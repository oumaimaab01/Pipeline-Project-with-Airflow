author,updated_at,like_count,text,public
@thiphan7023,2023-11-18T01:50:44Z,0,I can't access your website now,True
@CompThatHouse,2023-08-27T12:29:53Z,0,"Thankyou, Corey.  Your explanations are always complete and very helpful!",True
@arcy2056,2023-06-25T11:43:17Z,0,"You are the best, Corey ü•≥",True
@ahmed_ziada,2023-04-22T17:32:52Z,0,If I could give you a billion likes for this video I would. This is top quality content.,True
@pardener,2023-02-05T04:20:55Z,0,good video,True
@neowong7224,2023-01-12T03:14:52Z,0,Video id can be extracted by regex          (?ÔºúÔºùembed\/).+(?Ôºù\?),True
@enia1953,2023-01-04T13:59:54Z,1,Can you do a webscraping with python  and scrappy and xpath and Hidden API.,True
@user-qn5by5iv7u,2022-12-22T10:45:08Z,0,2022,True
@bangaloreshydrohome8003,2022-11-27T14:00:37Z,0,"Hi Corey, when I try this request-html, i get this html back as respose. It doens't make any sense to me. What's happening here? Is the website not allowing web-scraping? I tried with Amazon/Flipkart/Myntra online shopping website and I get back the same kind of response.   <!DOCTYPE html>  <html> <head> <meta charset=""utf-8""/> <meta content=""width=device-width, initial-scale=1, shrink-to-fit=no"" name=""viewport""/> <meta content=""5; URL='/12920472?bm-verify=AAQAAAAG_____5ErxOLzNQ9OUhsIjPw3oE4gZG98fYuiuPUY8FYnYbR6wCBS8-U90M3MShp2KG5HJhEjwMgFzDY1ZSWnuHSnce9FyMent8c58_Hk9FFiBiAzWvWu84tkRfFA7MEflo76fd4QGvqysxPBbVRk2rT6ErT0GUxAct80w0PFgpX98602aA6v6JvT3KVSunzQQPRJUohfzoWGBIs3mS_hzahxlctnX0mR9jY4KX20clpm'"" http-equiv=""refresh""> <title>¬†</title> <script>         var i = 1669556999;         var j = i + Number(""1171"" + ""23142"");     </script> </meta></head> <noscript> <iframe src="""" style=""border: none; height: 100%; width: 100%;""></iframe> </noscript> <script>           var xhr = new XMLHttpRequest();           xhr.withCredentials = true;           xhr.addEventListener(""loadend"", function() {               try {                   var data = JSON.parse(xhr.responseText);                   if (data.hasOwnProperty('reload')) {                       if (data[""reload""] == true) {                         window.location.replace(window.location.href.replace(/[&?]bm-verify=.*/, """"));                       }                   } else if (data.hasOwnProperty('location')) {                       window.location.replace(data[""location""]);                   } else {                       window.location.reload();                   }               } catch (e) {                   var data = {}                   window.location.reload();               }            });           xhr.open(""POST"", ""/_sec/verify?provider=interstitial"", false);           xhr.setRequestHeader(""Content-Type"", ""application/json"");           xhr.send(JSON.stringify({               ""bm-verify"": ""AAQAAAAG/////2wKoPGvDWv8hUStO9Tt1ZNti9zTvgyjfv/YpSL2BrKU8YpBF/+G32YtSEEAOxrpBvygSBlGmrERWD9UAlKz/+6WwjYChOrRAashpaYw8AtwrxygnSWPh8kwGlN/Jb1jch/Pon+9zkZuf6bpLKwvs4FgrCH2S74rvQbG2s4aT5b2L5itEMXtIsWdey24JXCDuLgic4u1kddqtHtHmAotR6wEhot2VSsVMt5qGSyJC+vbK4eimFWy9bcn7uRKeA=="",               ""pow"": j           }));     </script>  </html>   Process finished with exit code 0",True
@chashmal10,2022-09-17T19:54:29Z,0,"On the webpage/url that I call  session.get(url) on, there is a javascript script, one thing this script does is send a request of its own, how can I capture the  response to this request?",True
@africaplus7371,2022-07-30T17:41:54Z,1,Please can you make a  another video on how to scrape application?,True
@Seppiik,2022-07-27T10:26:45Z,0,Simply described to the point! Thanks,True
@quanlien8017,2022-06-25T06:42:20Z,0,Im confusing between this video and the web scraping with bs4. Can someone explain to me is this video the same as the other one or is it different?,True
@user-yd3ze4tq8z,2022-06-03T10:05:18Z,0,"Hi Corey, can u make a tutorial about how to call public java script api from python?",True
@gisleberge4363,2022-04-04T16:55:23Z,0,"Very thorough and complete on the topic, thanks for educational video üôÇ",True
@bayy4202,2022-03-23T15:00:50Z,0,"hi, im new about coding and stuff  i want to scrape url inside <a> tag with class .shortc-button but as you already know that 1 element can have multiple class...  in my case theres: .shortc-button medium green (for mediafire url) and  .shortc-button medium orange (for zippyshare url)  i want to access '.shortc- button medium green' that contains mediafire links  how i should write the code? 1. r.html.find('.shortc-button .medium .green') 2. r.html.find('.shortc-button medium green') 3.  shortc = r.html.find('.shortc-button') for medium in shortc:     medium.find('.medium')         for green in medium:             green.find('.green')",True
@imranullah7355,2022-03-09T02:29:25Z,0,"Sir I get empty list from soup.find_all(""div"",class_=""some class""), although there are some children of this class  What can be the reason?",True
@_ARIC_KAJI,2022-01-13T05:21:13Z,0,it's very useful thank you so much üíØ,True
@DI-xs3kh,2022-01-08T18:38:19Z,0,"Hi @Corey, for your tutorial related to AsyncHTMLSession.   I'm getting the      ""RuntimeError: This event loop is already running.""  I checked the documentation did not really see the reason for it. Could you please take a look if that is expected. I'm running in Windows 10. Python 3.10.",True
@harshitanailwal7016,2021-12-29T07:50:09Z,0,"i tried scraping one, but got a status code - 406 , can you please help, i can't find a solution!",True
@yubero2010,2021-12-26T16:27:58Z,0,"Daaamn this is the greatest video I‚Äôve ever seen about scrapping, nice I was looking for this kind of explanation for long time since I‚Äôm working on a project with python 3",True
@tatendaVIDZ90,2021-12-07T17:27:56Z,0,if anyone understands what html = HTML(html = source) is doing please assist? Any links to another video where it's explained would be welcome as well.  thanks,True
@HonkletonDonkleton,2021-10-25T15:57:22Z,0,top notch as usual thank you,True
@higheringai68,2021-10-01T00:59:56Z,0,how to scrap innerHTML content?,True
@helloworld-hv9oy,2021-08-26T15:36:13Z,0,tysm,True
@ricksegalCanada,2021-08-23T07:37:04Z,0,"12 minutes in, I can grab website information from this tutorial.  Why is this a big deal? I know next to nothing about Python.  Corey is high value in a very condensed time.  Others would take hours to get to his 12-minute mark.  Subscribed.",True
@theglobalconflict6904,2021-08-19T04:09:43Z,0,do you have any series about asynchronous programming in python ???,True
@prateeksarangi9187,2021-08-14T00:40:29Z,0,Wow detailed info !!  Request to go for coroutines and asyncio and async await please,True
@ericli9292,2021-07-27T13:19:47Z,0,What a great tutorial! I bet this is the first long tutorial that I ever watched nonstop.,True
@lohanisulav,2021-07-05T06:27:41Z,0,"Corey, it was really much informative. Can you clearify me what is the difference bet using BeautifulSoup and HTMLSession. Like for which types of sites, we use BeautifulSoup and for which type of site to use HTMLSession.",True
@eddiethinhvuong1607,2021-06-27T16:05:31Z,0,"Hi Corey, thanks for your video, it's really helpful.   I want to ask if the website requires log-in to see the data, how can we do that? I see there's a way to do it with normal request library but found none with requests-html. Thanks",True
@im4485,2021-06-21T21:11:14Z,0,Hi Corey. How does one find an element by its attribute and not by using css selector?,True
@tiger12506,2021-05-25T08:44:19Z,0,"This is really cool. I was looking for the ability to scrape a website and found requests_html. Quickly ran headlong into a wall as the site is a React.js site. :( Thought maybe I could find some information on performing clicks and such with requests_html, but looks like that is not possible. Your tutorial on the subject is great though. Really well thought out and explained, Great presentation!",True
@shivenkhajuria2488,2021-05-08T14:08:55Z,0,@coreyschaffer - Please provide code for web scrapping for this video.Github repo link doesnt contain the code files.,True
@compinerd732,2021-04-26T19:41:42Z,0,"17:23 sugar right there, i saw so many people just saying okay now we copy this, and put a 2 behind everything. oh my gawd. thank you - great tutorial so far",True
@stocksunlocked,2021-04-26T13:00:37Z,0,"Great stuff. Quick question. I'm able to scrape links but when they output on the HTML page it's just the text, not the clickable hyperlink. Any ideas on how to fix this so I can have a clickable link?",True
@nofyathp8075,2021-04-13T20:19:46Z,0,But how bout a login page? Is it still worth?,True
@Troglodyte2021,2021-01-31T06:45:33Z,0,Brilliant as usual! Salute!!!,True
@HarmanHundal01,2021-01-30T13:25:40Z,0,"Just a suggestion Corey. Can you please tag your videos 'Beginner', 'Intermediate', 'Advanced' for the benefit of noobs like me. Thanks already. Keep the awesome stuff coming.",True
@joy2000cyber,2021-01-21T13:45:59Z,0,How AsyncHTMLSession work with concurrent.futures? Don‚Äôt want to write a function for each thread.,True
@MindaugasV,2021-01-15T15:48:30Z,0,"I get printed pretified html with r.text method, you get type string, where r = session.get('https://coreyms.com/') or r.html.full_text method you get tree structure of html tags; and r.html.find('article', first=True).full_text returns tree of html tags.",True
@curruption018,2020-12-22T04:39:37Z,0,"Whenever I run .find(), the type thats returned is a list. For example the variable you have named ""headline"" would be a list. So I cant run .find() again. Also for some reason it's not recognizing  .html as a method of the r object. I even explicitly declared the variable type but it still cannot see .html as a method from whatever session.get returns. Any suggestions?",True
@RATANAGARWALITINFORMER,2020-12-02T12:29:19Z,0,nice tutorial,True
@joelprestonsmith,2020-11-12T22:53:13Z,0,"I'm learning a ton about webscraping from this tutorial, but I'm not able to run the code. Like many folks, I've got a few Python versions installed. I ran the code in the Thonny IDE, but I get a traceback on 'no requests_html module found.' Did some research on it, and discovered that requests_html is only supported on Python 3.6 (and my Thonny default was 3.7). I reset Thonny to run 3.6.5, but got the same error. Now I'm installing 3.6 to see if requests_html will be imported in that version. Anyone else see a similar issue with a traceback? What was your workaround?",True
@pythusiast4701,2020-11-09T01:45:17Z,0,"Hello Corey, can you please make a full on tutorial on webscraping using Scrapy? Thanks in advance.",True
@martingladis6225,2020-11-04T22:21:46Z,0,In my code I have this error:  There is no current event loop in thread 'Thread-1' My code(I use Django):         session = HTMLSession()         r = session.get(url)         r.html.render(),True
@stephenaborhey4214,2020-11-01T13:35:37Z,0,"i really love this video @Corey Schafer but i would like to learn about using the api to scrap data from social media like Facebook, twitter and the rest so if you do a video about that will be appreciated thank you",True
@surbhiagrawal3951,2020-09-22T20:36:55Z,0,"I think for splitting you can use parse_url or urlparse from parse.py , also we have urlsplit function",True
@rubenssantos7753,2020-09-13T19:09:41Z,0,"Hello, I would like to know if you can bring an icon and display it on the screen, I'm trying but none appears to me, when displaying  def previsao(local=''):          from requests_html import HTMLSession     import re      session = HTMLSession()      url = 'https://www.google.com.br/search?q=previsao+do+tempo&oq=previsao+do+tempo&ie=UTF-8'     if local != '':         local = local.replace(' ', '+')         url = url.replace('tempo', 'tempo+' + local)  selector_icon = 'wob_tci' icon = r.html.find(selector_icon, first=True)  return(""{}  "".format( icon))  import sys  local='' if len(sys.argv)>0:     sys.argv.pop(0)     local = ' '.join(sys.argv)  print(previsao(local))",True
@TheFrustratedProgrammer,2020-09-11T11:03:29Z,0,"hi @corey schafer even i found the same problem while working with requests_html where there is no prettify method but we can overcome that one with full_text method instead of text  thanks and hope i helped you",True
@SeamusHarper1234,2020-09-08T21:17:55Z,0,"Hi, what if the data that you are after only exists after some user interaction, e.g. clicking a button or triggering some other js event? Can you simulate this with requests-html?",True
@barrentheart2680,2020-09-08T13:21:33Z,0,Awesome content! Can you do a tutorial about Scrapy?,True
@yajantbhawsar2481,2020-08-26T05:06:15Z,0,"Hello corey, could you please make a video on desktop scrapping ?",True
@alenjose3903,2020-08-17T14:28:43Z,0,"when i used the html.render(), i got the error : RuntimeError: Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead",True
@nischalstha9,2020-08-07T11:56:17Z,1,"<td class=""coll-4 size mob-vip"">796.9 MB<span class=""seeds"">460</span></td> anyone know how to ignore text inside span?? please help! Thanks in Advance!",True
@rohannema9304,2020-08-04T07:53:28Z,0,"How do we click on the NEXT button that is generated within the javascript?   the inbuilt pagination doesnt seem to work and the next button generated in javascript doesnt redirect to a new link with a page number or for that matter doesnt change the link. Hence, unable to access second page.  Please, anyone who can help with this?",True
@pravegshrestha2610,2020-07-26T21:20:53Z,0,"Can I use requests_html in django?  I tried using it in my django app, but it is giving me following error: RuntimeError at / There is no current event loop in thread 'Thread-2'.",True
@suthejganjam1395,2020-07-11T03:47:09Z,0,Hi Thanks for the video.Can we get access to DOM object using this plugin,True
@LookNumber9,2020-07-08T21:04:19Z,1,Superb!  That's just what I needed for my next little project.  Thanks.,True
@melisaliu5727,2020-07-02T08:40:42Z,0,"¬øEs posible tener una herramienta automatizada para scrape los datos del sitio web, como Octoparse http://octoparse.es/? Es muy dif√≠cil escribir Python o usar Requests-HTML usted mismo. Los empleados en muchas industrias no hay experiencia de programaci√≥n pero tienen una gran necesidad de capturar y recopilar datos de web.",True
@compucademy,2020-06-23T21:13:18Z,0,"I can't find an answer for this anywhere, maybe you can help. Is it still worth learning to use Beautiful Soup, or has Requests-HTML basically superseded it, even though not many people have caught up?",True
@abhishekvaish6042,2020-06-20T15:07:27Z,0,Awesome video and I would love to have a video on how to keep changing ip while requesting,True
@kingstonpeng1076,2020-06-05T17:23:03Z,0,"I've found a solution.  I have another python project folder under VSCode: (venv) PS C:\Users\qkpen\PycharmProjects\training3\ even though my (venv) was enabled, and I cd to ""req-web-page"" folder:  (venv) PS C:\Users\qkpen\PycharmProjects\req-web-page> the venv was actually enabled under training3, but under ""req-web-page"", I didn't have venv setup:  (I used ""Git Bash"" terminal below -- so it looks like a Unix prompt, not Windows PS prompt)  $ cd /c/Users/qkpen/PycharmProjects/req-web-page $ ls first-web.py  password.py  simple.html  ten-tips.py  web-html.py  (<== no venv folder shown)  Solution:  enable venv under ""req-web-page"" folder: PS C:\Users\qkpen\PycharmProjects\req-web-page> python -m venv venv PS C:\Users\qkpen\PycharmProjects\req-web-page> pip install requests-html ...(last message said) Successfully installed appdirs-1.4.4 bs4-0.0.1 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.15.0 pyee-7.0.2 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 tqdm-4.46.1 w3lib-1.22.0 websockets-8.1  then I reran the above import code from python script, the issue resolved.",True
@kingstonpeng1076,2020-06-05T15:24:15Z,0,"Forgot to mention: I was using VSCode, not PyCharm when I ran the following one line python script.",True
@kingstonpeng1076,2020-06-05T15:19:06Z,0,"I tried to run the code, but hit an error, lookup google, don't see a solution:  (venv) PS C:\Users\qkpen\PycharmProjects\req-web-page> pip install requests-html  ... Successfully installed bs4-0.0.1 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.15.0 pyee-7.0.2 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0  so I code python script named web-html.py:  (narrow down to only one line -- that produce error) ------------------------------------------------------- from requests_html import HTML  ------------------------------------------------------- when I run this script, I got an error, output shows: Traceback (most recent call last):   File ""c:\Users\qkpen\PycharmProjects\req-web-page\web-html.py"", line 1, in <module>     from requests_html import HTML  ModuleNotFoundError: No module named 'requests_html' (venv) PS C:\Users\qkpen\PycharmProjects\req-web-page> pip list  Package     Version    ----------------- ---------- appdirs           1.4.4 astroid           2.4.1 attrs             19.3.0     pip               10.0.1 ... requests-html     0.10.0  Anyone has any suggestions? Thanks in advance.",True
@rodrigoillas6085,2020-05-25T01:54:15Z,0,"On 29:36 why are the first two classes written after a dot ""."" but the iframe class isn't?",True
@hirschheisstdermann,2020-05-23T00:37:00Z,2,I think I'll rewatch this gem some times....,True
@kinjalvora256,2020-05-21T13:48:56Z,0,"Hey Corey, Great tutorial I have a small doubt though articles = r.html.find('article') would this technically work for all websites? how to look for the information under inspect I was trying to use this to try and scrape a headline from BBC news and it does not seem to work I am not sure what information I should look at under inspect to make sure I have the right thing selected maybe you can help with that",True
@t-dsai,2020-05-19T13:37:54Z,1,"Hi Corey, Thank you very much for sharing your knowledge. Apart from your programming skills, your pedagogical skills are also of high order. I presume that you wanted to explain various aspects of splitting, and creating yt_link with string format. Otherwise, a shorter ways of getting the yt_link (from 29:55 to 34:45) could be the following yt_link = article.find('iframe', first=True).attrs['src'].split('?')[0].replace('embed/', 'watch?v=') It splits the whole source at '?', and then replace the 'embed/' part of the string by 'watch?v', and directly returns the link.",True
@kautilyab,2020-05-16T07:52:12Z,1,"#If anyone wants most of the code used in the above video   #Python Tutorial: Web Scraping with Requests-HTML #https://youtu.be/a6fIbtFB46g  #pip install requests-html   #import sys #sys.path #print(sys.path)  import os cwd = os.getcwd() ; print(cwd) # Get the current working directory (cwd) os.chdir(""/Users/kauti/Documents/Python course"")    # Change the current working Directory     cwd = os.getcwd() ; print(cwd)   #files = os.listdir(cwd)  # Get all the files in that directory #print(""Files in %r: %s"" % (cwd, files)) #os.path.exists(""C:\\Users\\kauti\\anaconda3\\lib\\site-packages"")    from requests_html import HTML  with open(""simple.html"") as html_file:     source=html_file.read()     html=HTML(html=source) #importing the html file and converting it into html  #for more about this, tutorial on objects in the link   print(html.html)     #to print out the html print(html.text) #to print out only the text in html   #with open(""simple.html"") as a: #    b=a.read() #    c=HTML(html=b)      #print(c.html)          #print(c.text)   #---------- #to only output particular parts of html match=html.find('title') print(match) #we are trying to output only the title, by seeing the html file we can say that #it is placed between <title> and </title> #find method uses CSS Selectors #Output: [<Element 'title' >] #found only one title in the html #Prints out a list of elements  #this is how you specify how you want to find in CSS Selectors  # instead of printing the whole list if we want only one element print(match[0])  #to see the html of the element print(match[0].html) print(match[0].text)   #to only search and stop after first element,instead of finding in the whole html match=html.find('title',first=True) #gives out one element instead of text print(match.text)   # finding element by certain id  #if we want to get element by certain id ,use the pound sign #to find a div with an id of footter match=html.find('#footer ',first=True)  print(match.text)  #it will be hard to read through hmtl sometmes as the code might be too big #so we learn few ways to access data easily  #to finding a div with a class""article"" use div.article (you can learn from css) article=html.find(""div.article"",first=True) print(article.text) #Output #Article 1 Headline #This is a summary of article 1  #To find headline and summary individually headline = article.find(""h2"",first=True) summary = article.find(""p"",first=True)  print(headline.text) print(summary.text)  #--------------- #to parse information from all the articles articles=html.find(""div.article"") #returns a list of articles for article in articles:     headline = article.find(""h2"",first=True).text #extracts text rightway     summary = article.find(""p"",first=True).text     print(headline) #so no need to add .text here again     print(summary)     print() #spreads the headling and summary while printing      #lets try grabbing data from a real URL , we need somehting like html session #to grab data from a url we need to import a different class from requests-html  #--------------------------------  from requests_html import HTML, HTMLSession  session= HTMLSession() r=session.get(""https://coreyms.com/"") #so this uses a request library to get response from website #r variable is the responce object  print(r.html) #<HTML url='https://coreyms.com/'> #here ""HTML"" attribute gives access to the html object of the obove url #the above action 'r.html' is same as html=HTML(html=source) but for a url   #to extract data from each video in the url, lets look at the html of website  #after inspecting the website, the first video is under a article #so lets look at the first article   article=r.html.find(""article"",first=True)  print(article.html) #this will give us all the html of first article       headline=article.find("".entry-title-link"",first=True).text print(headline) #finding the class ""entry-title-link"" and grabbing only the text #""entry-title-link"" is the class in which the title of the video is in #so we got the title of the video #mind the ""."" before the class in the string given to find , syntax for CSS  summary=article.find("".entry-content p"",first=True).text ;print(summary) #in the class of ""entry-contenr"" with a paragraph tag 'p'   #lets find the video source/link of the video #from the html we can see that it is in the iframe  #in some cases like this ,we take multiple steps to get the desired output      video_src=article.find(""iframe"",first=True) ; print(video_src.html)  #output #<iframe class=""youtube-player"" width=""640"" height=""360"" #src=""https://www.youtube.com/embed/z0gguhEmWiY?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent"" # allowfullscreen=""true"" style=""border:0;""/>  #as it is an embedded video a, we dont have the url #but youtube embedded videos has id and here in url its is #just after ""embed"" and before ""?"" i.e"" z0gguhEmWiY  #we need to extract that id print(video_src.attrs) #prints out a dictonary attributes in the iframe element print(video_src.attrs['src']) #to get a specific attribute src   video_src=article.find(""iframe"",first=True).attrs['src'] ; vid_id=video_src.split(""/"") ; print(vid_id)  #we need the 4th index as it has the id vid_id=video_src.split(""/"")[4]; print(vid_id)  #we can split with ""?"" as we need video id before that vid_id=vid_id.split(""?"")[0] ; print(vid_id) #we got the video id  #we can creat a youtube link from the aquired id #this is how to creat the youtube link  yt_link= f""https://www.youtube.com/watch?v={vid_id}"" ; print(yt_link)      #checked it in browser and it's working perfectly  #--------------------------------------- #now lets get all the videos in the url r=session.get(""https://coreyms.com/page/2"") #took page 2 at the time of wiriting as page 1 was giving errors articles=r.html.find(""article"")  for article in articles:          headline=article.find("".entry-title-link"",first=True).text     print(headline)               summary=article.find("".entry-content p"",first=True).text ;     print(summary)       video_src=article.find(""iframe"",first=True).attrs['src'] ;     vid_id=video_src.split(""/"") ;     vid_id=video_src.split(""/"")[4];      vid_id=vid_id.split(""?"")[0] ;              yt_link= f""https://www.youtube.com/watch?v={vid_id}"" ;      print(yt_link)     print()        #we would get an error if the post doesn't have video #so we make modifications to script so that it could handle that      r=session.get(""https://coreyms.com/"") articles=r.html.find(""article"")  for article in articles:          headline=article.find("".entry-title-link"",first=True).text     print(headline)               summary=article.find("".entry-content p"",first=True).text ;     print(summary)          try:         video_src=article.find(""iframe"",first=True).attrs['src'] ;         vid_id=video_src.split(""/"") ;         vid_id=video_src.split(""/"")[4];          vid_id=vid_id.split(""?"")[0] ;                      yt_link= f""https://www.youtube.com/watch?v={vid_id}"" ;      except Exception :         yt_link= None            print(yt_link)     print()  #--------------------------------------------     #WRITING TO A FILE      #we can save this scraped content to csv file      import csv csv_file=open(""cms_scrape.csv"",'w') csv_writer=csv.writer(csv_file) csv_writer.writerow(['headline','summary','video'])   r=session.get(""https://coreyms.com/"") articles=r.html.find(""article"")  for article in articles:          headline=article.find("".entry-title-link"",first=True).text     print(headline)               summary=article.find("".entry-content p"",first=True).text ;     print(summary)          try:         video_src=article.find(""iframe"",first=True).attrs['src'] ;         vid_id=video_src.split(""/"") ;         vid_id=video_src.split(""/"")[4];          vid_id=vid_id.split(""?"")[0] ;                      yt_link= f""https://www.youtube.com/watch?v={vid_id}"" ;      except Exception :         yt_link= None            print(yt_link)     print()     csv_writer.writerow([headline,summary,yt_link])  csv_file.close()  #---------------------------------------------------------------------  #To generate all the links from a website we can use HTMLSesson  from requests_html import HTML, HTMLSession  session= HTMLSession() r=session.get(""https://coreyms.com/"") print(r.html.links) #this is too clumsy to read   for link in r.html.links:     print(link) #this is easier to lead      #if you have relative links in the url you want to scrape the absolute links for link in r.html.absolute_links:     print(link)      #------------------------------ #to scrape the dynamanic data generated by java script #this is directly avaliable in requests-html    from requests_html import HTML  with open(""simple.html"") as html_file:     source=html_file.read()     html=HTML(html=source)  #on the webpage we can see that footer has text but in the actual simple.html      #we cannot find the text in the footer but has java script below the footer     #that adds text to the footer      #so the page has to run java script before the text gets aaded to the footer #so usually libraries have trouble getting data for this , # here requests-html get the job done  match=html.find(""#footer"",first=True) print(match.html) #no dyanmic text is in the response  #to render the dynamic data too in the text with open(""simple.html"") as html_file:     source=html_file.read()     html=HTML(html=source)     html.render() #adding render has made the dynamic data available  match=html.find(""#footer"",first=True) print(match.html)  #here you can see the data generated by javascript in footer   #Hope this helped few people",True
@varun83s,2020-05-08T18:46:41Z,0,"Hi There, When I am doing response.html.render() I am loosing authentication. When I do response.html.find('div') I get all the desired results however as you mentioned to use just html.render before find, I am not able to hold authentication when chromium is working to get the data. Any clues how to resolve this. Any pointers is highly appreciated.",True
@antonyalen2745,2020-04-25T15:58:30Z,2,"Hi Corey, I really enjoy watching your tutorials. Please make a tutorial on Asynchronous running of code.",True
@vinayagarwal1623,2020-04-25T03:37:32Z,1,A video about dynamic web scraping with Selenium would be super helpful. I was having trouble with moving buttons on a website. Thanks in advance.,True
@giancarlogalina1841,2020-04-19T19:04:34Z,0,Great video Corey! Thanks again. Does anyone know why when my code executes the render method it crashes? It would download up to a certain % and then crash.,True
@davebeckham5429,2020-03-31T17:40:00Z,1,Excellent tutorial as always. - Many thanks.,True
@digitalsols,2020-03-29T01:43:11Z,1,Nice,True
@mytimeincloud5263,2020-03-17T22:37:01Z,2,Just Supported  to show my gratitude.,True
@maraffio72,2020-03-10T17:08:31Z,0,"Hi Corey, thanks for this video! Could you please elaborate on how to use async functions which require an input parameter?  The command .run wants function objects in input. How can we pass parameters to those?",True
@laurentb6563,2020-02-29T10:30:06Z,0,"the javascript scraper not working in Jupiter notebooks, anybodyforund a solution? Error is ' loop already running' . Tried a few things but no go.",True
@PawelVerma,2020-02-28T15:23:16Z,0,"At 25:35  headline = article.find('.entry-title-link',first=True).text    # where did this dot come from before entry-title-link?  Why is this needed, did I miss some explanation?",True
@edemaehiz,2020-02-26T14:10:20Z,0,Hello Corey is it possible to create a spidering program using requests_html or do I need to use Scrapy for that,True
@shihabjamil7274,2020-02-25T07:26:36Z,0,"How can I remove a html tag by using this requests-html? Let's say: <section>   <div class=""to-remove>   </div> </section> I want to remove div.to-remove from section. How can I achieve this?",True
@edemaehiz,2020-02-13T13:23:18Z,2,"Menh. woke up this morning with a desire to reinforce my knowledge on Beautiful soup and APIs. Your videos did it for me.  In Bini, Nigeria we say ""Uwese kakabor"" meaning thank you so much",True
@avibas619,2020-02-09T11:18:48Z,0,"Could somebody help me figure out the following error?  AttributeError: 'list' object has no attribute 'html'   from requests_html import HTML, HTMLSession  session = HTMLSession() r = session.get('https://www.canada.ca/en/immigration-refugees-citizenship/services/immigrate-canada/express-entry/submit-profile/rounds-invitations.html')  invitations = r.html.find('mwsgeneric-base-html parbase section')  print(invitations.html)   I've tried the same code with two different sites and have the same error.",True
@drinkingineasterneurope6947,2020-01-01T16:25:11Z,0,I had some issues with beautiful soup but with requests_html nothing. For me this module is the way to go. Thank you for showing this alternative it turns to be better!,True
@feather1x,2019-12-21T04:46:17Z,0,"Hey corey, whats the difference between the video where you used beautiful soup to scrape information and this video?",True
@johnpopo2,2019-12-16T11:31:14Z,0,"I would have split the YT video URL like this : vid_id = vid_src.split('?')[0].split('/')[-1] It's easier to first get rid of the parameters (which may or may not be there) Instead of using hardcoded index 4 for the / split, why not just take the index of the last item (index -1) ? If the source URL were to be relative instead of absolute then index -1 should still work",True
@srinivasaraothammaneni4854,2019-12-13T05:16:10Z,0,all i saw is reading content from static HTML with web scraping.   i want to post a comment on public website on daily basis. Can i achieve this through web scraping ? is it possible ?,True
@875preeti,2019-11-29T17:07:46Z,0,"Hey Corey, How can i implement this web scraping in django? Thanks for your pervious videos.. They're of great help!",True
@farmakoxeris,2019-11-13T00:19:22Z,1,Great video buddy,True
@farmakoxeris,2019-11-13T00:18:45Z,0,45:53 How can I get the number of the html links contained inside r.html.links?,True
@randydiffenderfer7793,2019-11-06T22:25:31Z,0,"as your source is a known url, why not use a url parsing library to do the parsing?  can get last part of the route with little chicanery, i'd think! :D plus, you get a sanity check on what you've pulled out for free!",True
@Boy-tz3ow,2019-11-04T17:47:24Z,0,"Hey nice tutorial, but I have a question. How do you parse if the website uses Ajax calls to populate the data? More exactly, I am trying to parse a bet website. Any info or links would really help me. Have a nice day Corey.",True
@samirsarkar001,2019-10-28T06:09:59Z,0,"I like your tutorial most. I just have one question. For parsing the article headline and summary you used "".entry-title-link"" and & "".entry-content p"" in find method but at the time of vid_src you used ""iframe"" . So how we decide when to use . and when not ?",True
@samirsarkar001,2019-10-28T06:04:19Z,1,You rock man ü§òüèª,True
@robcarreon5743,2019-10-26T12:38:18Z,1,"Thank you for your videos!  They are very well organized, easy to follow and extremely helpful! I followed through on the above video and got it working perfectly on a site that uses javascript.  But I only got it to work when running via python command line or shell. When I put the code inside a very simple ""hello world"" django project, the .render() function causes a Thread Loop-1 error; if I comment out the r.html.render() line, I don't get the error, but the information I get back is incomplete.  I searched all over for results-HTML and django and this error and couldn't find much on a cause/solution.  Just curious if you've run into this and know  why it doesn't work?  Thanks again!",True
@naveenkumarreddykona8096,2019-10-22T11:55:16Z,0,"Hello Sir, can you please look into this, i am facing issue with django framework when i tried to implement requests_html while processing that for javascript execution  result.html.render()  i am getting this error : There is no current event loop in thread 'Thread-10'. This is my view method.  def search_movie(request):   if request.method=='POST':   movie_name = request.POST.get('movie_name').replace(' ','+')    session = HTMLSession()   results = session.get(f""https://isongs.info/search/?q={movie_name}"")   results.html.render()    return HttpResponse(results.html.html)   return render(request,'site/search_form.html')",True
@debbygram8153,2019-08-26T14:33:20Z,3,"real heroes don't wear capes they teach like Corey Schafer. You literarily make programming simpler than A, B, C.",True
@paulseldn,2019-08-23T20:03:44Z,0,"i keep getting this error. Can you please help Corey? MagicPython - Webscrape.py:9 Traceback (most recent call last):   File ""C:\Users\PAUL\Desktop\Test\Webscrape.py"", line 1, in <module>     from requests_html import HTML, HTMLSession   File ""C:\Python35-32\lib\site-packages\requests_html.py"", line 6, in <module>     import pyppeteer   File ""C:\Python35-32\lib\site-packages\pyppeteer\__init__.py"", line 30, in <module>     from pyppeteer.launcher import connect, launch, executablePath  # noqa: E402   File ""C:\Python35-32\lib\site-packages\pyppeteer\launcher.py"", line 20, in <module>     from pyppeteer.browser import Browser   File ""C:\Python35-32\lib\site-packages\pyppeteer\browser.py"", line 11, in <module>     from pyppeteer.target import Target   File ""C:\Python35-32\lib\site-packages\pyppeteer\target.py"", line 4, in <module>     from typing import Any, Callable, Coroutine, Dict, List, Optional ImportError: cannot import name 'Coroutine' [Finished in 0.872s]",True
@paulseldn,2019-08-23T18:28:52Z,0,Which python version are you using as i am getting errors all over the place?,True
@paulseldn,2019-08-23T18:15:15Z,0,Hi Corey. Can you please tell us what IDE you are using. it is so nice to be able to read large fonts and also resize the console. I cannot do this in Atom,True
@albertomedinarobredo,2019-08-11T17:44:26Z,0,"Hi Corey, this explanation is great, as always! I'd love to see a more difficult scraping example. Are you planning on doing something like that? Or do you have any recommendations of what to read/watch? Thanks!!",True
@AlexBerkk,2019-08-10T12:38:40Z,8,"Can we, please, have a tutorial on async/await and yield from? I kinda get it, but I really want to hear your explanation. Thank you so much for your vids!",True
@neoninsv,2019-08-03T02:52:44Z,2,"I am a big fan of you showcasing what it looks like first, then breaking it down in detail.",True
@ori61511,2019-08-01T11:24:58Z,0,"31:00 why not just use find(""embed/"") then find(""?"") like this: url[url.find('embed/')+6:url.find('?')]",True
@manpaalsingh,2019-07-30T06:09:14Z,0,Can you use python to automate and export every iteration of a searchable or filterable website?,True
@rockybhai-cn3qw,2019-07-13T03:54:42Z,0,python guru (India support),True
@earltan739,2019-07-10T02:53:06Z,0,Awesome!!!! Been looking for this man!,True
@hugogradvohl1549,2019-07-03T07:46:49Z,0,"Hi Corey, I was hitting a wall with asynchrone programmation and the last part has help me a lot. Since ""await"" doesn't work with all functions and the methods name have been upgraded, I was looking for something consistent. Has often I end up finding the right way to use the functionality in your video. I know that you have many projects but asynchrone and thread programmation are a very interesting topic, if you run out of projects (not happening ^^) it would be great if you could make a light series of video (by the way I reduce my download time from 58 sec to 18sec by applying async functionality).     Thank you!",True
@draco76xx,2019-06-27T12:42:34Z,1,Great video but would like to see more example on scrapping dynamic image etc using the render() function or something.,True
@anunayanu,2019-06-26T17:25:32Z,17,what is difference between using Beautifulsoup and Requests-HTML,True
@rahulsoni1969,2019-06-13T06:42:10Z,1,Sir can you please teach me how to use render() function properly.I am facing huge problem for scrapping data from a web which loads results dynamically using jacascript,True
@carlosmatosfanpage2856,2019-06-04T21:47:45Z,1,How do I display results of scraping on my own website using Django.I want to make a website that compares prices of products across different websites but don‚Äôt know how to put the data which I have scraped onto my website. Thanks,True
@hell1018,2019-05-25T13:00:48Z,60,46:32 Javascript rendering 49:42 Asynchronous requests,True
@roccococolombo2044,2019-05-14T17:24:14Z,1,Great videos and thanks for using large readable fonts.,True
@whilelab,2019-05-14T06:12:42Z,3,Just want to add that you can access a prettified version of the HTML using    r.html.html   BTW Great video.,True
@sarunassavickas5351,2019-05-11T15:22:41Z,2,"Very informative and useful tutorial. Thank you, Corey! Would love to see your approach on asyncio or multithreading :)",True
@ericklungle5715,2019-05-08T03:09:00Z,1,"Great tutorials.  Most people I have watched also will take requests.  I have been trying to figure out how to change an element on a webpage and as yet have not seen anything very useful THAT WORKS!!  It would be nice if someone would do a tutorial on how to do the following: change the following: <option selected=""selected"" value=""1""> &nbsp;&nbsp;Single Draw</option> to this:  (Notice the value="""" parameter) <option selected=""selected"" value=""100""> &nbsp;&nbsp;Single Draw</option>",True
@ramiboy1996,2019-04-27T23:03:59Z,0,great work. it helps me a lot . can you tell me how we extract the data from different pages without click or add link into the code from next pages tags.,True
@emasmach,2019-04-22T23:00:23Z,1,Nice video!,True
@emasmach,2019-04-22T23:00:22Z,1,Nice video!,True
@Hinstea,2019-04-08T10:09:38Z,0,any idea how we can implement it in dynamic websites?,True
@preetivasthava851,2019-03-27T07:27:58Z,0,hii.. i need a code for how to convert a pdf file to html file using python 3? kindly help..,True
@dheerajkura5914,2019-03-26T19:14:48Z,1,"Corey, you're a gift for the programmers. long live and spread the knowledge    Can you please do the Vidoe's on OpenCV as well which is useful for Computer Vision",True
@romankhripunov6550,2019-03-24T19:39:22Z,0,"–ö–∞–∫ –≤—Å–µ–≥–¥–∞ –∫—Ä—É—Ç–æ! –°–∞–º—ã–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç ) —Å–ø–∞—Å–∏–±–æ, –±—Ä–æ!)",True
@raffaelerimorso9671,2019-03-23T18:26:33Z,1,big lezion,True
@e-maxwell,2019-03-22T20:04:07Z,0,Can you please make a video about threads? I want to scrape a website and get pictures from it and I want to download several at the same time. I'm not sure if it would be better using threads or async.,True
@benedikthollenhorst2515,2019-03-21T13:06:52Z,0,"I usually use Selenium if I want to scrape JavaScript content. When would you use Requests-HTML instead of Selenium? As far as I know, Requests-HTML uses headless Chromium to scrape JS data.",True
@mrchatterjee_,2019-03-19T20:02:25Z,0,"Very useful video! I have a question though if I want to scrape a webpage where I have sublinks and that sublinks have many sublinks, how do I scrape that? Any suggestions would be appreciated!",True
@waichan4476,2019-03-19T07:51:49Z,37,"I don't usually comment in fact never, but I just wanna say thanks! the content you produce is by far the best I have seen",True
@josephsowah1678,2019-03-18T16:36:55Z,2,Thanks Corey.. you're the best...love all your tutorials. Looking forward to your tuts on rest api in python.,True
@denniskimani6810,2019-03-18T10:12:59Z,2,thanks for effort. your tutorials have been an important part of my journey as a programmer .,True
@hsumerfarooq5474,2019-03-18T09:16:48Z,1,"Please make scraping tutorials with scrapy and selenium too, BTW thanks for  your efforts.",True
@liangyumin9405,2019-03-17T17:01:51Z,1,good tutorial!,True
@sandeepvk,2019-03-17T03:28:16Z,0,"Hi, When I try to run the  code  : from requests_html import HTML with open('simple.html') as html:     source = html.read()     print(source) I can still seen and html body generated in the console. Why do have i got to them move the source variable into another with ""HTML(html = source) and print the result ? in both case I get the same result isn't it ?",True
@maciejZar,2019-03-16T12:49:17Z,2,"Hi Corey,  Great video! Thank you.  About your question concerning ‚Äúbeauty‚Äù of scraped webpage, I think that if you do  session = HTMLSession() r = session.get(url) print(r.text) not r.html  you have in fact html but ordered somehow. Best",True
@andrewmendela9065,2019-03-15T18:40:20Z,3,"Corey, you are a wizard! I dont know how are you doing this, but by far your videos are the best educational programming material that i have ever seen, thank you! When i get my first job you will be the first person i pay",True
@nishantchaturvedi7519,2019-03-15T17:36:17Z,1,"Hey Corey, I am little confused, from where should I practice python coding",True
@TedMaciag,2019-03-15T15:35:04Z,3,"Hey Corey,   BEST video for an older programmer to understand.   Thanks very much!",True
@christianrusso5142,2019-03-15T15:00:15Z,1,"Very clear and helpful, thank you!",True
@pythonocean7879,2019-03-14T20:34:18Z,0,"i have a request,plz make a series on machine learning and deep learning plzzzzzzzzzzzzzzz,i want to learn i tried many tutorials but i cant understand i watched your tutorial always.  plz sir its humble request :(",True
@kamranhussain4606,2019-03-14T16:10:36Z,0,"Corey , your videos very helpful for me . I want to access post from my website by current location of user. Please explain me or make a video . Thankü§ó",True
@ExplaineRKhaN,2019-03-14T08:47:03Z,0,"how to update foreign key  in django   Model  class Prdouct(models.Model):     PrdouctName = models.CharField(max_length=200)     PrdouctPurchasePrice = models.PositiveIntegerField()     ProductItemUnit = models.CharField(max_length=50)      TotalStock = models.IntegerField(default=1)     PurchasePriceTotal = models.PositiveIntegerField()     company = models.ForeignKey(         Company, on_delete=models.CASCADE, blank=True, null=True)     suppliers = models.ForeignKey(         Suppliers,  on_delete=models.CASCADE, blank=True, null=True)     category = models.ForeignKey(         Category, on_delete=models.CASCADE, blank=True, null=True)     Views  def Product_Update(request,pk):     product = get_object_or_404(Category,pk=pk)     form = Category_from(request.POST or None ,instance=product)     if form.is_valid():         form.save()         return redirect('product_list')      return render(request,'Product_form.html',{'from':form})",True
@winglau7713,2019-03-14T02:02:24Z,3,"A great video, everything is super clear, you are a gifted teacher, thx so much!",True
@DanielWeikert,2019-03-13T10:38:01Z,1,Once again great video. Thank you very much Corey. I don't know anyone else who is able to explain python that good. Could you consider to make a video on pathlib? Best regards,True
@speaktothepoint2108,2019-03-13T07:05:21Z,1,That‚Äôs fantastic. Corey makes it very simple to understand the complex topics.,True
@user-qs6dp2fw7q,2019-03-12T20:15:21Z,0,Corey Scraper,True
@younesboukroun3060,2019-03-12T19:49:28Z,1,Thank You for all <3,True
@morningfreeman,2019-03-12T18:45:51Z,0,"Hi Corey, first of all, thank you, I just started to learn python and your videos are super helpful. I'll love to see in the future a Dash tutorial :)",True
@infinitezymalny,2019-03-12T17:02:08Z,1,Could you show how to scrape more complex pages?,True
@archstampton5910,2019-03-12T15:11:39Z,3,"Thanks Corey, I just finished the video. I appreciate the fact that took some time  to go through some non-directly related topics (csv files , splitting links, etc ...) As soon as as I will be a bit more confortable on Python , I will read that American Doll Bed link of yours.",True
@josuefreire4442,2019-03-12T14:28:56Z,0,Sauce doesn't work,True
@bastmqt3641,2019-03-12T12:26:25Z,1,"How could we transpose this using TOR? As a user-agent can it use Tor Browser and request .onion?   What do you think about Scrapy & S√©l√©nium? :)   Thanks anyway, great video as Always!!!",True
@sandeepvk,2019-03-12T12:16:29Z,6,Pls do try to scrape a public website using their api,True
@micky8386,2019-03-12T12:06:55Z,0,Nice tutorial! Feel free to visit my channel and watch my tutorials! Have a nice day!,True
@cybergen2K,2019-03-12T11:10:46Z,0,"Hi Corey, you're the teacher we've always needed :) Now for a creepy request: Would you ever go back to your OOP Tutorials from 2 years ago and change them at all?",True
@michealhall7776,2019-03-12T10:12:12Z,7,Can you do a full video on asynchronous requests,True
@michealhall7776,2019-03-12T10:06:45Z,0,How does render work? Does it use a webdriver for chromium and run a headless ?,True
@silverzero9524,2019-03-12T09:24:07Z,0,"nice video corey   one other way of forming yt url :  x=""https://www.youtube.com/embed/a6fIbtFB46g?version=3&rel=1&fs=1&autohide=2&showsearch=0&showinfo=1&iv_load_policy=1&wmode=transparent""  link= x.split('?')[0].replace('embed/','watch?v=') print(link)",True
@YunikMaharjan,2019-03-12T09:19:10Z,2,please do a detailed video on async,True
@Khadijanoman328,2019-03-12T07:05:13Z,0,Hey corey please make on video django REST API thank you,True
@ytoh6408,2019-03-12T06:55:20Z,0,Thanks for the video. I have a website I would like to scrape but the data is kind of hidden until a user clicks on something. Imagine that the website is a shopping list with a (+) button. When the user clicks on the (+) button the shopping list expands and shows all the items in the list. Can I use this library to scrape this kind of websites?,True
@delllatitude299,2019-03-12T06:30:45Z,1,man i need a button to give 1M likes at a time to your video. JUst amazing specially the last part. You clear my big big big big concept <3 <3 Always glad to see your videos . Waiting for more as a greedy programmer :D,True
@benwalsh2825,2019-03-12T04:10:07Z,7,This is really top notch. Thanks so much for putting this together. Very well done!,True
@kristianfjeldepedersen4675,2019-03-12T00:47:30Z,5,"It is so satisfying how you always somehow cover the topics both me and the rest of the notification squad find most interesting. Keep up the excellent work, Corey.",True
@dev4life,2019-03-12T00:38:10Z,0,Corey few days ago I was trying to scrape YouTube video links from a YouTube playlist but the Youtube website only loads 100 videos at a time for a playlist so if a playlist has more videos than that then it won't be able to scrape it. Do you know how to do it?,True
@diniscab,2019-03-11T22:48:00Z,0,"You gotta try Selenium Corey, it's pretty neat!",True
@justinpopa9399,2019-03-11T21:57:37Z,2,You could not have timed this more perfectly for me. I was planning on working on figuring out how to do exactly this while relaxing this evening. Thank you!,True
@husseinalahmad429,2019-03-11T21:37:11Z,1,"Brilliant, thanks a lot Corey",True
@godfreynolottyogwu8562,2019-03-11T21:15:12Z,9,"The moment l see your notification    ,l start dancing because it's always a hit,thumb up Boss.",True
@nurashams4093,2019-03-11T20:20:04Z,2,"I requested that, if I'm not wrong..thanks a lot...",True
@arunsaivemula1300,2019-03-11T20:18:45Z,0,"Hello Corey.. Nice video, please start tutorials on machine learning.. Course",True
@coreycarter5668,2019-03-11T20:17:42Z,22,Hahaha! As soon as I see a notification that he has uploaded a video it‚Äôs like Christmas morning!,True
@moneymoves4955,2019-03-11T20:17:33Z,1,thanks Corey,True
@eeshsingh3336,2019-03-11T20:17:01Z,60,I see Corey's video. I hit like. 2am notification squad. ü§£ Hi from India!,True
@lgaan,2019-03-11T20:16:05Z,1,Notification gang,True
