author,updated_at,like_count,text,public
@MbuyiseloXaba,2024-05-31T13:39:29Z,0,and they say super heroes are a fiction,True
@JuanPerez-iu9vk,2024-05-31T13:13:04Z,0,"Pandas does that so much easier, on its own, with no need for beautifulsoup. Give it a try.",True
@timz2917,2024-05-29T12:10:10Z,0,"Beautifulsoup gets the prerendered html, thats why the jQuery class didnt persist.",True
@ashutoshranjan4644,2024-05-28T14:31:57Z,0,I like your way of teaching. Looking forward to learn from you. Thanks for making such content,True
@ayushsinghrawat1409,2024-05-26T17:06:00Z,0,I hands on to my 1st scrapping experience with your sir,True
@cbacca2999,2024-05-22T09:35:13Z,0,Hi Alex. In the Wikipedia revenue table there is a minus sign in some of the revenue rows. This is actually an extended ascii n-dash or m-dash which will appear as another character. Look for a funky character in those rows in the output. I work in the print industry and this is an inappropriate use of the n- or m-dash for us.,True
@ucthanhchu3688,2024-05-19T10:05:57Z,0,nice video! thanks,True
@friigu,2024-05-17T22:30:25Z,0,"it's the first video that i watch about web scraping. my know-hows about progamming are poor, but i find awesome the work to pick a code throw the localhost. how i can build it? or how i get a localhost like that? maybe you can link me to other videotutorial. thanks for your help in advance",True
@user-lb1fl7sh8m,2024-05-13T16:46:02Z,1,"df.to_csv(r'C:\Users\ANDREAS SOTIRIOU\Desktop\Assignment Data Mining\Companies.csv', index = False ) this is the only command that is not working for me , i mean i can not save the csv file in my Desktop ? Why ?",True
@hibayassir3118,2024-05-13T11:14:27Z,0,"I haven‚Äôt been following the series , i just want to start implementing this project is there anything i need to do beforehand?",True
@qurat_ul_ain495,2024-05-12T07:08:52Z,0,"i am getting error on ""table.find_all('tr')""...  error is  AttributeError                            Traceback (most recent call last) <ipython-input-52-0649fb3740d8> in <cell line: 1>() ----> 1 table.find_all('tr')   kindly suggest what to do...  video 14:29",True
@allah-swt,2024-05-12T03:43:14Z,0,Is it possible to notify us via email whenever there is modification to this table?,True
@amLife07,2024-05-10T12:26:02Z,0,This is one of the best video on web scraping using BeautifulSoup. Thank you soo much Sir!    Highly Recommended,True
@proud_indian0161,2024-05-10T12:06:46Z,0,"Great Tutorial, Got what i was looking for thanks",True
@bubblebath2892,2024-05-07T17:10:30Z,0,I have joined a course and the curriculum has assignments and their questions ....i want to scrape that dashboard to get these questions for solving in a word doc  but the site requires login . I have the login credentials and have logged in to the dashboard ...but the html code doesn't show up on inspect the way it does in the tutorial . I have been struggling with this,True
@vamshikrishnareddyLingam,2024-05-06T13:02:16Z,0,one word Beautiful video it actually helped to get the client,True
@Vikash-the-analyst,2024-05-05T11:00:02Z,0,"Honestly, very informative and this help me very well to learn this topic. Explanation of every code is very useful. Thanks for making this informative video.",True
@martinbolio257,2024-05-03T22:14:08Z,0,Very very useful! Great video.,True
@hilra,2024-05-03T05:04:32Z,0,Is it possible to scrape purchase count of a particular product?,True
@gabrielledatascience,2024-05-01T18:37:07Z,0,"If anyone is having issues around 13:31 when we state the dtaaframe columns, try adding   ,  dtype='object'  after world_table_titles so that the data type of the column headers can be set. mine had that issue and thought that I could share :)",True
@9199-Keith,2024-04-27T07:23:51Z,0,"For anyone getting duplicate rows, you can just use, df = df.drop_duplicates() and then type df to get only the distinct rows.",True
@sjb_s2003,2024-04-26T10:22:26Z,0,"this was really helpful, thankyou",True
@burkhardleuthner920,2024-04-25T15:57:47Z,0,"Hi Alex, thanks for sharing this tutorial! Unfortunately I got an error on step [17]: world_table_titles = [title.text for title in world_titles ] print(world_table_titles) like this: ['Rank\n', 'Name\n', 'Industry\n', 'Revenue (USD millions)\n', 'Revenue growth\n', 'Employees\n', 'Headquarters\n', 'Rank\n', 'Name\n', 'Industry\n', 'Revenue (USD billions)\n', 'Employees\n', 'Headquarters\n', 'Rank\n', 'Name\n', 'Industry\n', 'Profits(USD millions)\n'] .. and I'm not able to find the reason [for this 'triple' iteration ??? until that: everything works fine....",True
@AdnanAli-vm4ox,2024-04-24T07:00:06Z,1,"extract same table in just 4 lines of code, Thank me later. make sure to import pandas and request.  Code:  url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue' html = requests.get(url).content df2 = pd.read_html(html) df2[1]",True
@monicadesai7928,2024-04-19T09:41:46Z,0,Can you share video on scraping for company career page to scrape job details,True
@kuiwang3614,2024-04-16T07:01:37Z,1,"fantastic lesson, very clear",True
@isabelasantos7240,2024-04-15T14:58:47Z,0,"I switched ""soup"" for ""table"" when he committed the mistake, however, I kept getting an ERROR, the system did not allow me to use ""table"", but once I use ""soup"" it worked, what happened? how do I fix this?",True
@benjaminkaitany4464,2024-04-14T21:06:43Z,0,Amazing tutorial,True
@adiyansfuntime,2024-04-12T10:49:23Z,0,This is a fun project. Thanks for this.,True
@YahyaAhmed-yt7fg,2024-04-10T12:13:29Z,0,"When I run this :table class=""wikitable sortable jquery-tablesorter""> <captions> it gives me a syntax error does anyone know why?",True
@cuongdinhvan5327,2024-04-10T03:14:55Z,0,"thank very much. but how can we do if the table (data) in page 2, page3,.... next and next page?",True
@jorge.roques5533,2024-04-08T18:25:34Z,40,"Honestly I love that you include your missteps in your tutorials for several reasons. It makes coding seem more human, it also shows us that even content creators and great programmers can have missteps that they need to go back and fix which is usually edited out of other tutorial videos. Not to mention there might be people having the same issues without understanding why and you explain it so its almost a mini tutorial on debugging and your programmer thought process. Overall it was an easy 25 minutes to spend watching this. Thank you.",True
@tsubame1412,2024-04-08T10:21:52Z,0,"Thanks, this video is really helpful for me at this moment !",True
@mayanktiwari6997,2024-04-06T18:36:46Z,0,Why i'm getting NameError: name 'df' is not defined. I just replace the path & run the code. Kindly reply,True
@Dataciiiiiiiii,2024-04-05T21:53:54Z,0,thanks man,True
@sruthisrisai1867,2024-04-05T01:15:20Z,0,Does any one face issue with find function. It is showing 'list' object has no attribute 'find'. Can anyone help me fix this?,True
@sojourner5294,2024-04-04T04:00:49Z,0,"Completely quick, efficient and clear, really appreciate your effort and content Alex ! Thank You !",True
@palakjain521,2024-04-02T10:46:47Z,0,"Hey. after running the end piece of code my table is showing 403 rows and not 100, also the first 3 rows are displaying the last row value from the original table. Can anyone explain the error",True
@Kaura_Victor,2024-03-27T10:21:06Z,0,Funny Alex posted this on my birthday last year. ü§≠üôàüòÖ,True
@alicemtopcu743,2024-03-25T08:20:49Z,0,Thank you,True
@uncaged3076,2024-03-23T00:19:53Z,0,Thank you üôèüèø,True
@shankaricharan510,2024-03-21T09:43:09Z,0,Thanks a lot Alex - this helped a lot.,True
@Zenitsu-mq7fq,2024-03-16T13:31:08Z,0,56/74! I'm almost there Alex) Ty for your hard work. It is a really helpful bootcamp.  But I have only one question for you. Why are you still a Data Analyst and are not going to be a Data Science or Data Engineer?,True
@leewalton7403,2024-03-15T13:41:12Z,0,please.. which web browser are you using in this video to highlight all the stuff ??,True
@abrilgonzalez7892,2024-03-13T15:32:25Z,0,Does anyone has the  link from wikipedia?  Thanks!,True
@anirudh7150,2024-03-13T03:45:24Z,0,Thank you so much. It was really helpful,True
@shivamprajapati65,2024-03-09T14:50:14Z,0,very helpful!,True
@jmc1849,2024-03-08T11:04:46Z,0,"Hi Alex (as if!)  Thanks for all the content <3  This should be the last video of the playlist ^^",True
@EuricoAbel,2024-03-07T07:28:27Z,0,"Zeus Proxy facilitates seamless SEO monitoring and data scraping, enabling users to gather valuable insights.",True
@Karansingh-xw2ss,2024-03-05T10:22:36Z,0,"df.to_csv(r'C:\Users\saura\OneDrive\Desktop\Python Web Scrapping\Folder For Output\Companies_csv') when i am running this code ,In the folder it is not  in csv format. What should i do?",True
@ZeeshanAli-ds1tm,2024-03-02T08:14:26Z,0,A question. How we can scrape 'td' and 'th' at the same time within same tbody < tr tags.,True
@adrielaraileycastro7176,2024-03-01T20:31:00Z,0,I keep getting this error message when I try the request.get (url) command  JsException: NetworkError: Failed to execute 'send' on 'XMLHttpRequest': Failed to load  any ideas?,True
@saudtechtips8674,2024-02-29T12:15:45Z,2,my mind is blown after watching the whole video i didnt imagine this could be done by python.i have to watch it again!what a person you are Alex!,True
@nanamikentoyolo,2024-02-23T07:26:56Z,0,"Hello, Alex and everyone. Very informative session, however, I have a couple of issues (as on 23rd February,2024) 1. In the revenue growth column, there are companies with negative growth, but the dataframe doesn't reflect it 2. StoneX group has a link attached to it in the employees column , as 4000[2] which the dataframe is considering as it is  Do let me know how I can overcome these. Thanks a lot in advance!  PS : Really sorry if someone has asked this already, I couldn't find it in the comments",True
@user-ez6ti9vh6q,2024-02-22T17:00:30Z,0,"A little addition for commas stripping from numerical values and columns datatype conversion for further use of the DataFrame:  df.rename(columns={'Revenue growth': 'Revenue growth %'}, inplace=True) df[['Revenue growth %']] = df[['Revenue growth %']].replace('%', '', regex=True).astype(float)  df[['Revenue (USD millions)', 'Employees']] = df[['Revenue (USD millions)', 'Employees']].replace(',', '', regex=True) df[['Revenue (USD millions)']] = df[['Revenue (USD millions)']].astype(int)  df['Employees'] = df['Employees'].str.replace(r'[\[\]a-zA-Z]', '', regex=True).astype(int) df[['City', 'State']] = df['Headquarters'].str.split(',', n=1, expand=True)",True
@clovisstanford6515,2024-02-22T15:57:44Z,0,"17:51 I thought you're like every other guy , But you are special Alex",True
@boeingpete,2024-02-18T12:14:52Z,0,Excellent. Great video. Everything explained clearly and in a way I could follow. Thanks so much.,True
@premsawant3758,2024-02-17T14:58:59Z,0,Hahaha that mistake at 12:02 I didn't make it as I was coding....I watched the video till the list comprehension and went on to code from what I understood so I did not end up making the mistake,True
@margotonik,2024-02-16T20:03:45Z,0,I loved this!!! Very good practice I enjoyed working in this project including the mistakes. Is always good to know that having errors doesn't make myself an idiot and is part of the process. Thank you so much for everything Alex I am sure we all love you as well!!,True
@eatersdaily,2024-02-15T09:43:25Z,0,"dude it's awesome ! just keep teaching. short, empty of long stories, useful and update data! that's all i want always.",True
@niktos84,2024-02-08T20:48:41Z,0,Great job. I think I have something harder then that. Is there any chance that csv file will update data it self? I mean if it has only company name? I am looking for e-mail adress - by searching web?,True
@sergiysergiy8875,2024-02-08T11:09:02Z,0,That was a good one! Thx,True
@akshaybharadwaj,2024-02-07T19:22:33Z,0,This is super helpful! Thanks so much!,True
@amy_animated3674,2024-02-05T20:46:03Z,0,new to this! just actually was super helpful in a slight adjacent way... was wondering if there is a way to do scrapes for more broadly placed information? like by /div or something?,True
@paullemaron5258,2024-02-04T21:25:27Z,1,"Hey Alex, I am so proud of the amazing job you are doing, thank you for the amazing project, I am studying for a job interview tomorrow and I know I will ace it coz Alex is my teacher.",True
@rasoultk691,2024-02-03T19:15:38Z,0,Really thanks,True
@ezhankhan1035,2024-02-02T19:03:36Z,0,"Really helpful, thanks! You explain this muuuuch better than in the IBM Python Course haha.",True
@whitey9933,2024-01-31T07:54:03Z,0,"Thanks for the tutorial,  Was always told not to add to a dataframe row by row (probably slower for much larger data),  so I appended to a list and created a Dataframe off that - pd.DataFrame(company_list, columns=world_table_titles).set_index(['Rank'])",True
@J-B98,2024-01-30T11:38:07Z,0,Please HELP!!! Can anyone explain me why when I export the data to a CSV file it is all in one column???,True
@qlintdwayne9044,2024-01-29T11:19:56Z,0,Anyone else stuck at 21:00 no matter what index used it still brings up mismatched rows error?,True
@nguyenhuyhoangk18hcm37,2024-01-25T13:45:29Z,0,I am really like your project! I appreciated you,True
@KhushiSingh-vo9nf,2024-01-23T17:48:56Z,0,thanks a lot for guiding us,True
@Nalla-perumal,2024-01-21T16:02:54Z,0,Simply Wow!!! handsoff!,True
@iSky950,2024-01-16T15:09:06Z,0,"Very nice video Alex thanks for sharing! (I love that it's ""live"" and you make mistakes too, it's more human this way!)",True
@Autoscraping,2024-01-12T21:21:27Z,0,A fabulous video that has been of great help in orienting our new collaborators. Your generosity is highly valued!,True
@AceOnBase1,2024-01-12T16:02:05Z,0,‚ÄúWe‚Äôre doing this together I dont know what im doing‚Äù ‚ÄúI didnt encounter this issue when I wrote this lesson‚Äù ü§£,True
@Charlay_Charlay,2024-01-10T01:25:05Z,24,12:21 I literally stopped when i couldn't figure out why i was getting extra titles when i pulled the titles. I'm so glad that you showed your Rookie mistake. Everyone please watch Alex's videos in full before stopping the video. Thank you for showing your mistakes.,True
@hamrozjumaev,2024-01-03T15:54:05Z,0,Thanks Alex for this wonderful video! I just have a very quick qs: after defining the table variable in next line for instance world_titles = table.find_all('th') I am getting AttributeError. Would you mind to assist me with it? Once again thank you!,True
@startech3331,2024-01-02T14:18:27Z,0,What's your compiler name ?,True
@jacobdietertupactorresbart435,2023-12-31T22:27:11Z,0,df.doc(length) lve the trick: Works perfectly because indexing starts counting with zero. pretty cool,True
@sj1795,2023-12-28T23:20:13Z,16,"This was one of my FAVORITE projects in your series so far! It was SUPER interesting and HELPFUL/USEFUL. I can see using this info for many future projects.  P.S. I LOVE that you included the ""rooky mistake"" because that is definitely something I would do and then NOT be able to figure out for an hour. These included ""mistakes"" are such valuable lessons for people in your audience like me. :) P.P.S. I really appreciate how you summarize what we do in each video/project at the end. It's these extra details that make your instruction = A+, not just an A. Also, thank you for including the index = False. As always, THANK YOU ALEX!! You ROCK!",True
@AtharvChaulkar,2023-12-28T17:47:48Z,1,Perfect ü´∂‚ù§,True
@donovanmurray7005,2023-12-28T17:41:30Z,1,Thank you!,True
@gameaddict3068,2023-12-28T05:03:39Z,0,Hey check out my chanel for nice web scraping tools,True
@SilverMiraii,2023-12-25T13:33:00Z,1,"teach me how to scrape websites that have buttons that change the page, the DOM, through javascript, without requesting more data from the server, without selenium.",True
@sesoboy3,2023-12-24T05:48:11Z,0,Impressive Alex! Could you please help me about How can i scrap hidden data from the web?,True
@vishnupkumar2395,2023-12-23T19:50:11Z,1,"Hi, One quick question: instead of all this we can simple copy-paste the content. right?",True
@naimmomin5811,2023-12-21T06:50:50Z,0,So I just had this one question and this is at 12:27 -> Even if you were to switch the soup.find_all('th') to table.find_all('th'). Shouldnt it return the same thing as the last one. Since all the tables are from the same class? and they all also use <th> for the headers,True
@jg3040,2023-12-18T05:27:48Z,0,hm am I missing something - my index is up to 1000 rows and data keeps repeating itself from 1-100,True
@suryaprakash5555,2023-12-13T16:11:32Z,0,Hi Alex ... Is it work live data,True
@aaronklingensmith159,2023-12-11T23:04:16Z,39,"Alex: when I needed to learn SQL for my first analyst job as a career changer, you were there with videos to help me do so. Now I'm in a role that is using more python and once again, you're there! Really appreciate all the work you are putting into creating content to help people!",True
@abdulhamidalfani,2023-12-10T04:05:58Z,0,hi could you make similar video but for Shopee or Lazada website? thank you :'),True
@fowsi-88,2023-12-09T17:06:18Z,0,"Hei Alex, what about if i copy paste all da info directly from the web to my excel sheetüòÇ isnt that working .??",True
@EmuBoss,2023-12-07T20:59:01Z,0,"Here is the full code:  ====================== #imporitng the library   import requests from bs4 import BeautifulSoup import pandas as pd  #defining the url to start scraping  url='https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue'   r = requests.get (url)  #cooking the soup and finding the table  soup = BeautifulSoup(r.text, 'html')   print(soup)   soup.find(""table"")  #finidng the table with the index no. The table is in no 1. In python indexing start with 0 in a list.   soup.find_all('table')[1]  #selecting the table with the class="""". In html class is a selector or an atribute to identify one object.  soup.find(""table"", class_=""wikitable sortable"")   table=soup.find_all('table')[1]    print(table)  #after finding the table. We need the table heading as well. # Remember all tables do not have a th.  # so make sure you check the table top row html tag.  world_titles=table.find_all('th')  # so here all ""th"" is saved under world_titles variable world_titles  # now  the whole table heading is saved as a list  world_table_titles = [title.text.strip() for title in world_titles] print(world_table_titles)  # using pandas a dataframe is prepared with (world_table_titles list or varialbe) df = pd.DataFrame(columns = world_table_titles)  #after preparing the dataframe and its heading, we need to prepare the row data. column_data = table.find_all('tr')  #inside the whole table we need locate the table data of the row to complete the dataframe # here we needed to rung the loop to check the inde and collect the data and place it accordingly.   for row in column_data[1:]:     row_data = row.find_all('td')     each_row_data = [data.text.strip() for data in row_data]          length = len (df)      df.loc [length] = each_row_data   df  df.to_csv(r'/Users/techemporium/Downloads/first_csvfile.csv', index=False)",True
@David-ru8ut,2023-12-04T17:29:45Z,0,thanks,True
@prasad_create2687,2023-11-24T01:10:02Z,3,"Thank you, I learnt basics of python yesterday(had learnt C+ 8 yrs back so it was easy to relate) and I am a mechanical engineer but want to get into Product. This video was useful to learn and will modify it for other websites hopefully. Thanks again!",True
@pss6957,2023-11-22T15:38:54Z,1,"#AlexTheAnalyst  thanks a lot , how can we productize these kind of scipts rather than notebooks ?",True
@SupCortez,2023-11-21T22:10:47Z,3,"Just finished google data analyst certification, you about to help me make my portfolio look phat with scraping my own data before I do my whole hypothesis and data vis",True
@coorditecnologia,2023-11-21T13:57:45Z,0,Muchas gracias excelente contenido,True
@outhouse.wholesaler,2023-11-21T06:28:06Z,1,What if Wikipedia had split the list over two pages with page 1 & 2 hyperlink buttons at the bottom of the page.  How do you get python to click those links and continue scraping on page 2?,True
@jackspringsteen26,2023-11-15T01:23:04Z,0,I have an Error that says cannot set rows with mismatched columns can anyone help?,True
@ajibadeabdulateef2818,2023-11-10T12:32:05Z,1,"Let me start by thanking you for all the tutorials in this playlist, they are totally worth my time.  thank you. what would be the reason why I have double of the data on my own part I am having 200 instead of 100 data",True
@Larocaxx,2023-11-08T22:50:01Z,0,We love you too Alex ‚ô• thank you for such great videos,True
@francescab1413,2023-11-08T08:56:38Z,9,"I'm so glad you make mistakes and show us where to check if something goes wrong! It's my main problem when I have to work on my own after a tutorial, I mess up and don't ever know where to start to clean up my mess.",True
@liorpolak1391,2023-11-07T12:15:25Z,1,Who needs Web Scraping service here?,True
@richardtorrenueva5512,2023-11-04T14:13:10Z,0,I love this. Thank you Alex.,True
@saifilicious1749,2023-11-02T17:21:12Z,1,"Hello Alex.when I‚Äôm copying these codes it is saying requests is not defined ,name soup is not defined .What to do?Please help or can anyone help?",True
@EKTurduckin,2023-11-02T04:53:43Z,23,"Last year I got a job as a BI Analyst and I've been watching your stuff here and there. This video is hands down one of the best videos I've watched of yours.  I had to take multiple tables, pivot them, and label them with the table name and this video 100% helped me get there. I had run into my own set of issues, but not far removed from your sections of mistakes, so thank you for not letting those hit the cutting room floor.  Anyway, keep up the great work and thanks so much!",True
@joeche7461,2023-10-30T23:27:42Z,1,Thanks a lot for the video.,True
@izzyvickers6258,2023-10-30T13:49:47Z,2,You made this wayyyy easier than I thought it would be! Worth a sub from me sir!,True
@Photoshop729,2023-10-28T02:09:58Z,2,So far on my web scraping journey I don‚Äôt know if web scraping is any faster than just manual copy paste unless you have repeated scrape requests of the same site or structure,True
@efeberke681,2023-10-25T19:33:05Z,0,So helpful!,True
@prince5724,2023-10-24T22:35:07Z,0,why cant we just copy the table in excel and save it as a csv and then import in pandas for analysis ????,True
@patcher2944,2023-10-24T04:37:23Z,0,Sir alex is love,True
@SankarJankoti,2023-10-19T20:46:02Z,0,Thank you. But what is the usecase,True
@blackwidow2899,2023-10-19T17:48:36Z,0,"Wow, Alex  I totally enjoyed this. You make it so easy to understand. Now I need to go through your pandas tutorial and learn data manipulation. Thanks for being there!",True
@imnotragil,2023-10-14T14:14:38Z,0,why i can't do the table=soup.find_all() then world=table.find_all() its give me an error,True
@melissalopezdecastilla9817,2023-10-10T22:35:53Z,0,"Hey Alex! It seems like I am encountering an issue where the cell number increases, but you're not seeing the expected output in Jupyter Notebook. This happens after running the code of the 3rd cell ' print(soup)', I am not seeing the printed data in the cell output and only getting an increment in cell numbers. Why is that happening and how can I fix it?",True
@jimperkins9661,2023-10-09T23:11:48Z,0,What's the IDE you are using? Amazing.,True
@fmtdamian,2023-10-07T18:21:38Z,0,"How to get value 28 and 1,257 ( separately and without HP and lbs) from this code :  <div class=""tpw"">28 <i>HP</i>1,257 <i>lbs</i></div>. ? Any ideas ???",True
@ahmed-Alziyadi,2023-10-07T17:26:58Z,0,"Tables are easy, we need something a little harder like online pharmacy data..etc.",True
@nabilanwari2537,2023-10-05T18:26:32Z,0,"I would like to Thank you Alex for your continous endeavour. Just, we could procede like this, given that we may use the properties of the list type of the data, taht we may locate easily here : import pandas as pd  url = ""https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue"" data = pd.read_html(url) data[1]",True
@user-mh1ch3mq7h,2023-10-03T08:06:58Z,1,Interesting class!!,True
@cosmicstrays6864,2023-10-03T07:37:23Z,1,"Hi Alex, thanks a lot. Can you tell me how to do web scrapping where inspect option in chrome disabled ?",True
@sumanhachappa2822,2023-10-02T13:28:59Z,0,fantastic way of explaining things,True
@vnrd9,2023-10-02T09:26:03Z,0,"thank you so much, super helpful",True
@chomas001,2023-10-01T22:13:46Z,0,Nice,True
@adminravi,2023-09-28T13:31:10Z,1,"Exceptionally outstanding content. Hats off to Alex. Just one question can we instead of saying   column_data = table.find_all('tr')  we say,  rows = table.find_all('tr')  as in your words r-row. I know we can name variables by any name but I want to be logically correct. Will it(rows) be logically incorrect?  and while fetching 'td' we will save it in row_data variable. Will it be correct?",True
@dhanienugroho4323,2023-09-28T06:58:24Z,1,Thanks for the tutorial! I just found the channel and I like the way you explain it!,True
@pavithrac8192,2023-09-27T14:17:59Z,1,Heya... Ur videos are excellent and i have learned a lot from it... But in this web scraping... I got an error as --cannot set a row with mismatched columns... I have checked your other videos... could you please help me on that.,True
@wesrocha3293,2023-09-25T23:52:13Z,0,"Amazing, thanks!",True
@prezgrounds6170,2023-09-24T19:29:29Z,0,Thanks for this video helped me a lot. When I tried to pull the table headers only worked with tr not th. This might help others with the same issue,True
@MudassarAli-bx2pf,2023-09-23T16:46:11Z,0,Excellent Work Sir!!! I really Appreciated your work believe me You are a great mentor!,True
@YourYTHUB,2023-09-23T13:13:31Z,2,"Hey Alex, thank you so much for ur effort,,,its a really super helpful series üôè",True
@lilyfullery4779,2023-09-19T05:56:22Z,0,U r the best ‚ù§,True
@v1s1v,2023-09-18T20:24:35Z,1,"Nice tutorial, but there are AI tools now like Kadoa that can do all of this for you. In the time it takes for you to watch this video, you can get an AI scraper up and running.",True
@StopWhining491,2023-09-18T13:38:55Z,1,What benefits does scraping using Python have over Power BI or Power Query?,True
@anuradhamondal1601,2023-09-18T00:31:31Z,0,"02:26 lol.. as a beginner to this and already overwhelmed with all information i recently learned, it is exactly what i would had thought!",True
@ZeuSonRed,2023-09-16T13:50:31Z,1,This was from the Greatest Videos I have Ever seen Thank you! Very Much! üôÉüôÉüôÉüôÉüôÉüôÉüòä,True
@jhutchtor1,2023-09-15T08:21:37Z,1,"When I try this code  for row in column_data:     row_data = row.find_all('td')     individual_row_data = [data.text for data in row_data]     print(individual_row_data)  I get the following error  File ~\anaconda3\Lib\site-packages\bs4\element.py:984, in NavigableString.__getattr__(self, attr)     982     return self     983 else: --> 984     raise AttributeError(     985         ""'%s' object has no attribute '%s'"" % (     986             self.__class__.__name__, attr))  AttributeError: 'NavigableString' object has no attribute 'find_all' Not sure why or how to fixt it",True
@yunusaprianus736,2023-09-14T09:39:49Z,1,"I'm done with the tutorial today and end with awesome successful, i'm facing some trouble since i use different site but yeah, my scraping going well!  Thank you so much!",True
@chidinwosu-cc7ro,2023-09-13T08:58:05Z,0,please where did the title come from in the title.text,True
@Phenom5,2023-09-12T18:42:56Z,1,Hi ! How do you get to using this web interpreter ? Are there tutorials ? I was looking but  I couldn't find any not knowing what it actually is,True
@gh-sb1dy,2023-09-10T00:22:34Z,0,"hi  what are you using to type in? how do i open that ""python resources tab /page? where do i find that sheet to type in (the white sheet your typing the code in?",True
@khanglee6209,2023-09-09T04:33:58Z,1,"Hi guys, i'm here to asking about when i export the lastest data to csv, it went from beautiful visual in panda to ugly data in csv, how can Alex export the beautiful one to csv at last ? Pls help me",True
@toniivanov4419,2023-09-08T12:24:07Z,0,just copy paste the html to chatgpt and ask it to do it for you bro,True
@user-xb7og2ls5s,2023-09-07T21:30:24Z,1,"Thank You so so much for this video, Alex! It was super useful and easy to follow!",True
@jeet611_,2023-09-05T09:10:43Z,1,Thanks alot Alex it helped me alot to explore this Webscraping and thanks for making this interesting and on point,True
@Kicsa,2023-09-05T06:07:30Z,5,"I saw all the videos for this playlist and I am getting to this last one, I haven't felt so happy to learn in a while, thank you for your work and help!",True
@kakamoora7874,2023-09-04T19:05:58Z,1,Bro time changed üòÇ.. but you a teaching old method üò¢,True
@traetrae11,2023-09-04T16:45:27Z,2,"Thank you for doing this Alex. I learned a lot and followed along while watching this series so that I could learn how to do this as well. Now all I need to do is practice, practice, practice.",True
@gabinkundwa7215,2023-09-02T03:11:26Z,1,"Thank you Alex, I am new to web scrapping and this video was helpful to me! Keep the good work!",True
@clebersonosorio1398,2023-08-30T13:41:52Z,0,"Excellent tutorial, what would be this environment you are running?",True
@pritamlaskar7265,2023-08-30T04:57:40Z,1,Thank you so much! Very clear and well explained!,True
@noob4head,2023-08-29T14:15:23Z,4,Thank you for this video with a extremely clear explanation. I always wonder why my college professors can't explain something as clearly as some people on YouTube can.,True
@ivankostikov8377,2023-08-28T03:23:28Z,0,"Hi! isn't it easier to do everything through ""read_html""?  pd.read_html(url)[1]",True
@abutalha7184,2023-08-24T13:43:48Z,0,1000th like from me üòÖ,True
@abiol1542,2023-08-22T15:26:12Z,0,"Hi Alex, I need help scrapping some date from a website but it's been super challenging. Would you be able to help? Once I have an example I should be able to run from there. Thanks a lot.",True
@Best_Sellers001,2023-08-19T14:57:25Z,0,I need help this isnt working for me.......,True
@stingray3565,2023-08-18T04:26:43Z,0,Great video. Thank you...,True
@user-hx9is4uw5g,2023-08-14T01:03:45Z,0,"I think the jquery class is javascript, and the site also  uses bootstap for their class.",True
@ebamybass19,2023-08-11T09:30:31Z,2,Thank you Alex Frebeg ‚ù§‚ù§,True
@saimanisha4378,2023-08-10T22:20:23Z,0,Make videos on artificial intelligence,True
@dakshbhatnagar,2023-08-09T15:09:09Z,0,"Hey Alex, Can you do a selenium scraping tutorial? It would help a lot to scrape dynamic websites.",True
@martinologunja4003,2023-08-08T13:06:01Z,28,Alex telling us he loves us ü•π,True
@hooverzavala4560,2023-08-08T10:56:16Z,0,"The real problem is to access  websites that detects me as webscraper, do you have a tuto abouit this issue?",True
@user-vf1eu2rw6s,2023-08-07T00:06:52Z,0,"I tried this for the rows but id displays as one row    ['1', 'Walmart', 'Retail', '611,289', '6.7%', '2,100,000', 'Bentonville, Arkansas', '2', 'Amazon', 'Retail and Cloud Computing', '513,983', '9.4%', '1,540,000', 'Seattle, Washington', '3', 'Exxon Mobil', 'Petroleum industry', '413,680', '44.8%', '62,000', 'Spring, Texas', '4', 'A   how can I make it row by row",True
@ultimategohan2190,2023-08-04T06:08:29Z,0,Thanks Alex,True
@mikeg4691,2023-08-01T08:10:31Z,3,"I found out why the class names were different. It seems to be a common issue. Someone explained it on Stack Overflow,  ""The table class wikitable sortable jquery-tablesorter does not appear when navigating the website until the column is sorted. I was able to grab exactly one table by using the table class wikitable sortable.""",True
@cristianandrade2565,2023-07-31T20:04:22Z,2,"On the ""Revenue growth"" column, might be important to notate the sign of the growth (positive or negative one). I found a workaround for this:  revenueGrowth = [] table = soup.find('table', class_='wikitable') data = table.find_all('td')  for x in data:         if 'Decrease2.svg.png' in str(x):         decreasedGrowth = '-' + str(x.text.strip())         revenueGrowth.append(decreasedGrowth)",True
@prempatkar2372,2023-07-31T14:37:28Z,1,"Hey Alex,  It was a great video and I did find it to be very helpful and intresting . I would like to ask one question can we also do it for the second table and can we get the same table under the same excel csv file?",True
@Machiavelli698,2023-07-31T11:59:53Z,1,"I appreciated you, I love you üòÇ‚ù§",True
@yoshitamanavi530,2023-07-27T22:39:57Z,0,"I just have one comment, You are the best Alex ü§©",True
@ibikunleadekiitan9882,2023-07-27T10:17:51Z,0,Thanks Alex for making me a great value to the world,True
@givepain6247,2023-07-26T06:01:08Z,1,"Hey Alex, please I need your helpüòä. I made a python script to extract some products from a grocery store website made in html and it worked .  But when I tried to run it on other grocery store website didn't work.  I change the script to run in Javascript and the program runs but my excel file is empty. I did everything but still can not give me the product's in the excel.   Any thoughts?  Thanks in advance üòä",True
@Nomuz32,2023-07-24T08:14:37Z,19,"Hi Alex, thank you a lot  for all the videos. I'm currently doing a change of career to data analyst, and you are giving me more than just a little help with all your courses. Thanks for all",True
@ibrahimmohamoudbile3424,2023-07-22T19:51:12Z,0,You‚Äôre a ‚ÄòGod sent‚Äô  my g,True
@NewLeaf88,2023-07-22T02:58:03Z,2,"Hi Alex,  could you explain how I could create a file that could loop through a given URL to extract data from multiple pages?",True
@jsthaut,2023-07-21T17:11:51Z,0,"how do i make it, so that if it finds what i'm lookign for in a page it prints something, but if it doesn't it prints something else? thanks",True
@rarro_club,2023-07-19T18:25:07Z,0,How do you deal with rowspans? They're creating a mismatch that errors out.,True
@Ghazanfierce,2023-07-17T20:27:53Z,1,BTW have u ever used Scrapy or Selenium for data scraping and would u say that its beneficial to know it???,True
@himanshubisht5023,2023-07-16T10:46:50Z,2,"Hello Alex Sir! Thanks for the great video, super helpful as always! Could you do a video on how to convert PDF file to excel in python | OR | Data extraction from PDF File.  It will be really really helpful to me and other student/fresher...",True
@frybait0626,2023-07-16T10:38:25Z,1,"There is something wrong with the table2. Table2 only contains 20 rows of data, up until the point of for loop for Table2 is correct. Its outputting 20 rows of data but once you call df, then its outputting 100+. Something is wrong in there. Upon checking the CSV file once the data for table 2 has been saved, rows are being repeated over and over. I think there must be something wrong with the for loop I guess.",True
@atrallzerhas3157,2023-07-15T17:21:46Z,0,great video.Thank you,True
@oanhkieunguyen156,2023-07-15T14:45:03Z,0,Thanks so much for this video! I firstly understand the principle and the way to scrap data :),True
@UtiaGaxton,2023-07-14T11:40:56Z,0,Thank you sir. You got me going,True
@frybait0626,2023-07-14T11:30:04Z,0,I think you definitely did some behind the scenes cuts because just changing soup.find_all('th') to table.find_all('th') will definitely produced some error.,True
@Jt277277,2023-07-13T14:11:30Z,2,"Thank you Alex for such a great lecture. But i have a question: while the length of   the df will go from 0,1,2,3... every time we loop, so why can't we use df.iloc for index slicing instead of using df.loc? Thank you",True
@phillipjbates,2023-07-13T12:24:40Z,0,I get error on pages.txt: object has no attribute txt,True
@Url_jay,2023-07-12T15:18:50Z,0,"Thanks Alex... I also took the Whole Code and Prompted Chat Gpt to educate and Interpret it for insights... Amazing Stuff, How can one Submit a Portfolio for Reviews... Thanks Alex...",True
@korands,2023-07-12T12:18:10Z,1,could you web scraping tiktok?,True
@goutamsengupta5910,2023-07-12T10:32:47Z,0,Can u plz recommend some certification for cybersecurity in udemy and coursera,True
@assettemirkhan7087,2023-07-12T04:16:08Z,0,"Hi Alex, thanks for the video, it is very helpful",True
@neronova1176,2023-07-12T03:05:11Z,5,"Thanks, Alex!  This was a really helpful lesson and project. This helped me get a better understanding of web scrapping and restructuring the data. Now, I feel confident in applying this to a project I've been working on.",True
@artemboichenko743,2023-07-12T02:21:01Z,14,"Hi Alex! Super helpful video, thank you! One detail though: Growth index is not always positive. We may see in the wiki table negative and positive values are present in that column. Instead of using ‚Äò-‚Äò for negative value, that table uses small triangles. Could you show us how to manage that - to convert those triangles into positive or negative values accordingly?",True
@thememeguy7630,2023-07-11T23:26:56Z,2,"Hello Alex! Just finished your Data Analyst Bootcamp, will you be doing Data Science Bootcamp in the future? Thanks!",True
@santhoshram1724,2023-07-11T16:35:05Z,0,Many data in GitHub can't download. It makes difficult to practice Pls help!,True
@user-cc32vcg811,2023-07-11T16:32:51Z,3,"Hey Ale, how are you doing today? I wanted to ask you that, since data jobs in my area (Spain) require at least 3 years of experience and/or a bachelor's in a quant fielf, how should one break into the industry only with certificates? Is it by showcasing projects? Trying to network ppl in hr/data industry? Thanks in advance, and thanks for all of the amazing content!",True
@Anuj_Hindu_10k,2023-07-11T16:01:29Z,0,"Wow, amazing video sir....Thanks you",True
@raphael.dev13,2023-07-11T14:54:26Z,10,Hey Alex! Thanks for the great video as always! Could you do a video on the repercussions and impact on the Data Analyst career now that OpenAI released their GPT Code interpreter?,True
@leonardnewbill793,2023-07-11T14:44:32Z,1,Super excited to finish the lesson! Thank you sir. I appreciate it!,True
@Mvjesty23,2023-07-11T14:17:33Z,1,I‚Äôm going to do this today! Thank you Alex üòÑ,True
@MIRIAMIKECHUKWU,2023-07-11T14:10:29Z,0,Thanks Sir Alex,True
@anthonygordon5052,2023-07-11T13:30:39Z,0,Thanks for the videos as usual Alex !,True
@louisamkeyakala9420,2023-07-11T13:04:01Z,4,the way i was waiting for this videoüòÇ..thank you Alex,True
@aladiresoliu268,2023-07-11T12:43:00Z,1,"Hi Alex,  What steps would you advise one take in becoming a data analyst?",True
@muhammadabdullahbaig6932,2023-07-11T12:05:16Z,1,"Hey Alex, Is working as a trainee data scientist good as a fresh graduate ?",True
@moviesprobe6220,2023-07-11T12:03:31Z,1,Much needed video ‚ù§,True
@YouTubeVenJiX-zl4bj,2023-07-11T12:01:37Z,6,Sir you are a real hero ü§ó,True
