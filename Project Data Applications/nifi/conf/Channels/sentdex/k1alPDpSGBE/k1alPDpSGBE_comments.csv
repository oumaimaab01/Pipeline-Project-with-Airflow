author,updated_at,like_count,text,public
@user-rx4eq7mx4y,2024-03-03T08:45:44Z,0,"Hey harisson where should I go from here, continue this series or continue from your new deep learning flow,keras Playlist?",True
@user-rx4eq7mx4y,2024-03-01T06:07:07Z,0,"finally made  it 1/march/2024.thanks harisson as mention earlier best ml playlish in whole youtube. now time to do some project and  will continue my journey further (deep learning) from 8 march till then time to make some project .if someone who is thinking about taking this course .Go  for it Some advice i will give- 1-Be patient 2- Dont hesitate of vivewing same video in loop.for hours 3-Use chatgpt for doubt solving 4-If you are a smoker ,you may need some couple of cigg for (svm) part.  ""justKidd..'",True
@tresna132,2021-05-05T08:25:46Z,0,"finally,. p1 ~ p42 done,.",True
@cccloud3256,2021-03-15T10:18:10Z,0,"Please anybody help me understand this ""weight"" thing. I don't quite understand why do we need to duplicate many featureset into ""to_add"". Thx",True
@timvanderoest5741,2020-12-03T11:35:11Z,1,get you a blob that can do both,True
@decode0126,2020-08-31T11:12:11Z,0,u are a genius!!!!,True
@kevinziroldi3342,2020-07-07T08:36:50Z,0,"If you get a warning, use 'from sklearn.datasets import make_blobs' instead of 'from sklearn.datasets.samples_generator import make_blobs', which is deprecated",True
@ramakrishnasuresh6544,2020-04-27T17:34:31Z,0,yea into deep learning now,True
@nipunaviduranga6614,2020-01-27T14:36:43Z,0,clap clap clap,True
@cap4081,2019-10-27T17:52:28Z,6,Onto deep learning!,True
@rostyslav.,2019-09-15T07:39:38Z,0,"Hi! You have double centroids here, because you added to deleted array inner loop variable ""ii"" instead ""i"", replace that to ""i"" and that should work fine.               for i in uniques:                 for ii in [i for i in uniques]:                     if i == ii:                         pass                     elif np.linalg.norm(np.array(i)-np.array(ii)) <= self.radius:                         #print(np.array(i), np.array(ii))                         to_pop.append(ii)                         break  I improve some part of code and that works faster. I calculated average by hand without array of weight.   class MeanShift:     def __init__(self, radius=None, radius_norm_step=100):         self.radius = radius         self.radius_norm_step = radius_norm_step         self.centroids = set()         self.classifications = dict()      def fit(self, data):          if self.radius is None:             all_data_centroid = np.average(data, axis=0)             all_data_norm = np.linalg.norm(all_data_centroid)             self.radius = all_data_norm / self.radius_norm_step          optimized = False         centroids = set(tuple(v) for i, v in enumerate(data))         weights = list(range(self.radius_norm_step))[::-1]         while not optimized:             new_centroids = set()             for v in centroids:                 counts, in_bounds = 0, []                 for featureset in data:                     distance = np.linalg.norm(np.array(v) - featureset) or 0.00000000001                     weight = weights[min([int(distance / self.radius), self.radius_norm_step - 1])] ** 2                     counts += weight                     in_bounds.append(featureset * weight)                 new_centroids.add(tuple(np.sum(in_bounds, axis=0)/counts))              for c in new_centroids.copy():                 for cc in new_centroids.copy():                     if c == cc:                         continue                     if np.linalg.norm(np.array(c) - np.array(cc)) <= self.radius:                         new_centroids.remove(c)                         break             optimized = not centroids.symmetric_difference(new_centroids)             centroids = new_centroids          self.centroids = centroids         self.classifications = {i: [] for i in range(len(self.centroids))}         for featureset in data:             self.classifications[self.predict(featureset)].append(featureset)      def predict(self, data):         distances = [np.linalg.norm(c - data) for c in self.centroids]         return distances.index(min(distances))  Thank you for videos and sorry for my English :)",True
@lcoandrade,2019-08-30T14:31:34Z,0,Great video and great content! I'm just sorry for the keyboard.... that's abuse!!!! :),True
@venishpatidar3991,2019-07-29T10:49:00Z,1,"After watching K means from scratch. i decided for my own mean shit program. and ....... import numpy as np from sklearn.datasets.samples_generator import make_blobs import matplotlib matplotlib.use('TkAgg') import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot')  centres = [[2,3],[9,3],[10,9]]  X,_ = make_blobs(n_samples=100,centers=centres)   class MeanShift:     def __init__(self,tol=0.001,max_bandwidth=5,bandwidth=0.75):         self.tol = tol         self.max_bandwidth  = max_bandwidth         self.bandwidth = bandwidth      def fit(self,data):         self.centroids = []         tmp_data = np.array(data)         self.points = tmp_data           for point in self.points:             self.bandwidth = 0.5             self.point_location = []             #print(""loop activated================================="")             for i in range(self.max_bandwidth):                  distances = [np.linalg.norm(point-ii) for ii in tmp_data]                 #print(""----------------------\n"",distances,""------------------\n"")                 for i in range(len(distances)):                     if distances[i] < self.bandwidth:                         #print(distances[i],i,""\n"")                         self.point_location.append(i)                 if len(self.point_location)<=3:                     self.bandwidth+=0.5                     #print(""bandwidth increased"")             self.point_location = np.unique(self.point_location)             #print(self.point_location)             make_average = []             for i in self.point_location:                 make_average.append(tmp_data[i])              #print(make_average)             centroids = np.average(make_average,axis=0)             self.centroids.append(centroids)             numpydata = np.array(self.centroids)         #print(new_data,""\n"")         self.new_data = np.unique(numpydata,axis=0)         #print(final,""\n"")         #print(len(self.new_data),len(numpydata))          if len(numpydata) != len(self.new_data):             MeanShift.fit(self,self.new_data)         else:             #print(self.new_data)             print(len(self.new_data),""centres allocated."")             pass      def lables_(self,data):         self.classification = []         for point in data:             distance = [np.linalg.norm(centre-point) for centre in self.centroids]             self.classification.append(distance.index(min(distance)))         return self.classification      def centres_(self):         return self.new_data      ms = MeanShift() ms.fit(X) lables = ms.lables_(X) print(lables) centroids = ms.centres_() print(centroids)  colors = 10*[""r"",""g"",""b"",""c"",""k"",""o""] markers = 10*['o','*','x','.'] for i in range(len(centroids)):     plt.scatter(centroids[i][0],centroids[i][1], marker=markers[i],color=colors[i],s=75)  for i in range(len(lables)):     plt.scatter(X[i][0],X[i][1],color=colors[lables[i]],marker=markers[lables[i]],s=10)  plt.show()   ~~thanks for knowledge sentdex .",True
@amdreallyfast,2018-12-04T17:49:05Z,0,"Hello from 2018. Yes, that ""to_add"" with the weights * [featureset] is what is killing your performance. If weight=99, then 99**2=9801, which is a very large list to be appending. Granted 99 is not common, but weights of 40, 60, 80 could be, and those are still big. I worked out a way to do the average without the list appending, and that made performance much more acceptable. I couldn't figure out how to deal with centroids though that converged very close to each other but not quite within the min radius.",True
@ricardocancar6879,2018-08-21T19:59:51Z,0,"I know is late, but when you do to_add = (weights[weight_index]**2)*[featureset] you are creating a long list, and np.average have the options weigths which is faster   while True:             new_centroids = []             for i in  centroids:                 #print(i)                 weight = []                 in_bandwidth = []                 centroid = centroids[i]                                  for featureset in data:                                          distance = np.linalg.norm(featureset-centroid)                     if distance == 0:                         distance = 0.000000000001                     weight_index = int(distance/self.radius)                     if weight_index > self.radius_norm_step -1:                         weight_index = self.radius_norm_step-1                     weight.append([(weights[weight_index]**2)*featureset[0],                                   (weights[weight_index]**2)*featureset[1]])                     in_bandwidth.append(featureset)                                          new_centroid = np.average(in_bandwidth,weights=weight ,axis=0)#array                     new_centroids.append(tuple(new_centroid))#array to tuple  I try this on 50 samples dataset and it took 0.2 sec   my english is not that good so can't give a better explanation sorry.  Thanx for your videos.",True
@DerpRenz,2018-08-06T11:04:32Z,12,I think i found a solution to why the centroids and classification was so messy at the end. The radius was very low due to having positive and negative points so the magnitude of the average was nearer to 0. To solve this you need to use the abs(data) for the allDataCentroid because that is like getting the distance of each of the points from the origin and getting the average for AllDataNorm. The problem was that though points would be far away (either negatively or positively) from the origin the bandwidth was low. I tested it and it doesn't do any of the messy clustering anymore and is much more accurate.,True
@jimxiao9495,2018-07-19T08:55:14Z,1,"Hey . i found a little confused about the classification part:         for featureset in data:             distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]             classification = (distances.index(min(distances)))             self.classifications[classification].append(featureset)  Imagine a situation that point A belongs to class 1,but point A is closer to the centroid of class 2 than the centroid of class 1.According to your code , the A will be classified as class 2.I think the classification part should be in the While True loop .",True
@berksedatk,2018-07-16T11:20:09Z,17,If you watched all the series clap your hands!,True
@pritamitsyou,2018-06-29T13:21:12Z,0,if i add one more point to our data the third center just escapes from the picture,True
@saketdetroja9251,2018-05-25T07:48:36Z,0,"There is a bug in ur code sir... U can see the bug directly by removing any 1 of the data sample from ur X array for example removing the data [to [10,2], the code won't be able to classify in 3 clusters... rather there will be only 2...The problem comes from the nested for loop where u build to_pop array... There is repetitions and all that problems in that nested loop... so u can use i in range(len(uniques)) : And inner for loop for ii in range(i+1,len(uniques))  Thank you :)",True
@alexmorehead6723,2018-05-20T03:25:24Z,30,"If you made this far through the tutorial series and haven't skipped any videos, congratulate yourself! If you need any evidence of why you should, simply look at the number of views on this video compared to the number of views for the next video, part 43.",True
@martinneighbours9779,2018-03-14T10:51:43Z,0,"Love the series.  Not sure if this is an issue with Python 2.7 on a Mac only, but I needed to change the following line to get this to work:  from:                 for ii in [i for i in uniques]: to:                 for ii in [u for u in uniques]:  Using i seems to interfere with the i in the for loop before it and ends up not merging all the clusters that it should i.e. i always ends up as the last value in uniques and stays that way for the entirety of the loop.  Whereas the intended behaviour is to compare each i with each value in uniques (accepting that 1 will always match) and if any values are less than the radius drop them from the list.  I guess that a more robust solution would be to recalculate the average of the merged clusters, but not sure whether this would put it in an endless loop.",True
@YashChavanYC,2018-02-17T15:15:32Z,4,"In this and the previous one,  I am getting more cluster centers than in the video. Why! :(",True
@sarthakgarg4864,2017-12-01T13:37:58Z,1,Could someone please explain what the below line does: to_add = (weights[weight_index]**2)*[featureset]  Any help would be appreciated.,True
@dude2260,2017-08-17T17:48:25Z,0,thanks,True
@DM-py7pj,2017-08-01T06:22:13Z,3,"Some diagrams illustrating the logic being implemented, with explanations, would be really helpful for visual thinkers such as myself.",True
@ramasubramanian7862,2017-06-06T17:09:17Z,5,"I don't think we need to increase the size of the in_bandwidth list. Why not simply use the 'weights' parameter of np.average? (or am I missing something here?).  That part of the code could be like:  for i in centroids:           centroid=centroids[i]           in_bandwidth=[ ]           weight_list=[ ]  # To have weights for np.average           for featureset in data:                   distance = np.linalg.norm(featureset-centroid)                    if distance==0:                           distance=0.0001                     weight_index=int(distance/self.radius)                      if weight_index>self.radius_norm_step-1:                                weight_index=self.radius_norm_step-1                            weight_list.append(weights[weight_index])                     in_bandwidth.append(featureset)                     # to_add=(weights[weight_index]**2)*[featureset]                     # in_bandwidth+=to_add       new_centroid=np.average(in_bandwidth,axis=0,weights=np.array(weight_list))       new_centroids.append(tuple(new_centroid))",True
@hellopoop8888,2017-04-13T01:13:27Z,3,Again - thanks for your great work on this series.  Question: What if the average of all data points is the origin. (ie the all_data_norm = np.linalg.norm(all_data_centroid) = 0) I know this is unlikely but could happen (like 4 clusters each cluster in each quandrant - assuming a 2-d dataset). Does this mess up this implementation of mean_shift? And I guess this leads to my more general question of why exactly is the norm of the overall average vector of all data points a good starting point/estimate of what the overall radius should be?   Appreciate your time.,True
@sukumarh3646,2017-02-17T02:58:15Z,0,"I tried this as input X, y = make_blobs(n_samples=30, centers=10, n_features=2) The data I got clearly has 5 clusters but the result shows more than 10 clusters with  distance between few centroids less than 1 unit",True
@sukumarh3646,2017-02-17T01:31:37Z,5,"I think there is an error in the code for to_pop. Let's say there are two centroids a and b that have a distance less than radius. when i = a and ii = b, b is added to to_pop later when i=b and ii=a, a is added to to_pop  So your to_pop will have all centroids that have another centroid in their neighbourhood.",True
@mikeg6988,2017-01-06T21:32:24Z,0,"13:30 ish  Right before the to_pop step, you mention converging (i) and (ii), but I don't see that happening anywhere, rather you're just removing (ii), which shouldn't make any significant difference since (i) and (ii) are supposedly close (enough to justify converging), but it might be worth including this to maximize accuracy, even if it is by a very small amount.",True
@ranadiveomkar,2016-12-28T14:09:14Z,1,"A simple way to optimize the squaring of weights is to calculate the square of weights before the 'while True' loop instead of just setting weights = ith value.  I.e We do, weights = [i**2 for i in range(self.radius_norm_step)][::-1] instead of weights = [i for i in range(self.radius_norm_step)][::-1] and then reference them directly while multiplying the featureset.  So instead of having to square the weights for n iterations we will be squaring them only once initially.",True
@amyxst,2016-12-16T20:16:56Z,31,"Thanks for this series! Just want to note two things that hopefully aids in someone's understanding (hopefully I didn't make an error in my understanding as well):  1. The current vid implementation contains a logical error in the way centroids are popped. Suppose we have centroids A, B, and C, where B is in radius of A and C is in radius of B, if we're iterating from A->B->C in the current implementation, B will be added to to_pop first, and C will also be added to to_pop. But if B is already in to_pop, (i.e. it's set to be popped because B is close to A), then C technically should not be in to_pop because it's not in the radius of A. In other words, A and C should be the centroids at that point after the remove operation, as opposed to just A, which is what the current implementation yields.  2. Try/except is required in the vid implementation because duplicate centroids are still being added to to_pop. Going back to the example from 1., here in the vid implementation, B is added to to_pop while checking A's radius of close-by centroids, B is then added to to_pop again while surveying C's radius.  The following modification should address both of these points:  to_pop = []  for i in uniques: ----if i in to_pop: pass # we're not inspecting centroids in radius of i since i will be popped ----for ii in uniques: --------if i == ii: ------------pass --------elif np.linalg.norm(np.array(i)-np.array(ii)) <= self.radius and ii not in to_pop: # skipping already-added centroids ------------to_pop.append(ii)  for i in to_pop: ----uniques.remove(i)",True
@rajubhai-gl4qk,2016-12-16T15:01:13Z,0,"hello sentdex,  I have a list of numpy arrays of (float values) where each array in list are of Intensity values of the image of dimension (336,336,80,3) and of size like 103MB. And i need to apply Mean shift clustering to this list. Could you please suggest me in how to apply mean shift clustering to the list such that each clusters represents each array in list of numpy arrays?  Regards, Raj",True
@ehsanestiri6869,2016-10-01T01:22:02Z,0,"Hi thank you. I hope I find the answer by myself and delete my comment as soon as I found it but based on my knowledge that I learned from you, I am confused about this part  for i in uniques:                 for ii in uniques: as I know, here we assign i to any value in uniques so we can  populate it with something. so I dont understand why uniques, i and ii are all different. for instance when I print([(uniques),(i),(ii)]) I get  [[(-9.2639608167951923, 0.67730633975233001), (-9.2392077561082289, 0.78033299393753497), (-9.2204445369619794, 0.88140175205247395),  I dont get if i and ii are something we assign to one specific list, why we get different values.",True
@Grepoan,2016-09-26T23:57:33Z,1,"Hopefully this helps:  I modified the ""to_pop"" generation for-loop and no longer have duplicates.  Inspired by the ""if not optimized""-""if optimized"" double break statements, my ""to_pop"" generation for-loop now looks like this:  to_pop = [] for i in uniques:     for ii in uniques:         if i == ii:             pass         elif np.linalg.norm(np.array(i) - np.array(ii)) <= self.radius:             to_pop.append(ii)             break     if i in to_pop:         break  In this case, I add the ""if i in to_pop: break"" because it asks ""is there a duplicate unique? If True, then move on!",True
@oliverli9630,2016-07-17T10:00:50Z,0,"Hi, i always love your tutorials. While you take a break from the ML, i want to learn about apply python in commodities trading (my English isn't so good, cuz some people call it spot trading too, which is about the trading of cruide oil, gold, silver, etc.)  Do you have a series about that, from getting minute-level data to strategies and to the ML for that part? Thanks",True
@skhaaaan,2016-07-10T22:17:41Z,0,SUBD!,True
@michaelvinicius4034,2016-07-10T19:34:18Z,0,1 more sub,True
@LHG921,2016-07-10T13:22:06Z,1,Who needs magic when we have Python!,True
@Zwedgy,2016-07-07T16:49:59Z,0,42 Videos of this,True
@jonatanisse6362,2016-07-07T16:22:38Z,0,"Great upload as usual, but how long will this series be approximately?",True
