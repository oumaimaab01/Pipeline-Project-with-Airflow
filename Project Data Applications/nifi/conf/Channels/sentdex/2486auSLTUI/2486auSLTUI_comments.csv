author,updated_at,like_count,text,public
@user-dm9vr6ln3b,2024-03-28T09:13:54Z,0,"i would like to fine-tune the ""GPT2"" for task of ""generate title and description"" based on params. can you help me with that, please?",True
@Smallz2000i,2023-11-19T14:44:00Z,0,"my friend i really hope you reply to my comment, i made everything you did, i subscribed and is following you, i watched most of your videos, i am a self thought, i learnt from android app and YouTube tutorials, i know this comment may get lost in the comments section but i hope that one day you find it and replay. i also downloaded your model, im using lm studio, it says that it does not work, idk mann, ai is freaking cool but hard and easy at the same time",True
@Smallz2000i,2023-11-18T17:59:37Z,1,"from tokenizers import ByteLevelBPETokenizer from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, \     DataCollatorForLanguageModeling from datasets import load_dataset from transformers import Trainer, TrainingArguments  TRAIN_BASE = True  paths = [""output_data.txt""]  if TRAIN_BASE:     tokenizer = ByteLevelBPETokenizer()      tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[         ""<s>"",         ""<pad>"",         ""</s>"",         ""<unk>"",         ""<mask>"",     ])      # Save files to disk     tokenizer.save_model(""tokenizer"")  inp = ""print('making meth!')""  tokenizer = GPT2Tokenizer.from_pretrained('tokenizer')  tokenizer.add_special_tokens({     ""eos_token"": ""<s>"",     ""bos_token"": ""<pad>"",     ""unk_token"": ""</s>"",     ""pad_token"": ""<unk>"",     ""mask_token"": ""<mask>"" })  config = GPT2Config(     vocab_size=tokenizer.vocab_size,     bos_token_id=tokenizer.bos_token_id,     eos_token_id=tokenizer.eos_token_id ) model = GPT2LMHeadModel(config)  data = load_dataset(""text"", data_files=paths)  def encode(lines):     return tokenizer(lines['text'], add_special_tokens=True, truncation=True, max_length=512)  dataset = data['train'].map(encode)  # Move the set_transform line after the dataset is defined dataset.set_transform(encode)  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)  training_args = TrainingArguments(     output_dir=""./GPyT"",     overwrite_output_dir=True,     num_train_epochs=1,     per_gpu_train_batch_size=1,     save_steps=100,     save_total_limit=2,     prediction_loss_only=True, )  trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=dataset, )  trainer.train()  trainer.save_model(""./GPyT"")",True
@SvenTeresniak,2023-01-25T11:11:08Z,0,"dude, please use ssh keys (ssh-agent)! it hurts the eye to type in the pw to connect to a box behind you every time! :D",True
@ekpopromise2682,2022-12-19T18:44:20Z,0,"Hi, thanks for the tutorial, please where can i find the python file for this episode?",True
@atishayjain9053,2022-01-10T17:45:04Z,0,Is there anyplace where I can find all the code of this playlist. I was able to find the model.,True
@user-pz5xl2io5m,2021-08-13T16:48:14Z,0,"Hi! Sentdex. Im following your GPT tutorial and I'm doing part 5. Almost in the mid-video, I'm try to test my code just before loading Nvidia dgx. The problem I'm continuously facing is this error msg 'RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`'. may I ask for your requirements.txt because I did my own research but couldn't find the right versions for each library.  UPDATE1: Also, I'm using RTX 3090 (single). Just found that it's almost impossible to run GPT2 on 3090 alone? Is this true? Am I not be able to implement your tutorial at all? Cheers",True
@shaoxiongyuan2793,2021-07-21T15:05:29Z,0,Is there any way you can make the code and the model available?,True
@bappadityadey9244,2021-06-17T06:41:59Z,0,"Hi Harrison, awesome video. I am trying to reproduce the result as you are showing. But I am stuck as I am getting some ""IndexError: list index out of range"" with the parameters all set as per your video demonstration. Can you please help??  File ""train.py"", line 43, in encode     return(tokenizer(lines['text'], add_special_tokens=True, truncation=True,max_length=512))   File ""/home/tensorbook/miniconda3/envs/pyimg/lib/python3.6/site-packages/transformers/tokenization_utils_base.py"", line 2306, in __call__     **kwargs,   File ""/home/tensorbook/miniconda3/envs/pyimg/lib/python3.6/site-packages/transformers/tokenization_utils_base.py"", line 2491, in batch_encode_plus     **kwargs,   File ""/home/tensorbook/miniconda3/envs/pyimg/lib/python3.6/site-packages/transformers/tokenization_utils.py"", line 563, in _batch_encode_plus     verbose=verbose,   File ""/home/tensorbook/miniconda3/envs/pyimg/lib/python3.6/site-packages/transformers/tokenization_utils.py"", line 626, in _batch_prepare_for_model     return_attention_mask=return_attention_mask,   File ""/home/tensorbook/miniconda3/envs/pyimg/lib/python3.6/site-packages/transformers/tokenization_utils_base.py"", line 2619, in pad     while len(required_input[index]) == 0: IndexError: list index out of range   1%|‚ñç                                | 424/28608 [33:23<36:59:53,  4.73s/it]",True
@anishjain3663,2021-06-15T06:44:47Z,0,Firs of all you are insane and what about just freeze upper layers and unfreeze the rest so atleast model have knowledge about English because python is an English  I just thought,True
@nigmaxus,2021-05-27T15:30:20Z,0,Any chance we could get a requirments.txt file or a listing of the version for the libraries and python you are using?,True
@lukaswerner4390,2021-05-24T04:38:57Z,0,Lol writing your own Kite,True
@nano7586,2021-05-23T08:45:31Z,0,"Lovely video. I've been however thinking lately that it might not be the smartest decision to fully focus on coding for my career. I studied biotech but have been doing programming stuff for more than two years now. We all don't know how long it will take but models such as gpt-2 will basically take over our programming jobs. Biotech on the other hand doesn't have so much automation yet (I work in automation btw). So my thoughts are that right now, people working on ML are needed more than ever, in many fields, but this is a spike and I'm sure in one or more decades programming will be automated much more than now so that people like us aren't that needed anymore. The industrial revolution did bring more jobs in contrary to what was expected but I'm not sure if the AI revolution will be the same.",True
@pugboi8017,2021-05-23T03:58:12Z,1,What‚Äôs edward snowden doing here? Even sounds like him!,True
@chrispap5496,2021-05-23T00:47:32Z,0,Amazing video thanks!,True
@DoomSkullYT,2021-05-22T19:08:39Z,1,"the result was a lot cooler than I expected for the amount of data you used, nice",True
@alicemystery5332,2021-05-22T17:51:57Z,0,finally ordered the book today!,True
@Fjifjdjfkfjdhfijvu2,2021-05-22T17:30:23Z,0,Which ido do you use,True
@usamatahir7091,2021-05-22T16:17:30Z,0,Yoo!!,True
@mahimanzum,2021-05-22T15:57:30Z,1,I think mlm should be false. i mean gpt2 is trained using casual language modeling loss clm. :/ i am confused,True
@georgebassemfouad,2021-05-22T15:28:40Z,0,"I can't understand the max_length in the encode function, and the length of the data of any code must have the same length right? So when you you encode you get the same lengths?",True
@mineburgerl804,2021-05-22T15:26:35Z,0,"hi, nice vids",True
@kunalkushwaha6232,2021-05-22T15:17:02Z,0,too good sir,True
@ryan_chew97,2021-05-22T15:16:57Z,16,Imagine if you could train a neural network to write Python code to built a neural network LOL,True
@muhammetmantu276,2021-05-22T15:15:25Z,0,"Lol, u dont sit or st?",True
@adeeb12321,2021-05-22T15:10:03Z,2,yesssss new video <3,True
@HT79,2021-05-18T14:43:46Z,7,"In your 100GB dataset, did you do any filtering for the version of Python? Py2.7 code might have gotten mixed in while scraping. Won't that have a negative impact on the training?",True
@HT79,2021-05-16T17:52:10Z,21,"Wow Harrison, you're lightning fast with this series! Thanks for the quick updates.",True
@Stinosko,2021-05-16T16:13:03Z,1,Hello üëª,True
@Zebokdk,2021-05-16T15:51:01Z,1,I was just looking for something to watch!,True
