author,updated_at,like_count,text,public
@gregoriussteven6505,2023-08-14T07:59:59Z,0,"You can try this in the newest version: def save_sp500_tickers():   resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')   soup = bs.BeautifulSoup(resp.text, 'lxml')   table = soup.find('table', {'class': 'wikitable sortable'})   tickers = []   for row in table.findAll('tr')[1:]:     ticker = row.findAll('td')[0].text.strip()     tickers.append(ticker)    with open('sp500tickers.pickle', 'wb') as f:     pickle.dump(tickers, f)   print(tickers)    return tickers",True
@Gaat19,2022-08-08T23:10:25Z,3,"if anyone is having issues with characters being tagged at the end of the tickers like a ""\n"", use .strip('\n') after ticker = row.find('td').text",True
@noahbears9620,2021-12-17T21:31:48Z,0,"I get no errors, but no print either. Any ideas?",True
@akarshitmagotra4644,2021-07-27T17:22:04Z,0,@sentdex make a new finance playlist in 2021,True
@dwightledet5857,2021-05-22T06:12:29Z,0,"soup = bs.BeautifulSoup(resp.text, 'lxml') NameError: name 'resp' is not defined",True
@themonsterman229,2021-05-06T16:25:32Z,0,"Im getting the error: in line 12, in save_sp500_ticker     tickers.append(ticker) AttributeError: 'str' object has no attribute 'append'  I have checked and the code is copied word for word?   Any ideas?",True
@thalesgomes8451,2021-05-02T01:45:11Z,4,"Guys, there is a much simpler way of doing this.  import pandas as pd pd.read_html(r'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]  It's way easier to manipulate and grab data with pandas.",True
@AdamS111111111,2021-01-17T02:45:24Z,0,pickle.dump in Google Colab gives an error  `RecursionError: maximum recursion depth exceeded in comparison`. The solution is to change `import pickle` to `import dill as pickle`,True
@nitinrungta3320,2021-01-16T21:57:30Z,0,what if I already had data in excel file. how do I know if my format is correct,True
@jamiedunleavy2787,2020-11-09T15:06:54Z,0,"The below does not work. What is wrong? ********** Error code: Traceback (most recent call last):   File ""/Users/Jamie/Documents/SP500 Tickers.py"", line 25, in <module>     save_sp500_tickers()   File ""/Users/Jamie/Documents/SP500 Tickers.py"", line 14, in save_sp500_tickers     for row in table.findALL('tr')[1:]: TypeError: 'NoneType' object is not callable **********  import bs4 as bs import datetime as dt import os import pandas as pd import pandas_datareader.data as web import pickle import requests  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'id': 'constituents'})     tickers = []     for row in table.findALL('tr')[1:]:         ticker = row.find('td').text         tickers.append(ticker)      with open(""sp500tickers.pickle"",""wb"") as f:         pickle.dump(tickers, f)      print(tickers)      return tickers  save_sp500_tickers()",True
@romandavydov3888,2020-11-04T12:47:15Z,0,"someone can post a working code for 2020. I fixed it findAll on find, but it's still a bug",True
@vpee,2020-10-21T21:52:05Z,0,how about: import pandas as pd table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') df = table[0]........,True
@joshmatthews2031,2020-10-07T13:17:00Z,0,"can anyone tell me why ticker = row.findAll('td')[0].text is giving me a ""list index out of range"" error?",True
@davidnamai5077,2020-07-16T21:00:32Z,0,The code is returning None when it is run don't know where the problem is since everything is fine,True
@yogeshdubey5142,2020-06-20T15:52:11Z,0,'NoneType' object has no attribute 'findAll' getting this error can someone help,True
@vitordinis,2020-06-18T02:48:26Z,0,"2020 update: if you are having problems with ""from matplotlib.finance import candlestick_ohlc, try this: old API availability With this new mplfinance package installed, in addition to the new API, users can still access the old API. The old API may be removed some day, but for the foreseeable future we will keep it ... at least until we are very confident that users of the old API can accomplish the same things with the new API. To access the old API with the new mplfinance package installed, change the old import statments from:     from mpl_finance import <method> to:     from mplfinance.original_flavor import <method> where <method> indicates the method you want to import, for example:     from mplfinance.original_flavor import candlestick_ohlc source: https://github.com/matplotlib/mplfinance",True
@vulcanorosso3112,2020-06-13T20:49:59Z,6,Hi Great Job there: I am following your video carefully and wit great pleasure to learn form you. Just to comment two things: #1. I do not know for te others but it happensthat we need to strip the ticker got form the Beautifoulsoup process otherwise the pickle file has the '\n' new line in the Tickers list. This is what I did : tickers.append(ticker.strip()) #2: I also encounter an issue for stocks having '.' in the ticker name. Solution will be to save the failing tickers in a list and print that list later to see what really happened. At the end was not able to download 6 tickers,True
@KyleGW,2020-05-31T21:47:36Z,0,"I am using google colab, when i call print(tickers), it prints both 1x500 matrix and 500x1 matrix. Not really sure why",True
@skypickle29,2020-04-23T21:42:07Z,6,"why use bs4 when it can all be done like so:   url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies' stocks_df = pd.read_html(url, header=0)[0] stocks_df.head()    ?",True
@mohamedhassan-sr8ue,2020-04-18T17:25:02Z,0,in not getting any comapy. my result is just blank,True
@mohamedhassan-sr8ue,2020-04-18T17:20:01Z,2,im not getting ant error or any result with it. my shell just comes out blank.,True
@VS-uh5yq,2020-04-13T16:53:35Z,0,"Hello Sentdex, i got an IndexError: list index out of range :  ticker = row.find_all(""tr"")[0].text   Could you please help me?. The code looks exactly the same",True
@andrzejkowalczyk4064,2020-03-21T22:54:20Z,0,"Hi, I am having a problem because there are two tables there and I am getting data from the second table as well. Any idea of how can I narrow down the data to receive only tickers?",True
@samuelhasudungan,2020-02-18T03:36:07Z,1,"Thanks sentdex for the tutorial, helps me a lot! But, I have some problems. I want to get the Indonesian and Malaysian stock list, and later I will use them to get all company pricing data. In Indonesian an Malaysian stock list, they have some code after the company code. For example, ABCD.JK for Indonesian stock or ABCD.KL for Malaysian stock. My question is, how do I add the code (.JK) and (.KL) to all of the company code that I already have had? Thank you :)",True
@yinzhang7316,2020-01-17T21:54:37Z,0,"Hi Thanks for the great video! I'm trying to scrape another page, with two tables of the same class and am only able to pick up data from the first table, how can I specify which table I want to scrape from if they have the same class? thanks!",True
@EverythingTechWithMustafa,2020-01-17T20:13:53Z,0,Need the Source code,True
@eli123ry,2020-01-14T03:21:22Z,16,we need this kind of content in 2020. just found this guy @sentdex. thank you.,True
@guillecobo_,2020-01-05T18:13:42Z,2,"import bs4 as bs import requests # pickle library allows us to save any python object import pickle   def save_sp500_tickers():     rsp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(rsp.text, 'html.parser')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)      print(tickers)     return tickers   save_sp500_tickers()",True
@fedorezeev9923,2020-01-05T11:57:45Z,96,"I have two improvements to this code   1. Now, in 2020, this table on wikipedia have id, so we can find it not by class, but by id, which is more reliable. So instead of table = soup.find('table', {'class': 'wikitable sortable''})  you can write table = soup.find('table', {'id': 'constituents'})   2. If you want to get the very first object in list with BeautifulSoup, you can use 'find' method instead of 'findAll[0]' (exactly as you search the table three lines before) So instead of  ticker = row.findAll('td')[0].text you can write ticker = row.find('td').text which is prettier on my taste   Anyway, thank you for such a beautiful lessons",True
@ranjithkumar2521,2019-12-03T17:43:35Z,0,"Hi want to plot chart in amibroker using python , can you tell me how to do that ?",True
@ajromero87,2019-11-24T16:05:30Z,1,"hi, running the code successfully but there is nothing printed. I am using sublime text and below code: import bs4 as bs import pickle import requests  def save_sp500_tickers():  resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')  soup = bs.BeautifulSoup(resp.text, ""lxml"")  table = soup.find('table', {'class':'wikitable sortable'})  tickers = []  for row in table.findAll('tr')[1:]:   # dtr=tablerow esde fila uno en adelante, para no agarrar titulos   ticker = row.findAll('td')[0].text   # td table data la primer columna por eso 0   tickers.append(ticker)    with open(""sp500tickers.pickle"",""wb"") as f:    pickle.fump(tickers, f)      return tickers     save_sp500_tickers()         print(tickers)   any help?",True
@9BoStOnGeOrGe,2019-11-10T22:18:14Z,1,i'm getting \n after each ticker...for ex. ''AAPL\n'  why is that likely?,True
@jamesdunn6968,2019-08-18T02:51:21Z,0,"I'm getting an error message 'tickers is not defined'. Does anybody have  a solution?    def save_sp500_tickers():     resp = requests.get(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")     soup = bs.BeautifulSoup(resp.txt)     table = soup.find('table', {'class,':'wikitable sortable'})     tickers = []     for row in table.findAll ('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker) with open(""sp500tickers.pickle"", ""wb"") as f:     pickle.dump(tickers, f)   NameError                                 Traceback (most recent call last) <ipython-input-13-8e5142848e40> in <module>       8         tickers.append(ticker)       9 with open(""sp500tickers.pickle"", ""wb"") as f: ---> 10     pickle.dump(tickers, f)  NameError: name 'tickers' is not defined   Any help would be really appreciated!",True
@jeffreymcneary3055,2019-08-14T22:17:29Z,24,every single ticker of mine is followed by \n. any idea why?,True
@nicklewis5223,2019-07-07T14:42:22Z,0,"I am having trouble getting my script to run and I'm kind of at the end of my trouble-shooting know-how. Can anyone help me out?   To start, I am using Atom, saving down the file and running it thru powershell.   python sd9.py 3M Company Traceback (most recent call last):   File ""C:\Users\X\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 133, in _read_one_data     data = j['context']['dispatcher']['stores']['HistoricalPriceStore'] KeyError: 'HistoricalPriceStore'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""sd9.py"", line 50, in <module>     get_data_from_yahoo()   File ""sd9.py"", line 45, in get_data_from_yahoo     df = web.DataReader(ticker, 'yahoo', start, end)   File ""C:\Users\X\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\data.py"", line 310, in DataReader     session=session).read()   File ""C:\Users\X\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Users\X\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 136, in _read_one_data     raise RemoteDataError(msg.format(symbol, self.__class__.__name__)) pandas_datareader._utils.RemoteDataError: No data fetched for symbol 3M Company using YahooDailyReader    Did anyone else run into this issue? I haven't had any problems up until this module.",True
@fsrdiaz,2019-07-07T02:49:39Z,0,"This first part of my code ending in save_sp500_tickers() returns:  TypeError: object of type 'module' has no len()  I can't seem to find where the error is, could someone give some insight? Thanks!",True
@Somethinggolfer123,2019-07-05T12:58:05Z,0,"So I was able to make this work, but in my test output, each ticker has the new-line character after it.  (Example, [...'GE\n', 'GIS\n', 'GM\n'...]  How does one get rid of that character?  Thank you very much.",True
@gnkarn00,2019-06-21T12:16:14Z,0,"hi , thanks for your videos , This one is the first i have a problem that  couldnt solve yet : the error is AttributeError: module 'bs4' has no attribute 'BeatifulSoup' , and have tested everything i red without luck . im using anaconda on a mac . any comment is appreciated, thanks again",True
@jake_steele,2019-06-19T19:48:32Z,3,"Hi all, there is an easier way to get the tickers at the current time, which he covers in another video. This example brings them in as a dataframe. You can pickle them the same way if select the single column. Not sure if this method existed at the time of this videos creation.   import pandas as pd def save_sp500_tickers():     df_companies = pd.read_html('url')     tickers = df_companies[0]['Symbol']     with open (""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers",True
@alancedeno9452,2019-05-17T21:47:21Z,0,I got a print stament saying [u’MMM\n’] why? I did as you did except i had to include the id=constituents,True
@ryanflanigan1153,2019-05-16T19:20:30Z,1,why do I get a NameError that 'tickers' is not defined?,True
@epicmonckey25001,2019-04-25T00:29:30Z,0,"I have a question. After running the program in PyCharm, I don't get the same output as you. I don't get the array of the tickers. All I get is the response that it has finished code 0",True
@mariusklbo7187,2019-04-11T17:41:40Z,0,"If you have issues with the tables make sure you have the right class. Ex. mine  was ""sortable wikitable"" and not ""wikitable sortable""",True
@martiniyongo7576,2019-04-07T15:37:12Z,2,"Thank you for this video man. I receibed on error : ""RemoteDataError: No data fetched for symbol 3M Company using YahooDailyReader"" could you help me?",True
@iClaWnN,2019-03-14T07:36:17Z,0,sentdex weeb confirmed. You're very saucy guy,True
@strawtak3265,2019-03-04T12:53:26Z,0,"it warns me to pass the additional argument 'features=""lxml"" ' to beautifulsoup constructor, which line should I type in?and it shows attributeError : 'tuple' object has no attribute 'find' at the lastwhen I fixed it, it shows the whole name of the stock like""3M Company"", ""Abbott Laboratories"".......",True
@camillezajac1233,2019-01-13T09:06:45Z,3,What did I do wrong if all the tickers print with a u in front of them: u'AMD' for example.?,True
@LoganCTanner,2018-12-29T04:49:26Z,2,"I'm getting a strange hiccup at BRK.B, must be buffet bots harassing me. The error comes from pandas and the last line is KeyError: 'Date', any thoughts?",True
@jroth8607,2018-12-25T13:59:17Z,0,Is it possible to create the pickle file from a local csv file as apposed to a web address such as wiki sp500 page?,True
@kennethwong580,2018-12-14T13:17:12Z,0,stuck at evrg ! its driving me crazy,True
@stephentomaszewski8501,2018-12-07T03:10:19Z,0,what is the advantage of saving the list in a pickle vs a csv file?,True
@enersha,2018-11-22T03:25:51Z,0,im getting pretty lost here. is it actually possible to make money off this without any qualifications?,True
@Oyarsa797,2018-11-07T03:50:40Z,0,"Has anyone tried to run this recently?  I was able to run my code previously without issue but now I get a ""KeyError: 'Date'"" when it hits the stock 'LIN'.   Any ideas why it's causing it to crash?",True
@jej5557,2018-10-26T06:44:25Z,0,"Hi, i keep getting ""  File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'"" on ticker BRK.B.  Can anyone help me to resolved the error.",True
@anmolchaudhary3893,2018-10-25T13:32:08Z,0,why did you not use a similar method as in your pandas tutorial ?  https://www.youtube.com/watch?v=WkIW0YLoQEE&index=7&list=PLQVvvaa0QuDc-3szzjeP6N6b0aDrrKyL-,True
@adamhoar6802,2018-10-05T23:15:17Z,1,Just wanted to say thanks for putting these tutorials out there! Finding this channel has been one of the best thing to happen to my python learning experience. Way better to learn through actually trying things. Also helped me to 'waste' a lot of time finding my own damn typo when indexing through the table,True
@jems5000,2018-08-28T09:33:59Z,2,"For table specific data, another way you can go about scraping data without using BS is using the inbuilt pandas function read_html, which will read all tables within the webpage  essentially the following code:  import pandas as pd table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') tables = tables[0]0][1:] # first [0] is to choose the main table of which the data is located, second [0] is just choosing the column tickers= [] for i in tables:     tickers.append(i)  with open('sp500pickle','wb') as f:     pickle.dump(tickers,f)  return ticker_list  Hope this helps!",True
@PrakashPraaku,2018-08-19T09:44:37Z,1,Thank you again for this wonderful video sir! I became your fan sir. Your work is truly helpful for the needy folks like me.,True
@asd12304,2018-07-12T15:48:00Z,0,"Hi guys, does anyone know how to navigate between wikipedia sortable tables and say select the 2nd table (in a page with two) and extract data from there? Cheers!",True
@blacksiddis,2018-06-24T19:12:48Z,0,You are a fucking beast.,True
@Golden_B1,2018-04-30T03:44:09Z,0,new to programming  I get this error AttributeError: 'NoneType' object has no attribute 'findAll'any help pls,True
@MrAden1997,2018-04-04T22:33:32Z,0,"Hi! im getting stuck at this point, my code is;   The error message is AttributeError: module 'bs4' has no attribute 'Beautifulsoup'.  import bs4 as bs import pickle import requests  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.Beautifulsoup(resp.text)     table = soup.find('table',{'class':'wikitable sortable'})     tickers = []     for row in table.findALL('tr')[1:]:         ticker = row.findALL('td')[0].text         tickers.aappend(ticker)      with open('sp500tickers.pickle','wb') as f:         pickle.dump(tickers, f)      print(save_sp500_tickers())      return tickers save_sp500_tickers()",True
@ChEkOv,2018-03-31T10:52:02Z,0,"Nor google noy yahoo wants to give us free data now(The Google Finance API has not been stable since late 2017. Requests seem to fail at random. Failure is especially common when bulk downloading.) In order to continue nicely this tutorial(like I think we all did in the others) , could you provide us with another api & wrapping method OR maybe just upload  the files iinto a zip to your site? Thanks again for your work Harrison (or any other helpful person out there).",True
@BadrAzo,2018-02-27T17:31:59Z,0,"Hi,  Everytime I try to run the code I get the following error: ""SyntaxError: invalid syntax"" on line 2 for the word ""import"".  This is the code (copy-paste): import bs4 as bs from import pickle import requests  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text)     table = soup.find('table', {'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker)      with open(""sp500tickers.pickle"",""wb"") as f:         pickle.dump(tickers,f)      print(tickers)      return tickers  save_sp500_tickers()",True
@Lu_Ca,2018-02-20T19:22:44Z,0,"Hello, can you provide the code, it doesen't work anymore! ""tickers not defined""  please help",True
@gautam.UnfoldingCrypto,2018-02-20T03:46:58Z,1,"Hey Sentdex. Lovely work : I am having output with a list of only dots ([...],[...],[...],[...],[...],[...],[...],[...],)--> something like this. I am quite new to coding and don't know what to do",True
@nicolorisciotti8608,2018-02-12T11:16:52Z,3,Is beatifulsoup4 supported on the new  Python version 3.6.4? Cause I installed the module but I keep getting the error that there is no module called bs4 . Thanks,True
@jacobwilkins7728,2018-02-09T16:28:08Z,0,"This is a very inspiring series, thanks sentdex!!",True
@theJESUSofcallofduty,2018-02-09T02:28:12Z,0,"when i do this in spyder ide, it runs but pulls up nothing, none of the tickers show, is there anything i can do about this?",True
@mathewjohnson7623,2017-12-21T08:26:32Z,1,I am getting the following AttributeError: 'Response' object has no attribute 'txt' and can't seem to find a solution online anywhere. I have tried upgrading my requests package which i can now confirm is at Version: 2.18.4 but that hasn't solved my problem. Any suggestions?,True
@hamildub,2017-11-03T17:08:05Z,1,"I had error ""name error 'resp' not defined"". It appears that it has now been changed to response",True
@nicolalosa7580,2017-10-30T09:53:53Z,0,"Hello Harrison, let's say that I wanna download just the  Information Technology constituents of the SP500: how do I need to modify the code?",True
@indianchange,2017-10-22T13:08:58Z,0,"using python 3.6.3 programm rus without error and result :( import bs4 as bs import pickle import requests  def save_sp500_tickets():     resp = request.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text)     table = soup.find('table',{'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker)      with open(""sp500tickers.pickel"",""wb"") as f:         pickle.dump(tickers,f)              print(tickers)               return tickers          save_sp500_tickers()",True
@NewGameMultimedia,2017-10-14T03:03:45Z,0,@sentdex Sentdex! THANK YOU! You're giving such wonderful information God bless you! I am curious! If I use yahoo will it constantly update in real time when the market is going? Thanks!,True
@taylornichols1756,2017-10-08T23:57:37Z,0,Hey does anyone know where I can find the pickle library?,True
@daitavan297,2017-08-01T09:20:13Z,0,"One thing, I would like to compliment. You have to upgrade the html5lib, beautifulsoup4 using - pip install --upgrade <package> in order to execute the scripts. Good luck",True
@simonchan2394,2017-07-23T17:14:23Z,0,"Hello. I ran your code but I got these types of tickers returned back: u'MMM', u'ABT',u'ABBV'. I can see the ticker but it has a U in front of it, how do I remove the U?",True
@hans6973,2017-07-05T20:30:09Z,2,"Another way to get the s&p 500 list: df = pd.read_html(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")[0] df.columns = df.ix[0] df.drop(df.index[0], inplace=True) tickers = df['Ticker symbol'].tolist()",True
@sylvainli1928,2017-05-24T14:08:09Z,2,does anyone know how to fix : bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?  I am on macOS,True
@EvilSpeculator,2017-05-03T22:45:25Z,15,"Hey guys - I received one error: ""TypeError: 'NoneType' object is not callable"" Solution: ticker = row.find_all('td')[0].text",True
@man.h,2017-04-16T10:46:08Z,3,"im just getting in my list ['MMM'], know anyone the solution ? ( Beginner with python)",True
@4052,2017-02-10T05:47:23Z,9,"Thanks sentdex for your nice work! In my case Yahoo wasn't able to find alll the tickers. I had to replace the dots within the tickers, now it's running perfectly! Cheers ticker = row.findAll('td')[0].text.replace('.','-')",True
@bquinn1992,2017-02-08T02:05:21Z,3,SNP 500 csv here https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv  :),True
@sef83,2017-02-07T16:59:10Z,0,"thanks for the videos :) really helpful  ""https://www.kap.org.tr/en/bist-sirketler"" how can we get the tickers from this website? i could not see a table class in the page source?",True
@NoBSFX,2017-01-20T15:53:58Z,0,"hey @sentdex , will you cover backtesting ? in the near future ?",True
@MentiSnap,2017-01-18T15:51:26Z,1,"Why don't you use the ""inspect"" and console options of the DevTools to see the code of the websites?",True
@nitishchauhan7377,2017-01-18T10:35:57Z,0,Hi! Can you make a tutorial series of web scraping using scrapy ?,True
@joshwilson7470,2017-01-18T05:45:00Z,2,"Wouldn't it be more accurate to have the input come from the live actual stock market values somehow, as opposed to relying on Wikipedia?",True
@russici,2017-01-18T01:12:36Z,8,"Hello Harrison, thanks for the video. Another way to get the table is to use pandas: 1) df = pd.read_html(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")[0]   2) df.columns = df.ix[0] 3) df.drop(df.index[0], inplace=True)",True
@porlando12,2017-01-18T01:06:50Z,1,Really great tutorial! BeautifulSoup is a lot more fun when you are scraping important data!,True
@kyleconn4679,2017-01-17T22:57:11Z,0,http://www.nasdaq.com/screening/company-list.aspx  The link takes you to the NASDAQ website. There you can get CSV data for just about any company for free. Just click the link to download the CSV that's next to the exchange name..   I used these CSV files to compile my company list. They have much more than just the ticker data too in the CSV's.,True
@VishalSingh-ly9vh,2017-01-17T18:40:27Z,0,Thanks so much sir...your videos have been extremely helpful...love from India <3,True
