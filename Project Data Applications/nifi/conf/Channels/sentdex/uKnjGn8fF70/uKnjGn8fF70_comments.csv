author,updated_at,like_count,text,public
@pedrowangler97,2024-01-17T19:44:23Z,0,"You want to have the rendering done separately and not within the step function. When training the model, the number of epochs( steps performed) needed to achieve good accuracy is usually very high (10K+ depending on the task complexity), so the number of computations in each step must be kept to a minimum. Rendering is only really used for visualization purposes and has no impact on the agent or its environment, so rendering within the step function is a big no-no!",True
@JPy90,2024-01-08T14:31:12Z,0,"First comment, dude your content it's awesome just like that, don't use copilot, it only follow the rithm of your head, but for me the progression of the coding it's exelent!",True
@phantomBlurrrrr,2024-01-06T22:13:15Z,0,"By using the past actions, isn't that making this implementation stop being markovian? Since you are using the past...?",True
@ApexArtistX,2023-11-26T20:47:52Z,0,this isnt a real custom environment. Do a web browser game environment,True
@michpo1445,2023-11-14T20:47:36Z,0,"This no longer works, since gym was changed to v0.26. However, !pip install gym==0.21 command is broken",True
@user-xv5sp9lq5r,2023-10-30T17:08:51Z,0,"Hi i have one error when execute the code, the reset funcion in PPO mlpPolicies need a seed argument, this is the error: TypeError: SnakeEnv.reset() got an unexpected keyword argument 'seed' i tried to set in the  definition of the reset function a default argument seed=None like this: def reset(self, seed=None):     ....  but now i have this value error: ValueError: too many values to unpack (expected 2) can you help me? thanks for the video!",True
@user-kn8tp7jo3c,2023-10-29T19:00:54Z,0,"this tutorial series was much needed for me. Thank you so much!in you're script you define self.action_space and self.observation_space but i don't see them used anywhere else. this confuses me. Do you really have to use the gym spaces to feed the algorithm? because i have a lot of trouble with those things... ah ok i found the answer(copied from forum): The observation_space defines the structure of the observations your environment will be returning. Learning agents usually need to know this before they start running, in order to set up the policy function. Some general-purpose learning agents can handle a wide range of observation types: Discrete, Box, or pixels (which is usually a Box(0, 255, [height, width, 3]) for RGB pixels).",True
@paulokafor3781,2023-10-20T16:56:33Z,0,"hi sentdex  i tried making  a custom env  but ran into a problem not being able to use image as observation, if you have any info on how or this is done please that would be great. love your videos by the way you single handily taught me ml.",True
@Hazit90,2023-08-25T05:35:29Z,0,"""There you go you cell phone watchers"" üòÇ",True
@peterhpchen,2023-08-11T14:30:34Z,0,Remove OpenAI and Create custom environment is most difficult for me. Thanks a lot,True
@omidalekasir4736,2023-04-27T06:22:47Z,0,That laugh could be a meme 3:08,True
@Alex-Marshall,2023-04-04T11:40:19Z,2,"I would like to remind you that there is no rationality check of the current action based on the pre_action in the env code: for example, when the snake is walking to the right, the current action is not allowed to go to the left, but can only go to the right, up and down. If this check is not performed, then randomly sampled actions will cause the snake to suddenly go from the right to the left, then collide with itself, then done = True, and return reward = -10, which may cause the model to often use -10 reward exit, may be detrimental to learning? I don't know, but if the learning is long enough, the model should come to this conclusion by itself: the action opposite to the current direction of motion cannot be performedüòÄüòÄüòÄüòÄ",True
@phono1231,2023-02-01T13:27:26Z,0,"can you guys show agent that actually lratned something? I mean agent that really play game and sent go to wall? actually real case must be like I have some working strategy, which I provide to agent and gym learn only improvements for my strategy. is such simple example available anywhere?",True
@udik1,2023-02-01T02:48:32Z,3,"Getting the following error for checkenv: AssertionError: The observation returned by the `reset()` method does not match the given observation space  Solved using the following -         #self.observation_space = spaces.Box(low=-500, high=500, 	#									shape=(5+SNAKE_LEN_GOAL,), dtype=np.float32)                                              	self.observation_space = spaces.Box(low=-500, high=500, 										shape=(5+SNAKE_LEN_GOAL,), dtype=np.float64)",True
@samnman1,2023-01-17T21:13:46Z,1,Thanks for the videos! It‚Äôs super helpful to see someone run through the entire process when starting out.   What are your thoughts on imitation learning? Is there a similar library that you could demo?,True
@muhammadsohailnisar6600,2023-01-13T23:00:10Z,4,"I am getting an error even though I copied it from the written tutorial. The error mentions, ""AssertionError: The observation returned by the `reset()` method does not match the given observation space"". how to resolve this ?",True
@HaikuGuy,2023-01-07T15:49:39Z,0,Awesome content as always. You are the greatest tech teacher!,True
@niklasdamm6900,2022-12-20T14:35:27Z,0,20.12.22  16:00,True
@vitaly1085,2022-10-28T14:22:00Z,0,does this men know that he was recorded?)))),True
@galenholland3872,2022-09-13T21:10:35Z,0,"every time i try to load a trained snake env model from the .zip, i get a NotImplementedError for the env.render() line",True
@markcai8130,2022-08-30T08:02:24Z,2,copy and past in win10. Check with error: The observation returned by the `reset()` method does not match the given observation space,True
@vaizerdgrey,2022-07-26T05:01:31Z,0,Can you make a video on custom policy?,True
@Nerdimo,2022-05-24T19:12:30Z,6,I came back and I believe I found an issue. When self.img is set to ‚Äúuint8‚Äù it causes the arrays to be cast as that (I think) instead of np.float32. When I set every self.img as float32 and the observation a float 32 using astype() it ran without issues. I don‚Äôt know if it‚Äôs an OS thing or how my  numpy array module is set but it it could cause issues,True
@iceman1125,2022-05-18T17:16:01Z,0,"could you make a simpler custom environment version, this snake game is too confusing as a start.",True
@oysteinmb1,2022-05-03T23:32:00Z,1,"Great video! I have one question i would greatly appreciate if anyone could help me with.  If my observation space is a 5*6 grid (list in list; [[None] * 6] * 5) and a int, how would i make the spaces.Box? And how should i best structure the grid, and how should i return the observation?  Thanks for any help!",True
@jonclement,2022-04-25T01:11:36Z,1,Nice.  I'm surprised you don't use Vim.  I find any mouse movements to slow my fingers down.,True
@peschebichsu,2022-03-27T20:27:55Z,0,"7:49 why did noone comment :o (I don't know it). Btw, using Ctrl d (select next occurrence) and multi curses would make the self. much more convenient :D",True
@jonasls,2022-03-09T18:43:08Z,1,"I really wouldn't mind GitHub Copilot, although I think it might make you skip/forget to mention some things you wouldn't otherwise. But again, would prefer it speed wise. (bias, Copilot user here üòõ)",True
@adamjones6916,2022-03-03T01:06:02Z,0,"Haha, I spotted the observation bug as soon as you copied it and thought it was going to be a nightmare to catch.",True
@hendrixkid2362,2022-02-25T02:53:58Z,0,"This is super informative! Did you study/plan a lot prior to this video or is this you going off the cuff? Either way, impressive work.",True
@azleezlee,2022-02-15T20:32:17Z,0,"Anyone tried Stable Baseline 3 with a NES game (e.g., Mario Bros.)? In comparison to prior Stable Baselines, it appears that scenario files are no longer operating.",True
@justinmoore1136,2022-02-15T19:22:54Z,0,"Please, feel free to use copilot. It showcases a cool tool and the content is pretty much exactly the same! Love this channel <3",True
@JamesWattMusic,2022-02-12T13:51:39Z,0,Is it worth trying to use RL to see if it can make a supervised learning regression model better? or does that not make sense,True
@nielsencs,2022-02-10T08:32:18Z,0,With the indentation can't you just tell Python to use 2 spaces?,True
@enriquesnetwork,2022-02-08T12:30:19Z,0,This is just awesome man! Thank u for tutorial!!,True
@Magnathia,2022-02-08T01:54:52Z,0,"Personally, especially for people learning how to use the language and tools, I strongly discourage the use of Co-Pilot. The reason behind this is that you become dependent on Co-Pilot for the answers and autocorrect, and many people just picking up the language can't write a single line of code without assistance. I do have real world  experience within the last 3-6 months to draw on to form this conclusion. Normally when I mentor someone, we start with IDLE, but that would be silly with the amount of content that you have put together. A full IDE such as VS Code or Pycharm is great for speed and line by line explanation and if you were to stop every few lines and continue to explain the use of the line, then you would have taken my argument away from me. In conclusion, Co-Pilot is a great tool to add to your tool box / dev environment if you are iterating over things very quickly, but is a very poor teaching / tutorial tool.",True
@raven9057,2022-02-08T00:09:05Z,0,"Is Harrison an avid iRacer?  Cheers for the videos man, really enjoying this series. üëç",True
@randywelt8210,2022-02-07T20:06:55Z,0,This is a great episode! How u develop ur own observation concept is super interesting. I guess the reward concept in the next episode will become even better!!,True
@godwyllaikins2075,2022-02-07T18:23:20Z,4,I love these videos on reinforcement learning. GIVE ME MORE!!!!,True
@TECHN01200,2022-02-07T17:49:14Z,4,"If you want to make copying lines easier, I have mapped the visual studio keybinds in vscode/vscodium to cut down on this. Also Ctrl+Alt+up/down enables multi-edit allowing you to edit multiple lines, escape to exit.",True
@TECHN01200,2022-02-07T17:35:26Z,3,We need a code bullet colab with this.,True
@akarshrastogi3682,2022-02-07T16:18:33Z,0,Amazing series,True
@georgebassemfouad,2022-02-07T15:52:33Z,0,Finally you are the best,True
@Brysett,2022-02-07T15:49:32Z,0,Quick guess about making the model learn better: Small reward for moving towards the apple (and possibly a small punishment for moving away).,True
@Giant-Axe,2022-02-07T15:34:06Z,4,"when is part 10 of the ""Neural Networks from Scratch in Python"" series?",True
@laurence1320,2022-02-07T15:19:55Z,0,"Hello, I'm very new to Stable Baseline 3 and was wondering if it would be possible to use this alongside AirSim within Unreal Engine?",True
@vatsalshukla5434,2022-02-07T15:16:09Z,0,hey-o! Thanks!,True
@JazevoAudiosurf,2022-02-07T15:15:27Z,1,2:10 google is listening,True
@Stinosko,2022-02-07T15:05:28Z,1,Hello again üëãüëãüëãüëã,True
@esra_erimez,2022-02-07T15:03:33Z,31,This video on reinforcement learning has reinformed my learning of reinforcement learning. Thank you.,True
@tljstewart,2022-02-07T15:03:16Z,1,"Heck ya, been looking forward to this! Thanks big man!",True
@martis9453,2022-02-07T15:01:56Z,0,2nd yay,True
@judedavis92,2022-02-07T15:00:42Z,2,"Hi, when‚Äôs nnfs coming back?",True
