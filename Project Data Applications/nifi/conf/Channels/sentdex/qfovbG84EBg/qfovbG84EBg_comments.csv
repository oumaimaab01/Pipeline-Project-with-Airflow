author,updated_at,like_count,text,public
@user-tc4kr9nh9b,2024-03-27T21:44:09Z,0,That helped a lot!,True
@homataha5626,2024-02-19T03:46:22Z,0,Can it be done with gym now?,True
@JPy90,2024-01-08T21:23:43Z,0,"37:06 oh! I feel so represented. Thank you for your great work making content, I had a great time!",True
@raydelmiranda1548,2023-12-08T19:15:24Z,0,"This is a handy tutorial!!! Thank you very much for your job! 4 years later it might be an opportunity to revisit this with the new keras/tf and of course with yourself more 4 years experienced. And yes, it would be great to actually see the implementation of the environment :D",True
@pablogomezgracia7058,2023-11-06T10:59:20Z,0,"Hello!! Thanks for the tutorial, it was really helpful and easy to follow!! I have one question regarding normalize/scale data...I want to normalize/scale the input of the neural network (the agent model), but usually in order to normalize/scale, you do it over a set of training data previously obtained. In my case, I don't have a previous training data set, this data is dinamycally obtained as the agent makes actions and moves through diferent states. Is there a way to normalize/scale data 'dinamically'?  without having a training data set? Should I generate a dataset while training the model and the normalize the data in future iterations?",True
@Sickkkkiddddd,2023-07-11T14:12:27Z,0,"In some textbooks, the deque data type in referred to as a ""replay buffer""",True
@Nova-Rift,2022-10-29T22:31:09Z,0,Can you do this in Pytorch? Just a simple request :),True
@lucasmadrassi1295,2022-10-13T02:41:03Z,0,Nice vid. is it posible to configure the agent in order it to move on each pixel only once?,True
@zhongzhongclock,2022-03-14T17:16:47Z,0,"Python 3.7.12 (default, Jan 15 2022, 18:42:10) >>> import keras  Using TensorFlow backend. >>> print(tf.__version__) 1.15.2 >>> print(keras.__version__) 2.3.1  The above version could work correctly with those codes.",True
@zhongzhongclock,2022-03-08T15:32:40Z,0,"Oh, my god, I'm using TF2.8 to try your program, found many many API compatible issues on tensorflow. After I go through the comments, and found many others also report similar API compatible issues on TF2.0 TF2.2, I feel a little better.",True
@utilelearning5349,2022-02-21T01:15:43Z,1,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@utilelearning5349,2022-02-21T01:15:35Z,1,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@utilelearning5349,2022-02-21T01:15:30Z,0,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@utilelearning5349,2022-02-21T01:15:25Z,0,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@utilelearning5349,2022-02-21T01:15:20Z,0,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@utilelearning5349,2022-02-21T01:15:14Z,0,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@utilelearning5349,2022-02-21T01:15:09Z,0,"Hello, I hope you are well. Please help me . In which part of the code should we increase the number of ENEMYs?",True
@sergiveramartinez2685,2022-02-05T00:40:32Z,1,"Hello guys! Does anybody know why my computer is taking ages to run even 5000 episodes? I have an iMac from 2014. It has to do with the GPU or CPU? Thanks in advance, I need it for a project!!",True
@chaotang2166,2022-01-03T02:43:33Z,0,"Thanks for the great tutorial and I have learned a lot from it! One question: why there is no learning rate in DQN? In Q learning, learning rate weights how much the model trusts the  future information. But in DQN, new q totally depends on  max future q, so I didn't see any learning rate. I'm new to RL, so the question might sound stupid : )",True
@sblancozen,2021-11-03T10:25:20Z,1,"First of all thanks for the tutorials sentdex!!! They are amazing!!!! Just trying to implement DQN, but wondering why it is necesary to use ""terminal_state"" in ""train(self, terminal_state, step)"", if afterwards we do a random sample selection for filling the minibatch, so the state that gave the terminal_step=TRUE (or FALSE), it has no more chances to be used in the minibatch that any other one for fitting the model? Also the ""step"" parameter also it is not used inside ""train"" function,, right? Thanks in advance!!! And good research to all!",True
@eastwoodsamuel4,2021-10-16T07:44:33Z,0,"Was anyone able to train their model? I'm using RTX3060, it is taking nearly 1.5 hr.",True
@tzookil,2021-09-29T18:13:59Z,1,"In order to run on latest tensorflow the following changes need to be done: 1. The imports: import numpy as np import keras.backend as backend #(This actually gives a warning that it is not in use) from keras.models import Sequential from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten from tensorflow.keras.optimizers import Adam from keras.callbacks import TensorBoard import tensorflow as tf from collections import deque import time import random from tqdm import tqdm import os from PIL import Image import cv2  2. Line 196: tf.random.set_seed(1)  instead of   3. on ModifiedTensorBoard add the following:     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.step = 1         self.writer = tf.summary.create_file_writer(self.log_dir)         self._log_write_dir = self.log_dir         def set_model(self, model):         self.model = model          self._train_dir = os.path.join(self._log_write_dir, 'train')         self._train_step = self.model._train_counter              self._val_dir = os.path.join(self._log_write_dir, 'validation')         self._val_step = self.model._test_counter              self._should_write_train_graph = False            def _write_logs(self, logs, index):         with self.writer.as_default():             for name, value in logs.items():                 tf.summary.scalar(name, value, step=index)                 self.step += 1                 self.writer.flush()  I think this is pretty much it. for each error I got immediately the right solution on stackoverflow.",True
@pisoiorfan,2021-09-27T08:58:42Z,0,Should have try NEAT against the simpler environment (distances not image),True
@satyamedh,2021-09-17T10:09:31Z,0,37:11  no I'm following along working on mineRL,True
@cromi4194,2021-08-06T20:18:23Z,0,"I came here because I bet my friend that I will program an AI that will beat him in Poker. I have 2 months to succeed. Next step will be to adjust this code to my environment. Challenge accepted! I'm quite the noob in programming (only 1 year experience). So I don't really understand much of what is going on here, but hope to be intelligent enough to do the necessary adaptations. Should be fun.",True
@TheThunderSpirit,2021-07-22T09:46:35Z,0,none of code is working outside blob environment,True
@burgos135,2021-06-21T18:36:08Z,0,You are the Hammer. Thank you bro!,True
@alibehfarnia4782,2021-06-01T19:31:35Z,0,How long does this take to be run? It's like forever for me. it takes like more than 8 hours!,True
@prashantsharmastunning,2021-05-07T09:28:25Z,0,hey anyone know how to make it use gpu on colab?,True
@daniil2815,2021-04-27T19:13:49Z,1,28:26 Me converting the tutorial to pytorch: 8-),True
@chaitanyaprasad6924,2021-04-11T08:35:55Z,0,Amazing Content!!,True
@Diego01201,2021-03-30T19:17:26Z,0,"Cool tutorial, but I'll never know if my implementation is correct, because it takes forever to train...",True
@jorgefelipegaviriafierro705,2021-02-12T04:28:14Z,0,"You are the best man, I know probably this didn¬¥t make you a lot of views but its awesome and you deserve even more subscribers than you currently have :)",True
@nicholaszivkovic1312,2021-01-29T17:51:25Z,0,*teleports to ubuntu*,True
@sebgrootus,2021-01-06T11:44:32Z,1,"You know the videos are quality and amazing, when you have a million followers with almost no views. Like anyone who sees them just subscribes.",True
@Sercil00,2021-01-05T17:51:52Z,0,Any chance you could post an update code that runs on Tensorflow 2.3.0 (current version as I write this)? I've gone through all the comments and tried all the fixes suggested here. They already don't work anymore. They just give me more errors. I wasted hours on this.,True
@farshadshamsyahoo,2021-01-02T14:13:51Z,0,"Hi, I have a question. In ""def train(self, terminal_state, step):"", where is the ""transition"" defined?",True
@erictheawesomest,2020-12-31T17:17:08Z,0,is there a reason to have X capitalised and y lowercase at 7:50 ?,True
@milkteamx7183,2020-12-01T17:08:32Z,1,"For me training the program said it would take over 130 hours?!  Can anyone tell me maybe why it takes so long compared to his. I'm sure my computer isn't as good as his but it is a fairly good laptop (16gb ram, etc). Why would it take so long compared to his training time?",True
@hussainalaaedi,2020-11-30T10:27:16Z,0,"Hi, which version of the Ubuntu Linux Operating system do you use to run this code, please?",True
@codyquist4908,2020-11-25T00:17:55Z,0,"AttributeError: 'ModifiedTensorBoard' object has no attribute '_train_step'  upon `agent.train(done, step)`",True
@Jaegg,2020-11-22T23:05:59Z,0,"Any one facing a VEEEERY slow training of this project? Estimator says 24h - 48h for training. I can't imagine that this is normal?! I am running a RTX 2060 Super, linux 5.8, TF 2.3.1, Cuda 10.1, Keras 2.4.3",True
@Jaegg,2020-11-22T22:17:59Z,2,How about redo all that stuff on an actual current version of TF?! No one who watches these tutorials will have TF 1.x installed... Sadly nothing in regards to Tensorboard is working for me on tf 2.3 ... I completely deactivated Tensorboard.,True
@timothec.8216,2020-11-17T21:15:40Z,1,"My training takes 7-9 seconds per episode, even though I run it on my GPU.. And I got the same results on Google Colab. Would someone have any idea of what could be the cause of this slowness ?",True
@user-oe2he7hg5y,2020-10-16T05:35:14Z,0,"It's actually pronounced ""paste"" not pasta",True
@boongbaang482,2020-10-13T00:47:31Z,0,how did he get the tensorboard display?,True
@charleswarren2089,2020-09-29T04:14:37Z,10,AttributeError: 'ModifiedTensorBoard' object has no attribute '_train_dir'  Any idea?,True
@jerinjose4185,2020-09-14T16:29:04Z,0,I built an agent which converged at 120 episodes. The model that I checkpointed at 120th episode was perfect it gave 200 score every time I ran it. But after 180 episodes it became very bad. Like it was only scoring 10 to 30 points,True
@Razorhunter9,2020-08-21T02:32:02Z,4,"Hello, I am getting this error:  AttributeError: 'ModifiedTensorBoard' object has no attribute '_train_dir'   Any help on this issue?",True
@RavinderSingh-ys8rt,2020-08-17T03:20:53Z,0,"QUESTION: Every piece of code runs fine, but I don't see this TensorBoard window containing all plots. Can somebody help?",True
@oOnihazOo,2020-08-06T09:03:55Z,0,"SUPER helpful. You're right, most tutorials on RL sucks; this one got me through my last assignment. Thanks!",True
@MsFearco,2020-07-30T11:30:34Z,3,Hey! Any way to speed up the training? It 'only' takes 1.5 hours but its only using 1% gpu and 30% cpu :/,True
@prashantmehta2832,2020-07-25T10:32:17Z,0,Hi.. i am learning reinforcement learning for industrial robotics. so. i want to ask that pre-envirnments(gym.make()) are necessary to learn? and what should i do or learn for industrial robotics? plss help bczz i have been findind answer from many weeks(about 4 weeks)..,True
@revanttiwari4669,2020-07-10T15:01:52Z,0,"might sound like a dumb question but why do we use the ""model"" to get current_qs_list and ""target_model"" for new_current_states?",True
@garlandzhang7201,2020-06-13T21:24:42Z,0,"i still dont understand why we have two models. if we always periodically update the target model to become the current model, and the current model tries to fit to the same values the target model is outputting, wouldnt it just remain unchanged?",True
@kastin83,2020-06-09T14:27:08Z,0,After all the TF2 updates i still get this error tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: logs/2x256-1591712683\train\plugins\profile\2020-06-09_22-25-00; No such file or directory,True
@alanjohnstone8766,2020-06-06T20:21:09Z,0,Why do you do a fit after every action? With a reasonably big memory the chances of the data from that action being in the training batch are small. I would have thought that making it more  like ‚Äúnormal‚Äù training by perhaps waiting until the end of the game and then doing one epoch of training over the whole memory replay would have been better.,True
@IlllIIIlllIIIlll,2020-06-04T12:55:00Z,0,"Sorry, another question: How is the blue line so good if you re-decayed epsilon? Shouldn't it be some random noise atleast until around 2.5k, when epsilon reaches aroun 0.5 (so we are taking random actions only half the time).",True
@IlllIIIlllIIIlll,2020-06-02T23:33:52Z,0,"Another question: Why, when we make model.fit(), are we dividing the X again with 255? Didn't we divide the states already in the code (when we did list comprehension to extract the states from the minibatch)?",True
@IlllIIIlllIIIlll,2020-06-02T21:39:00Z,0,"What is the point of the DISCOUNT? Wouldn't it be better to just not have it? If DISCOUNT tells us ""how much we value future reward"" it should be ""we value the end reward the most"" since we want to win in the end. Havin DISCOUNT just seems to do basically nothing.",True
@Kayoya1000,2020-05-31T08:23:55Z,0,"Your videos are awesome, easy to follow and learn these topics, great job man!",True
@mehediahsanpritom9113,2020-05-23T11:54:44Z,0,"Thanks for the video, it's quite detail and you explained it really well. I was just to know if DQN is a good choice for doing multiclass or binary classification problems. And if possible, then in that case how the 'actions' are changing?",True
@garychen6367,2020-05-20T19:50:45Z,1,"Hi Sentdex, thank you for your awesome tutorial, it really helps me a lot.  I have a question, what parameters need to be saved and how to save them before we run the code again (i.e., run 20_000 episode for like 5 times.)?",True
@AieeeNazeebo,2020-05-14T00:49:36Z,27,"Changes I had to make for Tensorflow 2.2.0: You should be able to copy-paste the portions within the `` quotes.  in class ModifiedTensorBoard:  in __init__: rewrote `self.writer = tf.summary.create_file_writer(self.log_dir)` added `self._log_write_dir = self.log_dir`  rewrote update_stats: `with self.writer.as_default():             for key, value in stats.items():                 tf.summary.scalar(key,value,step=self.step)                 self.writer.flush()`   in class Blob: rewrote `def move(self, x=None, y=None)` in move: rewrote `if x == None:` rewrote `if y == None:`  in main script: rewrote tf.set_random_seed(1) to `tf.random.set_seed(1)`",True
@MegaGippie,2020-05-05T08:59:36Z,2,"Hi man,   I have a problem with the ModifiedTensorBoard:  AttributeError: 'ModifiedTensorBoard' object has no attribute '_write_logs'   Do you have an update to the latest version?    Greetings friom Germany, David",True
@julsoles7886,2020-04-27T15:00:38Z,1,What GPU are you using for training? I have a local GTX 1060 and it is very slow. Just curious about your hardware. Thanks for the amazing video!,True
@5pellcast3r,2020-04-24T12:46:25Z,0,DO MOST OF U SERIOUSLY WATCH IT JUST FOR FUN ???? ANYWAYS I NEED TO USE A NATURAL LANGUAGE ACTION SPACE  FOR IMPLEMENTING THIS ON SO ANY IDEAS ON HOW I COULD DO THAT ??? OR USE THIS DIRECTLY INTO A SEQUENCE MODEL???. THANKS,True
@tidorpricope7140,2020-04-17T20:26:45Z,0,"Whatch out as the code has a bug for BlobEnv... x and y can also be 0 but they enter the positive ramification at the ""if not y"" which is ""if 1"", so instead of moving in vertical direction, it chooses a random action!",True
@kae4881,2020-04-13T15:13:51Z,0,"Shouldn't it be if not terminal_state in the self.model.fit in the train function? Because we want to train when the episode is not ending, right? Anyway, Thanx so much for these tutorials! I've been trying to learn about DQNs since the past month, ans this is the only place where i could understand its explanation!",True
@nathantsang5806,2020-04-12T21:47:01Z,0,I don't think you need to divide np.array(x) / 255 within self.model.fit in the train() method. We already dive all of the current_states and new_currents_states by 255,True
@TheRealAfroRick,2020-04-09T07:47:24Z,1,Huge fan. I know the theory to all of this and I'm building out my first DQN implementation. I understood everything you were doing until I got to the train. Was a bit hard to follow the implementation because you jumped around a bit and it wasn't entirely clear what you were doing. After watching it a few times it started to make a lot more sense.,True
@naveen3046,2020-04-08T14:37:45Z,0,Could you give us the trained model bruh,True
@nathantsang5806,2020-04-07T16:43:44Z,0,"thanks @sentdex for the great series! This was really helpful and I appreciate the time you took to make these vids. I'd love see a series on policy-gradients, actor-critic or another RL algo! It could even be on the same environment. Anyways, thanks again!",True
@cristianbergamo,2020-03-30T11:42:53Z,0,"HI! Can anybody tell me why using the fit() method instead of train_on_batch(), if we want the optimizer to keep track of the number of iterations we made?  Thank you",True
@rajcivils,2020-03-16T08:46:57Z,0,"Hello I tried some of the solutions below they seem to work, however they give me one more error -  ```TypeError: 'ResourceSummaryWriter' object is not callable ``` At this line -  ```     def update_stats(self, **stats):         self.writer(stats, self.step) ```",True
@MaxwellOmwenga,2020-03-03T23:59:40Z,0,"Is current_state = grid world image?   https://youtu.be/qfovbG84EBg?list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7&t=761 if yes, then the replay memory stores the grid world image as numpy array, right? Thank you.",True
@jorostuff,2020-02-16T17:51:21Z,0,"Sorry sentdex, will there be an A3C tutorial? I read that it's even better than DQNs.",True
@KishoreKumar-uz8ir,2020-02-15T16:31:38Z,0,23:19 There is an end of page button (ctrl+end),True
@mariushav,2020-02-10T10:21:29Z,0,"Hey! Thanks for a great tutorial, it nicely combines theory and hands on coding. Just a quick question: you mention the model - i.e. the neural network in this case - ouputs a touple of predicitons. In the illustration, it outputs a four-touple, and the action corresponding to the maximum value is chose.   I imagined the approach would be to pull a set of all possible actions for the agent from the environment given the current state, and to feed each of the possible actions into the model to retrieve a predicted Q-value for each action.   For example, if you want to mimic a RL chess playing agent, some states have very few possible actions, wile others have countless, that is the number of legal moves to make for the agent. Maybe i misunderstood your approach? :)",True
@user-lf7lr9gi7d,2020-02-07T00:51:35Z,4,"14:19 ""We're gonna fit all of this if we are on a terminal state, else we aren't gonna fit nothing"" that's a misspell, In reality, you meant: ""We're gonna fit all of this and if we are on a terminal state we will have a log taken""",True
@markd964,2020-01-21T08:50:18Z,2,"First run through an epsilon decay gave results similar to shown in video, with increasing reward as model learns how to get food quicker. Saved model. Then re-loaded model for second run through an epsilon decay, but on tensorboard reward does not improve from around the -200 (whereas in video the second run should improve on first run? did you change some parameters for second run during the copy / paste sections at end of video? Code at end of video is different from the earlier video and from the written code on pythonprogramming.net. thanks - superb series!",True
@anonn7248,2020-01-20T15:24:59Z,0,"After 20k, 50k and 100k episodes, the average reward definitely improves (exact same code as the tutorial). However the minimum reward doesn't seem to improve at all. Its always around -480~-500. Any guesses why?",True
@xiaoyang4521,2020-01-12T10:32:38Z,0,"Thank you sentdex. I've been following thru your machine learning series and have learned a ton through your channel. I think a lot of the viewers shared similar thoughts. Though the number of views do not reflect the value of the channel, I believe if you weight by your viewers, I think it has a major impact. Every view is like one student behind the monitor. Great job!",True
@douglasferreira3506,2020-01-06T01:38:49Z,0,how is the relationship between earlier steps and future rewards linked in the random sampling method utilized?,True
@axelsolhall5830,2020-01-05T22:17:21Z,1,"For those of you having problems with the TensorBoard writer with never versions of TF, here is my two step solution: Change self.writer to: self.writer = tf.summary.create_file_writer(self.log_dir) Thanks to @ChiDotPhi for this code And add this to the ModifiedTensorBoard Class:     def _write_logs(self, logs, index):         with self.writer.as_default():             for name, value in logs.items():                 tf.summary.scalar(name, value, step=index)                 self.step += 1                 self.writer.flush() Thanks to @Maad Ebrahim for this code It seems to be working with these changes, I will comment tomorrow when I have tested it.",True
@JordanMetroidManiac,2020-01-04T11:19:04Z,1,You should look into adaptive resonance theory (ART) for neural networks. Very cool stuff.,True
@mochilata,2019-12-14T17:22:38Z,1,"""from keras import model"" is missing in both the video and the website full code. I'm thinking most people who watches the video doesn't actually follow through it.",True
@sunnyguha2,2019-12-11T06:47:02Z,104,"for people using TF 2.0. Here are the changes you need to make (All are Tensorboard related): change self.writer as, self.writer = tf.summary.create_file_writer(self.log_dir)  and  in def update_stats(self, **stats): you need to update logs via: tf.summary.scalar('loss',stats['loss'], step=self.step) etc. Thats it.",True
@dankman7603,2019-11-30T03:31:04Z,0,"I'm having so much fun watching these, thanks",True
@kks2105,2019-11-27T05:20:16Z,3,One of the best tutorial you get for free. Thank you. :-)  Any plans to release actor critic ?,True
@nadavpotasman7534,2019-11-18T13:35:11Z,2,"Thanks for this video. One queation,  you suppose to predict the next action, in get_qs from the target network instead of the dummy one?",True
@leonshamsschaal,2019-11-10T21:22:38Z,1,Shouldn't the get_qs() use the target model instead of the main model for the predictions?,True
@bhaskarsyamal,2019-10-31T00:58:47Z,0,"Revisited the series. This now inspires me to do some python plays NES games sorta projects!! And also, waiting for you to reach that 1M mark. A warm thank you and good luck from us subs.",True
@MollyelevenTOO,2019-10-29T08:58:08Z,0,"Great series! Just want to understand why learning rate is never used in this DQN model when updating q values, while in basic q learning and theories it is used.  Did I miss out anything?",True
@gamblingguy6879,2019-10-18T13:25:47Z,1,"absolutely amazing series, can't stop watching. kudos!!! now that tensorflow2.0 is up and running, how about a rl series using tf-agents? it packs quite some stuff (from built-in models to parallel environment processing). Right now there are no tutorial about that anywhere, you would be the first one.",True
@arturasdruteika2628,2019-10-05T13:33:09Z,19,"am i the only one that's getting: ""AttributeError: 'ModifiedTensorBoard' object has no attribute '_write_logs"" ? Tried looking for similar problems but couldn't find one. Pls help.",True
@sharpbit,2019-10-01T03:20:40Z,0,"How do you determine env.OBSERVATION_SPACE_VALUES? You put (SIZE, SIZE, 3). I understand the SIZE part but how did you determine the 3?",True
@Jgolbstein,2019-08-31T15:20:27Z,1,"""step"" variable in train method is not used. intentionally? did u miss something? I can't reproduce your results either..",True
@alejandrofuster4528,2019-08-30T14:46:26Z,0,It would be cool if you extended your starcraft series with new RL algorithms,True
@vandortamas2010,2019-08-25T13:48:35Z,1,How can be maximum reward greater than 25 in the case of more than 1 enemies? The graph reward_max shows that the values of max rewards are close to 300.,True
@miicro,2019-08-18T13:18:14Z,4,"Hey, I've been studying DQN for some months, but theoretically mostly. While I did understand it, I had really hard time implementing it and overall had no clue where to begin with implementation. These tutorials have been GREAT and gave me a good idea where to begin. Thank you so much!",True
@RAPTURE,2019-08-14T00:03:50Z,0,"SethBling did a video where he used genetic programming to play through Super Mario World. Very interesting, I highly recommend checking it out if you haven't seen it before. He didn't mention Deep Q-Learning, but I think that could be a medium for people to test their understanding of this series.",True
@damiancaza-cleypool1088,2019-08-13T01:57:55Z,0,"Hey Sentdex,  I've experienced a slow down when training a model on my GPU. For some reason the CPU is almost twice as fast for training times. Any thoughts on this?",True
@anilkumarch91,2019-08-03T17:26:12Z,0,"@sentdex - I think for a long tutorial as this, may be jupyter notebook would be a good choice. As we can run blocks of code and understand the same.",True
@arsalanahmad1832,2019-08-02T07:38:39Z,0,Thanks for sharing such great knowledge in best way. Was going to ask is another lesson coming ?,True
@guilhermedelgado4077,2019-07-29T02:20:34Z,0,"@sentdex Hi, I've made a env with 2 agents and 5 actions (e.g. atk, defend, something like that). The game is over once a player hits hp=0, anyway, how should I reward a game with a series of actions? I was thinking to reward a player once he wins and record all the actions he took, and then train the network with that...but i just didn't thought it was correct enough to move forward with that... do you (or anybody) have any thoughts about this metodology?",True
@rockikz6965,2019-07-22T01:12:09Z,1,"Thank you for the tutorials, and by the way you was able to make the NN inputs as coordinates of all blobs concatenated, tried it and got huge training performance and yet the agent is converging !",True
@freelancer42,2019-07-21T20:06:08Z,1,"Hooray! I'm unique!  I'm just getting into machine learning, and this has been an excellent introduction. Thank you!",True
@alagaika8515,2019-07-20T20:45:25Z,2,"The added movement options for your Blob class are broken, aren't they? If you want to move down, you pass x=0 to move(), which, due to the check ""if not x:"" will perform a random move in the x direction.",True
@genbreaker2208,2019-07-19T13:17:13Z,0,"Hello, can you help me about of my problem about PyQt5? How to implement a closeEvent on a UI which was generated by qt designer?",True
@eddierichardson5740,2019-07-19T11:35:48Z,0,"I keep getting an error when running this     0%|          | 0/20000 [00:00<?, ?episode/s] Traceback (most recent call last):   File ""DeepQLearning.py"", line 353, in <module>     agent.model.save(f'models\\{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')   File ""C:\Users\follo\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\engine\network.py"", line 1088, in save     raise NotImplementedError NotImplementedError",True
@dudeguy8864,2019-07-13T21:00:31Z,1,i don‚Äôt understand how you don‚Äôt have millions of subs,True
@olithefirst,2019-07-13T19:45:57Z,6,"You don't need Netflix or TV when you can watch these tutorials! Really love them! @sentdex you can press CTRL + END to jump to the very bottom of your file, or CTRL + HOME to go to the beginning of the file :D",True
@RutgerMusicOnline,2019-07-09T14:57:01Z,5,Could we maybe do soft actor critic in the long run since its one of the best algorithms currently?,True
@VaibhavMishraBCS,2019-07-08T15:03:40Z,1,Hey Sentdex! Thanks for the amazing tutorial !! Can you make a series on applying RL on some real Hardware like drone or Line Following Bot! It would be great.,True
@daedalus4021,2019-07-07T09:02:10Z,0,My model takes over 5 days to train. Is this just because I'm not using tensorflow-gpu (I have an Intel HD 620) or is something else wrong?,True
@utkarshharit4719,2019-07-03T20:56:10Z,8,Hey... Any chance of NEAT coming up?? This series is just amazing!!,True
@fuba44,2019-06-30T21:15:59Z,0,"Yaay, I feel special.",True
@siddharthkale97,2019-06-28T10:39:56Z,1,"Your videos are of very good quality and I have followed some of your series and learnt python, django and machine learning. Thanks for making such elaborate and insightful videos. If you can make a video on neuro evolution it would be great. Please do, it would be very helpful.",True
@9gaggertons476,2019-06-26T18:17:56Z,0,Who is Daniel?!,True
@soldjaboy9670,2019-06-26T08:59:13Z,3,Can you do an application of reinforcement learning to finance. Like stock price prediction or optimising trade execution!! Thanks for the videos,True
@BryceChudomelka,2019-06-26T05:24:05Z,0,"Hey Sentdex. Thank you for your time. This has been a great video. I am trying to implement 'walls' for a maze in the environment, in order to create a maze. I first tried adding multiple enemies but got confused when I ran into errors. Do you think this is the right approach? Just create a Blob for each wall?",True
@ernestassimutis6239,2019-06-25T19:20:34Z,1,your content is amazing dude,True
@lorenzogiardi7930,2019-06-25T13:56:48Z,1,"Hey @sentdex, what do you think about libraries like Baselines, RLLIB and many others that provide prepackaged RL algorithms? By the way, I think that It would be great if you could cover multi-agent RL in a future video (or series)! Thank you very much for the great job that you're doing!!",True
@brandoncardillo5359,2019-06-24T11:17:13Z,0,Can we learn how to use DQN for Binary option trading on intraday trading,True
@arjunbemarkar7414,2019-06-24T06:42:47Z,1,Do another q/a,True
@ayushtiwari3819,2019-06-23T15:49:39Z,3,Please do NEAT! I can't stand watching those CodeBullet videos and never learning how to implement them. You're by far the best teacher so pleeeaaaaaaaaaaaase! :),True
@andreamassacci7942,2019-06-23T11:09:13Z,0,I'm sorry I might have misunderstood something. There is a way to have these videos before you release them in the main channel? if so what is it this method?? (Asking for a friend),True
@tomasmerva7978,2019-06-23T09:57:50Z,9,Is it planned to do algorithms such as PPO or DDPG in the future?,True
@jeanjacquesstrydom,2019-06-23T07:15:28Z,0,YEAY! MORE LINUX,True
@Totial,2019-06-23T01:15:38Z,71,Sadly this channel isn't watched as much as it is worth!! Im learning so much out of it and I'm so happy its free and accesible. Amazing job Sentdex!,True
@Karzacan,2019-06-22T21:25:14Z,1,Could you do a video about odoo?,True
@StuartHolliday,2019-06-22T21:11:50Z,3,@sentdex Thanks for making these videos!,True
@SandwichMitGurke,2019-06-22T20:09:12Z,1,I just donated a little bit via PayPal. I don't earn money yet but these videos are just too great :D,True
@davidg421,2019-06-22T19:58:08Z,7,Thanks to you I got excited about learning RL and now I'm already in chapter 7 of the sutton & barto book and following your tutorials,True
@Smikay,2019-06-22T17:56:31Z,3,I disliked the copy and pasting but I would have disliked watching you rewrite the whole thing even more lol,True
@EctoMorpheus,2019-06-22T17:40:06Z,8,"I'm already familiar with RL (in particular DQN) so I'm not watching any of the videos so far, but I just wanted to thank you for making this series. Even though they may not be useful to myself anymore, I wish these videos existed when I was looking for them a year ago!  If you plan on ever covering policy gradient methods, I'll be sure to join in though!",True
@thomaslukenyanja6242,2019-06-22T17:22:35Z,0,Hey #sentdex am from UGANDA,True
@pixel7038,2019-06-22T17:15:21Z,0,Please change the title for ur hate on udemy,True
@ujjawalsinha8968,2019-06-22T15:45:25Z,0,What about scaling the image between 1-2 instead of 0-1 so that the conv layers has something to operate on?,True
@sirynka,2019-06-22T15:26:42Z,0,You can use Ctrl+A and Left Arrow to scroll to the end of script in any editor or use page up page down buttons.,True
@liangyumin9405,2019-06-22T15:12:56Z,0,great!,True
@AniketKumar-bm8gi,2019-06-22T14:56:21Z,0,Awesome video sir,True
@chiquetcamille,2019-06-22T14:47:01Z,1,"Thank you for your tutorials ! Have you heard about this competition ? : https://www.aicrowd.com/challenges/neurips-2019-minerl-competition They provide a gym environment to let your agent play in Minecraft ! They also provide a decent dataset with tons of videos of real players (12Go of 64x64 videos) with an associated goal and reward/success. The ultimate goal is to dig a diamond but there are many intermediate steps like chop trees or build axes (the env also give the associated rewards of course). You should take a look, it could be a really nice env (for DQN or other algos) to play with :)",True
@RabeezRiaz,2019-06-22T14:42:30Z,2,"If anyone is interested in learning more about why the 'cyclic learning rate decay' (37:37)  actually works better than just a simple one-time decay method, you can read this (https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b) article which I found very interesting.",True
@devstash830,2019-06-22T14:34:58Z,1,"i like your tutorial as always from philippines üòä, i create my own Desktop virtual assistant because of you, thanks keep it up idol",True
@jimdelsol1941,2019-06-22T14:23:50Z,0,Do you plan on doing any video on DeepFaceLab explaining the code and how to use it ?,True
@aakarshan01,2019-06-22T14:13:42Z,0,Here early to ask a question. What does Q mean or stand for,True
@digitalboltwebdesign,2019-06-22T14:03:46Z,1,Please show how we can use this with cryptocurrency and get the output results for live trading i have learned python cause of your tutorials i have been able to make my first scripts and now im eager to master machine learning thank you for all your tutorials.,True
@zotec5464,2019-06-22T14:02:13Z,0,"Hey Sentdex can you check out your and Daniels nmt chatbot, and more accurately the issues section of the GitHub page. Much appreciated",True
@girish7914,2019-06-22T13:40:55Z,0,can you create more DQN turorials on real world problems..,True
@Anysecur,2019-06-22T13:26:10Z,0,"oh, video come out right when I was trying to estimate how many weeks I have to wait for the next part after finishing p. 5",True
@RutgerMusicOnline,2019-06-22T13:24:56Z,15,"A3C, PPO here we come!",True
@utkarsh1874,2019-06-22T13:22:36Z,35,"Thank you so much, love all your series I feel like I get more content and practical training here than any of the  courses at coursera.  Please keep going on, I am a Machine Learning Engineer and python lover all thanks to you. :D",True
@mikopiko,2019-06-22T13:17:13Z,0,Cool,True
@Terror500,2019-06-22T13:16:47Z,0,:),True
@shahzerbaig1397,2019-06-22T13:16:31Z,0,Second,True
@Stinosko,2019-06-19T06:21:01Z,0,Yeay üéä,True
