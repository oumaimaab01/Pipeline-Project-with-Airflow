author,updated_at,like_count,text,public
@SD-gw5vm,2024-03-26T18:05:54Z,0,"For those of you who may struggle with the code above due to the yahoo API and its list of issues you may want to consider this.   def get_data_yahoo(reload_sp500=False):          if reload_sp500:         tickets=save_SP500_tickers()     else:             with open (""sp500tickers.pickle"", ""rb"") as f:                 tickers=pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')          start=dt.datetime(2020,1,1)     end=dt.datetime(2023,12,31)          for ticker in tickers:         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             yf.pdr_override()             df=web.get_data_yahoo(ticker,start=start,end=end)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have'.format(ticker)) get_data_yahoo()      It worked after I attempted to run the one I had multiple times.",True
@Chris-si4ox,2023-11-23T15:53:41Z,0,is this now out of date?  I guess need pip install yfinance now,True
@Iearnwithme,2023-09-09T11:43:09Z,0,"Has anyone got the following error/ know how to solve:  File ~/anaconda3/lib/python3.11/site-packages/pandas_datareader/yahoo/daily.py:153 in _read_one_data data = j[""context""][""dispatcher""][""stores""][""HistoricalPriceStore""]  TypeError: string indices must be integers, not 'str'",True
@kba155,2022-12-28T17:35:40Z,2,"As of December 2022, this works:  def save_dow30_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'id': 'constituents'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[1].text.strip().replace('.','-')          tickers.append(ticker)      with open('dow30tickers.pickle','wb') as f:         pickle.dump(tickers, f)         print(tickers)      return tickers    #save_dow30_tickers()   def get_data_from_yahoo(reload_dow30=False):     if reload_dow30:         tickers = save_dow30_tickers()     else:          with open('dow30tickers.pickle', 'rb') as f:             tickers= pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(y,m,d) #year,month,date     end = dt.datetime(y,m,d) #same      for ticker in tickers:         print(ticker)         if not os.path.exists(f'stock_dfs/{ticker}.csv'):             df = pdr.get_data_yahoo(ticker, start, end)             df.to_csv(f'stock_dfs/{ticker}.csv')         else:             print(f'Already have {ticker}')               get_data_from_yahoo()",True
@Timotheos42,2022-12-04T16:11:38Z,0,"Heads up for everyone watching recently,  Do you know how I would write in the def loop a line of code saying to continue if the ticker is not find in Yahoo.  It would be nice and help me correct this error : RemoteDataError: No data fetched for symbol LAK-USD using YahooDailyReader",True
@rednibkb1,2021-12-10T13:02:21Z,0,sentex  gave you I like this...but counter went from 841 down to 840 seems to me - count should increase ??? Python Programming for Finance p.6,True
@senteix2,2021-04-18T01:56:52Z,0,why not use quandl ?,True
@chinzorigtds1796,2021-03-12T14:59:48Z,0,Round all columns into 2 decimal places for smaller memory space: df = df.round(2),True
@isaakimaliev5584,2021-02-28T12:20:06Z,1,No data fetched for symbol MMM using YahooDailyReader,True
@durstiespace2270,2021-02-04T06:49:04Z,5,"If anyone is hitting this in 2021, yahoo finance doesn't use ""."" in their tickers.  Replace any ""."" with ""-"" instead.      for m_ticker in m_tickers:         print(m_ticker)         # Yahoo finance uses ""-"" instead of "".""         m_ticker = m_ticker.replace('.', '-')         if not os.path.exists(f'stock_dfs/{m_ticker}.csv'):             try:                 df = web.DataReader(m_ticker, 'yahoo', m_start, m_end)             except Exception as e:                 print(f'Could not get data for {m_ticker}')                 continue             df.to_csv(f'stock_dfs/{m_ticker}.csv')         else:             print(f'Already have {m_ticker}')",True
@jameshuangmusic,2021-01-30T00:01:59Z,0,"if the start and end date do not align with the historical data (i.e. the company was not in the S&P during the time frame, then the pull would not work - does anyone know how to make an exception for these cases?",True
@rodneysimpson4604,2020-12-21T09:54:25Z,0,"Hi Everyone, bit new to this and hoping there might be a few Aussies out there.  Trying to run same code as sentdex, but on the ASX200 (Australian top 200 companies).  Everything seems to work perfectly... tickers list created ok and ticker.csv files created  upto PPT.AX.csv  (140th csv file ) then it crashes with following error: FileNotFoundError: [Errno 2] No such file or directory: 'ASXstock_dfs/PRN.AX.csv'  .....exact error dump below.  Very confused to why it would just crash after successfully executing 140 times.  hmmmm.... Any help greatly appreciated.  Already have PNV.AX ['A2M.AX', 'ABC.AX', 'ABP.AX', 'AGL.AX', 'ALL.AX', 'ALQ.AX', 'ALU.AX', 'ALX.AX', 'AMC.AX', 'AMP.AX', 'ANN.AX', 'ANZ.AX', 'APA.AX', 'APD.AX', 'APE.AX', 'APT.AX', 'APX.AX', 'ARB.AX', 'ASB.AX', 'AST.AX', 'ASX.AX', 'AVH.AX', 'AWC.AX', 'AZJ.AX', 'BAP.AX', 'BEN.AX', 'BGA.AX', 'BHP.AX', 'BIN.AX', 'BKL.AX', 'BKW.AX', 'BLD.AX', 'BOQ.AX', 'BPT.AX', 'BRG.AX', 'BSL.AX', 'BVS.AX', 'BWP.AX', 'BXB.AX', 'CAR.AX', 'CBA.AX', 'CCL.AX', 'CCP.AX', 'CGC.AX', 'CGF.AX', 'CHC.AX', 'CIM.AX', 'CKF.AX', 'CLW.AX', 'CMW.AX', 'CNU.AX', 'COE.AX', 'COH.AX', 'COL.AX', 'CPU.AX', 'CQR.AX', 'CSL.AX', 'CSR.AX', 'CTD.AX', 'CUV.AX', 'CWN.AX', 'CWY.AX', 'DHG.AX', 'DMP.AX', 'DOW.AX', 'DXS.AX', 'EHE.AX', 'ELD.AX', 'EML.AX', 'EVN.AX', 'FBU.AX', 'FLT.AX', 'FMG.AX', 'FPH.AX', 'GEM.AX', 'GMG.AX', 'GNC.AX', 'GOR.AX', 'GOZ.AX', 'GPT.AX', 'GUD.AX', 'GWA.AX', 'HLS.AX', 'HUB.AX', 'HVN.AX', 'IAG.AX', 'IEL.AX', 'IFL.AX', 'IGO.AX', 'ILU.AX', 'INA.AX', 'ING.AX', 'IPH.AX', 'IPL.AX', 'IRE.AX', 'IVC.AX', 'JBH.AX', 'JHG.AX', 'JHX.AX', 'JIN.AX', 'LLC.AX', 'LNK.AX', 'LYC.AX', 'MFG.AX', 'MGR.AX', 'MIN.AX', 'MMS.AX', 'MND.AX', 'MPL.AX', 'MQG.AX', 'MTS.AX', 'MYX.AX', 'NAB.AX', 'NAN.AX', 'NCM.AX', 'NEA.AX', 'NEC.AX', 'NHC.AX', 'NHF.AX', 'NSR.AX', 'NST.AX', 'NUF.AX', 'NWH.AX', 'NWL.AX', 'NWS.AX', 'NXT.AX', 'OML.AX', 'ORA.AX', 'ORE.AX', 'ORG.AX', 'ORI.AX', 'OSH.AX', 'OZL.AX', 'PDL.AX', 'PLS.AX', 'PME.AX', 'PMV.AX', 'PNI.AX', 'PNV.AX', 'PPT.AX', 'PRN.AX', 'PTM.AX', 'QAN.AX', 'QBE.AX', 'QUB.AX', 'REA.AX', 'RHC.AX', 'RIO.AX', 'RMD.AX', 'RRL.AX', 'RSG.AX', 'RWC.AX', 'S32.AX', 'SAR.AX', 'SBM.AX', 'SCG.AX', 'SCP.AX', 'SDF.AX', 'SEK.AX', 'SFR.AX', 'SGM.AX', 'SGP.AX', 'SGR.AX', 'SHL.AX', 'SIQ.AX', 'SKC.AX', 'SKI.AX', 'SLR.AX', 'SOL.AX', 'SPK.AX', 'SSM.AX', 'STO.AX', 'SUL.AX', 'SUN.AX', 'SVW.AX', 'SXL.AX', 'SYD.AX', 'TAH.AX', 'TCL.AX', 'TGR.AX', 'TLS.AX', 'TNE.AX', 'TPG.AX', 'TWE.AX', 'URW.AX', 'VCX.AX', 'VEA.AX', 'VOC.AX', 'VUK.AX', 'VVR.AX', 'WBC.AX', 'WEB.AX', 'WES.AX', 'WHC.AX', 'WOR.AX', 'WOW.AX', 'WPL.AX', 'WSA.AX', 'WTC.AX', 'XRO.AX'] <module 'ntpath' from 'C:\\Python\\P38\\lib\\ntpath.py'> ['A2M.AX', 'ABC.AX', 'ABP.AX', 'AGL.AX', 'ALL.AX', 'ALQ.AX', 'ALU.AX', 'ALX.AX', 'AMC.AX', 'AMP.AX', 'ANN.AX', 'ANZ.AX', 'APA.AX', 'APD.AX', 'APE.AX', 'APT.AX', 'APX.AX', 'ARB.AX', 'ASB.AX', 'AST.AX', 'ASX.AX', 'AVH.AX', 'AWC.AX', 'AZJ.AX', 'BAP.AX', 'BEN.AX', 'BGA.AX', 'BHP.AX', 'BIN.AX', 'BKL.AX', 'BKW.AX', 'BLD.AX', 'BOQ.AX', 'BPT.AX', 'BRG.AX', 'BSL.AX', 'BVS.AX', 'BWP.AX', 'BXB.AX', 'CAR.AX', 'CBA.AX', 'CCL.AX', 'CCP.AX', 'CGC.AX', 'CGF.AX', 'CHC.AX', 'CIM.AX', 'CKF.AX', 'CLW.AX', 'CMW.AX', 'CNU.AX', 'COE.AX', 'COH.AX', 'COL.AX', 'CPU.AX', 'CQR.AX', 'CSL.AX', 'CSR.AX', 'CTD.AX', 'CUV.AX', 'CWN.AX', 'CWY.AX', 'DHG.AX', 'DMP.AX', 'DOW.AX', 'DXS.AX', 'EHE.AX', 'ELD.AX', 'EML.AX', 'EVN.AX', 'FBU.AX', 'FLT.AX', 'FMG.AX', 'FPH.AX', 'GEM.AX', 'GMG.AX', 'GNC.AX', 'GOR.AX', 'GOZ.AX', 'GPT.AX', 'GUD.AX', 'GWA.AX', 'HLS.AX', 'HUB.AX', 'HVN.AX', 'IAG.AX', 'IEL.AX', 'IFL.AX', 'IGO.AX', 'ILU.AX', 'INA.AX', 'ING.AX', 'IPH.AX', 'IPL.AX', 'IRE.AX', 'IVC.AX', 'JBH.AX', 'JHG.AX', 'JHX.AX', 'JIN.AX', 'LLC.AX', 'LNK.AX', 'LYC.AX', 'MFG.AX', 'MGR.AX', 'MIN.AX', 'MMS.AX', 'MND.AX', 'MPL.AX', 'MQG.AX', 'MTS.AX', 'MYX.AX', 'NAB.AX', 'NAN.AX', 'NCM.AX', 'NEA.AX', 'NEC.AX', 'NHC.AX', 'NHF.AX', 'NSR.AX', 'NST.AX', 'NUF.AX', 'NWH.AX', 'NWL.AX', 'NWS.AX', 'NXT.AX', 'OML.AX', 'ORA.AX', 'ORE.AX', 'ORG.AX', 'ORI.AX', 'OSH.AX', 'OZL.AX', 'PDL.AX', 'PLS.AX', 'PME.AX', 'PMV.AX', 'PNI.AX', 'PNV.AX', 'PPT.AX', 'PRN.AX', 'PTM.AX', 'QAN.AX', 'QBE.AX', 'QUB.AX', 'REA.AX', 'RHC.AX', 'RIO.AX', 'RMD.AX', 'RRL.AX', 'RSG.AX', 'RWC.AX', 'S32.AX', 'SAR.AX', 'SBM.AX', 'SCG.AX', 'SCP.AX', 'SDF.AX', 'SEK.AX', 'SFR.AX', 'SGM.AX', 'SGP.AX', 'SGR.AX', 'SHL.AX', 'SIQ.AX', 'SKC.AX', 'SKI.AX', 'SLR.AX', 'SOL.AX', 'SPK.AX', 'SSM.AX', 'STO.AX', 'SUL.AX', 'SUN.AX', 'SVW.AX', 'SXL.AX', 'SYD.AX', 'TAH.AX', 'TCL.AX', 'TGR.AX', 'TLS.AX', 'TNE.AX', 'TPG.AX', 'TWE.AX', 'URW.AX', 'VCX.AX', 'VEA.AX', 'VOC.AX', 'VUK.AX', 'VVR.AX', 'WBC.AX', 'WEB.AX', 'WES.AX', 'WHC.AX', 'WOR.AX', 'WOW.AX', 'WPL.AX', 'WSA.AX', 'WTC.AX', 'XRO.AX'] Traceback (most recent call last):   File ""C:/Users/Simpson M17X Admin/Finance 3 P38/FinanceASX.py"", line 66, in <module>     get_data_from_yahoo()  # takes long time to run to create all 200 csv files   File ""C:/Users/Simpson M17X Admin/Finance 3 P38/FinanceASX.py"", line 60, in get_data_from_yahoo     df.to_csv('ASXstock_dfs/{}.csv'.format(ticker))   File ""C:\Users\Simpson M17X Admin\Finance 3 P38\lib\site-packages\pandas\core\generic.py"", line 3170, in to_csv     formatter.save()   File ""C:\Users\Simpson M17X Admin\Finance 3 P38\lib\site-packages\pandas\io\formats\csvs.py"", line 185, in save     f, handles = get_handle(   File ""C:\Users\Simpson M17X Admin\Finance 3 P38\lib\site-packages\pandas\io\common.py"", line 493, in get_handle     f = open(path_or_buf, mode, encoding=encoding, errors=errors, newline="""") FileNotFoundError: [Errno 2] No such file or directory: 'ASXstock_dfs/PRN.AX.csv'",True
@cesarbodden1162,2020-12-01T15:07:43Z,0,"This is a very helpful series.  Thank you for posting.  I have a question, in this series we downloaded the historical data for all the S&P 500 stocks.  Is there a way to pull the pricing data for a single day for each stock and then put it in a separate file?  This way I can analyze a single day's activity.    Thanks,",True
@corentinw.1145,2020-11-26T08:23:46Z,6,"As of november 26th 2020, this works :  import bs4 as bs import datetime as dt import os import pandas as pd  from pandas_datareader import data as pdr import pickle import requests import yfinance as yf  yf.pdr_override  def save_sp500_tickers() :     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text)     table = soup.find('table', {'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.strip().replace('.', '-')          tickers.append(ticker)          with open('sp500tickers.pickle', 'wb') as f:         pickle.dump(tickers, f)     print(tickers)     return tickers  save_sp500_tickers()  def get_data_from_yahoo(reload_sp500=False):      if reload_sp500:         tickers = save_sp500_tickers()     else:         with open('sp500tickers.pickle', 'rb') as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')          start = dt.datetime(2000,1,1)     end = dt.datetime(2020,11,26)      for ticker in tickers[:100]: #Should grab all 500 later, but takes time         print(ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = pdr.get_data_yahoo(ticker, start, end)             df.reset_index(inplace=True)             df.set_index('Date', inplace=True)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker)) get_data_from_yahoo()",True
@tzookil,2020-11-24T19:38:09Z,0,"Hi, I'm having a strange behave with get_data_from_yahoo().  when I execute the func in the line: df = web.DataReader(ticker,'yahoo',start ,end) it's returning an error that say ""pandas_datareader._utils.RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader"" I run the same command with the stock name instead of ticker and it pass... df = web.DataReader('MMM','yahoo',start ,end)  has anybody encountered this issue before? thanks,",True
@alihazem8820,2020-10-07T22:18:48Z,1,"No data fetched for symbol MMM  using YahooDailyReader pls help",True
@astaragmohapatra9,2020-10-03T15:15:26Z,0,"If anyone following this in 2020, directly using ""yahoo"" will not give your results. So follow this thread https://stackoverflow.com/questions/58890570/python-yahoo-finance-download-all-sp-500-stocks",True
@GoredGored,2020-09-30T02:35:01Z,0,Thanks sentdex I am lost right by  def get_data_from_yahoo(reload_sp500=False): Can you explain the logic behind this function?     if reload_sp500: (that is if reload_sp500 is True then the below)         tickers = save_sp500_tickers() but why tickers is assigned by calling the function save_sp500_tickers() and then the else?  I appreciate if anyone can help explain the logic behind this function.,True
@edgardolopez6908,2020-09-13T08:49:13Z,1,"For anyone getting the HistoricPriceStore Error for some tickers, you can skip them with a try/except. (From StackOverflow)  def get_data_from_yahoo(reload_sp500=False): ...      if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):           try:                df = web.DataReader(ticker,'yahoo',start,end)                df.to_csv('stock_dfs/{}.csv'.format(ticker))           except Exception as ex:                print('Error', ex)      else:           print('Already have {}'.format(ticker)) ...",True
@joycelynnnelson9316,2020-09-12T23:57:39Z,0,Hi! Where was the sp500 saved to?  I am using Colaboratory.,True
@matt_t937,2020-09-09T09:34:43Z,0,"For some tickers I get that yahoo cannot fetch the data required, how can I solve this?",True
@michaelschneck6161,2020-09-03T16:11:34Z,0,I added yfinance and the program worked-Halleluah,True
@michaelschneck6161,2020-09-03T13:59:49Z,0,"I have been using Spyder under Anaconda and have successfully copied the program line for line.  When I run it, it will give me the list of s%p500 stocks.  However after that it does not do what it should do.  After going through a number of ""Already have BDX"", it then gives me the following errors which I cannot make sense of.  I hope somebody can help.  Any advice would be appreciated Already have BDX Traceback (most recent call last):    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 2646, in get_loc     return self._engine.get_loc(key)    File ""pandas\_libs\index.pyx"", line 111, in pandas._libs.index.IndexEngine.get_loc    File ""pandas\_libs\index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc    File ""pandas\_libs\hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item    File ""pandas\_libs\hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item  KeyError: 'Date' During handling of the above exception, another exception occurred:  Traceback (most recent call last):    File ""C:\Users\mksch\anaconda3\SP500-names-2.py"", line 52, in <module>     get_data_from_yahoo()    File ""C:\Users\mksch\anaconda3\SP500-names-2.py"", line 47, in get_data_from_yahoo     df = web.DataReader(ticker, 'yahoo', start, end)    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas\util\_decorators.py"", line 214, in wrapper     return func(*args, **kwargs)    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas_datareader\data.py"", line 376, in DataReader     return YahooDailyReader(    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas_datareader\base.py"", line 253, in read     df = self._read_one_data(self.url, params=self._get_params(self.symbols))    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 165, in _read_one_data     prices[""Date""] = to_datetime(to_datetime(prices[""Date""], unit=""s"").dt.date)    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas\core\frame.py"", line 2800, in __getitem__     indexer = self.columns.get_loc(key)    File ""C:\Users\mksch\anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 2648, in get_loc     return self._engine.get_loc(self._maybe_cast_indexer(key))    File ""pandas\_libs\index.pyx"", line 111, in pandas._libs.index.IndexEngine.get_loc    File ""pandas\_libs\index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc    File ""pandas\_libs\hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item    File ""pandas\_libs\hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item  KeyError: 'Date'",True
@broy.,2020-08-26T11:07:50Z,1,How can I update the data on a daily basis without reloading it from Yahoo?,True
@jamesbevan1,2020-08-25T12:39:45Z,0,"I ran this code and only got the tickers for 66 companies. Then I added some of the code changes mentioned in the comments as fixes to certain errors.  Now I have been able to get 206/505 of the tickers.  However,  the same two lines of code keep causing error messages. 1)  	df = web.DataReader(ticker, 'yahoo', start, end) 2) 	get_data_from_yahoo()  What is it about these lines of code that cause me issues?  Why does the error message not tell you how to fix it?  Update:  For some reason I can now get 257/505 of the tickers, however I'm also getting this in the error message: ""pandas_datareader._utils.RemoteDataError: No data fetched for symbol ISRG using YahooDailyReader""  Is there a way to skip if there is an error? For example if it can't fetch the data for a ticker, it just skips it and continues with collecting the rest, rather than terminating the task?",True
@akshatjain8052,2020-08-15T08:41:21Z,0,"I keep getting the error- ""OSError: [Errno 22] Invalid argument: 'stock_dfs/MMM\n.csv' "". i am using forward slash. Also, tried double slash. The problem doesnt seem to solve. Can someone help me out!?",True
@michakonik730,2020-08-02T08:56:59Z,0,Executing that in mid 2020 the script could not fetch MMM data.  Here is the solution: https://stackoverflow.com/questions/54854276/no-data-fetched-web-datareader-panda,True
@akshatjain8052,2020-07-27T13:25:59Z,1,"I am getting this error- OSError: [Errno 22] Invalid argument: 'stock_dfs\\MMM\n.csv' Can anyone explain to me what this error means and what is the fix?",True
@stephenhobson1618,2020-07-11T15:17:01Z,0,"Many thanks for taking the time to make and share your knowledge, you inspired me to learn Python. Trouble is I've hit a wall, regarding accessing non US exchanges. I'm in the UK and so, want to access London Stock Exchange (FTSE All Share etc) stocks. Any hints or ideas would be appreciated. Thanks again and I can't wait to see what you'll do next",True
@chenzhang2921,2020-06-30T09:08:26Z,4,"For anyone with the ""HistoricalPriceStore"" error (I had it with ""EXC"", ""MAA"", ""NFLX""). I really don't know why, but trying to add them individually outside of the function seems to work:  ticker = 'EXC' #or any other problematic ticker df = web.DataReader(ticker,""yahoo"", dt.datetime(2000,1,1), dt.datetime(2020,6,30)) df.to_csv('stock_dfs/{}.csv'.format(ticker))",True
@yogeshdubey5142,2020-06-20T19:45:37Z,0,there are no stocks in my stock_dfs folder can someone help ?,True
@trevorhinchliffe2195,2020-06-01T21:01:56Z,0,"I am getting this error when I run the code.  C:\python\Python38\lib\site-packages\pandas_datareader\compat\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.   from pandas.util.testing import assert_frame_equal MSFT Traceback (most recent call last):   File ""s&p500tickers.py"", line 57, in <module>     get_data_from_yahoo()   File ""s&p500tickers.py"", line 52, in get_data_from_yahoo     df = dataReader(ticker,'yahoo', start, end) NameError: name 'dataReader' is not defined  my version of datareader is 0.8.1 pandas is 1.0.4 and python version 3.8.3   can anyone help ?? TIA",True
@kwangyangchia9408,2020-05-31T04:41:36Z,0,"Hi sentdex! I don't know if you still read these comments, but I'm just trying to shoot my shot here. I've received an EOFError saying that I've ran out of input, and while there was a comment that states i should have a stocks_dfs file in wherever my .py file is, I still receive that error. I'm not too sure how to proceed properly, if there is any guidance it would be greatly appreciated! :)",True
@Mr1337Fabulous,2020-05-26T07:50:37Z,0,"For those getting the error reading BRK.B, you have to replace . (dot) with - (dash) to match Yahoo's format.",True
@clanbull3t,2020-05-20T10:43:31Z,3,"In case someone ran into a problem like me recently: The code breaks right after processing CCL (Carnival Corp.) when reaching CARR (Carrier Global). Looking into the table we can see it started trading 2020-04-03. If our end of crawling is earlier, the code breaks. Fix: set end to a later date i.e:               end=dt.datetime(2020,4,30) Warning : your dataset will be much bigger and probably need way more computing power /time .",True
@vickys2555,2020-05-18T12:57:12Z,3,"when you are  getting data from html table just use pandas read_html method, just one line of code . It grabs HTML tables from the html page then iterate over list or just pass index number to the list as below. Pandas does the job of requests and beautifulsoup with respect to html tables with out any additional libraries. KEEP IT SIMPLE.  df  = pd.read_html(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")[0]   That's all!!",True
@theaxel1231,2020-05-14T13:26:06Z,0,"I really thought he was kidding when he said ""if you've slow internet just grab 10 or something like that"", then while running the script  I realized he was right about spending too much time getting everything.",True
@vd_1990,2020-05-08T04:22:44Z,0,"For anyone running into issues of how wikipedia has some of the tickers with a period and how that causes your function to fail, just need to add the following two lines;     for ticker in tickers:           ticker_unadjused=ticker           if ticker_unadjused.find(""."")>0:                ticker_unadjused=ticker_unadjused.replace(""."",""-"")          if not os.path.exists('stock_dfs/{},csv'.format(ticker_unadjused)):              df=pdr.DataReader(ticker_unadjused,'yahoo',start,end)              df.to_csv('stock_dfs/{},csv'.format(ticker_unadjused))         else:             print(""already have the {}"".format(ticker_unadjused))",True
@christosroniotis1903,2020-05-02T14:22:50Z,0,"Hello,   Congrats on your amazing work!I modified a little bit the algorithm  (S&P100, one year data 2018-1-1 / 2018/12/31) however in the stock_dfs file I get only the first 34 csv's of stocks, in alphabetical order.Then I am getting a huge error that ends with the note 'KeyError: 'Date' .Any idea?   Best regards!!!!",True
@gehtomacgyver,2020-04-20T01:12:44Z,11,"Full code, working as of 4-19-2020. Corrects for  those who kept getting an error that ""no data fetched for symbol MMM (or another stock ticker ) using yahoodailyreader""     import bs4 as bs import datetime as dt import os from pandas_datareader import data as pdr import pickle import requests import fix_yahoo_finance as yf  yf.pdr_override  def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.replace('.', '-')         ticker = ticker[:-1]         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')     start = dt.datetime(2019, 6, 8)     end = dt.datetime.now()     for ticker in tickers:         print(ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = pdr.get_data_yahoo(ticker, start, end)             df.reset_index(inplace=True)             df.set_index(""Date"", inplace=True)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   save_sp500_tickers() get_data_from_yahoo()",True
@czhang870,2020-04-16T19:31:29Z,1,"Hi, guys! I have updated codings for anyone who watch this tutorial recently. And THANKS to Sentdex and  predecessors. Thanks for your guidance.   import bs4 as bs import pickle import requests import os import datetime as dt import pandas as pd import pandas_datareader.data as web  def save_sp500_tickers():     resp=requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup =bs.BeautifulSoup(resp.text,'lxml')     table = soup.find('table', {'id': 'constituents'})     tickers=[]     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.strip().replace(""."", ""-"")         if ""."" in ticker:             ticker = ticker.replace('.','-')             print('ticker replaced to', ticker)          tickers.append(ticker)      with open(""sp500tickers.pickle"",""wb"") as f:             pickle.dump(tickers,f)      print(tickers)     return tickers #save_sp500_tickers()  def get_data_from_yahoo(reload_sp500= False):      if reload_sp500:         tickers=save_sp500_tickers()     else:                  with open ('sp500tickers.pickle','rb')as f:                 tickers=pickle.load(f)                      if not os.path.exists('stock_dfs'):            os.makedirs('stock_dfs')     start=dt.datetime(2018,4,16)     end=dt.datetime(2020,4,16)      for ticker in tickers:         print (ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):               df=web.DataReader(ticker,'yahoo',start,end)               df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have{}'.format(ticker)) get_data_from_yahoo()",True
@VS-uh5yq,2020-04-13T17:17:06Z,0,Unfortunately I cant create the dir stocks_dfs. I always get the already exists error,True
@bolaolapade7402,2020-04-05T14:58:55Z,0,"Hey, sentdex or anyone able to help. I keep getting the error below although my coding is the same as what is shown in the tutorial   line 31, in get_data_from_yahoo     with open(""sp500tickers.pickle"", ""rb"") as f: FileNotFoundError: [Errno 2] No such file or directory: 'sp500tickers.pickle'",True
@dhruvilmatalia719,2020-04-01T14:12:50Z,0,I am getting Key data error:'Historical Price Store' @sentdex please tell me the solution and any other person who can help.,True
@bobsviiu9481,2020-03-09T04:16:56Z,0,"i can't load data of stocks  everything went well untill fetching the data from yahoo a error is shown at  get_data_from_yahoo data=web.DataReader(tickers, 'yahoo', start, end )",True
@ardaavc3830,2020-02-04T21:53:22Z,27,"For those who keep getting an error that ""no data fetched for symbol MMM (or another stock ticker ) using yahoodailyreader"" I've found a solution on stack overflow in case of need here is the link : https://stackoverflow.com/questions/54854276/no-data-fetched-web-datareader-panda",True
@ericlougheed9440,2020-01-30T20:11:37Z,0,"I keep getting this Error:  RemoteDataError: No data fetched for symbol MMM using YahooDailyReader   Help!",True
@chegu071,2020-01-30T17:37:14Z,0,"I was having a problem with ADBE - the wiki table has '12' next to it in the same table cell so it was pulling ADBE12 through as the ticker and looking for that with Yahoo which caused an error. I fixed it by using an if statement to check for ADBE12 then ticker = ""ADBE"" which seems a bit of a lazy way to do it - is there a better way to fix that? I realise you can't just cut numeric values out because tickers like 3M wouldn't work then?",True
@vishalkanojia9634,2020-01-30T03:02:47Z,0,"After return tickers mine is showing Ticker is not defined   with open (""nasdaq_tickers.pickle"",""wb"") as f:     pickle.dump(tickers, f) return tickers    --------------------------------------------------------------------------- NameError                                 Traceback (most recent call last) <ipython-input-3-cab730796468> in <module>       1 with open (""nasdaq_tickers.pickle"",""wb"") as f: ----> 2     pickle.dump(tickers, f)       3 return tickers  NameError: name 'tickers' is not defined",True
@PhysicistGamer,2020-01-25T22:02:21Z,1,"For anyone watching in 2020, this code wont work, you have to modify it a bit:   and install pip of yahoo finance and yfinance   import bs4 as bs  import datetime as dt import os from pandas_datareader import data as pdr import pickle import requests import yfinance as yf  yf.pdr_override  def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.replace('.', '-')         ticker = ticker[:-1]         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')     start = dt.datetime(2019, 6, 8)     end = dt.datetime.now()     for ticker in tickers:         print(ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = pdr.get_data_yahoo(ticker, start, end)             df.reset_index(inplace=True)             df.set_index(""Date"", inplace=True)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   save_sp500_tickers() get_data_from_yahoo()",True
@ZectorHolme,2020-01-24T20:14:22Z,0,"Hi, what if I would like to update the database that I already have? Is there a way to write only the new lines on an already existing .csv file? Thanks",True
@dustinwi,2020-01-17T03:44:49Z,1,"My twist on the solutions for ""Keyerror: 'Date'"", extra white space in the ticker names, and ""."" instead of ""-"" in the Wikipedia page:  Keyerror: 'Date' - Another user suggested changing the end date to ""today"" but hard coded the date in his example.  Anyone watching it later could run into the same issue.  I used these as my start and end so it will theoretically always work, and still gathers roughly the same amount of data used in this video.  end = dt.date.today()  start = end - dt.timedelta(days=int(16*365.25))   Extra white space and wrong separator in the ticker names - I did a one line update, instead of the suggested multi-line if statements mentioned below.  Use strip to strip off leading and trailing whitespace.  Use replace to replace the ""."" with a ""-"".  I prefer it all in one line.  Just my preference.      Change this: ticker = row.findAll('td')[0].text      To this: ticker = row.findAll('td')[0].text.strip().replace(""."", ""-"")",True
@user-ze4qq8mm1q,2020-01-16T21:18:20Z,1,"Kept getting hung up on CTVA when scanning from Yahoo, only to realize that CTVA was added a year ago so there is no data for it in the January 1, 2000 to December 12, 2016 date range... took me a while to realize so hopefully this comment helps someone out there.",True
@tanishagupta7303,2020-01-12T14:35:32Z,0,"@sentdex i am getting the output with error   ['MMM', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADBE', 'AMD', 'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALXN', 'ALGN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AMCR', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'AIV', 'AAPL', 'AMAT', 'APTV', 'ADM', 'ARNC', 'ANET', 'AJG', 'AIZ', 'ATO', 'T', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'BKR', 'BLL', 'BAC', 'BK', 'BAX', 'BDX', 'BRK-B', 'BBY', 'BIIB', 'BLK', 'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BF-B', 'CHRW', 'COG', 'CDNS', 'CPB', 'COF', 'CPRI', 'CAH', 'KMX', 'CCL', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'COO', 'CPRT', 'GLW', 'CTVA', 'COST', 'COTY', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'FANG', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DISH', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DTE', 'DUK', 'DRE', 'DD', 'DXC', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EOG', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'EVRG', 'ES', 'RE', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FE', 'FRC', 'FISV', 'FLT', 'FLIR', 'FLS', 'FMC', 'F', 'FTNT', 'FTV', 'FBHS', 'FOXA', 'FOX', 'BEN', 'FCX', 'GPS', 'GRMN', 'IT', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GL', 'GPN', 'GS', 'GWW', 'HRB', 'HAL', 'HBI', 'HOG', 'HIG', 'HAS', 'HCA', 'PEAK', 'HP', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HFC', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'HII', 'IEX', 'IDXX', 'INFO', 'ITW', 'ILMN', 'IR', 'INTC', 'ICE', 'IBM', 'INCY', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IPGP', 'IQV', 'IRM', 'JKHY', 'J', 'JBHT', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LEG', 'LDOS', 'LEN', 'LLY', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'M', 'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MKC', 'MXIM', 'MCD', 'MCK', 'MDT', 'MRK', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'MYL', 'NDAQ', 'NOV', 'NTAP', 'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NLOK', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'ORLY', 'OXY', 'ODFL', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PKG', 'PH', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTN', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE', 'NOW', 'SHW', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STE', 'SYK', 'SIVB', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TGT', 'TEL', 'FTI', 'TFX', 'TXN', 'TXT', 'TMO', 'TIF', 'TJX', 'TSCO', 'TDG', 'TRV', 'TFC', 'TWTR', 'TSN', 'UDR', 'ULTA', 'USB', 'UAA', 'UA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAC', 'V', 'VNO', 'VMC', 'WRB', 'WAB', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'WEC', 'WCG', 'WFC', 'WELL', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WMB', 'WLTW', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS'] MMM Exception in thread Thread-1: Traceback (most recent call last):   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\threading.py"", line 917, in _bootstrap_inner     self.run()   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\threading.py"", line 865, in run     self._target(*self._args, **self._kwargs)   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\site-packages\multitasking\__init__.py"", line 102, in _run_via_pool     return callee(*args, **kwargs)   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\site-packages\yfinance\multi.py"", line 167, in _download_one_threaded     actions, period, interval, prepost, proxy, rounding)   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\site-packages\yfinance\multi.py"", line 182, in _download_one     rounding=rounding, many=True)   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\site-packages\yfinance\base.py"", line 121, in history     _time.strptime(str(start), '%Y-%m-%d')))   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\_strptime.py"", line 571, in _strptime_time     tt = _strptime(data_string, format)[0]   File ""C:\Users\Harshita\AppData\Local\Programs\Python\Python37\lib\_strptime.py"", line 359, in _strptime     (data_string, format)) ValueError: time data 'yahoo' does not match format '%Y-%m-%d'",True
@MrGuiassis,2019-12-30T23:49:42Z,0,It worked for me! :D Anyone can help about how to generate the csv files by a stock list in a .txt file (instead of a pickle)?,True
@juhaszat,2019-12-29T12:26:55Z,5,"code works for me as of 29th Dec 2019   import bs4 as bs import datetime as dt import os import pandas_datareader.data as web import pickle import requests   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.replace('.','-')         ticker = row.find_all ('td')[0].text.strip()         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')     start = dt.datetime(2019, 6, 8)     end = dt.datetime.now()      for ticker in tickers:          print(""Scraping: ""+ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             try:                                df = web.DataReader(ticker,'yahoo',start,end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             except KeyError:                 pass         else:              print('Already have {}'.format(ticker))  save_sp500_tickers() get_data_from_yahoo()",True
@konradkwiatkowski1008,2019-12-28T11:14:05Z,3,"Hi  I got an ""EOFError: Ran out of input"". Does anyone have solution for this?  EDIT: I found this solution https://stackoverflow.com/questions/54854276/no-data-fetched-web-datareader-panda",True
@PhiaNova,2019-12-24T11:15:42Z,0,"This is what worked for me: I am new to this all so forgive me if there's something missing... but this gives me the output we were looking for.  first install pip install yfinance    import bs4 as bs import datetime as dt import os import yfinance as yf from pandas_datareader import data as pdr yf.pdr_override() import pickle import requests   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.find_all('td')[0].text            str(ticker).replace('.','-')         tickers.append(ticker)                           with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)      return tickers                    def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2018,1,1)     end = dt.datetime(2019,1,1)     for ticker in tickers:         # just in case your connection breaks, we'd like to save our progress!         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = pdr.get_data_yahoo(ticker, start, end)             df.reset_index(inplace=True)              df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   get_data_from_yahoo()",True
@privateeye242,2019-12-23T18:15:57Z,0,"I am stuck.  I get the sp500tickers.pickle filled with the tickers. I get the directory stocks_dfs filled with the ticker data. When running the Compile_data() module I get the count printed from 10 to 500.  But then I get the error: ""ValueError: columns overlap but no suffix specified: Index(['MMM'], dtype='object')"". I copied another csv file and gave it the name  MMM.csv. Still with the same result.  Where should I look at?",True
@1xxxtylerxxx1,2019-12-17T00:06:24Z,120,"For anyone getting an error  KeyError: 'Date', here are two things that I did to fix the problem:  1. Switch the end date to today - some of the S&P 500 stocks weren't added until after December 31, 2016. So just make the end date today (for you).  code:        start = dt.datetime(2000,1,1)     end = dt.datetime(2019,12,15)   2. Some of the stock tickers output a period instead of a hyphen. ex: in Wikipedia Brown-Forman Corp is listed BF.B, but in Yahoo Finance, it's listed BF-B. To fix that, just do a find and replace. So add this if statement to the for loop in the first function ""save_sp500_tickers()"":  for row in table.findAll('tr')[1:]:         ticker = row.find_all('td')[0].text.replace('\n','')         if ""."" in ticker:             ticker = ticker.replace('.','-')             print('ticker replaced to', ticker)          tickers.append(ticker)   After you add that if statement, un-comment save_sp500_tickers() and re-run it so there will be no errors down the road.",True
@uncommonsense6022,2019-12-14T19:30:23Z,0,is there a way to append new data to you df?,True
@inakigoenaga9811,2019-12-06T19:08:50Z,0,"For the issue of yahoo i used fix yahoo finance and it works, i installed it like this pip install yfinance — upgrade —no-cache-dir and the imported as import yfinance as yf. It now runs correctly, im not a programer so if theres something wrong plese tell me, hope i can help",True
@user-um8sw6ii5z,2019-11-10T01:28:18Z,0,"Not sure if anyone facing a strange issue , there have 2 companies, BRK-B and BF-B, which i cannot read the history data from Yahoo,  but i manually search it, they are really exits and have all time data. also ,    i guess the index of SP500 from WIKI ,the format is BRK.B, but in Yahoo, the company data is stored as BRK-B. same thing happened to BF-B or BF.B,    if I run the code , there have an error :   FileNotFoundError: [Errno 2] File b'stock_dfs/BF.B.csv' does not exist: b'stock_dfs/BF.B.csv   one by one come out , does anyone else facing that problem? i am using VScode to program code.   thanks",True
@cedriklegrand,2019-11-07T23:05:10Z,3,"For people doing this with newer version of panda_datareader, if the date is outside the actual life possible data date, it will trow a KeyError : Date. This is a bug that you can see on the github of pandas_datareader. A way to avoid it is to go with a try and except (or copy me :p): for ticker in tickers:          print(""Scraping: ""+ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             try:                                df = web.DataReader(ticker,'yahoo',start,end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             except KeyError:                 pass         else:              print('Already have {}'.format(ticker))",True
@henrycwcw,2019-10-31T06:36:18Z,0,"What to do if I want to get the data for all 500 companies but im stuck due to the variation of list dates? I cant have only 1 start date can I? Because it would return error when the start date is too early, which happens a lot. Would there be a way for me to store the ticker and list_date? Can I index the list_dates with ticker names and then call it during get_data_from_yahoo() function for start date?",True
@euniceng_glorytohk,2019-10-31T06:00:36Z,0,NotImplementedError: data_source='morningstar' is not implemented I am really confused...,True
@iceman13687,2019-09-29T12:54:23Z,2,05:50 any help on how to add a list for updating stock data everyday? (Sentdex a BIG thank you),True
@uncommonsense6022,2019-09-06T00:36:26Z,0,Once I hit BRK.B i get an error?,True
@VismundCygnus421,2019-08-29T19:34:47Z,0,can anyone help me with this error?     tickers = pickle.load(f) _pickle.UnpicklingError: unpickling stack underflow,True
@user-no2mv1zv9r,2019-08-27T19:57:40Z,0,"yoo, im getting pandas_datareader._utils.RemoteDataError: No data fetched for symbol 3M Compan using YahooDailyReader",True
@Li-hm6re,2019-08-17T09:51:42Z,0,"anyone can help me with thi s part? i write  def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"",""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')          start = '2000-01-01'     end   = '2019-08-01'          for ticker in tickers:         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df= web.get_data_yahoo(ticker, start, end)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('already have {}'.format(ticker))   get_data_from_yahoo()   ValueError: zero-size array to reduction operation maximum which has no identity",True
@gabriellevaillant5153,2019-08-10T21:26:01Z,0,"I did the same with S&P 500 with no problem. Then i tried the same with the CAC40 and no problem to get the data from wikipedia, but i couldn't get any data from yahoo :( I had this KeyError: 'HistoricalPriceStore' can someone help me ? thanks",True
@chasewillis297,2019-08-06T17:38:17Z,0,How can I add the part he was talking about at 5:30? I have downloaded all the data and would like to append new data to my existing files.,True
@freetube7767,2019-07-20T15:33:44Z,0,*** `fix_yahoo_finance` was renamed to `yfinance`. *** Please install and use `yfinance` directly using `pip install yfinance -U`  More information: https://github.com/ranaroussi/yfinance,True
@Salirbeber1,2019-07-19T11:29:06Z,0,"I have some problems downloading the data from yahoo. When I indicated some dates when there is no data of a particular ticket an exception from external libreries jumps out. The programm continue to run without problems and in fact a file with the data of the particular ticket is save without any information (thats correct). But I dont want to have the exception while the programm is running. I tried some try/except blocks but as the ""problem"" is from external libreries is very difficult to catch it. The exception is:   Exception in thread Thread-105: ... KeyError: 'timestamp'  During handling of the above exception, another exception occurred: ... ValueError: ('EGN', 'No data found for this date range, symbol may be delisted')   I dont know if someone has had the same problem.",True
@ashleyez2022,2019-07-19T02:16:08Z,0,"I have a question. why do we use wb and rb, why not just write and read without the bytes? what role do the bytes play?",True
@smartoolscr4371,2019-07-16T20:24:29Z,0,"Hello, new at this. Having this error using Jupiter, anyone can help? Tks :   --------------------------------------------------------------------------- KeyError                                  Traceback (most recent call last) ~/anaconda3/lib/python3.7/site-packages/pandas_datareader/yahoo/daily.py in _read_one_data(self, url, params)     132             j = json.loads(re.search(ptrn, resp.text, re.DOTALL).group(1)) --> 133             data = j['context']['dispatcher']['stores']['HistoricalPriceStore']     134         except KeyError:   KeyError: 'HistoricalPriceStore'   During handling of the above exception, another exception occurred:   RemoteDataError                           Traceback (most recent call last) <ipython-input-33-54f7d1f5b225> in <module>      42         else:      43             print(""Already have {}"".format(ticker)) ---> 44 get_data_from_yahoo()   <ipython-input-33-54f7d1f5b225> in get_data_from_yahoo(reload_sp500)      38         print(ticker)      39         if not os.path.exists(""stock_dfs/{}.csv"".format(ticker)): ---> 40             df = web.DataReader(ticker, ""yahoo"", start,end)      41             df.to_csv(""stock_dfs/{}.csv"".format(ticker))      42         else:   ~/anaconda3/lib/python3.7/site-packages/pandas_datareader/data.py in DataReader(name, data_source, start, end, retry_count, pause, session, access_key)     308                                 adjust_price=False, chunksize=25,     309                                 retry_count=retry_count, pause=pause, --> 310                                 session=session).read()     311      312     elif data_source == ""google"":   ~/anaconda3/lib/python3.7/site-packages/pandas_datareader/base.py in read(self)     208         if isinstance(self.symbols, (compat.string_types, int)):     209             df = self._read_one_data(self.url, --> 210                                      params=self._get_params(self.symbols))     211         # Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])     212         elif isinstance(self.symbols, DataFrame):   ~/anaconda3/lib/python3.7/site-packages/pandas_datareader/yahoo/daily.py in _read_one_data(self, url, params)     134         except KeyError:     135             msg = 'No data fetched for symbol {} using {}' --> 136             raise RemoteDataError(msg.format(symbol, self.__class__.__name__))     137      138         # price data   RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader",True
@ericabao1279,2019-07-08T16:34:24Z,0,"Hello， I am sorry to trouble you but I got  some same errors: RemoteDataError: Unable to read URL: https://query1.finance.yahoo.com/v7/finance/download/ADBE?period1=946706400&period2=1562389199&interval=1d&events=history&crumb=S6cESkXkpT%5Cu002F   that I could not obtain the df in the loop from yahoo. But when I tried to obtain the df next time, it was ok. Could you  explain it? Here is my code:   def save_sp500_tickers():     response= requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup= bs.BeautifulSoup(response.text)          table=soup.find('table',{'class':""wikitable sortable""})     tickers=[]          for row in table.findAll('tr')[1:]:         ticker=row.findAll('td')[0].text                  ticker=ticker[:-1]         '''         mapping=str.maketrans(""."",""-"")         ticker=ticker.translate(mapping)         '''         ticker=str(ticker).replace('.','-')         tickers.append(ticker)              with open('sp500tickers.pickle','wb') as f:         pickle.dump(tickers,f)      print(tickers)     return tickers  save_sp500_tickers()   # above okay   def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers=save_sp500_tickers()              else:         with open('sp500tickers.pickle','rb') as f:             tickers=pickle.load(f)          if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')       start_date=dt.datetime(2000,1,1)     end_date=dt.datetime(2019,7,5)               for ticker in tickers:                  if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df=web.DataReader(ticker,'yahoo', start_date, end_date)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print(""Already have {}"".format(ticker))          get_data_from_yahoo()   got error!",True
@michaelciraco7890,2019-07-08T03:24:03Z,0,if anyone has this working as of 7/7/2019 pls link or paste below. Thank you,True
@zanehutchison7304,2019-07-05T01:29:57Z,1,"For everyone who is getting stuck on the stock CTVA, it's because the stock only opened 3 June 2019. Since there is no data the 'Date' issue occurs. To fix this I just adjusted the end date to be 30 June 2019: end = dt.datetime(2019,6,30)",True
@kitaatkar5814,2019-06-21T13:27:27Z,2,Anyone know if YahooDailyReader is still working? I get stuck at the first ticker MMM with an error saying   RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader  Is there a better alternative out there anyone knows about? I tried changing the dates to be only the year of 2019 and still nothing..   Update:  It was because I wasn't removing the \n from the ticker...duh!,True
@bifflohman1,2019-06-08T10:04:17Z,0,"Complete nood here.  I can't seem to get the program to run.  I hit f5, the shell pops up and nothing happens. Anyone else do the same?",True
@davidbarrar5968,2019-06-05T10:21:01Z,0,I had to add ticker.strip() to remove white spaces from the ticker item in my list to make the yahoo search work,True
@waynewatson7970,2019-05-28T16:09:14Z,0,"Sentdex, if that is what I should call you.  Thank you so much for sharing.  That is such a cool thing that people share what they know.  All the time you have spent of your own learning all these things and your willing to share.  Thanks again.",True
@vieuxstradivari133,2019-05-27T14:09:28Z,2,"RemoteDataError: No data fetched for symbol  Try this, the only change made is that you want to have MMM and not MMM/n, therefore add: ticker = ticker[:-1]   I'm a begginer, hence sorry if this will not help but for me surely did.      import bs4 as bs import datetime as dt import os import pandas_datareader.data as web import pickle import requests   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         ticker = ticker[:-1]         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2010, 1, 1)     end = dt.datetime.now()     for ticker in tickers:         # just in case your connection breaks, we'd like to save our progress!         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = web.DataReader(ticker, 'yahoo', start, end)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   get_data_from_yahoo()",True
@alancedeno9452,2019-05-18T21:36:43Z,0,RemoteDataError: No data fetched for symbol MMM   using YahooDailyReader,True
@vivek9917333300,2019-05-07T02:07:16Z,0,"I run the code and get error. Traceback (most recent call last):   File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 133, in _read_one_data     data = j['context']['dispatcher']['stores']['HistoricalPriceStore'] KeyError: 'HistoricalPriceStore'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python37-32\Program_5.py"", line 48, in <module>     get_data_from_yahoo()   File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python37-32\Program_5.py"", line 39, in get_data_from_yahoo     df = web.DataReader(ticker, 'yahoo', start, end)   File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas_datareader\data.py"", line 310, in DataReader     session=session).read()   File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 136, in _read_one_data     raise RemoteDataError(msg.format(symbol, self.__class__.__name__)) pandas_datareader._utils.RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader",True
@epicmonckey25001,2019-05-03T20:58:41Z,0,"If anyone has any problems with a date error, it is because the stock was added after 2016, or when ever you specified.",True
@Humanstartups,2019-05-01T04:59:59Z,0,"I have an excel code that I am trying to convert into python so I can scrape morningstar website for apple financial information. Thanks     =arrayformula(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(IMPORTHTML(""http://financials.morningstar.com/finan/financials/getFinancePart.html?t=AAPL&region=usa&culture=en-US&ops=clear"",""table"", 1), ""<\/td>"" , """" ),""<\/tr>"",""""),""<\/th>"",""""),""<\/thead>"",""""),""<\/span>"",""""))",True
@bread639,2019-05-01T01:17:21Z,17,I don't know if you're getting the same error as me but next to line:  >> ticker = row.findAll('td')[0].text  you'll need: >> ticker= ticker[:-1]  to get rid of the '\n' character,True
@seanstills1755,2019-04-28T22:42:56Z,0,"I am having issues with an error being thrown  ""RemoteDataError: No data fetched for symbol MMM using YahooDailyReader"" here is my code...thanks for the help:   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.replace('.','-')         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers  save_sp500_tickers()   def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2018, 1, 1)     end = dt.datetime.now()          for ticker in tickers[:100]:         print(ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):                 df = web.DataReader(ticker, 'yahoo', start, end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))              get_data_from_yahoo()",True
@andrewrooney1,2019-04-22T04:35:39Z,3,"anyone facing pandas error KeyError: 'Date' while running this? already translated ""."" into ""-""",True
@joshuabeauchamp4525,2019-04-19T02:39:43Z,3,"for everyone having the ""EOF"" error, make sure you have created a 'stock_dfs' folder wherever you have your .py file.",True
@fullmooncorp,2019-04-19T00:40:06Z,0,"Super weird but im getting:   C:\Users\PewPew\PycharmProjects\Tutorial\venv\Scripts\python.exe ""C:/Users/PewPew/PycharmProjects/Finance Proyect/Python S&P500.py"" Already have Z Already have o Already have e Already have t Already have i Already have s  Process finished with exit code 0   But that`s all i get....anyone got an idea on how to fix this?",True
@OliverOliverio1,2019-04-15T04:10:15Z,0,"getting these errors running this: ""KeyError: 'HistoricalPriceStore'"" followed by ""pandas_datareader._utils.RemoteDataError: No data fetched for symbol 3M Company using YahooDailyReader""",True
@aldoseitlli6034,2019-04-11T14:13:45Z,0,"When i write in  df = web.DataReader('TSLA', 'yahoo', start, end) i get the data but for tickers i cant where is the problem? for ticker in tickers:         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             print(ticker)             try:                 df = web.DataReader(ticker, 'yahoo', start, end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             except:                 print('Couldnt Reload {}'.format(ticker))         else:             print('Already have {}'.format(ticker))",True
@tarangsomani1827,2019-04-11T06:01:09Z,0,"Can you please explain me what is this reload_sp500 in - ""def get_data_from_yahoo(reload_sp500=False): "" ?",True
@sebastiandrozd18,2019-04-08T02:29:07Z,0,"okay so i spent the last hour trying to fix this lmao. for some reason when i was connecting to the api's if it was yahoo or morningstar it kept saying no results.   i changed the       for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[1].text   where this is a one here use to be a 0 right after the ('td) Having left it how it was it was sending the full stock name to the api not just the ticker so now its taking the 2nd column which is just the symbol. hope it helps! and its working fine on yahoo. just downloaded 100 stocks! thanks for the tutorial!!",True
@rap2back,2019-03-23T00:08:07Z,8,"Heads up for everyone watching recently, wiki may have changed the table formatting so you have to type the beginning as:    for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[1].text.replace('.','-')   He's got a ('td')[0] but you need ('td')[1] for the file to save the stock abbreviations, ('td')[0] saves the full titles and it doesn't work for the next step.",True
@troys9099,2019-03-09T12:38:20Z,0,IndexError: tuple index out of range .  ..,True
@mattcostantino346,2019-03-07T19:52:21Z,0,"I'm new to python and would really appreciate the help here. I am getting an error: KeyError: 'HistoricalPriceStore'. Also, another exception occurs which references many anaconda3 paths and lines within each of the references. Finally, it gives me a RemoteDataError: No data fetched for symbol ... Any reason why I'm receiving this? Is it because of my geography and the georouting by Yahoo. Or maybe too many requests from my IP?",True
@strawtak3265,2019-03-05T10:22:28Z,0,"Anyone same case with this downstairs?Traceback (most recent call last):   File ""C:\Users\straw\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 133, in _read_one_data     data = j['context']['dispatcher']['stores']['HistoricalPriceStore'] KeyError: 'HistoricalPriceStore'During handling of the above exception, another exception occurred:Traceback (most recent call last):   File ""C:\Users\straw\Desktop\Stock\finance.py"", line 130, in <module>     get_data_from_yahoo()   File ""C:\Users\straw\Desktop\Stock\finance.py"", line 124, in get_data_from_yahoo     df = web.DataReader(ticker,'yahoo',start,end)   File ""C:\Users\straw\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\data.py"", line 310, in DataReader     session=session).read()   File ""C:\Users\straw\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Users\straw\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 136, in _read_one_data     raise RemoteDataError(msg.format(symbol, self.__class__.__name__)) pandas_datareader._utils.RemoteDataError: No data fetched for symbol 3M Company using YahooDailyReader",True
@KiKi-rw7ou,2019-03-01T04:07:48Z,0,"I got this ""raise RemoteDataError(msg.format(symbol, self.__class__.__name__))  pandas_datareader._utils.RemoteDataError: No data fetched for symbol Allegion using YahooDailyReader"". Could someone help me?",True
@AjayKumar-id7mb,2019-02-26T00:10:32Z,0,"This is the updated code, just delete your pickle file and rerun save_sp500_tickers()  import bs4 as bs import datetime as dt import os import pandas_datareader.data as web import pickle import requests   def save_sp500_tickers():     resp = requests.get(         'http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[1].text         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_iex(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2018, 1, 1)     end = dt.datetime.now()     for ticker in tickers:         # just in case your connection breaks, we'd like to save our progress!         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = web.DataReader(ticker, 'iex', start, end)             df.reset_index(inplace=True)             df.set_index(""date"", inplace=True)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   get_data_from_iex()",True
@AjayKumar-id7mb,2019-02-25T21:54:51Z,0,"It's giving me an error that   Traceback (most recent call last):   File ""C:\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 133, in _read_one_data     data = j['context']['dispatcher']['stores']['HistoricalPriceStore'] KeyError: 'HistoricalPriceStore'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""D:/PYTHON CODES/PYTHON FOR FINANCE/6.10.py"", line 46, in <module>     get_data_from_yahoo()   File ""D:/PYTHON CODES/PYTHON FOR FINANCE/6.10.py"", line 37, in get_data_from_yahoo     df = web.DataReader(ticker, 'yahoo', start, end)   File ""C:\Python37\lib\site-packages\pandas_datareader\data.py"", line 310, in DataReader     session=session).read()   File ""C:\Python37\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 136, in _read_one_data     raise RemoteDataError(msg.format(symbol, self.__class__.__name__)) pandas_datareader._utils.RemoteDataError: No data fetched for symbol 3M Company using YahooDailyReader",True
@AjayKumar-id7mb,2019-02-25T21:53:36Z,0,"Your code is not working it    import bs4 as bs import datetime as dt import os import pandas_datareader.data as web import pickle import requests   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2010, 1, 1)     end = dt.datetime.now()     for ticker in tickers:         # just in case your connection breaks, we'd like to save our progress!         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = web.DataReader(ticker, 'morningstar', start, end)             df.reset_index(inplace=True)             df.set_index(""Date"", inplace=True)             df = df.drop(""Symbol"", axis=1)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   get_data_from_yahoo()",True
@malcolmtan2501,2019-02-16T03:20:10Z,0,"Hi SentDex! A big fan fan of your work. ive been following u for the past 6 months and my goal is to go through all your tutorials. Also, is it possible to update this (or at least the code in pythongprogramming.net) because im unable to continue with any of these section's tutorial because im not able to access the S&P500 data for some reason.",True
@richardbedoyavillegas9820,2019-02-13T20:49:21Z,0,"Hello, I have a problem with 3M Company, this error No data fetched for symbol 3M Company using YahooDayliReader",True
@cosimocolacone9414,2019-02-08T13:56:05Z,0,So by now what is the right way to write the code? Becuase I still cannot fix the problem with yahoo and Google API and the code in the website (https://pythonprogramming.net/sp500-company-price-data-python-programming-for-finance/) still doesn't work!,True
@qalbepoems,2019-01-21T18:00:19Z,2,"For anyone who wants their CSV files to be updated the next time you run the program, my code is as follows:      for ticker in tickers[:]:         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = web.DataReader(ticker, 'yahoo', start, end)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             temp = pd.read_csv(""stock_dfs/{}.csv"".format(ticker))             date = [int(temp['Date'][len(temp)-1][:4]), int(temp['Date'][len(temp)-1][5:7]),                      int(temp['Date'][len(temp)-1][8:])]             X = dt.datetime(date[0], date[1], date[2])             Update_start = dt.datetime(end.year, end.month, end.day + 1)             if X >= Update_start:                 print('Already have {}'.format(ticker))             else:                 print('{} is updating'.format(ticker))                                  Update_end = dt.datetime(now.year, now.month, now.day)                 df = web.DataReader(ticker, 'yahoo', Update_start, Update_end)                 with open('stock_dfs/{}.csv'.format(ticker), 'a') as f:                     df.to_csv(f, header=False)",True
@godfreywp,2019-01-18T07:03:16Z,0,"It gives me an error saying Invalid syntax with the else statment   if reload_sp500:         tickers = save_sp500_tickers()         else:                      with open(""sp500tickers.pickle"",""rb"") as f:                 tickers = pickle.load(f)   does anyone know if i closed off  anything wrong or did something wrong?",True
@amirvahid7143,2019-01-17T03:10:43Z,0,"Here is the working code as of today 1/16/19:  import bs4 as bs import datetime as dt import os import pandas_datareader.data as web import pickle import requests import fix_yahoo_finance as yf   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers  # AV: 1/16/19: These are sved before in the previous function  # save_sp500_tickers() # AV: 1/16/19: I had to use Tingo from reddit recommendation b/c quandl doesn't # work for this purpose as SPY is not defined there!!! # https://www.reddit.com/r/Python/comments/7zxptg/pulling_stock_market_data_yahoo_and_google_dont/ # Even Tiingo doesn't work. I have to stick with quandl for now until I learn more # AV: 1/16/19: I learned how to fix it by reding the youTube comments using # pip install yahoo_finance_fix # Note: You might have to run the code twice due to the big data memory issues # I guess!!!  def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2010, 1, 1)     end = dt.datetime.now()     for ticker in tickers:         # just in case your connection breaks, we'd like to save our progress!         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             yf.pdr_override()             df = web.get_data_yahoo(ticker, start=start, end=end)             # AV: 1/16/19: Had to replace this by the yahoo_finance_fix             # attributes             # df = web.DataReader(ticker, 'morningstar', start, end)             df.reset_index(inplace=True)             df.set_index(""Date"", inplace=True)             # AV: 1/16/19: changint the data to quandl. This is             # no longer necessary             # df = df.drop(""Symbol"", axis=1)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))   get_data_from_yahoo()",True
@kevalmistry7749,2018-12-31T05:29:47Z,0,"Traceback (most recent call last):   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas\core\indexes\base.py"", line 3078, in get_loc     return self._engine.get_loc(key)   File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\s&p500_list.py"", line 47, in <module>     get_data_from_yahoo()   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\s&p500_list.py"", line 42, in get_data_from_yahoo     df = web.DataReader(ticker, 'yahoo', start, end)   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\data.py"", line 310, in DataReader     session=session).read()   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 142, in _read_one_data     to_datetime(prices['Date'], unit='s').dt.date)   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas\core\frame.py"", line 2688, in __getitem__     return self._getitem_column(key)   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas\core\frame.py"", line 2695, in _getitem_column     return self._get_item_cache(key)   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas\core\generic.py"", line 2489, in _get_item_cache     values = self._data.get(item)   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas\core\internals.py"", line 4115, in get     loc = self.items.get_loc(item)   File ""C:\Users\MUKESH_DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\pandas\core\indexes\base.py"", line 3080, in get_loc     return self._engine.get_loc(self._maybe_cast_indexer(key))   File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'  Hey Sentdex, i love your content and in am doing great  with your help, but i am stuck at this while i am collecting data from wiki page. i would like to hear back from you on this. Again thanks for the content",True
@bradengee5093,2018-12-27T16:48:21Z,0,"To get the csv files to download, I had to remove "" if not os.path.exists('stock_dfs/{}.csv').format(ticker):"" why would that be?  Thank you, im enjoying this series even though I have almost no coding experience.",True
@tubehossein,2018-12-24T08:18:49Z,0,How can I download the data under the Statistics tab for all the S&P500 companies in Yahoo Finance?,True
@user-ix4jp8sh5o,2018-12-01T13:06:19Z,1,"Nowadays Quandl Solution  Hi, Guys! I'm from Russia so would like sorry for my English, just a little tip  how to make this code works through Quandl request:  1. Quandl query needs dataset, where Quandl  tries to find data, so I simply choose wiki for this. ( I think you could try another datasets) To indicate dataset we need something like this : qdl.get(''database""+''/""+""ticker,"" start_date, end_date) So i used the easiest way to this just adding the ""WIKI/"" to each ticker in query :)  2. Sometimes Quandl can't done the request cause the error and script stops working, to avoid this I googled fix by try/except Quandl NotFounderror (it was my case), so you need a little bit correct your code in request part and import this thing : ""from quandl.errors.quandl_error import NotFoundError""  My working script here:  from quandl.errors.quandl_error import NotFoundError def get_data_from_quandl(reload_sp500 = False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open (""sp500tickers.pickle"",'rb') as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')              start = dt.datetime(2000,1,1)     end = dt.datetime(2016,12,31)          for ticker in tickers:         print(ticker)         try:             if not os.path.exists('stock_dfs/{}.cvs'.format(ticker)):                 df = qdl.get(""WIKI/""+ticker, start_date = start, end_date = end,use_retries = False)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             else:                 print('Already have {}'.format(ticker))         except NotFoundError as e:             print('error in : {} '.format(ticker)) get_data_from_quandl()  Hope it helps you!",True
@CyborgGaming99,2018-11-28T11:49:40Z,2,"I ran into bunch of different errors while following this, and after an hour of searching, i made it work: import bs4 as bs import datetime as dt import os import pandas_datareader.data as web import pickle import requests import fix_yahoo_finance as yf   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         mapping = str.maketrans(""."",""-"")         ticker = ticker.translate(mapping)         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)         print(tickers)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2000, 1, 1)     end = dt.datetime(2016, 12, 31)     for ticker in tickers:         # just in case your connection breaks, we'd like to save our progress!         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):              try:                 df = yf.download(ticker, start=start, end=end)                 df.reset_index(inplace=True)                 df.set_index(""Date"", inplace=True)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             except ValueError:                     pass         else:             print('Already have {}'.format(ticker))   get_data_from_yahoo()  Keep in mind if you're gonna follow this code, you first need to install fixyahoo in cmd: pip install fix_yahoo_finance --upgrade --no-cache-dir",True
@shivajishinde1496,2018-11-05T11:29:38Z,0,"I have never seen such teaching with great energy so far. you are amazing!!!.  God Bless You! I tried same code but getting this error. "" FileExistsError(17, 'Cannot create a file when that file already exists')"". Could you please help me in this?",True
@simonromano,2018-10-31T16:27:48Z,2,"For some reason I am getting a KeyError: 'Date' when the function gets to the EVRG ticker does anyone have any idea why???  Also @stendex, awesome videos",True
@dr.mikeybee,2018-10-24T20:08:09Z,0,You need a quandl account and api key.,True
@dr.mikeybee,2018-10-24T20:07:27Z,0,"Also, note that the column names are different from Quandl with all lower case letters and underscores.",True
@alpharomeo401,2018-10-09T04:31:56Z,38,"Sentdex is one of a kind, really appreciate his lessons. For the new people like me, I had to make a few changes in order to make script functional:   On terminal:   pip install fix-yahoo-finance  Add the following to your import modules list:  import fix_yahoo_finance as yf  ""from pandas_datareader import data as pdr"" instead of ""import pandas_datareader.data as web""  yf.pdr_override()  Use: ""df = pdr.get_data_yahoo(ticker, start, end)"" instead of ""df = web.DataReader(ticker, 'yahoo', start, end)""  Add:   ticker = ""str(ticker).replace('.','-')"" right before the ""tickers.append(ticker)""",True
@zo62,2018-10-06T21:39:27Z,0,"anyone else getting error Traceback (most recent call last):    File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\pandas\core\indexes\base.py"", line 3078, in get_loc      return self._engine.get_loc(key)    File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc    File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc    File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item    File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item  KeyError: 'Date'",True
@robertloop7847,2018-09-21T07:01:31Z,2,holy hell I can't wait to try this shit out.... I just need to learn basic programming all of a sudden.,True
@fbmemar,2018-09-13T03:30:06Z,3,"Anybody knows why I'm getting this error after running the get_data_from_yahoo()? It happens in the middle of syncing tickers. ... Already have BAX BBT Already have BBT BDX Already have BDX BRK.B Traceback (most recent call last):   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\indexes\base.py"", line 3078, in get_loc     return self._engine.get_loc(key)   File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""C:\Files\Projects\Python\SP500 Webscraping\SP500 Scraping yahoo.py"", line 55, in <module>     get_data_from_yahoo()   File ""C:\Files\Projects\Python\SP500 Webscraping\SP500 Scraping yahoo.py"", line 50, in get_data_from_yahoo     df = web.DataReader(ticker,'yahoo',start,end) #query google for historical data   File ""C:\Python\Python37-32\lib\site-packages\pandas_datareader\data.py"", line 311, in DataReader     session=session).read()   File ""C:\Python\Python37-32\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Python\Python37-32\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 142, in _read_one_data     to_datetime(prices['Date'], unit='s').dt.date)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\frame.py"", line 2688, in _getitem_     return self._getitem_column(key)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\frame.py"", line 2695, in _getitem_column     return self._get_item_cache(key)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\generic.py"", line 2489, in _get_item_cache     values = self._data.get(item)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\internals.py"", line 4115, in get     loc = self.items.get_loc(item)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\indexes\base.py"", line 3080, in get_loc     return self._engine.get_loc(self._maybe_cast_indexer(key))   File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'",True
@petealwayslovesu,2018-08-31T06:54:46Z,2,"import bs4 as bs4 import pickle import requests import datetime as datetime import os import pandas as pandas pandas.core.common.is_list_like = pandas.api.types.is_list_like import pandas_datareader.data as data   # save standard & poor's 500 ticker list  def save_sp500_tickers():     response = requests.get(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")     soup = bs4.BeautifulSoup(response.text, ""lxml"")     table = soup.find(""table"", {""class"": ""wikitable sortable""})     tickers = []     for row in table.findAll(""tr"")[1:]:         ticker = row.findAll(""td"")[0].text         tickers.append(ticker)      with open(""sp500tickers.pickle"", ""wb"") as file:         pickle.dump(tickers, file)      print(tickers)      return tickers   # save_sp500_tickers()   def morning_star():      if not os.path.exists(""stock_data_frames""):         os.makedirs(""stock_data_frames"")      with open(""sp500tickers.pickle"", ""rb"") as file:         tickers = pickle.load(file)      start = datetime.datetime(2000, 1, 1)     end = datetime.datetime.now()      count = 0     count_e = 0     no_such_tickers = []     for ticker in tickers:         count += 1         print(str(count) + "". "" + ticker)          if not os.path.exists(""stock_data_frames/{}.csv"".format(ticker)):             while True:                 try:                     data_frame = data.DataReader(ticker, ""morningstar"", start, end)                      if str(data_frame.head()):                         data_frame.to_csv(""stock_data_frames/{}.csv"".format(ticker))                         count_e = 0                         break                 except Exception as e:                     count_e += 1                     print(str(count_e) + "", "" + ticker + "" "" + str(e))                      # no such tickers in morningstar                     if count_e >= 100:                         no_such_tickers.append(ticker)                         print(no_such_tickers)                         count_e = 0                         break         else:             print(""already have {}"".format(ticker))   morning_star()",True
@liangyumin9405,2018-08-23T14:24:59Z,0,yahoo api has been blocked...,True
@brk2719,2018-08-12T12:06:56Z,0,"Hey @sentdex, Kindly help with the below error , as I  am stuck.   EOFError                                  Traceback (most recent call last) <ipython-input-34-f0737bfbf47c> in <module>()      21             print ('Already have {}'.format(ticker))      22  ---> 23 get_data_from_yahoo()  <ipython-input-34-f0737bfbf47c> in get_data_from_quandl(reload_sp500)       5     else:       6         with open ('sp500tickers.pickle','rb') as f: ----> 7             tickers = pickle.load(f)       8        9     if not os.path.exists('stock_dfs'):  EOFError: Ran out of input  Code: def get_data_from_yahoo(reload_sp500=False):         if  reload_sp500:         tickers = save_sp500_tickers()     else:         with open ('sp500tickers.pickle','rb') as f:             tickers = pickle.load(f)          if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')              start = dt.datetime(2000,1,1)     end = dt.datetime(2016,12,31)          for ticker in tickers:         print (ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = web.DataReader(ticker,'quandl',start,end)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print ('Already have {}'.format(ticker))  get_data_from_yahoo()   Thanks in advance",True
@melancholybrocoli9868,2018-08-05T18:29:41Z,6,I wish I found this sooner. I have watched 5 already. Thanks so much for uploading this stuff man.,True
@Daniel-to5jd,2018-08-02T00:38:27Z,1,"yahoo api doesn't work, and for some reason neither morningstar worked for me today, so i used ""robinghood"":  df = web.DataReader(ticker, 'robinhood', start, end) and it worked  http://pandas-datareader.readthedocs.io/en/latest/remote_data.html",True
@peterjosepharienza2330,2018-07-13T20:31:13Z,0,Why are my files in notepad and not CSV although I used to_csv,True
@step7steveX,2018-07-12T10:56:15Z,0,"Sendex, thank you for your tutorials. very useful!  I am using python 3.6 and pandas datareader 0.6.0. / when fetching csv form morningstar API it seams like I am getting throttled, I followed your advise and added a time.sleep(0.5) but I am still stuck.. do you have any other ways or api data sources that  could overcome this problem ? thank you!",True
@samg7247,2018-07-02T00:29:49Z,0,"Can someone maybe explain to me why I am having issues with panda and datareader? I was able to recreate the code from the last part with no issues but from this video, it is giving me issues with lines (in order of traceback) 5, 2, 14, and 1. At the bottom of the error it says Importerror: cannot import name 'is_list_like_' I am trying to use morning star data as I have noticed comments saying there are issues with google and yahoo APIs",True
@farhadshadmand5785,2018-06-25T14:13:42Z,0,"give me error that kernel is dead, what should I do?",True
@daitavan297,2018-05-23T09:36:26Z,0,I can not used Yahoo Finance API to get the data. Could anyone tell me the solution. Or share with me the data set.  Thank you very much.,True
@jackkoh6174,2018-05-04T10:04:07Z,0,"I'm having problem with ""if reload_s&p500: "", it shows ""undefined name"". I'm using python 3.6. Do I need to import library to use it?",True
@mgrotheer,2018-05-03T15:26:06Z,2,So when I run the script the program sorta stalls out at ANDV. I hadn't seen anyone else run into this issue. I included a one second delay with time.sleep() and reran it and still have the same issue. So I'm able to get 44 of the companies and that's it. I didn't see anyone else mention this specific issue...any ideas?,True
@adeshmallaHQ,2018-04-19T06:58:03Z,1,"Hi I believe the yahoo and google finance API does not work anymore, and quandl API has some kind of problem, has any one tried the morningstar API for all of the pricing data? I tried and I got stuck on some tickers ANDV in particular, don't know whats the problems, it gets stuck there and doesn't return any response.  Hope to get some help, thanks in Advance",True
@ruonaizuagbala3948,2018-03-29T12:40:03Z,1,"I tried using the google API as of 3/29/2018 and I got errors. Try using the quandl API instead.  Working Code.  import bs4 as bs import pickle import requests import datetime as dt import os import pandas as pd import pandas_datareader.data as web  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, ""lxml"")     table = soup.find('table', {'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:             ticker = row.findAll('td')[0].text             tickers.append(ticker)      with open(""sp500tickers.pickle"", ""wb"") as f:             pickle.dump(tickers, f)      print(tickers)      return tickers  #save_sp500_tickers()  def get_data_from_quandl(reload_sp500=True):      if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)      if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2000,1,1)     end = dt.datetime(2016,12,31)      for ticker in tickers[:10]:         try:             print(ticker)             if not os.path.exists('stocks_dfs/{}.csv'.format(ticker)):                 df = web.DataReader(ticker, 'quandl', start, end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             else:                 print('Already have {}'.format(ticker))         except:             print('Cannot obtain data for ' +ticker)              get_data_from_quandl()",True
@Fiddelush,2018-03-26T06:39:18Z,0,"Got the error ""FileNotFoundError: [Errno 2] No such file or directory: 'stocks_dfs/MMM.csv'"" when using quandl as a API. Dont know how to come around this issue..",True
@Lu_Ca,2018-02-20T20:11:28Z,0,NameError: name 'tickers' is not defined ANYONE ?,True
@nikifoxy69,2018-01-30T16:19:49Z,0,Don't know where i am going wrong. please help. below is the error statement  Unable to read URL: https://query1.finance.yahoo.com/v7/finance/download/ALGN?period2=1483208999&events=history&crumb=7HJis7F6lr4&period1=946665000&interval=1d,True
@eshwark5153,2018-01-29T14:42:49Z,0,"Hi  I am New for python i have copied same code but i am getting below error  BRK.B Traceback (most recent call last):   File ""G:\Data Analysis\into one DF 7.py"", line 46, in <module>     get_data_from_yahoo()   File ""G:\Data Analysis\into one DF 7.py"", line 41, in get_data_from_yahoo     df = web.DataReader(ticker,'yahoo',start,end)   File ""C:\python\lib\site-packages\pandas_datareader\data.py"", line 121, in DataReader     session=session).read()   File ""C:\python\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 115, in read     df = super(YahooDailyReader, self).read()   File ""C:\python\lib\site-packages\pandas_datareader\base.py"", line 181, in read     params=self._get_params(self.symbols))   File ""C:\python\lib\site-packages\pandas_datareader\base.py"", line 79, in _read_one_data     out = self._read_url_as_StringIO(url, params=params)   File ""C:\python\lib\site-packages\pandas_datareader\base.py"", line 90, in _read_url_as_StringIO     response = self._get_response(url, params=params)   File ""C:\python\lib\site-packages\pandas_datareader\base.py"", line 139, in _get_response     raise RemoteDataError('Unable to read URL: {0}'.format(url)) pandas_datareader._utils.RemoteDataError: Unable to read URL: https://query1.finance.yahoo.com/v7/finance/download/BRK.B?period1=946665000&period2=1517077799&interval=1d&events=history&crumb=Jds5htuRRQ9",True
@ThePRANAVGAUR,2018-01-17T14:05:41Z,0,I am getting an error after downloading around 10 companies data. what to do please help.,True
@nikolezhang583,2018-01-09T11:25:50Z,0,"hey thank you for the awesome video, appreciate you sharing all this knowledge with everyone. I have a question if you or any of the fellows here can help me with. When I run to grab the SP500 historical price files from Yahoo, I often get error messages for Bad Yahoo URL links, just wondering if anyone else is experiencing this error, and how do you guys deal with it? thanks!!!",True
@user-yl4om9ho5b,2018-01-01T13:51:46Z,0,"for me, inorder to get all the 500 tickets, I have to run the same code again and again. The strange thing is that for the same code, some ticket can not be generated for the first time running, but when i run the second time, it can be generated",True
@joerich10,2017-12-20T04:03:23Z,0,"I'm also getting the following error, after successfully loading the first 7 ticker csvs..  RemoteDataError: Unable to read URL: http://www.google.com/finance/historical?q=ADBE&startdate=Jan+01%2C+2000&enddate=Dec+01%2C+2017&output=csv.   I've tried the solutions below but to no avail. Has anyone else here overcome this issue?",True
@beansgoya,2017-11-30T03:37:27Z,0,anyone else getting this error when running get_data_from_google() or from_yahoo()?  UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 42445: invalid start byte,True
@MrJuvette,2017-11-13T21:03:34Z,3,"here is the fully working code to extract FTSE 100 tickers from wikipedia: import bs4 as bs  import datetime as dt  import os import pandas as pd  import pandas_datareader.data as web import pickle import requests  def save_ftse100_tickers():  resp = requests.get('https://en.wikipedia.org/wiki/FTSE_100_Index')  soup = bs.BeautifulSoup(resp.text, 'lxml')  table = soup.find('table', {'class':'wikitable sortable'})  tickers = []  for row in table.findAll('tr')[1:]:   ticker = row.findAll('td')[1].text   tickers.append(ticker)    if ticker.find(""_""):     ticker = ticker.replace(""_"",""-"")    print(""trying to read modified {}"".format(ticker))   with open('ftse100tickers.pickle','wb') as f:   pickle.dump(tickers, f)   print(tickers)   return tickers  #save_ftse100_tickers()  def get_data_from_google(reload_ftse100=False):  if reload_ftse100:   tickers = save_ftse100_tickers()  else:   with open('ftse100tickers.pickle','rb') as f:    tickers = pickle.load(f)   if not os.path.exists('stock_dfs'):   os.makedirs('stock_dfs')   start = dt.datetime(2016,1,4)  end = dt.datetime(2017,11,10)   for ticker in tickers:   if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):    df = web.DataReader(ticker, 'google', start, end)    df.to_csv('stock_dfs/{}.csv'.format(ticker))   else:    print('Already have {}'.format(ticker))   get_data_from_google()",True
@jiaxiangding1386,2017-11-06T11:48:27Z,1,"I am using Jupyter notebook and I am getting an error like below:   EOFError                                  Traceback (most recent call last) <ipython-input-4-49419fb70966> in <module>()      47             print('Alreader have {}'.format(ticker))      48  ---> 49 get_data_from_yahoo()      50       51   <ipython-input-4-49419fb70966> in get_data_from_yahoo(reload_sp500)      32     else:      33         with open(""sp500tickers.pickle"",""rb"") as f: ---> 34             tickers = pickle.load(f)      35     if not os.path.exists('stock_dfs'):      36         os.makedirs('stock_dfs')  EOFError: Ran out of input",True
@primetime6504,2017-10-21T20:22:49Z,0,"why is it not working?  import bs4 as bs import pickle import requests         def save_sp500_tickers():     resp = requests.get(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")     soup = bs.BeautifulSoup(resp.text, ""lxml"")     table = soup.find(""table"", {""class"": ""wikitable sortable""})     tickers = []     for row in table.findAll(""tr"")[1:]:         ticker = row.findAll(""td"")[0].text         tickers.append(ticker)         with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)         print(tickers)         return tickers         def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)         if not os.path.exists(""stock_dfs""):         os.makedirs(""stock_dfs"")         start = dt.datetime(2000, 1, 6)     end = dt.datetime(2016, 12, 5)         for ticker in tickers:         print(ticker)         if not os.path.exists(""stock_dfs/{}.csv"".format(ticker)):             df = web.DataReader(ticker, ""yahoo"", start, end)             df.to_csv(""stock_dfs/{}.csv"".format(ticker))         else:             print(""Already have{}"".format(ticker))     get_data_from_yahoo()",True
@danidin77,2017-10-14T19:07:51Z,0,But I get 'pandas_datareader._utils.RemoteDataError: Unable to read URL' after a few stocks each time...:( What can be done ???,True
@kunomels,2017-10-10T14:43:16Z,2,"since Yahoo is working for some tickers and google for some others, I made someting like this :   for ticker in tickers:         if not os.path.exists(""stock_dfs/{}.csv"".format(ticker)):             #df = web.DataReader(ticker,""quandl"",start,end)             try:                 df = web.get_data_yahoo(ticker,start,end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             except:                  print('Cannot obtain data for ' +ticker+"" with yahoo"")                 print(""Trying with google : "")                 try :                     df = web.DataReader(ticker,""google"",start,end)                     df.to_csv('stock_dfs/{}.csv'.format(ticker))                 except:                     print(""still not working ! added to notworking pickle"")                                      bad_tickers.append(ticker)                     with open(""notworking.pickle"",""wb"") as u:                         pickle.dump(bad_tickers,u)                          else:             print('Already have {}'.format(ticker))  and then you can try to get the tickers in the notwoking pickle with different methods (like with quandl or alpha_vantage apis)",True
@uditvashisht878,2017-10-07T03:49:22Z,0,"Hi, I have question that if, i fetch the data daily to create the CSV, will it just append the data of the current date on the already downloaded CSV file or will it replace the CSV file with new CSV having data upto the current date. And if it is not appending the data and replacing the data with new data, is there any way to append it ?",True
@dpoort64,2017-10-01T04:29:25Z,0,"I am downloading S&P500 historical data. I get about 3 downloaded and then get the following error: RemoteDataError: Unable to read URL: https://query1.finance.yahoo.com/v7/finance/download/ALLE?period1=1262322000&period2=1506830399&interval=1d&events=history&crumb=I97QkZ%5Cu002FgwCi  I re-run - and I can usually get 3, 4, or 5 more, but not more. I have made it as far as the ""B"" companies.  Any thoughts?  Thanks  in advance",True
@RYAN-ww2ql,2017-09-27T13:42:29Z,0,"Run the get_data_from_yahoo function, but is not working.  Pls help.  10x Traceback (most recent call last):   File ""C:/Users/Mom/AppData/Local/Programs/Python/Python36/getting pricing data in SP500.py"", line 49, in <module>     get_data_from_yahoo()",True
@peterkoppelman6232,2017-09-13T22:13:50Z,3,"I was getting a lot of errors so I changed save_sp500_tickers. Right before the append statement I added the code ticker = ticker.replace('.','-').strip(). This seemed to fix the problems with wikipedia using a ""."" instead of a ""-"" and I no longer had issues with any tickers greater than 3 characters. Then I changed get_data_from_yahoo. I put df = web.DataReader(ticker,'yahoo', start, end) inside a try statement with the error trap (except clause) checking for RemoteDataError.  The first time that I ran everything through get_data_from _yahoo 79 tickers went to the error trap. The second time I ran get_data_from_yahoo, the code there were 11 errors (the code caught 68 of the 79 tickers). The third time there  were 2 errors (the code caught 9 or 11 tickers). The fourth time there was 1 error. The ticker symbol that still fails is FTV.  I can't figure out why the code works some of the time and not all of the time. It bothers me that it's not consistent. I'm using the same list each time. Is this a problem with yahoo or the code? If it's a problem with yahoo, is there a better source of data?",True
@Nickiziboy,2017-09-12T18:01:38Z,9,"For anyone that's struggling - here's the entire code that bypasses LMT using google and a try/except:  import bs4 as bs import pickle import requests import datetime as dt import os import pandas as pd import pandas_datareader.data as web  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, ""lxml"")     table = soup.find('table', {'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:             ticker = row.findAll('td')[0].text             tickers.append(ticker)      with open(""sp500tickers.pickle"", ""wb"") as f:             pickle.dump(tickers, f)      print(tickers)      return tickers  save_sp500_tickers()  def get_data_from_google(reload_sp500=True):      if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)      if not os.path.exists('stock_dfs'):          os.makedirs('stock_dfs')      start = dt.datetime(2000,1,1)     end = dt.datetime(2016,12,31)      for ticker in tickers:         try:             print(ticker)             if not os.path.exists('stocks_dfs/{}.csv'.format(ticker)):                 df = web.DataReader(ticker, 'google', start, end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             else:                 print('Already have {}'.format(ticker))         except:             print('Cannot obtain data for ' +ticker)  get_data_from_google()",True
@liruixue403,2017-09-11T00:29:17Z,0,"HI Sentdex, very powerful introduction!  i do have a question though: when I first tried to download the stock prices, there are around 100 stocks that showed error message. i used try/except to handle the error and save the failed tickers to a list and rerun the same function for the failed tickers and this time it worked for another 88 tickers.  i had to redo the failed tickers again and again but still fail to get the data for the following 4:['BRK.B', 'BF.B', 'DWDP', 'STI'].... any suggestions?",True
@epicmeeltime,2017-09-10T20:36:23Z,2,I keep getting this error:  pandas_datareader._utils.RemoteDataError: Unable to read URL: http://www.google.com/finance/historical?q=MMM&startdate=Jan+01%2C+2010&enddate=Dec+31%2C+2016&output=csv  Does anyone know the problem? I am not using yahoo because 9/10 times it spits out an error,True
@one_lettersandnumbers,2017-08-29T21:03:01Z,2,"I get this error all the time from yahoo: raise RemoteDataError('Unable to read URL: {0}'.format(url)) pandas_datareader._utils.RemoteDataError: Unable to read URL: https://query1.finance.yahoo.com/v7/finance/download/AME?period1=946681200&period2=1503871199&interval=1d&events=history&crumb=By7%5Cu002F2Uk16xG  I get a few tickers (usually just one), and then it throws the error again. Anyone has any idea how to fix it?",True
@saadahmed9239,2017-08-13T07:00:21Z,0,"I was able to fix the yahoo api issue by  looking at the comments but I have one more problem When I download the stock data most of the files in Stock Df's folder are 1Kb in size, why is it like this any suggestions?",True
@kennyPAGC,2017-07-17T15:41:30Z,1,how come the sp500 table has more than 500 entries? 505 to be exact,True
@hans6973,2017-07-05T20:52:46Z,1,"This should work downloading all the tickets data:(instead of 288 with limitations) import datetime as dt import os import pandas as pd from pandas_datareader import data as pdr import fix_yahoo_finance  df = pd.read_html(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")[0] df.columns = df.ix[0] df.drop(df.index[0], inplace=True) tickers = df['Ticker symbol'].tolist()   def get_data_from_yahoo():     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start = dt.datetime(2000, 1, 1)     end = dt.datetime(2017, 6, 29)      for ticker in tickers:         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             list = pdr.get_data_yahoo(ticker, start, end)             list.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))  get_data_from_yahoo()",True
@hans6973,2017-06-30T17:02:24Z,0,"why i only get S.csv, T.csv, Z.csv in my stock_dfs folfer?",True
@bjornnilsson4648,2017-06-30T09:04:50Z,2,"Hi, i get this error message:     raise RemoteDataError('Unable to read URL: {0}'.format(url)) pandas_datareader._utils.RemoteDataError: Unable to read URL: http://www.google.com/finance/historical?q=HUM&startdate=Jan+01%2C+2000&enddate=Dec+31%2C+2016&output=csv  I get the same typ of error message when I try to pull data from omxs30 list. What could be the problem?",True
@JeremAl,2017-06-28T14:33:42Z,0,"I had the EOF error with pickle. So I switched in the FIRST function: pickle.dump(tickers, f, protocol=pickle.HIGHEST_PROTOCOL)  I erased the .pickle file and rewrote it with that method.  Then it loaded ok.  Also I used google instead of Yahoo which gave me error in the previous videos and here too:  df = web.DataReader(ticker, 'google', start, end)",True
@daitavan297,2017-06-28T06:23:37Z,0,"Thanks very much for your great lecture video. Anyone got the same problem. I just changed ''yahoo"" by ""google"" but only got 288 tickets instead of 500.",True
@c-spam9581,2017-06-15T10:58:46Z,0,The Yahoo api is no longer working. The Google API is but doesn't provide an adjusted close. Do you have any suggestions on how I can move forward with this tutorial?,True
@PEPOSOPREYES,2017-06-14T18:19:01Z,9,"Hey newbie here. For some reason, I get hung up on LMT stock. I get the next error:  ""pandas_datareader._utils.RemoteDataError: Unable to read URL"" I tried running an exception block but with no result.  I am not sure how to resolve this please let me know if someone else is experiencing †he same thing and has a solution.  Cheers !",True
@xuxutravel,2017-06-06T10:34:46Z,0,Is there any solution for the yahoo finance disable ichart function?,True
@asivolobov,2017-06-05T15:29:16Z,25,"Yahoo changed API so there are some runtime errors in current version of code. To repair:  1. All information is here: https://pypi.python.org/pypi/fix-yahoo-finance  2. Install fix_yahoo_finance using pip: $ pip install fix_yahoo_finance --upgrade --no-cache-dir  3. Import fix_yahoo_finance into your code (add this at top of your file after ""import pandas_datareader.data as web""): import fix_yahoo_finance   4. Change a line with 'yahoo' string to:             df = web.get_data_yahoo(ticker, start, end)",True
@adamgelaw1382,2017-05-29T02:42:09Z,6,Any body else get held up on LMT stock using google? And how would I fix it . Any,True
@ZakharovInvest,2017-05-21T15:20:30Z,0,"a little update. Whoever said to add  if ticker.find(""_""):             ticker = ticker.replace(""_"",""-"")             print(""trying to read modified {}"".format(ticker))  he was right. But also try to pull the data from google instead of yahoo. it may work for you perfectly. This worked for my case",True
@thomaselder4076,2017-05-09T06:58:39Z,0,1:31 Aren't you worried about cascading failures if you ever have to go back and make changes.,True
@mattkilner1669,2017-05-07T12:56:43Z,2,"Hi all, can someone please help, I'm stuck in this lesson when trying to run all this code, it gives me the error: Traceback (most recent call last):   File ""/Users/Matt/Documents/Python Sets/Finance list.py"", line 49, in <module>     get_data_from_yahoo()   File ""/Users/Matt/Documents/Python Sets/Finance list.py"", line 33, in get_data_from_yahoo     tickers = pickle.load(f) EOFError: Ran out of input  I've tried a few solutions I found around the internet but none seem to work.",True
@philipbein3586,2017-04-25T02:53:34Z,0,"Hey I am having the same ""Ran out of imput"" issue others seem to be having, does anyone have a solution?",True
@sheldonlavis,2017-03-30T22:41:42Z,1,"hey sendex i am getting the following error, do you know why? on a mac thank yoU!!  EOFError                                  Traceback (most recent call last) <ipython-input-2-184aeee33af5> in <module>()      38             print('Already have {}'.format(ticker))      39  ---> 40 get_data_from_yahoo()  <ipython-input-2-184aeee33af5> in get_data_from_yahoo(reload_sp500)      22     else:      23         with open(""sp500tickers.pickle"",""rb"") as f: ---> 24             tickers = pickle.load(f)      25       26     if not os.path.exists('stock_dfs'):  EOFError: Ran out of input",True
@alainl3887,2017-03-25T13:44:40Z,0,"Hi Sentdex, what you are doing for the community is really incredible. I've been watching you videos all week. I'm not new to programming but I'm new to Python and I've been ripping of my head for a day or two to do something that I could have coded in 5 min in VB back in the days.   How can I update the data every day (append the csv files) with only missing bars (daily in this case) and not trigger full load every day.  I saw Alex Rodgers code in the comments here https://github.com/AlexanderRodgers/stocks/blob/master/finance_for_python.py  I tried it (without understanding it fully) and it doesn't seem to work.   That would be great if I could run this script once a day or a week to only append the missing data to the csv files.  Thank you",True
@nunosilvestre3197,2017-03-11T09:40:09Z,0,"Hi!. For tickers with a dot ""."" things go bad. How can one get around it?",True
@GodsNode,2017-03-05T12:02:00Z,0,checked my code accuracy to the videos like 5 times and I still receive 6 errors and only one ticker output. Anyone have this repo'd on git?,True
@gogytdk6215,2017-03-05T07:18:43Z,1,"This is what I get when i copy paste final code   Traceback (most recent call last):   File ""finance4.py"", line 48, in <module>     get_data_from_yahoo()   File ""finance4.py"", line 32, in get_data_from_yahoo     tickers = pickle.load(f)   File ""/usr/lib/python2.7/pickle.py"", line 1384, in load     return Unpickler(file).load()   File ""/usr/lib/python2.7/pickle.py"", line 864, in load     dispatch[key](self)   File ""/usr/lib/python2.7/pickle.py"", line 886, in load_eof     raise EOFError EOFError",True
@abluntdaily,2017-02-17T03:09:24Z,0,if I rerun the function again will it create duplicate files or continue where it left off? its taking a really long time so I think I might be getting throttled. On my last attempt I was able to get 60 files but then I deleted the folder and it started right up again when I ran the function. I have 256 files now so I don't want to delete the folder but I also don't want to rerun the function if it will create duplicate files. Its been about an hour that I am stuck at 256 files,True
@bigfoothines,2017-02-09T14:24:55Z,4,"Hey Sentdex, when pulling my data from Yahoo, an error occurred wherein one of the tickers (cant remember which one) wouldn't load from its URL on Yahoo I changed to google api in the web.DataReader(ticker, ""yahoo"", start, end), which then ran into difficulties with Endo International plc, so I then changed back to yahoo which got the job done.... any ideas why certain stocks wouldn't pull? got the data now but just curious Thanks P.S awesome tutorials... having started just a few weeks ago on python i've been incessantly watching your channel and I think maybe now I at least have some idea about Python!",True
@zcrib3,2017-02-09T08:12:32Z,68,"Wikipedia uses ""."" instead of ""-"" in their list. Had to translate ""."" to ""-"" so it would get past Berkshire Hathaway. Just for anyone running into this.",True
@eal1,2017-02-07T11:57:20Z,0,"Why as f: What is f and what are ""df s"" what does it stand for, stock 'data fields""?",True
@lacunasports134,2017-02-04T11:05:21Z,1,Awesome series - super coaching for us absolute noobs! I have a CSV for all of the Australian tickers and just want to replace the pickle to call the CSV list - can someone please point me in the right direction?,True
@savirien4266,2017-01-24T20:00:33Z,4,"Working on trading software myself and slowly going through your new series when I have time.  It's nice to see someone actually cover the fundamentals of financial computing.  It's also nice to validate some things are on the right track, at least for my own program.  I had one suggestion though, why use pickle?  Why not place your data into some flavor of an SQL database?  Then you can design your functions to simply update that information as time goes on.  This way would benefit us later on as our program becomes multi-threaded.  Anywho, thank you for these videos!",True
@tahalv85,2017-01-23T04:19:59Z,0,"Great video (and series).  Quick question: is it possible to pull back other historical metrics, e.g. trailing-12 Earnings per Share, through pandas?",True
@AndersGustafsson87,2017-01-22T22:04:54Z,1,excellent stuff!,True
@kuatroka,2017-01-20T22:31:48Z,1,Great knowledge sharing! thanks.  Throwing two ideas for topics: 1. How to update the data every day with only missing bars (daily in this case) and not trigger full load every day. 2. How to use TA-lib library for example MACDEXT and use it as filter for the stock screener.,True
@skmn07,2017-01-20T16:20:02Z,0,When would u be releasing the further videos in this series,True
@samwisegamgee5184,2017-01-19T17:22:57Z,0,"Hey Sentdex, how big is this series , i mean what is the extent of it. How many more videos are there in this series?",True
@mr.youtube7195,2017-01-18T16:25:56Z,15,"Sentdex! Can you do a tutorial on using google api with python? Uploading docs, sheets, etc. You have the best Python tutorials  by far",True
@boratarhan,2017-01-18T06:16:57Z,0,"Harrison thank you for all your great contributions. Are you planning to extend this tutorial series to real time data? Getting tick data, updating last ohlc data, visualizing it, saving the data in a db? It would be great if you could either share your opinion about how to efficiently do this or put together a demo.",True
@tranngochiep4673,2017-01-18T03:45:11Z,0,Thanks,True
@porlando12,2017-01-18T03:12:02Z,0,It would be interesting to compare these historical stock data against google finance's api.   Do you know of an api that provides more metrics than yahoo's ohlc values?,True
@porlando12,2017-01-18T01:38:56Z,7,Watching the stock_dfs directory fill up was the most satisfying moment of all time.  Great video!,True
@femaledeer,2017-01-17T23:05:21Z,0,It would be interesting to see how to load closing prices into to a SQL database and append the database every day with the new closing price.,True
@audacious1876,2017-01-17T21:46:04Z,0,Great video and Channel too! I'm so glad to you for your explanation!,True
@pmunin,2017-01-17T21:30:39Z,7,"Sentdex, How come you still don't have an autonomous self-training neural network trading for you fulltime? :) Looks like you have all the tools for that.",True
@realking4918,2017-01-17T19:18:54Z,1,"Great video series, love it!",True
