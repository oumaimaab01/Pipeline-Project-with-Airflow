author,updated_at,like_count,text,public
@mrpedrao02,2024-01-05T14:14:09Z,0,I actually got pretty happy when you said I'm a robot lol. I've a bachelor in physics btw. Usually I've hard times with the code,True
@derencakr9450,2023-09-08T07:25:25Z,0,It was really bored :/,True
@ayushshukla6823,2022-01-10T19:08:47Z,0,shouldn't it be 2.w/||w|| instead of 2/||w|| at  (4:47) ?,True
@rajendransp8074,2021-06-13T11:55:02Z,0,Great dude,True
@mrfrozen97-despicable,2021-01-26T08:43:49Z,0,I think I get it. But I don't get it. Life sucks,True
@viniciusVS8v,2020-10-10T20:28:39Z,0,continuo sem entender porra nenhuma,True
@skillmonks.intern235,2020-09-16T14:04:07Z,0,Major Skills You Need to Master Machine Learning and Deep Learning. Read Full Blog Article https://blog.skillmonks.com/deep-tech/major-skills-you-need-to-master-machine-learning-and-deep-learning/,True
@edilgin622,2020-08-25T15:23:37Z,0,IIWII why did that turn into W when we applied derivative?,True
@anilkumar-cu6we,2020-08-24T16:30:33Z,1,Thank me later! - https://youtu.be/efR1C6CvhmE  beautiful graphical  illustration for svm.,True
@sanjivgautam9063,2020-06-29T14:41:48Z,0,"b is never maximized.  Correct me if I am wrong, but we use Langrange so that our objective function (which is to maximize the margin (m), which is same as minimizing w), with our constraint. So W and b just play the role of being parameters for the hyperplane. So our job is to find W and b, keeping the constraints. So when we train them, we have W and b, and for any unknown, if W.Unknown + b >=0 , we say it is positive and for <=0, we say it is negative. I am not sure why you said b is something we need to maximize, but I see it and W being just the parameters for hyperplane.",True
@meetmerchant4733,2020-06-05T19:32:52Z,2,"he had me at ""we, the scientist""",True
@bhavyar2986,2020-01-17T17:28:38Z,0,isn't width = (2/||w||) * w(bar) ??? what's the value of w(bar) ?,True
@bidhanmajhi,2019-08-14T10:06:40Z,0,"Anyone who's confused, do watch multiple sources.",True
@harrypapadopoulos8428,2019-07-16T10:53:19Z,2,Let's have a play :)   Cast: Me (14 years old greek teen that want to learn ML and DL ) Tutor (sentdex) Brain (my brain)   Me: ok lest see until p21 it was ok lest see the rest  -watches part 22- Tutor = ok and so we do that that that I am a wizard blah blah blah Me: ... my brain:; working.exe stopped responding Me: :/ -Now I came with a math dictionary),True
@focker0000,2019-03-27T15:00:20Z,1,it's not very clear why you need to maximize b,True
@madmuffin2511,2019-02-28T17:25:19Z,0,"I would say I get this from the maths point of view, but... maybe not.  How come minimizing norm ||w|| achieves anything ? Say I have the ""best"" separating plane, then my classification wouldnt change if I just change the length of the normal vector, because it is still facing in the same direction, so it does not change the plane at all? While writing I notice that it actually does change the plane. Usually you define planes with a normalized nomal vector (so unit lentgh), so the shortest distance Origin-Plane would be ""b"". But now it is ""b / ||w||"". ?",True
@rafaelsofi-zada1850,2019-02-10T12:43:52Z,3,"SVM has a ton of challenging (at least for me as a 1st-year undegrad) but very interesting maths, and I believe it needs to be studied in-depth to understand everything that is going on. I found Sentdex' explanations insufficient, so I looked up and the best tutorial for math behind SVMs is https://www.svm-tutorial.com/svm-tutorial/ There is also a free e-book from the same author on the same website, and all I can say is it's absolutely amazing.",True
@CoolDude911,2019-02-08T10:22:12Z,0,"You assume there is an equal number of of positive instances to negative instances around 4:00.    This works out in the Lagrangian of the problem as we can derive that there is equal total ""cost"" or importance placed on each class.  That is to say the bias is selected so that sum y_i * alpha_i = 0. w is selected so that alpha_i * (( x_i * w + b ) y_i - 1 ) = 0) (*).    We minimise w so that we get the smallest possible scaling of the axis ( x_i * w + b ) while maintaining a fixed separation between the axes of 1. It is equivalent to ask for a large separation but keep ||w|| fixed. Which is basically what (*) says.",True
@hillbct,2019-01-30T12:31:54Z,1,though I was following step by step but seems i missed somewhere and now its all over. Can I skip this and come back later to this portion,True
@Anas-gu2qg,2019-01-05T08:31:13Z,77,"If  you're having a hard time following the math on this video, MIT OCW lecture about SVM is very helpful and does a great job explaining the mathematical concepts and formulas. Things should make sense after you watch this.  However, before delving into that, I advise you to learn the basics about Lagrange multiplier in order to understand the reason why and how it was put to use in the calculations. I put the two links to the videos below for you. Links: OCW lecture : https://www.youtube.com/watch?v=_PwhiWxHK8o Lagrange multiplier : https://www.youtube.com/watch?v=yuqB-d5MjZA",True
@ashishsharma612,2018-12-17T19:05:13Z,1,12:25 where this negative come from,True
@aravamuthanl2715,2018-11-13T20:14:33Z,0,"@sentdex, Great video really helped me at the exact time",True
@leshamokhov,2018-11-05T06:39:14Z,3,"If  you're confused go check out MIT OCW lecture about SVM. It's the same math formulas covered in MIT lecture, but it is more clear, I can say.",True
@TheOlderSoldier,2018-08-15T23:44:58Z,1,"Can you think of what model would be best for predicting the next answer in a coin toss that is generated by a basic PRNG? I get that this wouldn't be possible with a true coin toss as each toss has no bearing on the previous, but PRNGs might show a pattern over time. I have data from 3000 sequential coin tosses.",True
@kingking-bv3vh,2018-07-25T03:12:00Z,11,"I took 1 hour to figure out a 15 minutes tutorial, finally",True
@SanketPatole,2018-07-14T14:39:07Z,1,"Isn't the projection of vector u on vector w = u.w/||w||  ? Why did you consider only the dot product?  If we consider ||w|| also in the equation, Value of width comes to be exactly 2 (which is actually true since we have assumed length of support vector from decision boundary is exactly one). How can we find extremes of this width (2) ?  Am I interpreting something wrong? Please help.   Also why did you take sum of constraint function instead of just the constraint function in the Lagrangian?",True
@thesilenthero422,2018-07-13T00:01:03Z,2,"Everything after 6:40 flew over my head, moved too fast. These principles deserve their own video.",True
@tormentedbacon4573,2018-07-03T01:50:42Z,48,Nobody told me there would be math.,True
@kristiansmith5298,2018-06-28T18:06:24Z,0,You've lost me - too much maths I don't understand,True
@AladdinPersson,2018-06-21T15:58:11Z,19,"I find your explanation pretty good, but I don't understand how you can jump to Lagrange multipliers and not even mention that most of you probably won't understand this. I mean you had a video when you explained what norm of a vector and the dot product is, giving the idea that you can follow these videos without knowledge of calculus / linear algebra",True
@austinhaider105,2018-06-16T20:48:19Z,65,There's a knock on the door... it's Lagrange and he brought his multipliers ðŸ˜‚,True
@guyitayev6359,2018-05-17T14:00:09Z,1,how is it different from regression(best fit line)?,True
@chandeepadissanayake6975,2018-04-02T19:33:32Z,2,"Someone without the math knowledge to this series may stop at the end of KNN and find some other tutorials teaching you the math. I really dislike the people who follow something without any intuition about what they are following. Still I hate when you say ""we know that"".. It's totally ridiculous to assume that your audience know something prior to anything you do, even without mentioning about the prerequisites at least in the description. DISLIKED",True
@Adaministrator,2018-03-24T02:47:45Z,0,I had trouble understanding the maximising values step and found this explanation a helpful: https://www.youtube.com/watch?v=9tPHHEAP9io,True
@mini_frank,2018-03-19T07:47:48Z,0,"if you guys find the math part confusing, here's a link that helped me, just remember dot product is the projection of the first vector onto the unit length second vector, norm(a)*norm(b)*cos(theta): https://www.cs.utexas.edu/~mooney/cs391L/slides/svm.ppt lagranian multiplier:https://en.wikipedia.org/wiki/Lagrange_multiplier",True
@ArminXtosis,2018-03-14T17:45:34Z,4,PLAGIARISM !!!!!!!!,True
@manojagarwal3441,2018-01-10T20:35:58Z,0,hi i have a doubt my doubt is that after taking the differentiation of Lagrangian equation with respect to w and b how does it lead to a quadratic problem  can you please explain the equation at 11: 42 second and why do we need to maximize this,True
@erichexin3637,2018-01-06T23:59:48Z,1,maybe the theory introduction is more clear in this open course https://www.youtube.com/watch?v=_PwhiWxHK8o&t=23s,True
@TheMrArtmaster,2017-11-21T22:48:04Z,1,For those looking to get a better grasp of the mathematics behind svm's I would look at this series of articles. It gives a very thorough explanation of how svm's work. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/,True
@jadetan6330,2017-10-24T07:33:29Z,9,im a uni student doing machine learning for my final year project and your videos have really helped me ALOT. More than my teachers could've taught me as this is an independent project. Thank you sooooooo much!!!!!!!,True
@juleshenri8117,2017-09-17T11:08:46Z,3,"A little too slow sometimes so it gets really boring, and in contrast sometimes too quick and without enough mathematical explanations. But still helpful :)",True
@ReddSpark,2017-08-13T09:29:05Z,1,What does x_i and x_j represent?,True
@aashishpaudel9178,2017-07-07T17:25:57Z,2,"You said you took philosophy and criminology as majors in college....If so, where did you get to learn these stuffs???",True
@JacobSmithodc,2017-06-01T19:42:02Z,111,If you're not lost or confused . . . you're a robot.,True
@nevermore7755,2017-05-31T03:59:01Z,6,5:38 Can anyone explain why would you want to minimize 1/2||W||^2? And how did he derive that? He just kind of mentioned it like it was trivial...,True
@ZarreenNaowalReza,2017-05-23T11:21:33Z,13,Why do we need to maximize b?,True
@_musk7407,2017-04-13T06:56:45Z,2,Very much hope to have Chinese subtitles,True
@saurabhkhodake,2017-03-14T13:12:54Z,4,Bouncer :/,True
@prakharmishra6697,2017-01-15T09:15:29Z,79,"I believe SVM needs a better mathematical explanation to be understood properly. If any of you guys need further help with SVM or want to revisit the math, I would suggest you check out Professor Patrick Winston's SVM lecture on the MIT Open Courseware YouTube channel. Its pretty helpful.",True
@theycallmemorphine,2016-12-23T06:19:35Z,2,"Wow, I am a robot.. WOW.. I could understand it easily for the first time as I have done a course on OR Optimization.. It's easy for me. . others be like Â¯\_(ãƒ„) _/Â¯",True
@xoxUnD3R0aThxox,2016-11-19T12:02:57Z,5,"At 11 minutes, u  throw that L at us, u didn't explain where it comes from, could u please elaborate?",True
@ericsheetz,2016-10-05T12:47:59Z,10,@6:30 - 6:38. Lol :-),True
@XWRIATAKOS,2016-09-22T11:12:06Z,0,"what exactly is ai,ay? you say it is Lagrange multiplier but how this come from?",True
