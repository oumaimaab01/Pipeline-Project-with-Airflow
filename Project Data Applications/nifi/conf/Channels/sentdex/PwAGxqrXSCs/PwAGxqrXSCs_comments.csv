author,updated_at,like_count,text,public
@sentdex,2017-02-17T13:45:31Z,227,"Here are some updates to the code to support TensorFlow version 1.0:  def train_neural_network(x):     prediction = neural_network_model(x)     # OLD VERSION:     #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )     # NEW:     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )     optimizer = tf.train.AdamOptimizer().minimize(cost)          hm_epochs = 10     with tf.Session() as sess:         # OLD:         #sess.run(tf.initialize_all_variables())         # NEW:         sess.run(tf.global_variables_initializer())",True
@presente9501,2023-10-06T00:32:34Z,1,"The full code in TensorFlow v2: import tensorflow as tf import numpy as np import matplotlib.pyplot as plt  # Load data (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()  # Normalize data x_train = tf.keras.utils.normalize(x_train, axis=1) x_test = tf.keras.utils.normalize(x_test, axis=1)  # Build model model = tf.keras.models.Sequential() model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) model.add(tf.keras.layers.Dense(units=128, activation=tf.nn.relu)) model.add(tf.keras.layers.Dense(units=128, activation=tf.nn.relu)) model.add(tf.keras.layers.Dense(units=10, activation=tf.nn.softmax))  # Compile model model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Train model model.fit(x_train, y_train, epochs=3)  # Evaluate model val_loss, val_acc = model.evaluate(x_test, y_test) print(""val_loss : "", val_loss) print(""val_acc : "", val_acc)",True
@Faisal-jo5vk,2022-07-19T20:36:59Z,0,"it didnt work for me, this code is outdated, can someone  give new version ?",True
@qw4316,2020-12-26T18:00:30Z,3,"Hi，Dear I got error when I ran the code :  for _ in range(int(mnist.train.num_examples/batch_size)):AttributeError: 'dict' object has no attribute 'train'",True
@mrfrozen97-despicable,2020-12-21T17:14:50Z,0,Compiled first time... Xdxdxd That never happens with me.... Thanks )))),True
@akif6233,2020-07-27T13:33:53Z,1,i didnt understand we are starting with train_neural_network(x)  and first process is prediction= neural_network_model(x) but we didnt feed x it is just placeholder and has no data? how its gonna work?,True
@mtahirrasheed5538,2020-07-23T08:56:34Z,0,I have some suggestions. I really like your videos that's why i want to give suggestions. 1) a lot of material is available on internet related to classification and mnist. But you should work more on customize data. 2) image segmentation related video 3) Video on which API is best for convolutional network. Because selection of API is also difficult for me. 4) Comparison of writing some code in both tensorflow 1.x and 2.x. What are the differences in between two of them. because a lot of things have changed.,True
@anonymosranger4759,2020-02-27T11:23:53Z,1,Where and how did you learn machine learning,True
@VascoCC95,2019-12-07T19:13:31Z,0,"Hey everyone! Does anybody know how to stop Tensorflow from printing every single step to the screen??? I've filled the code with lots of ""verbose=False"" and commented out all the print's but it still insists on slowing the hell out of my computer just for posting every little step it makes... So annoying!!!",True
@strawhatdragon100,2019-12-02T11:36:35Z,3,Is this tutorial relevant from a coding point of view as many things have changed in tensorflow 2.0?,True
@aldendelvecchio5123,2019-12-01T01:28:38Z,2,downloading tensorflow no longer comes with an examples folder with the mnist data set,True
@eastwoodsamuel4,2019-10-22T05:41:28Z,0,Is it okay to not understand anything the first time I watch this video?,True
@fregenelazarus9946,2019-09-20T20:43:27Z,0,Thank you so much for your presentation and the deep insight for beginners like me. I'II will appreciate practicing with your codes. Please my email: floritseexcel@gmail.com,True
@ATXpert,2019-08-17T13:38:22Z,0,where do we save the weight and biases?,True
@xxMpEGxx,2019-05-28T22:06:48Z,1,"could someone explain to me what the functions in these 2 lines exactly do? correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct, 'float'))",True
@nano7586,2019-05-22T14:36:56Z,0,"It really is amazing how you can just throw in data and it ""magically"" (as you say it a lot :D) decreases the loss function so you get good predictions (95% is crazy if you think about it).",True
@hamidsadeghi3248,2019-05-21T12:19:39Z,0,"I got this error. can someone help me please?  Traceback (most recent call last):     mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)",True
@mplm28,2019-05-10T11:19:05Z,0,how we can run model for one test data and get predication like what you do in tflearn tutorial (model.predict),True
@gracemalcom7358,2019-05-07T18:01:45Z,11,"If sentdex has errors, it makes me feel less depressed when I get errors. Thank your for all your hard work. Your tutorials are helping me help the local law enforcement I work for.  Thank you for all you do!",True
@Hirotaka93,2019-04-24T16:16:14Z,0,can you create a version where you import data from a csv file?,True
@neerajgarg6481,2019-03-09T19:41:07Z,0,I am getting an error in:  optimizer = tf.train.AdamOptimizer.minimize(loss = cost)  it syas:  minimize() missing 1 required positional argument: 'self' can anyone help fix this?,True
@jmanh128,2019-03-09T14:30:01Z,0,"When I do it, it doesn't train nearly as accurately :/",True
@eturkoz,2019-02-22T06:31:11Z,0,"I like the reaction you made after you said, ""please don't give me an error"" at 13:44 :D Thank you for the videos. That reaction was ""epoch"" :D",True
@igs8949,2019-02-17T12:33:08Z,0,lol first time coding with tf but i didnt make that mistake wiht the brackets,True
@nadaalqahtani2341,2019-02-03T16:22:44Z,0,thanks man,True
@jamesbarker6373,2019-01-27T03:13:06Z,1,How can we test the program? It claims 95% accuracy but I can't see anything. How can we use this  in the real world? (Sorry if this is explained later),True
@stacksonchain9320,2019-01-21T06:03:49Z,0,is it epics or epOchs?,True
@ubaidmanzoorwani7491,2019-01-20T15:41:33Z,0,N Layer Neural Network in TensorFlow Using The Same Dataset https://github.com/Ubaid-Manzoor/HandWritten-Digit-Recognization/blob/master/Mnist.ipynb,True
@nr9885,2019-01-17T23:04:06Z,0,why no softax(output) ?,True
@shawnstafford9664,2019-01-15T03:58:37Z,0,prob figured it out but to make text smaller in Linux is ctrl - and to make it larger is ctrl shift +,True
@attyuttamsaha8,2019-01-12T15:04:04Z,0,"here you perform epoch_loss += c , what is c basically ??",True
@attyuttamsaha8,2019-01-09T11:18:13Z,2,we call the train_neural_network(x) with the parameter x but x has still not been loaded with any data ! are we just passing the placeholder ?,True
@AhmedIsam,2019-01-06T10:34:15Z,0,"A couple of suspicious practices. (I'm learning from you for the first time, but they are glaring!) First: @ 0:26 it shows the neural_network_model that you defined in the earlier video. You're defining the hidden layers each time this model is called! You need to take those lines out of the model.  I  did it, it worked perfectly.  Second: @ 13:27 line 70, you're passing x to the function. It is absolutely meaningless. x is just a tensor, a tensorflow variable, it has no value at the moment unless you run the session. Why do would you pass it? Well, because you when you defined the function train_neural_network, the definition included some use of x, thus, Python will yell at you unless declare it, or, pass it as you did. The best thing is just declare it inside the function.  So, the lines: x=tf.Variable('float') y=tf.Variable('float') need to be moved from their current useless location to inside the aforementioned function. And you don't need to pass anything. (I did it, it worked) Thank you.",True
@MoinBayern,2018-12-27T11:49:39Z,0,I don't understand how we can have a loss in the range of 1.7 million (1st Epoch) while only using a set of 55000 images as testing data. What does that loss number actually mean?,True
@hehuang3374,2018-12-25T06:11:28Z,2,I have local mnist file and I have X_train and y_train ready for using.  Is there any function to replace the .next_batch()?,True
@martykump8180,2018-12-23T03:07:08Z,0,"Sentdex, the variable y is not defined locally in, or passed to, the function train_neural_network.  Am I right to assume the line y = tf.placeholder... is effectively a Global then?  I'm getting an error on the line cost = tf.reduce_mean(...), even after I changed to epoch_x and epoch_y.",True
@naq8405,2018-12-21T05:31:52Z,0,Is this code done by Tensorflow or created by you?,True
@vishnusumanth350,2018-12-10T15:03:16Z,0,Hey how to save and restore trained model,True
@SudeepDasguptaiginition,2018-11-27T20:05:48Z,1,And every one is copying code from every where i just subscribed three channels in udemy and guess what same source code is shown as example line by line exactly not even variable or constant name is different hahaha LOL,True
@Hslifelearner,2018-11-24T01:51:59Z,1,Is this code available on github?,True
@ramreddy6115,2018-11-19T08:30:37Z,0,How to save this model??,True
@mosesarthur9743,2018-10-24T09:28:54Z,0,"I have install tensorflow in my computer, when i run the above codes..i am getting this error -     from tensorflow.examples.tutorials.mnist import input_data ModuleNotFoundError: No module named 'tensorflow.examples'; 'tensorflow' is not a package  #so went into sitepackages and checked if tensorflow examples is present - it is present. but it gives the above error. any suggestion ?",True
@jamesbarker6373,2018-10-24T04:33:42Z,0,"Silly question.., but how can we test the program has learned the hand drawn numbers? It just claims it has 95% accuracy.., but no details on what, lol",True
@atrumluminarium,2018-10-16T21:21:11Z,0,No output activation?,True
@aabhishek4911,2018-10-08T15:59:21Z,0,Isnt the hidden_1_layer actually the input layer ?,True
@puthenrohit,2018-10-08T07:34:01Z,3,"In the new version use: softmax_cross_entropy_with_logits_v2  Like, tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y)  Or the old, softmax_cross_entropy_with_logits  Like, tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)  Using logits and labels is necessary else you'll have an error.",True
@jeremyliu4602,2018-10-01T08:32:47Z,0,Hi! This tutorial is so so useful! Is it all possible you can send me the source code & dataset? --> liuleny@outlook.com  Thx so much!,True
@lee_land_y69,2018-09-27T07:20:38Z,0,"Hey, thanks. Can you please do a function of back-propagation of error and weights update? Instead of using tensorflow magic. That would really help clear how dnns work.",True
@desmondcodes93,2018-09-24T09:55:31Z,0,"Isn't one epoch equivalent to going through the whole dataset one time? Meaning if the dataset contains 10 datapoint, after feedforward and backprop for all 10 of them, it is considered as 1 epoch. Please correct me if im wrong",True
@tijsp.8162,2018-09-23T09:53:00Z,1,"After 30 epochs, I got an accuracy of 0.9685  Edit: after going fullon YOLO and raising the epochs to 100, I achieved an accuracy of 0.9757",True
@zahidulislam9812,2018-09-21T18:14:48Z,0,hey sentdex will you please make a video on speech recognition and chatbot manually,True
@kingmoni1,2018-09-18T09:37:01Z,0,working code as of 18/9/2018 https://github.com/humayuntanwar/100-days-of-Machine-Learning/blob/master/Deep%20Learning/Deepnet-day18-19.py,True
@linsthankachan5506,2018-09-14T17:18:33Z,0,"i didn't know why i'm getting this errors ...i'm a noobie....so please help me...  --------------------------------------------------------------------------- NameError                                 Traceback (most recent call last) <ipython-input-1-7c80e09e37bd> in <module>()      15     hidden_layer_3 = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}      16     output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),'biases':tf.Variable(tf.random_normal([n_classes]))} ---> 17 l1 = tf.add(tf.matmul(data,hidden_layer_1['weights']) , hidden_layer_1['biases'])      18 l1 = tf.nn.relu(l1)      19 l2 = tf.add(tf.matmul(l1,hidden_layer_2['weights']) , hidden_layer_2['biases'])  NameError: name 'data' is not defined",True
@toobaafreen3976,2018-09-14T04:25:46Z,0,"My data is in a csv file, all floats. What function should I use to train the data? Will data.train.next_batch(batch_size)  work, where data is a numpy array?",True
@toobaafreen3976,2018-09-14T04:25:43Z,0,"My data is in a csv file, all floats. What function should I use to train the data? Will data.train.next_batch(batch_size)  work, where data is a numpy array?",True
@chuanjiang6931,2018-09-06T03:00:38Z,1,"Hi，man. Good video, I got it work and it seems like some functions are deprecated. How should we know how to update the code?  import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  minst = input_data.read_data_sets(""/tmp/data/"", one_hot=True)  n_nodes_hl1 = 500 n_nodes_hl2 = 500 n_nodes_hl3 = 500  n_class = 10 batch_size = 200  x = tf.placeholder('float', [None, 784]) y = tf.placeholder('float')   def neural_network_model(data):     hidden_1_layer = {         'weight': tf.Variable(tf.random_normal([784, n_nodes_hl1])),         'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))     }      hidden_2_layer = {         'weight': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),         'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))     }      hidden_3_layer = {         'weight': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),         'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))     }      output_layer = {         'weight': tf.Variable(tf.random_normal([n_nodes_hl3, n_class])),         'biases': tf.Variable(tf.random_normal([n_class]))     }      l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['biases'])     l1 = tf.nn.relu(l1)      l2 = tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['biases'])     l2 = tf.nn.relu(l2)      l3 = tf.add(tf.matmul(l2, hidden_3_layer['weight']), hidden_3_layer['biases'])     l3 = tf.nn.relu(l3)      output = tf.matmul(l3, output_layer['weight']) + output_layer['biases']     return output  def train_neural_network(x):     prediction = neural_network_model(x)     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))     optimizer = tf.train.AdamOptimizer().minimize(cost)     hm_epochs = 100      with tf.Session() as sess:         sess.run(tf.global_variables_initializer())          for epoch in range(hm_epochs):             epoch_loss = 0             for _ in range(int(minst.train.num_examples/batch_size)):                 epoch_x, epoch_y = minst.train.next_batch(batch_size)                 _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})                 epoch_loss += c             print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)          correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))         print('Accuracy: ', accuracy.eval({x: minst.test.images, y: minst.test.labels}))   train_neural_network(x)",True
@dineshchoudhary6122,2018-09-04T11:09:53Z,0,I am getting epoch loss as 0.00 for every iteration.... Why is this so?,True
@Naufal331,2018-09-02T14:06:44Z,0,"Hello,  I'm getting error   TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.  based on feed_dict = {x: x, y: y}  Pls help!",True
@ngsamuel5385,2018-08-28T09:47:20Z,0,What do the values of the loss signify? The values shown in the video seem too big.,True
@dilanpeiris3034,2018-08-24T04:48:24Z,0,"plz explain whts this error mean   return _eval_using_default_session(self, feed_dict, self.graph, session)   File ""C:\Users\Admin\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\framework\ops.py"", line 4937, in _eval_using_default_session     raise ValueError(""Cannot evaluate tensor using `eval()`: No default "" ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",True
@dilanpeiris3034,2018-08-23T08:30:40Z,0,how to dwnload mnist data sets??,True
@dilanpeiris3034,2018-08-21T05:21:09Z,0,syntax error in l1 = tf.nn.relu(l1),True
@icebrennan,2018-08-19T23:30:30Z,1,"Is it strange that I'm getting 95% accuracy for just training this network for 3-5 epochs, whilst getting 18% accuracy for training this network for 5000 times. Shouldn't training the network for longer, yield better accuracies. I don't know what other people are getting, when they try out this network.",True
@patrickadjei9676,2018-08-13T06:01:28Z,0,"After sess.run(tf.initialize_all_variables()), Could someone give a thorough explanation as to what is happening? I know I'm late but I need to understand please Ty.",True
@anurag6674,2018-07-31T04:10:45Z,0,"My model is trained and tested on test data ! Now how should i apply to real data , like image or something?",True
@piethelderop8085,2018-07-24T22:03:40Z,0,"Thank you so much for your spices time, it works fine. only a few stupid type errors from my side.",True
@vivekssj3,2018-07-24T13:15:45Z,0,"can anyone explain what last line of code is doing? print(""accuracy"",accuracy.eval({x:mnist.test.images,y:mnist.test.labels})) like it seems like it is calculating calculating accuracy only on those 10,000 testing data sets and ldk i am a bit confused my only query is there should be something prediction which should be compared to y test labels!!",True
@pritamitsyou,2018-07-21T14:55:46Z,0,"Am getting invalid argument error: InvalidArgumentError                      Traceback (most recent call last) C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)    1566   try: -> 1567     c_op = c_api.TF_FinishOperation(op_desc)    1568   except errors.InvalidArgumentError as e:  InvalidArgumentError: Shapes must be equal rank, but are 2 and 1  From merging shape 0 with other shapes. for 'softmax_cross_entropy_with_logits_sg_6/logits' (op: 'Pack') with input shapes: [?,10], [10].  During handling of the above exception, another exception occurred:  ValueError                                Traceback (most recent call last) <ipython-input-60-7c7cbdae9b34> in <module>() ----> 1 train_neural_network(x)  <ipython-input-59-94c20b7ce93e> in train_neural_network(x)      28 def train_neural_network(x):      29     logits = neural_network_model(x) ---> 30     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y,logits = logits))      31     optimizer = tf.train.AdamOptimizer().minimize(cost)      32     hm_epochs = 10  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py in new_func(*args, **kwargs)     248               'in a future version' if date is None else ('after %s' % date),     249               instructions) --> 250       return func(*args, **kwargs)     251     return tf_decorator.make_decorator(     252         func, new_func, 'deprecated',  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\ops\nn_ops.py in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)    1957     1958   return softmax_cross_entropy_with_logits_v2( -> 1959       labels=labels, logits=logits, dim=dim, name=name)    1960     1961   C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\ops\nn_ops.py in softmax_cross_entropy_with_logits_v2(_sentinel, labels, logits, dim, name)    1835   with ops.name_scope(name, ""softmax_cross_entropy_with_logits"",    1836                       [logits, labels]) as name: -> 1837     logits = ops.convert_to_tensor(logits, name=""logits"")    1838     labels = ops.convert_to_tensor(labels, name=""labels"")    1839     convert_to_float32 = (  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)    1012       name=name,    1013       preferred_dtype=preferred_dtype, -> 1014       as_ref=False)    1015     1016   C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)    1102     1103     if ret is None: -> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    1105     1106     if ret is NotImplemented:  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)    1032   if dtype is not None and dtype != inferred_dtype:    1033     return NotImplemented -> 1034   return _autopacking_helper(v, inferred_dtype, name or ""packed"")    1035     1036   C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)     995           elems_as_tensors.append(     996               constant_op.constant(elem, dtype=dtype, name=str(i))) --> 997       return gen_array_ops.pack(elems_as_tensors, name=scope)     998     else:     999       return converted_elems  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py in pack(values, axis, name)    4514     axis = _execute.make_int(axis, ""axis"")    4515     _, _, _op = _op_def_lib._apply_op_helper( -> 4516         ""Pack"", values=values, axis=axis, name=name)    4517     _result = _op.outputs[:]    4518     _inputs_flat = _op.inputs  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)     785         op = g.create_op(op_type_name, inputs, output_types, name=scope,     786                          input_types=input_types, attrs=attr_protos, --> 787                          op_def=op_def)     788       return output_structure, op_def.is_stateful, op     789   C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)    3390           input_types=input_types,    3391           original_op=self._default_original_op, -> 3392           op_def=op_def)    3393     3394       # Note: shapes are lazily computed with the C API enabled.  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)    1732           op_def, inputs, node_def.attr)    1733       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs, -> 1734                                 control_input_ops)    1735     else:    1736       self._c_op = None  C:\Users\welcome\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)    1568   except errors.InvalidArgumentError as e:    1569     # Convert to ValueError for backwards compatibility. -> 1570     raise ValueError(str(e))    1571     1572   return c_op  ValueError: Shapes must be equal rank, but are 2 and 1  From merging shape 0 with other shapes. for 'softmax_cross_entropy_with_logits_sg_6/logits' (op: 'Pack') with input shapes: [?,10], [10].   pls help",True
@srijalshrestha7380,2018-07-20T02:49:24Z,0,Can i find your code in GIThub?,True
@abdkumar1300,2018-07-18T18:00:25Z,0,"sir do videos on Exploratory data analysis, how to get insights from data,  data preprocessing. it would help a lot thank you",True
@sebastianairaksinen1816,2018-07-17T07:08:16Z,0,I have this type of problem any suggestions what to do? read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models.,True
@sehaba9531,2018-07-10T11:31:06Z,1,There is a lot of modifications on Tensorflow :/,True
@kw_cooper,2018-07-06T06:36:19Z,0,"did anyone ever figure out the for loop, or even better a way to set the network hyperparameters at the start without having to hardcode in the network structure?",True
@subratkumarsahoo4849,2018-07-04T09:27:17Z,0,Without understanding of TensorFlow  these tutorial will be no use. First need to understand TensorFlow all functions and there uses correctly then only you can apply.,True
@kw_cooper,2018-07-04T04:55:03Z,0,for recent updates look at https://stackoverflow.com/questions/45835292/shapes-must-be-equal-rank-but-are-2-and-1/47824480 as well as possibly downgrading your tensorflow version,True
@vishnuyadav4-yrb.tech.chem245,2018-06-20T06:52:44Z,0,could you please tell me that how to predict the concentration of co2 everywhere in the room if we have the values for co2 concentration at the centre of the room after every 30 seconds in a day from 8am to 6 pm.If possible please suggest me the approach which ca do this,True
@derekkneisel90,2018-06-19T14:59:48Z,1,"I have the updated code for June 2018. There are quite a few warnings since TensorFlow was updated but the code compiles and works for me. Here is a link to the code, let me know if you want to be a contributor: https://github.com/Derek-Kneisel/MNIST-Deep-Neural-Network",True
@shubhambhattacharjee6715,2018-06-19T04:56:24Z,0,How would I write my own next_batch()  for a csv dataset ?,True
@rev_pheonix7254,2018-06-16T19:36:14Z,0,If you use anaconda for coding you can get rid of those syntax errors.,True
@moilui1664,2018-06-15T14:20:21Z,0,"i don't know how i got that, but it just happened: good luck to solve it...  Warning (from warnings module):   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py"", line 36     from ._conv import register_converters as _register_converters FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. WARNING:tensorflow:From /Users/olivierpartensky/Desktop/Programme/GitHub/python_projects/Deep learning Sendex 3.py:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From /Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please write your own downloading logic. WARNING:tensorflow:From /Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please use urllib or similar directly. Traceback (most recent call last):   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1318, in do_open     encode_chunked=req.has_header('Transfer-encoding'))   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1239, in request     self._send_request(method, url, body, headers, encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1285, in _send_request     self.endheaders(body, encode_chunked=encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1234, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1026, in _send_output     self.send(msg)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 964, in send     self.connect()   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1400, in connect     server_hostname=server_hostname)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 407, in wrap_socket     _context=self, _session=session)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 814, in __init__     self.do_handshake()   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 1068, in do_handshake     self._sslobj.do_handshake()   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 689, in do_handshake     self._sslobj.do_handshake() ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""/Users/olivierpartensky/Desktop/Programme/GitHub/python_projects/Deep learning Sendex 3.py"", line 4, in <module>     mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func     return func(*args, **kwargs)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 260, in read_data_sets     source_url + TRAIN_IMAGES)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func     return func(*args, **kwargs)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 252, in maybe_download     temp_file_name, _ = urlretrieve_with_retry(source_url)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func     return func(*args, **kwargs)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 205, in wrapped_fn     return fn(*args, **kwargs)   File ""/Users/olivierpartensky/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 233, in urlretrieve_with_retry     return urllib.request.urlretrieve(url, filename)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 248, in urlretrieve     with contextlib.closing(urlopen(url, data)) as fp:   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen     return opener.open(url, data, timeout)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 526, in open     response = self._open(req, data)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 544, in _open     '_open', req)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain     result = func(*args)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1361, in https_open     context=self._context, check_hostname=self._check_hostname)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1320, in do_open     raise URLError(err) urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)> >>>",True
@nuusain996,2018-06-14T11:49:16Z,0,Super basic question; How would I go about plotting the cost value over time/training iterations. I get that that I need to make a list of costs with the remainder function but Im struggling to see how I'd populate that list.,True
@kotaprolucharan4032,2018-06-14T11:19:35Z,0,Can anyone say me what is the function of tf.reduce_normal(),True
@abcdxx1059,2018-06-12T18:17:55Z,0,I was so frustrated but in the end its worth it don't give it up to the errors you may be a idiot but remember your not a loser,True
@kotaprolucharan4032,2018-06-12T07:20:48Z,0,"WARNING:tensorflow:From deepneural.py:19: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From /home/kotaprolumanisai/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please write your own downloading logic. WARNING:tensorflow:From /home/kotaprolumanisai/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting /tmp/data/train-images-idx3-ubyte.gz WARNING:tensorflow:From /home/kotaprolumanisai/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting /tmp/data/train-labels-idx1-ubyte.gz WARNING:tensorflow:From /home/kotaprolumanisai/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.one_hot on tensors. Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz WARNING:tensorflow:From /home/kotaprolumanisai/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From deepneural.py:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating:  Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default.  See @{tf.nn.softmax_cross_entropy_with_logits_v2}.  Traceback (most recent call last):   File ""deepneural.py"", line 110, in <module>     train_neural_network(x)   File ""deepneural.py"", line 85, in train_neural_network     optimizer = tf.train.AdamsOptimizer().minimize(cost) AttributeError: module 'tensorflow.tools.api.generator.api.train' has no attribute 'AdamsOptimizer'  CAN YOU PLEASE FIGURE OUT THE ERRORS FOR ME",True
@Elzelgator,2018-06-09T09:13:43Z,0,FUck this Udemy ads,True
@sayyamjain1607,2018-06-05T21:18:13Z,0,Achieved more than 99% using GradientDescentOptimizer,True
@_Nibi,2018-06-02T22:46:06Z,0,"Has anyone else gotten this error, i can't figure out how it fix it:  Expected bool for argument 'transpose_a' not <tf.Variable 'Variable_17:0' shape=(500,) dtype=float32_ref>.",True
@hrithikraj4277,2018-06-01T06:21:33Z,0,"UnboundLocalError                         Traceback (most recent call last) <ipython-input-14-0b1339788380> in <module>()      47         print('accuracy',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))      48  ---> 49 train_neural_network(x)  <ipython-input-14-0b1339788380> in train_neural_network(x)      29 def train_neural_network(x):      30     prediction=neural_network_model(x) ---> 31     cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))      32     optimizer=tf.train.AdamOptimizer().minimize(cost)      33     epochs=10  UnboundLocalError: local variable 'y' referenced before assignment getting this error ..can anybody help me out?",True
@manankumawat8364,2018-05-30T13:49:15Z,2,how you remember that much :o,True
@GURPARKASH06,2018-05-25T16:08:39Z,0,Can somebody tell me why the output of the model function is just x*w+b and not g(x*w+b)? where g is a sigmoid or softmax function,True
@keerthana7777,2018-05-22T07:06:27Z,0,"Is this what is meant by building your network from scratch? in research, can we say that this is building the network from scratch,?",True
@andrew1257,2018-05-22T02:53:30Z,0,"For my hidden_2_layer, I keep getting an indentation error, even though it's indented the same way as the first layer and it's identical to the code you wrote. I'm used to Java so these indentation issues are quite new to me...",True
@lukasmrvecka1275,2018-05-09T20:48:41Z,0,Nice tutorial. Code works fine but I have problem because when I run it after 3 or 4 epochs program stop working and restart pc. Spend hours searching for problem and got nothing. Could someone help?,True
@aishasiddiqa8717,2018-05-04T15:16:43Z,0,If I want to save this model and then test it using my own image.. how exactly can I do that?,True
@TheOfficialJeppezon,2018-05-04T14:28:15Z,0,ran it 100 times got 97.47 this is so fun :D,True
@klaatu77,2018-04-26T13:42:59Z,0,Great tutorial series. I am definitely a fan of TensorFlow now. I was pleasantly surprised by the depth of discussion of the code and the actual coding which makes it so much easier to follow along. Keep up the great work.,True
@potatohater5599,2018-04-22T15:32:36Z,0,"Traceback (most recent call last):   File ""E:/Ermia/Projects/Artificial Intelligence/Main.py"", line 62, in <module>     train_neural_network(x)   File ""E:/Ermia/Projects/Artificial Intelligence/Main.py"", line 42, in train_neural_network     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))   File ""E:\Ermia\Projects\Artificial Intelligence\venv\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func     return func(*args, **kwargs)   File ""E:\Ermia\Projects\Artificial Intelligence\venv\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1954, in softmax_cross_entropy_with_logits     logits)   File ""E:\Ermia\Projects\Artificial Intelligence\venv\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1779, in _ensure_xent_args     ""named arguments (labels=..., logits=..., ...)"" % name) ValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)  help?",True
@tanmaysingh8562,2018-04-22T10:40:16Z,5,"How to use the model which we have created? How to give any input from 0-9 as images and check if it predicting correct or not, how to do that?",True
@sgrouge,2018-04-19T18:08:18Z,0,"Im a bit consudef between pythons' variables and tensorflows' variables, I understand tf variables beyond to the model and pythons variables are used at a high level programmatic. Im also confused with the optimiser I understand it can make the link with the weights and biases because the output is linked to them but not sure.",True
@OGMasterPiece,2018-04-17T20:23:43Z,0,"Always had a good taste for the algorithm but it is time to move on to machine learning. Idea itself is very exciting and that's the whole reason i will give a shot, just because as you train it, you watch it performing actions no human algorithm (hand coded) can manage and figure it's task by just itself. Even it comes with the ideas of it's own which are better then humans for instance in board games.",True
@KS-fs1nl,2018-04-14T19:36:15Z,0,I do not know why nobody wants to play with some other dataset :( Everyone is doing it on MNIST,True
@CoolDude911,2018-04-12T21:32:32Z,0,"Thank you for your help. I especially like who you've put the updated version of the tensor flow code. I have managed to get your code to work with arbitrarily many layers (at least it seems to give the same results). Basically using a dictionary for the weightings and a list for the layers. I tried doing it with exec, but bad things happened.  hm_epochs = 10 columns_in_data = len(data[0]) n_classes = len(labels[0]) nodes = [500,500,500] n_nodes = len(nodes) batch_size = 100  def neural_network_model(data):     # define weights and biases for each hidden layer     inputs = {}     inputs['w0'] = tf.Variable(tf.random_normal([columns_in_data, nodes[0]]))     inputs['b0'] = tf.Variable(tf.random_normal([nodes[0]]))      i = 0     while (i+1) < n_nodes:         i += 1         inputs['w'+str(i)] = tf.Variable(tf.random_normal([nodes[i-1], nodes[i]]))         inputs['b'+str(i)] = tf.Variable(tf.random_normal([nodes[i]]))     i += 1     inputs['w'+str(i)] = tf.Variable(tf.random_normal([nodes[i-1],n_classes]))     inputs['b'+str(i)] = tf.Variable(tf.random_normal([n_classes]))          layers=[]     layers.append(tf.nn.relu(tf.add(tf.matmul(data, inputs['w0']),inputs['b0'])))     for i in range(1,n_nodes):         layers.append(tf.nn.relu(tf.add(tf.matmul(layers[-1], inputs['w'+str(i)]),inputs['b'+str(i)])))          output=tf.add(tf.matmul(layers[-1], inputs['w'+str(n_nodes)]),inputs['b'+str(n_nodes)])          return output",True
@Jack-dx7qb,2018-04-11T07:06:20Z,1,"Here is my code for creating a 10-hidden-layer n.n. model, with 500 nodes per hidden layer. It works well for me.   def neural_network_model(data):     hidden_layer_num = 10      hidden_node = [500]*hidden_layer_num     node = [784] + hidden_node      pre_layer = data      for i in range(len(node)-1):         hidden_layer = {         'weights':tf.Variable(tf.random_normal([node[i], node[i+1]])),         'biases':tf.Variable(tf.random_normal([node[i+1]]))}         layer = tf.add(tf.matmul(pre_layer, hidden_layer['weights']), hidden_layer['biases'])         layer = tf.nn.relu(layer)         pre_layer = layer          output_layer = {         'weights':tf.Variable(tf.random_normal([node[-1], n_classes])),         'biases':tf.Variable(tf.random_normal([n_classes]))}     output = tf.matmul(pre_layer, output_layer['weights']) + output_layer['biases']     return output",True
@sajad3320,2018-04-08T06:15:52Z,0,Perfectttttt!!!! Love your videos!!!! I recommend everyone to see these!,True
@divyanshujhawar3671,2018-04-03T12:43:17Z,1,I want to know one question. Can anyone help me? Please.,True
@AbhishekJain-zu1uf,2018-04-01T09:37:21Z,0,How can we pickle the training part in a pickle file as it takes too much time to run it even once??,True
@aaditkapoor,2018-03-28T16:59:34Z,0,Please read the fundamentals!,True
@himansuodedra2201,2018-03-28T16:08:27Z,0,why do we use tf.reduce_mean()??? line 48,True
@RabeeQiblawi,2018-03-24T19:19:27Z,0,how to use gradient decent optimizer instead of Adam ?,True
@nilsaha8021,2018-03-22T21:04:36Z,0,"When I am calculating input_data with weights and adding biases (my codes are exactly similar as mentioned  l1, l2, and l3. Even though the data is defined in the previous step neural_network_model (data),  I am getting an error in the l1 step that name 'data' is not defined. Any suggestion will be appreciated.",True
@retatzyandast5093,2018-03-21T03:56:16Z,0,"Hi,  I am runnning the exact  same code and i am getting this error:  (py2018) C:\Users\Ted\Anaconda3\Scripts>python test_tensor_2.py   File ""test_tensor_2.py"", line 92     train_neural_network(x)                        ^ SyntaxError: invalid syntax  i know maybe i am doing somehwere a lame mistake .   import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist=input_data.read_data_sets(""/tmp/data/"", one_hot=True)  n_nodes_hl1=500 n_nodes_hl2=500 n_nodes_hl3=500  n_classes=10 batch_size=100  x=tf.placeholder('float',[None, 784])  #data y=tf.placeholder('float')              ##label of the data  def neural_network_model(data):                hidden_1_layer={'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),                     'biases':tf.Variable(tf.random_normal([n_nodes_h11]))}      hidden_2_layer={'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),                     'biases':tf.Variable(tf.random_normal([n_nodes_h12]))}       hidden_3_layer={'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),                     'biases':tf.Variable(tf.random_normal([n_nodes_h13]))}       output_layer={'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),                     'biases':tf.Variable(tf.random_normal([n_classes]))}      ##model structure for each layer      l1=tf.add(tf.matmul(data, hidden_1_layer['weights']+ hidden_1_layer['biases']))     l1=tf.nn.relu(l1)       l2=tf.add(tf.matmul(l1, hidden_2_layer['weights'], hidden_2_layer['biases']))     l2=tf.nn.relu(l2)       l3=tf.add(tf.matmul(l2, hidden_3_layer['weights'], hidden_3_layer['biases']))     l3=tf.nn.relu(l3)       output=tf.add(tf.matmul(l3, output_layer['weights']+ output_layer['biases']))  return output   def train_neural_network(x):     prediction=neural_network_model(x)      #cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_logits(prediction,y))      cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )     ##learning rate= 0.001      optimizer=tf.train.AdamOptimizer().minimize(cost)     ##cycles feedforward + backprop     hm_epochs=10      with tf.Session() as sess:         ##sess.run(tf.initialize_all_variables())         sess.run(tf.global_variables_initializer())      ##training the model                  for epoch in range(hm_epochs):             epoch_loss=0             for _ in range(int(mnist.train.num_examples/batch_size)):                 epoch_x,epoch_y=mnist.train.next_batch(batch_size)                 _,c=sess.run([optimizer,cost],feed_dict={x:epoch_x,y:epoch_y})                  epoch_loss += c              print('Epoch',epoch,'completed out of',hm_epochs,'loss',epoch_loss)     ##                   correct=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))              accuracy=tf.reduce_mean(tf.cast(correct,'float'))              print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels})                     train_neural_network(x)            please help here!",True
@joshpceeg,2018-03-18T16:39:30Z,0,Could this tutorial use a CSV imported data set and how would I replace MNIST with it?,True
@tariqkhasawneh4536,2018-03-17T19:53:20Z,0,This is honestly useless,True
@nathanburley3245,2018-03-14T02:24:51Z,0,"This code works with Anaconda 3 (Python 3.6) and TensorFlow 1.6.0:  import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)  # Number of nodes in our hidden layers num_nodes_h1 = 500 num_nodes_h2 = 500 num_nodes_h3 = 500  # Number of classes (written digits 0-9) num_classes = 10  # Batch size (feeds in this many features (images) every time) batch_size = 100  # X: Picture has shape of no height, ""width"" of 784 pixels (ensures error thrown if wrong shape passed in) # Y: label of data (just an integer) features = tf.placeholder('float', [None, 784]) label = tf.placeholder('float')  def neural_network_model(data):     # LAYERS DEFINED ###########################################################      # HIDDEN LAYER 1     # weights: shape 784 input values connected to...however many nodes in the next layer!     # biases: just added onto every node     # For every node: (input_data * weights) + biases     hidden_layer1 = {'weights':tf.Variable(tf.random_normal([784,  num_nodes_h1])),                      'biases':tf.Variable(tf.random_normal([num_nodes_h1]))}      # HIDDEN LAYER 2     # weights: shape 784 input values connected to...however many nodes in the next layer!     # biases: just added onto every node     # For every node: (input_data * weights) + biases     hidden_layer2 = {'weights':tf.Variable(tf.random_normal([num_nodes_h1, num_nodes_h2])),                      'biases':tf.Variable(tf.random_normal([num_nodes_h2]))}      # HIDDEN LAYER 3     # weights: shape 784 input values connected to...however many nodes in the next layer!     # biases: just added onto every node     # For every node: (input_data * weights) + biases     hidden_layer3 = {'weights':tf.Variable(tf.random_normal([num_nodes_h2, num_nodes_h3])),                      'biases':tf.Variable(tf.random_normal([num_nodes_h3]))}      # OUTPUT LAYER     output_layer = {'weights':tf.Variable(tf.random_normal([num_nodes_h3, num_classes])),                      'biases':tf.Variable(tf.random_normal([num_classes]))}      # RUNS THE MODEL ###########################################################      # Runs the first layer (takes in input data, multiplies by weights, adds biases, feeds into activation function)     layer_1 = tf.add(tf.matmul(data, hidden_layer1['weights']), hidden_layer1['biases'])     layer_1 = tf.nn.relu(layer_1)      # Runs the second layer (takes in first layer output, multiplies by weights, adds biases, feeds into activation function)     layer_2 = tf.add(tf.matmul(layer_1, hidden_layer2['weights']), hidden_layer2['biases'])     layer_2 = tf.nn.relu(layer_2)      # Runs the third layer (takes in second layer output, multiplies by weights, adds biases, feeds into activation function)     layer_3 = tf.add(tf.matmul(layer_2, hidden_layer3['weights']), hidden_layer3['biases'])     layer_3 = tf.nn.relu(layer_3)      # Runs the output layer     output = tf.matmul(layer_3, output_layer['weights']) + output_layer['biases']      return output  def train_neural_network(features):     prediction = neural_network_model(features)     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=label))     # Minimizes cost with an Adam Optimizer. You can also use gradient descent, and/or pass in a learning rate     optimizer = tf.train.AdamOptimizer().minimize(cost)     # Cycles of feed forward, and then backprop     num_epochs = 10      with tf.Session() as sesh:         sesh.run(tf.global_variables_initializer())          # TRAINING #############################################################         for epoch in range(num_epochs):             cost_this_epoch = 0             # Divides up the dataset (total images / number of batches)             for _ in range(int(mnist.train.num_examples/batch_size)):                 x_features, y_labels = mnist.train.next_batch(batch_size)                 _, c = sesh.run([optimizer, cost], feed_dict = {features: x_features, label: y_labels})                 cost_this_epoch += c             print(""Epoch {} completed. Loss: {}"".format(epoch, cost_this_epoch))          # TESTING- Evaluates performance of our network after training #########         correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(label, 1))         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))         print(""Accuracy: {}"".format(accuracy.eval({features:mnist.test.images, label:mnist.test.labels})))  train_neural_network(features)",True
@nathanburley3245,2018-03-14T01:56:51Z,0,"Anyone have an update for Python3 and TensorFlow 1.6? I'm getting this error:   File ""tf_practice.py"", line 97, in <module>     train_neural_network(features, label)   File ""tf_practice.py"", line 72, in train_neural_network     prediction = neural_network_model(features)   File ""tf_practice.py"", line 30, in neural_network_model     'biases':tf.Variable(tf.random_normal(num_nodes_h1))}   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py"", line 78, in random_normal     shape_tensor, dtype, seed=seed1, seed2=seed2)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py"", line 384, in _random_standard_normal     seed2=seed2, name=name)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper     op_def=op_def)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3273, in create_op     compute_device=compute_device)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3313, in _create_op_helper     set_shapes_for_outputs(op)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2501, in set_shapes_for_outputs     return _set_shapes_for_outputs(op)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2474, in _set_shapes_for_outputs     shapes = shape_func(op)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2404, in call_with_requiring     return call_cpp_shape_fn(op, require_shape_fn=True)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 627, in call_cpp_shape_fn     require_shape_fn)   File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 691, in _call_cpp_shape_fn_impl     raise ValueError(err.message) ValueError: Shape must be rank 1 but is rank 0 for 'random_normal_1/RandomStandardNormal' (op: 'RandomStandardNormal') with input shapes: [].",True
@niteshkumarsharma8305,2018-03-11T16:36:37Z,0,"Hey sentdex i am getting error 'unhashable type: 'numpy.ndarray'  on line ' _ , c = sess.run([optimizer,cost], feed_dict = { x : epoch_x , y : epoch_y}) i checked dtype for mnist.train.labels and its is float.64 and so i made sure that placeholder for y has same the dtype.But its still throwing the same error. Any advise?",True
@CasanovaKingpin,2018-03-10T09:49:41Z,0,"hey i had written a comment on your video : ""RUNNING OUR NETWORK"" please sir give a reply on that. I had confusion on line 64 please it took me very long but i haven't understood that yet. There my doubt is you reinitialized y as epoch_y and on line 64 you used y in argmax , so my doubt is , is that y or epoch_y on line 64?",True
@CasanovaKingpin,2018-03-10T09:48:35Z,0,"hey i had written a comment on your video : ""RUNNING OUR NETWORK"" please sir give a reply on that. I had confusion on line 64 please it took me very long but i haven't understood that yet. There my doubt is you reinitialized y as epoch_y and on line 64 you used y in argmax , so my doubt is , is that y or epoch_y on line 64?",True
@CasanovaKingpin,2018-03-09T21:01:03Z,0,"on line 64 : the argument you were passing (y,1) ,here variable ""Y"" is Y which you defined earlier or the updated y which is epoch_y ??",True
@jerryenebeli1238,2018-03-09T17:46:54Z,0,still getting ValueError: Both labels and logits must be provided.,True
@opstube,2018-03-08T17:53:12Z,0,"Thanks for sharing.  Regarding the confusion about tf.add and the + operator: The tensorflow python API overloads several operators, if you use ""x+y"" and x (or y) is a tensor, then tf.add will be used. In general, you want to use tf.add, so the code is clearer about a tensorflow function being used, and in case you want to use the ""name"" parameter.",True
@timotheemenais3437,2018-03-08T16:49:25Z,0,"It seems that mnist.train.num_examples is not in the mnist database anymore, just replace it by: mnist.train.images.shape[0]",True
@tedp9146,2018-02-27T17:37:32Z,0,2 questions: Does the dataset get downloaded every time im running the program (probably not)? And why is the loss that big? i hope im getting answers on even tough its an older video... Thanks!,True
@brianlink391,2018-02-27T01:01:19Z,1,"You got to watch out for them Global Variables.  Try to only use the ""ABC"" and ""XYZ"" variables within your definitions. This will help you avoid errors in the future.  :-) Thanks for the vid! I'm learning a lot about Tensorflow!",True
@TheFefiiiii,2018-02-20T00:49:03Z,0,"at 19:45 i have the same code as him but it gives me this  Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz Traceback (most recent call last):   File ""/home/starrboy/Desktop/deep-net.py"", line 102, in <module>     train_neural_network(x)   File ""/home/starrboy/Desktop/deep-net.py"", line 78, in train_neural_network     prediction = neural_network_model(x) # it outputs variable x   File ""/home/starrboy/Desktop/deep-net.py"", line 73, in neural_network_model     output = tf.add(tf.matmul(l3, output_layer['weights']) + output_layer['biases']) TypeError: add() missing 1 required positional argument: 'y'",True
@simpleman1546,2018-02-17T19:56:10Z,0,"Hey, I'm quite new to TF, but I was wondering can we apply ANN on  regression problems say for eg. Predicting house prices. If so please make something on that. Since i have been following your series, I don't know yet whether you have done it already or not, don't sue me for that. Great stuff mate. Thanks.",True
@brandonmerrithew317,2018-02-12T03:07:10Z,1,"@17:10 ""Hello everyone! My name is sentdex, and welcome to debugging with Python"" lol",True
@leonwiesen4439,2018-02-09T15:14:34Z,0,"Very good tutorial. The whole playlist is well done. didn't make the bracket and range mistake, but had loads of other errors like the missing args ""logits"" and ""labels"" in the softmax... Looking forward for more tutorials",True
@cemox007,2018-01-29T20:36:04Z,0,Accuracy is 0.9672 in 3 minutes on GeForce GTX 1050 GPU with batch = 50 and hm_epochs=30. CPU version must be frustrating.,True
@rohansakhuja9774,2018-01-28T13:23:45Z,0,"After struggling for days trying to understand how TensorFlow works , came to your tutorial and this was by far the best tutorial i've been through, thanx alot",True
@wolfisraging,2018-01-28T04:04:36Z,0,"I got 99.97% accuracy over test with adam with 0.001 lr, and 2 hidden layers,     with softmax as output layer, but only 92% in validation, what I am missing??? I thought that is overfitting, but even after using l2 regularization and dropout, still i am stuck with only 94.2% validation accuracy!!! Plz help",True
@bradyhuang5606,2018-01-26T07:57:39Z,0,"Sir, I don't quite understand this line. cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))  I thought y should be the labels of the training data, but y here is equivalent to y = tf.placeholder('float')   Isn't this meaning this is just a placeholder not labels? Or is there something wrong of my understanding?  Thanks",True
@mohankumaru3683,2018-01-24T12:57:26Z,0,"x,y=mnist.train.next_batch(batch_size) can someone explain me this.",True
@samarthkaluskar2352,2018-01-22T01:41:47Z,0,"I just downloaded tensorflow through Anaconda prompt using pip installation. I am able to import tensorflow using ""import tensorflow as tf"" command. Unfortunately, I can not do the same for input_data ('import input_data""). I get following error message. ""No module named input_data"". I went to the ""\conda\envs\tensorflow\Lib\site-packages\tensorflow\examples\tutorials\mnist"" directory. I can see input_data.py file. Can someone help me to overcome this problem? Thanks a lot.",True
@suileungmak9325,2018-01-17T13:57:09Z,0,Why the output layer does not have activation function ?,True
@chiranshuadik9818,2018-01-14T07:19:43Z,0,better than Siraj Raval,True
@SpecKROELLchen,2018-01-13T08:31:28Z,0,"Thank you for your nice tutorials. I tested the same program for three and also for a simplification by just two layers and the accuracy at the end was still 0.948 (the same as for three layers). Does this mean that in this exact example, two layers are enough or does it have to do with how the layers are constructed and that additional layers should contain maybe different distributions? I also tested it for just one layer and the accuracy just went down by a little to 0.9469",True
@karthickraja8711,2018-01-10T07:16:58Z,0,I did same as he did starting from installing virtual box to this code....but im only acheiving accuracy of 0.0982 why??,True
@aayushpathak6365,2018-01-08T17:05:03Z,0,"Thanx for the video .I am getting this error on my Windows 10 laptop  ""your cpu supports instructions that this tensorflow binary was not compiled to use: avx avx2""",True
@bluelight9508,2018-01-08T07:30:42Z,0,Great content.  I think you should use a softmax activation function on the output layer.,True
@skrmnghrd4520,2018-01-06T22:31:30Z,0,I hope its not yet too late ummm C+ to make text bigger on terminal. (CONTROL AND PLUS ) :),True
@anjopag31,2018-01-05T01:52:41Z,0,I am confused,True
@arkoraa,2017-12-26T19:23:58Z,0,"Hi all, I'm a bit late in the game, but how does tf.cast convert 'correct' to a float? Doesn't tf.equal return a tensor of type bool?",True
@cryhav0k2112,2017-12-25T09:40:30Z,0,"I am a beginner who basically just has Python syntax skills mastered. Everything else is on a 'google it' basis. At first was kinda mad that you had so many errors, but the fact that you left them in made me go back and rewatch a bunch of times, question my code, google a bunch of stuff but overall I had a better grasp, made it not just mindless copying of code. The more time I spend looking at the code the more I understand",True
@aonoymousandy7467,2017-12-24T06:59:12Z,0,"I have the whole code complete however, it gives me the error:   Tensor(""add:0"", shape=(?, 10), dtype=float32) Traceback (most recent call last):   File ""sentdex2.py"", line 71, in <module>     train_neural_network(x)   File ""sentdex2.py"", line 53, in train_neural_network     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))   File ""/home/archimedes/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 1744, in softmax_cross_entropy_with_logits     labels, logits)   File ""/home/archimedes/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 1700, in _ensure_xent_args     raise ValueError(""Both labels and logits must be provided."") ValueError: Both labels and logits must be provided.  not sure how to fix it",True
@mpunt5482,2017-12-20T19:28:15Z,0,"Awesome. You are making so many excellent tutorials! Many, Many thanks",True
@abidtest1103,2017-12-19T07:19:31Z,0,How can i input my own image in this and get the digit as output ?,True
@maharshichowdaryyeluri4330,2017-12-13T13:24:16Z,0,Thanx!  for the useful shit......,True
@padhisandeep96,2017-12-13T08:13:52Z,1,how can we save our tensorflow model so that we can use for futher projects or use it for transfer learning?,True
@Prometheus_Storm,2017-12-13T06:48:59Z,0,"I have an error like this: TypeError: Expected bool for argument 'transpose_a' not <tf.Variable 'Variable_1:0' shape=(500,) dtype=float32_ref>.",True
@sunnyrugg6584,2017-12-10T14:27:02Z,0,"hey i am getting this error, help me out>>>>    Traceback (most recent call last):   File ""yolo.py"", line 65, in <module>     train_neural_network(x)   File ""yolo.py"", line 55, in train_neural_network     temp, c = sess.run([optimizer,cost], feed_dict = {x: ex, ey: y}) TypeError: unhashable type: 'numpy.ndarray'",True
@jjsridharan,2017-12-06T08:29:57Z,0,"What might be the value of prediction? ""prediction = neural_network(x)"" I think it might be one_hot output?",True
@siddharthsinghgaur4764,2017-12-06T06:05:56Z,0,How am I suppose to learn all these function names and all ? Plz someone suggest...,True
@waqasraj1867,2017-12-05T17:30:46Z,0,i got this error.  ValueError: Shape must be rank 1 but is rank 0 for 'random_normal_9/RandomStandardNormal' (op: 'RandomStandardNormal') with input shapes: [].,True
@bryanpicchiottino2180,2017-12-05T01:42:19Z,0,"Just wanted to send thanks for making these videos, they are much appreciated.",True
@kennyadetiloye204,2017-11-28T01:40:33Z,0,"If you are using another data set, would we have something like mnist.train.num_examples? How we get that?",True
@ManintheMakingPodcast,2017-11-25T18:46:54Z,0,"During execution of:  l1 = tf.add(tf.matmul(data, hidden_1_layer['weights'], hidden_1_layer['biases']))  The error is: TypeError: Expected bool for argument 'transpose_a' not <tf.Variable 'Variable_1:0' shape=(500,) dtype=float32_ref>.  Could anyone help?",True
@aron987iji,2017-11-22T05:00:52Z,2,"Hi, I'm getting this error :   tf.Session as sess: AttributeError: __enter__  which would be the cause? :/",True
@AnupumPant,2017-11-20T06:41:09Z,0,prediction = train_neural_network(x)   [Previous line repeated 995 more times] I'm getting an error saying: RecursionError: maximum recursion depth exceeded  Any idea how can I get rid of it?,True
@AnupumPant,2017-11-20T06:09:32Z,0,Has anyone used the keyshot rendering program? Has someone made something like that for making the neural network model. I think that would be really useful GUI way of making the model.,True
@gallegom58,2017-11-18T01:50:01Z,0,in the function call of train_neural_network(x). How was data into the x place holder? Isn't it just an empty float of shape 784? Please help! I'm not understanding.,True
@quantummath,2017-11-08T13:27:17Z,1,Dude ... this is a great way of teaching how to code. Greetings from Hamburg.,True
@chetangupta8794,2017-11-08T06:49:21Z,0,"How to run this code on float 16, float 32, float64.... that is on different precisions",True
@mramzanshahidkhan3917,2017-11-06T19:24:06Z,0,"This tutorial i will say just awesome,,,,,,, always much better learning lessons, thanks",True
@razbotics,2017-11-03T21:46:06Z,0,"Their is an inbuilt DNN classifier in tensorflow ,if i am not wrong ,can't we use that? Or you maybe not using that because you are explaining the code.",True
@TheLordcenzin,2017-10-30T08:50:58Z,0,"Hi, thanks a lot for your tutorials. They are really great. But i have some question. How can I test an image using the already training model? I mean, given 4.png, how can i load the model and test if it correctly recognizes my image? Is there already a tutorial/example on this? Thanks",True
@azdcxdf,2017-10-25T21:21:47Z,0,Thanks. I couldn't understand Tensorflow's tutorial on building nn for mnist. It uses convolutional network which is a bit more complicated. Your tutorial was a step in between that I needed right now :),True
@nashitbabber7360,2017-10-25T05:02:05Z,0,What if I make number of epochs to say 500 or 1000? would that increase my accuracy or overfit the data?,True
@youtubspabr,2017-10-23T04:41:51Z,0,"Thanks for these great tutorials.  Created a version of the code where the layer definitions were done in a loop, thus allowing flexibility in the number of layers.  Code at : https://drive.google.com/open?id=0BzyV6_GhllUSZ283M3Jvc29LbDg",True
@IstiyakHossainPro,2017-10-21T20:24:06Z,0,"I am trying to understand about deep learning for last a month, but your video save me in one day. Thanks for such kind of video.",True
@TheSaksham2,2017-10-21T17:11:24Z,0,"In the last line while calling the function train_neural_network(x), what is x? Wasn't x a placeholder with nothing in it?",True
@ahmadfitri6035,2017-10-15T10:20:25Z,0,"why is the loss seems really big, what is the unit for the loss. is it percentage cause that is really big number there",True
@itrebla1,2017-10-14T20:12:18Z,0,"FOR loop for (N) HIDDEN LAYERS ..  def nearal_network_model(data,layer_nodes_sizes,n_classes):     datasize = int(data.shape[1])     numlayers = len(layer_nodes_sizes)                    hidden_layers = []     inputlayer = {'weights':tf.Variable(tf.random_normal([datasize,layer_nodes_sizes[0]])),'biases':tf.Variable(tf.random_normal([layer_nodes_sizes[0]]))}         hidden_layers.append(inputlayer)             for counter in range(1,numlayers) :         hidden_layers.append({'weights':tf.Variable(tf.random_normal([layer_nodes_sizes[counter-1],layer_nodes_sizes[counter]])),         'biases':tf.Variable(tf.random_normal([layer_nodes_sizes[counter]]))})                           hidden_layers.append({'weights':tf.Variable(tf.random_normal([layer_nodes_sizes[numlayers-1],n_classes])),                      'biases':tf.Variable(tf.random_normal([n_classes]))})          ln=[];     tmp=tf.add(tf.matmul(data,hidden_layers[0]['weights']),hidden_layers[0]['biases'])     ln.append(tf.nn.relu(tmp))                for counter1 in range(1,numlayers) :         tmp = tf.add(tf.matmul(ln[counter1-1],hidden_layers[counter1]['weights']),hidden_layers[counter1]['biases'])               tmp = tf.nn.relu(tmp)         ln.append(tmp)              output =  tf.add(tf.matmul(ln[numlayers-1],hidden_layers[numlayers]['weights']),hidden_layers[numlayers]['biases'])         return output    def train_neural_network(x,layer_nodes,n_classes,nm_epochs,batch_size):     prediction = nearal_network_model(x,layer_nodes,n_classes)          cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))     optimizer = tf.train.AdamOptimizer().minimize(cost)          with tf.Session() as sess:         sess.run(tf.global_variables_initializer())          for epoch in range(nm_epochs):             epoch_loss=0             for _ in range(int(mnist.train.num_examples/batch_size)):                 epoch_x,epoch_y = mnist.train.next_batch(batch_size)                 _,c = sess.run([optimizer,cost],feed_dict = {x: epoch_x,y: epoch_y})                 epoch_loss +=c             print('Epoch',epoch,'completed out of', nm_epochs,'loss',epoch_loss)                                           correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))                     accuracy = tf.reduce_mean(tf.cast(correct,'float'))         print('Accuracy:',accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))              batch_size= 100             nm_epochs =10 nm_classes =10 layer_nodes = [500,500,500];  train_neural_network(x,layer_nodes,10,nm_epochs,batch_size)",True
@lyan2759,2017-10-13T01:18:09Z,0,"the ' print('accuracy :',accuracy.eval(feed_dict={x:mnist.test.images,y:mnist.test.labels},session=sess)) ' doesn't work for me. eval is not even shown in code completion... any suggestions?",True
@vishnuprem4393,2017-10-09T17:12:57Z,0,Hi I need some help figuring this out! I tried many things but tensorflow seems so confusing! Suppose I want to test the classifier on my own image of a digit(28*28 pixel) using the trained model. How do I do that? (tensorflow noob) Thanks in advance!,True
@ravishankard2819,2017-10-08T22:13:13Z,0,"Very helpful, especially the debugging.Thanks.",True
@wktodd,2017-10-08T18:21:23Z,6,epic.  - e pik. : epoch - Ee. pok,True
@siddharthpanjwani8948,2017-10-07T10:34:06Z,0,"This is an awesome tutorial, although I must concur that the way tensorflow treats variables is best understood if you have smoked something before. I used the code in the tutorial and tweaked it to my own inputs and output data, which are not from mnist. I could not for the life of me figure out how to use the prediction given a new input (not in the training data). I dont care about calculating accuracy against labelled data at this point.  My current approach is to use the code below: predicted_tensor = sess.run(prediction, feed_dict = {x: f1})  This is after the training and before the accuracy code. I want to see what is the prediction of the last row in the input data - f1. However, I dont get a one-hot array. I was hoping to get a one-hot array classifying the last row of the input data into some class. But the array has non-zero numbers. Any help would be hugely appreciated.",True
@RastaZak,2017-10-05T14:16:37Z,0,"Need Help! I keep Getting the following error: File ""DeepNet.py"", line 22, in nural_network_model     hidden_1_layer = {'weights': tf.Variable(tf.random_normal([784,n_nodes_hl1])), 'biases':tf.variable(tf.random_normal(n_nodes_hl1))} AttributeError: module 'tensorflow' has no attribute 'variable'   Tensorflow (gpu) is installed correctly as far as I can See.  TF version = 1.3, Python Version = 3.5, Windows 10.   Any Ideas Guys?",True
@nurkanatkhametov6225,2017-09-28T19:10:29Z,0,How tensorflow graph defines input data ? HOW THE HELL it distinct input data from another kind?,True
@mdasadullahturja1481,2017-09-28T17:07:29Z,0,"I am getting different accuracy for different batch sizes. And the difference is significant I think. For example, for batch_size = 100, I am getting 95% accuracy while for batch_size = 1000, the accuracy falls down to 91%. Any explanation??",True
@ganeshjayaraman2588,2017-09-26T11:42:42Z,0,"If you guys have any kind of doubt in understanding this code, please go through the official site  https://www.tensorflow.org/get_started/mnist/beginners",True
@shivamkeshri487,2017-09-26T07:12:02Z,0,"this is the code need to change in version 1.3 of tensorflow: def train_network(x):  prediction=neural_network_flow(x)  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))  optimize=tf.train.AdamOptimizer().minimize(cost)   cycles=10  sess=tf.Session()  sess.run(tf.global_variables_initializer())  for roundnum in range(cycles):    round_loss=0    for _ in range(int(mnist.train.num_examples/batch_size)):     x_echo,y_echo=mnist.train.next_batch(batch_size)     u,c=sess.run([optimize,cost],feed_dict={x:x_echo,y:y_echo})     round_loss+=c    print('round ',roundnum,'completed out of ',cycles,'loss: ',round_loss)  correct=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))  accuracy=tf.reduce_mean(tf.cast(correct,'float'))  print('accuracy :',accuracy.eval(feed_dict={x:mnist.test.images,y:mnist.test.labels},session=sess))  train_network(x)",True
@brucezhu8332,2017-09-26T06:19:24Z,0,why it does not work if I change the optimizer from Adam to Gradient Descent?,True
@shivamkeshri487,2017-09-25T15:15:44Z,0,ye i found the solution .....i am doing a mistake thanks for the tutorial,True
@shivamkeshri487,2017-09-25T14:36:39Z,0,actually i have tensor 1.3 version so i am getting error in  one line: optimizer=tf.train.AdamOptimizer.minimize(loss=cost) i even tried with this: optimizer=tf.train.AdamOptimizer.minimize(cost) and this : optimizer=tf.train.AdamOptimizer().minimize(cost) it gives me  typeerror:minimize() missing 1 required positional argument : 'self' and i tried so many things but can't find solution.. please provide complete train_nueral_network function according to tensorflow version 1.3......  and thanks for this series..,True
@nasrinsultana227,2017-09-25T06:18:01Z,1,"File ""deepnet.py"", line 48      optimizer = tf.train.AdamOptimizer().minimize(cost)                                                       ^ IndentationError: unindent does not match any outer indentation level  anyone please explain whats the problem with this line?",True
@cagdask,2017-09-24T05:59:34Z,0,"This is how I modified model creation so that it now runs as a loop  def create_network_model(X):      nClasses = 10     nLayers = 4;     nNeurons = [28*28, 500,500,500,nClasses]     layer = [None]*nLayers     l = [None] * (nLayers+1)     l[0] = X          for i in range(nLayers):         layer[i] = {'W':tf.Variable(tf.random_normal([nNeurons[i], nNeurons[i+1]])),                       'b':tf.Variable(tf.zeros([nNeurons[i+1]]))}         l[i+1] = tf.add(tf.matmul(l[i],layer[i]['W']),layer[i]['b'])         if(i+1!=nLayers):             l[i+1] = tf.nn.relu(l[i+1])            return l[nLayers]",True
@adithya_ganesh,2017-09-15T06:07:52Z,0,@sentdex Funny thing when I tried without biases i got a higher accuracy,True
@chaitu723931,2017-09-09T18:23:39Z,0,"To increase the size of text in terminal use ctrl+shift+""+""",True
@janoxley,2017-09-08T14:32:11Z,0,So when do we see it recognise your writing?,True
@soumyodey1795,2017-09-07T08:37:05Z,0,"Hey, how to find out the recall score of your model ?",True
@georgewilliam6001,2017-09-05T06:49:18Z,0,In what part of the code you supply the test dataset to the model....you specify the line of code where training is occurring ....how about the testing part?,True
@davemadethis5568,2017-09-01T16:44:41Z,0,"why do we need a 1 in tf.argmax(prediction, 1)? Do we really need it?",True
@nikhilraghava,2017-08-31T03:31:22Z,0,"I recommend you use tf.float32 instead of 'float' , since the docs use them.",True
@beesent8582,2017-08-30T08:20:44Z,0,WHATS GOING ON EVERYBODY?!!,True
@nsv.2344,2017-08-28T00:17:26Z,0,"can someone explain this line of code:   _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y}) specifically how this gets translated to backpropagation?",True
@nsv.2344,2017-08-28T00:09:38Z,0,"When you call the neural_network_model(x) you are running the training set through the entire neural network just once and then in train_neural_network(x) you capture the cost, optimize it and backpropagate the new weights, but this time you run the same training set in batches, am I understanding correctly? Why not the run the neural_network_model(x) in batch the first time?",True
@zepeng8503,2017-08-23T08:49:26Z,0,"Hey sentdex, Thx for the video ! by following alone the code i got this error...   ValueError: Shapes must be equal rank, but are 2 and 1 From merging shape 0 with other shapes. for 'SparseSoftmaxCrossEntropyWithLogits/packed' (op: 'Pack') with input shapes: [?,10], [10].  Could you give me some hints?  Thanks!",True
@desync0134,2017-08-17T05:45:39Z,0,"Did anyone else run into ""TypeError: 'int' object is not iterable' on line 'for epoch in hm_epochs:'  ? I was searching around but examples are very basic, i understand that it is saying an integer is not iterable, but im not sure where it is sourcing the integer in question from.",True
@ryanleonard1027,2017-08-15T20:07:08Z,0,"Hey Harrison, in response to your comment about making the text bigger, I've found that a general rule of thumb when using the linux command line, that most of the usual keyboard shortcuts, cut, paste, increase text size, etc are the same as usual, but with the shift key added.  So, for example: copy is usually ctrl C but in the command line it becomes ctrl shift C paste is usually ctrl V but bcomes ctrl shift V increase text size is usually ctrl + but becomes ctrl shift +  This isnt true for everything, but in general, its a good thing to try.  Thanks for the tutorial",True
@nihil1,2017-08-15T17:50:48Z,0,"How does that even work? I mean... ...if I were implementing that myself, I'd make a model function that received data AND parameters, so that the optimizer would be varying the parameters until the cost was at a minimum - just like doing a MCMC fitting. How does tensorflow know what to change, and where is that knowledge (that is, all the weights) stored afterwards?",True
@llJoDall,2017-08-13T09:46:50Z,0,Ctrl + to make the text bigger. Ctrl - to make it smaller,True
@farzanjafeh,2017-08-03T17:31:33Z,0,"Anyone knows how to compute precision and recall for this data set and this code? I'm using          argmax_prediction = tf.argmax(prediction, 1)         argmax_y = tf.argmax(y, 1)          TP = tf.count_nonzero(argmax_prediction * argmax_y, dtype=tf.float32)         TN = tf.count_nonzero((argmax_prediction - 1) * (argmax_y - 1), dtype=tf.float32)         FP = tf.count_nonzero(argmax_prediction * (argmax_y - 1), dtype=tf.float32)         FN = tf.count_nonzero((argmax_prediction - 1) * argmax_y, dtype=tf.float32)           precision = TP / (TP + FP)         recall = TP / (TP + FN)                  print (""Precision"", precision)           print (""Recall"", recall)  and get  Precision Tensor(""truediv:0"", dtype=float32) Recall Tensor(""truediv_1:0"", dtype=float32)",True
@maikbialas7691,2017-08-03T11:03:55Z,1,Updated for TensorFlow 1.0 and dynamic layer creation. https://github.com/Hellekin87/mnist_tensorflow/blob/master/mnist.py,True
@iLLt0m,2017-08-02T06:07:55Z,0,Most of the time I'm only getting an accuracy of 32%. Often 14%. Here's my code: https://hastebin.com/uhahanovok.py,True
@hussamshamek6331,2017-07-31T02:07:13Z,0,Do you have any videos for using TensorFlow for image classification? Thanks,True
@rajatdubey6854,2017-07-28T11:35:58Z,0,One of my Best tutorial..!! Good Job,True
@tarakaramsinghbondili6871,2017-07-25T03:49:42Z,0,"Hellow,  After execution of my code i could not able to see thee epochs after these lines   Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz >>>   Can someone please help me how to solve this problem..? I used the same code which is explained in above tutorial.",True
@onepercentswe,2017-07-23T08:40:38Z,0,"for 100 epochs, got around 0.972 accuracy.",True
@revanttiwari4669,2017-07-22T11:49:25Z,0,AttributeError: 'DataSet' object has no attribute 'num'  i am getting this error please help!!,True
@revanttiwari4669,2017-07-22T11:40:52Z,0,AttributeError: 'DataSet' object has no attribute 'num'  i am getting this error,True
@tospig,2017-07-19T04:42:27Z,0,is 'epic' an American pronunciation of 'epoch' ?,True
@JordanShackelford,2017-07-14T10:57:03Z,0,"Is a recurrent neural network the same as a ""recursive"" neural network? I was thinking it would be cool to train a network to find the optimal activation function, the optimal number of layers, etc. This would probably increase computation time massively but it could be interesting for small neural nets",True
@curtlyblows7287,2017-07-11T17:31:39Z,0,"Is it possible to use   y=tf.placeholder('float',[None,10])  in line 15?",True
@hello12345720795131,2017-07-10T00:36:20Z,0,Does anyone know where I can actually find the pictures to see them for myself?,True
@neil659,2017-07-06T15:30:46Z,0,Hi I've been getting the following error code TypeError: add() missing 1 required positional argument: 'y',True
@cchen7359,2017-07-05T20:59:03Z,0,"It is a very useful video!!  How can I find the dataset in the Video? Now I have a large dataset with 15 attributes, it is OK to use this model?",True
@dishanilahiri9536,2017-07-02T16:59:32Z,0,"I'm trying t to work out the for loop using this..  num_hl =3 for i in range(num_hl-1):         hi =  {'weights':tf.Variable(tf.truncated_normal([n_nodes_hl[i], n_nodes_hl[i+1]],stddev = 0.1)),                       'biases':tf.Variable(tf.constant(0.1,shape=[n_nodes_hl[i+1]]))}         hidden_layers.append(hi)         l[i] = tf.add(tf.matmul(data,hidden_layers[i]['weights']), hidden_layers[i]['biases'])         l[i] = tf.nn.relu(l[i]) and the output layer separately but I'm getting this error on this line..  l[i] = tf.add(tf.matmul(data,hidden_layers[i]['weights']), hidden_layers[i]['biases'])   IndexError: list assignment index out of range",True
@vincentsung5972,2017-06-29T09:03:10Z,0,"Can i ask why the neural network model returns a bunch of zeros and a one at the output layer? What i am think is since we are doing (features*weights + biases) and feed through a ReLU function, it doesn't seem to return 0 or 1",True
@hans6973,2017-06-27T19:38:09Z,12,"line 48 should be  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))",True
@unknownboy4060,2017-06-26T16:40:59Z,0,use gradient decent optimizer instead....and put learning rate as 0.01...youll get a 98% accuracy,True
@ranjanp2955,2017-06-23T11:38:29Z,0,"I have a question. The activation function used here is RELU, but the cost function used here is cross entropy and its related to softmax function. How is cross entropy different from mean sqaured error and why is this preferred.  Thanks a lot for the help with tensorflow .",True
@user-ic7ii8fs2j,2017-06-22T17:23:37Z,0,Great tutorial! Thank you!,True
@ravimaurya2533,2017-06-22T06:39:10Z,1,"Hey I am getting this error on softmax_cross_entropy line :  tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[500,100] labels_size=[100,10]   [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape, Reshape_1)]]  Can any one help me out. I have updated to the new code... Please help me :)",True
@harshadannina7197,2017-06-16T16:17:09Z,0,"can u place all your code in Git-Hub repository and add that link in description?  Because when you getting errors we cannot able to catch up with you. Please sir....!!!!!!  And your tutorials are really awesome>>>>>>>  After this why don't you make tutorials on Mat-Lab for Neural Networks which involves artificial neurons, might be interesting!!!!!",True
@tzq33tdq,2017-06-15T15:11:13Z,0,"Tomorrow, I'm going to set up my rpi and see how it runs, I presume slowly",True
@shyamalvaderia7473,2017-06-15T08:14:45Z,0,Here is a implementation of the code without exact deceleration of each and every layer. (Solve the problem using list comprehension)  https://gist.github.com/svaderia/93de434ad3bb23da704a106b2e8349c5,True
@zwwang5395,2017-06-14T13:21:22Z,0,"Since ""x"" was a parameter of ""train neural network"", why don't you put ""y"" to be the parameter of ""train neural network""?",True
@Kim8612,2017-06-13T00:56:29Z,0,"I get lost  often in the lecture,,, would it help for me to understand better that I watched the lecture from the start?",True
@haraldfielker4635,2017-06-10T22:04:56Z,0,"Thank you of being human and let us take part of making errors! I really love this because event the best coders make brakets, comma and array index errors! So I loved this by watching! Being a python virgin this really helped me.",True
@sudhanvamg733,2017-06-06T14:22:06Z,0,"@sentdex , Is it possible to write a use_neural_network function for this network ? If yes , how do I pass an image and classify it ?",True
@Akshatgiri,2017-06-03T07:41:08Z,0,I debug my own errors.,True
@amirshukayev6934,2017-06-01T05:35:52Z,0,So I'm getting an error where the MatMul won't multiply because the matrix dimensions aren't the same. I had to set the number of nodes to 784. Any help?,True
@FranciscoCrespoOM,2017-05-31T13:29:19Z,0,"I think the variable 'output' in the function 'def neural_network_model(data)' should be also passed as an argument to the function 'relu(...)'. I think this vector should pass through the activation function too. So, the final statement in that function might be 'return tf.nn.relu(output)' instead of 'return output'. With that little change I got 98% accuracy instead of 94.85%.",True
@justwoody6511,2017-05-29T20:05:39Z,0,Your videos are amazing! Thank you for all your excellent tutorials!!!!,True
@anikethyadav6815,2017-05-29T10:46:56Z,4,"If anyone gets an error regarding softmax_cross_entrpoty......logits and labels missing, do this:  def train_neural_network(x):     prediction=neural_network_model(x)     cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y,name=None))     optimizer=tf.train.AdamOptimizer().minimize(cost)  hope this will help you guys",True
@dancluderay1304,2017-05-29T08:36:27Z,0,@sentdex can you explain the relevance of 500 in the nodes? I don't understand why its 500 and not 10. I can see 10 has a terrible accuacy!  Love the tutorials  Thanks Dan Sheffield UK,True
@deepitpatil,2017-05-28T06:48:56Z,1,"I'm getting this error. Please help me :( File ""C:/Users/deepc/PycharmProjects/HelloPython/neural network.py"", line 51, in train_neural_network     with tf.Session as sess: AttributeError: __exit__",True
@Zerksis79,2017-05-21T19:58:06Z,0,"Thanks for making this!  I'm playing with the code and I thought that increasing batch size would speed things up, and it did, but it killed accuracy.  I can't wrap my head around this...  any thoughts?",True
@shideh.naderi,2017-05-20T23:12:32Z,0,"Hi, I'm getting this error: Shape must be rank 1 but is rank 0 for 'random_normal_19/RandomStandardNormal' (op: 'RandomStandardNormal') with input shapes: []. Is it because I'm using a new version of tensorflow 1.0.0.0?",True
@tothesun,2017-05-20T19:53:35Z,0,Question about line 60: is this code actually evaluating cost twice (being that cost is a part of the optimizer declaration)? Or is TensorFlow just smart enough to evaluate only what it needs to in the computational graph and nothing extra?,True
@THEMithrandir09,2017-05-16T10:33:06Z,0,instead of int(somedevisionresult) you should do e.g. 25//2. The double / stands for integer division.,True
@endland1,2017-05-15T22:07:27Z,1,thanks for sharing your precious knowledge. it's really helpful 👍👍👍,True
@raphaelbaur4335,2017-05-05T07:50:34Z,0,"What nice tutorial. But i get a strange error (using PyCharm):  tensorflow.python.framework.errors_impl.NotFoundError: C:\Users\<and so on>\Python\Python35\lib\site-packages\tensorflow\contrib\rnn\python\ops\_gru_ops.dll  The problem is: When i copy the exact same path into my explorer, the  _gru_ops.dll file does actually exist ... What am I doing wrong? An answer would be much appreciated.",True
@joechu7935,2017-05-04T23:54:18Z,0,"Hi, where are the codes available?",True
@KshitizRimal,2017-05-04T06:36:53Z,0,"Thanx for the tutorial, it was great. I understood most of it but in some part i got confused. You talked about stochastic gradient descent and others but you haven't fully talked about gradient descents and how it is helping to reduce cost functions, i would really like to know relationship about them in a clear explanation that you always give. I haven't watched future videos but up to now i have been following machine learning playlist sequentially, so if it is in future videos.. ignore my comment :)",True
@jacheto,2017-05-01T11:40:12Z,6,is it normal to not understand almost nothing what it is actually going on yet on this train function? T.T,True
@dependent-wafer-177,2017-04-30T11:52:14Z,0,"What is a better accuracy level??? You say 95% accuracy is laughable, in reality 95% is pretty darn good so you have to tell us what then is a preferred accuracy level. Because there's not enough room left for more points when you talking about 95%.",True
@NeoKailthas,2017-04-29T18:41:09Z,0,"so after doing this, what did we achieve exactly? just learning how to work with tensorflow or is there a real benefit?",True
@janlukasr.2141,2017-04-28T12:42:39Z,1,"Hey sentdex - Thanks for your amazing videos! Regarding the errors: Please leave them in, they are super helpful and actually really motivating knowing you also make them. ;-) Keep it up! :)",True
@bebensiganteng,2017-04-26T01:41:25Z,0,"Thank you for this, but unfortunately, some of the code has changed, and how come are you sharing the code through youtube comments?",True
@mickelodiansurname9578,2017-04-19T15:35:15Z,0,"So increasing the epoch number doesn't really have a huge effect there...well, in my case an extra 2% accuracy at 20 epochs...so 2% extra for a 100% increase in time... thats not a good trade off although I accept that this is just training time so the ability could be reused again and again. But is  there some sort of known curve on this where one can calculate in advance the accuracy over x generations?  The determining factors are the number of levels, the randomness of the training data (or starting position) and the number of epochs right?",True
@balupabbi,2017-04-18T03:50:17Z,0,"Hi Sentdex, thanks for creating these tutorial, really appreciate ur work. Do output layers generally have linear activation function or can there be a sigmoid activation function as well? Just wondering",True
@sampathdatascientist3545,2017-04-13T05:00:49Z,0,"@sentdex  # my revised model for neural_network_model. Please share your thoughts and thanks for your posts.  def neural_network_model(data):     dimensions=[784, 500, 500, 500, 10]     neural_layer_fns = [tf.nn.relu for i in range(len(dimensions) - 1)]     num_hidden_layers = len(dimensions) - 2     layers = []     neural_layer = defaultdict(dict)          def add_layer_weight_bias(data, weights, biases):         return tf.add(tf.matmul(data, weights), biases)      for i, weights_dimension in enumerate(zip(dimensions[:-1], dimensions[1:])):         print('Preparing Layer:', i + 1, '....')         weights = tf.Variable(tf.random_normal(weights_dimension))         biases =  tf.Variable(tf.random_normal(weights_dimension[1:]))         layer_fns = neural_layer_fns[i]          if not layers: # first layer             layer = add_layer_weight_bias(data, weights, biases)         else:             layer = add_layer_weight_bias(layers[-1], weights, biases)          if i == num_hidden_layers: # outer layer             print('Model preparation is complete!\n')             return layer          layers.append(layer_fns(layer))         print('Preparing Layer:', i + 1, '.... Done')",True
@ApostropheShy,2017-04-06T00:50:53Z,0,"If you don't want the epoch loss, you don't have to run the 'cost' in sess.run, right?",True
@Fledron,2017-04-04T19:29:20Z,0,"I have the same code, but when I run it, I get different results. I have a loss of 1250 after 10 epochs. But it says my accuracy was 0.1249!?",True
@vijayabhaskarj3095,2017-04-03T10:56:20Z,0,how to choose the best value for the batch size? because if I change the value the accuracy varies a lot?,True
@Jr-Programmer,2017-03-30T15:47:37Z,1,"Hello Harrison, thank you for this series. What beast of a pc do you have? your model was done while mine was at epoch 5",True
@alignedbyprinciple,2017-03-30T05:12:58Z,0,Thanks for this great tutorial dude. I am a newbie who searched all over the internet for an easy and clear explanation of this subject. I found your video to do just that. Keep up the good work brother. I have one question though. I come from Java and the fact that you are calling the initial train_network function by passing x didn't make any sense to me. What value does x has?,True
@dhidhi1000,2017-03-26T03:11:13Z,0,"I got so many errors because I mistyped this ""labels"" as ""lables"" w the heck, ""lables"" makes much more sense",True
@jcamdr,2017-03-25T19:26:09Z,0,Take to me 295 epochs to get zero loss.,True
@amyobi1027,2017-03-22T22:23:07Z,0,"Hi!! I am learning a lot from your videos.. However in this code I keep on getting this error Nameerror: optimizer not defined in the line _,c=sess.run",True
@gauravsharma-zb1ku,2017-03-22T16:39:59Z,0,Tensorflow is the most fucked up library when it comes to syntax.,True
@gauravsharma-zb1ku,2017-03-22T16:30:18Z,0,"Shouldn't y be  tf.placeholder('float', [None, 10]) ? as one hot=True",True
@8o8inSquares,2017-03-21T16:36:38Z,0,"I get:   urllib.error.URLError: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond apparently the mnist servers are down or something?",True
@abhinavkumar7743,2017-03-18T20:33:37Z,0,Shouldnt we pass the output in neural_network_mode()  to the relu acivation as that will bring it to 0 -1 scale (as we do in sigmoid)  to act as logits or prediction.,True
@jefferygriffith,2017-03-18T02:40:55Z,17,"I'm a python noob but took a stab at the ""for loop"". I scrapped the weights and biases dict and created them on demand as i wired up the layers doing a look-ahead by 1 to hook the layers together. I hope I did this right - have a look. The accuracy remains over 95% so hopefully it is correct. https://github.com/jeffgriffith/mnist-tensorflow-demo/blob/master/mnist_tf_demo.py",True
@zevminsky-primus9367,2017-03-17T23:32:13Z,0,"Shouldn't the cost be related more to the shape then the value of the number? For instance, 1 would be closer to 7 than 0",True
@xro7117,2017-03-14T15:02:49Z,0,can someone explain why we need the tf.add() function when computing each layer's output?,True
@MexRexAngel,2017-03-11T18:24:19Z,0,"If anyone is getting an error like: ""failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED""  Here is your solution: https://stackoverflow.com/questions/41117740/tensorflow-crashes-with-cublas-status-alloc-failed",True
@luckyboylb,2017-03-08T14:42:16Z,0,Thanks. Great video. Is there a way to send a picture as an entry and get back the print as number on the console?,True
@cyl5207,2017-03-07T11:59:25Z,0,"the code did not work on my mac. it shows 'TimeoutError: [Errno60] Operation time out.' another error, 'urlib.error.URLError: <urlopen error [Errno 60] Operation timed out>' does anyone know how to fix this?",True
@Rochbenritter,2017-03-07T11:23:24Z,0,"Under Windows I am getting ""socket.py"", line 702, in create_connection     sock.connect(sa) TimeoutError: [WinError 10060] "" I do not use a Proxy so I do not know why urllib library is not getting a response from the Host..Someone got an idea? (or currently connection issue under other OS)",True
@lucmissoum5721,2017-03-02T20:36:12Z,1,Anyone knows what the tf.reduce_mean in the cost function is for?,True
@arivuazhagan6697,2017-03-02T08:44:58Z,14,"Did anyone getting error like ValueError: Dimensions must be equal, but are 784 and 500 for 'MatMul_1' (op: 'MatMul') with input shapes: [?,784], [500,500]. Any help/pointers are highly appreciated.",True
@garbilan0,2017-03-02T07:26:33Z,3,@sentdex the MNIST database is unavailable (the site is down) so the tutorial doesnt work. i looked for hours now for the original files and cant find them anywhere. if you could upload them to your website that would be wonderful.,True
@bach1792,2017-02-28T23:08:57Z,0,"Great tutorial. I am a pythoner now learning tensorflow. as per your request here how you can generate a model without copy and paste  # init number of nodes for n different layers with different number of nodes nNodesLayers=[500,500,500]  # set number of classes 0 -> 9 nClasses         = 10  def neural_network_model(data, nNodesLayers, nClasses):     # create  layers       output = data     xnNode = int(data.shape[1])         for nNode innNodesLayers:         layer = { 'weights':tf.Variable(tf.random_normal([xnNode, nNode])),                   'biases' :tf.Variable(tf.random_normal([nNode])) }         xnNode = nNode                      # update output         output = tf.add( tf.matmul(output, layer['weights']), layer['biases'] )         output = tf.nn.relu(output)     # create output layer     outputLayer = { 'weights':tf.Variable(tf.random_normal([xnNode, nClasses])),                     'biases' :tf.Variable(tf.random_normal([nClasses])) }     output = tf.add( tf.matmul(output,outputLayer['weights']) , outputLayer['biases'] )     # return     return output",True
@yuanzuo5642,2017-02-26T16:50:07Z,5,"Hi, Thank you for your tutorials. I am watching your other tutorials, which help me a lot. While I am following this tutorial, I get confused about the variable passing mechanism. In terms of your function ""train_neural_network(x)"", the ""y"" of ""cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))"" is not passed through the interface, but ""x"" in ""train_neural_network(x)"". Actually, I have tried three ways, 1, def train_neural_network():, 2 def train_neural_network(x):, 3 def train_neural_network(x, y):. They all works. So I wonder if this is the mechanism of Python or it is the mechanism of tensorflow, which means once I define placeholders, I can pass them globally. Personally, I interpret this as the same mechanism of C language.  The placeholders ""x = tf.placeholder('float',[None, 784]) y = tf.placeholder('float') ""  are both outside of function ""train_neural_network"" making themselves global. Is it right?",True
@jayvala8368,2017-02-24T12:40:05Z,0,It would have been awesome if you could have shown what values of test dataset were taken during training and what it predicted instead of just accuracy!  It would have been more useful and had given an insight to what the model is actually doing and how could it be made more accurate in terms of what images it had problems identifying or predicting so it could be made more efficient and could be implemented in some real life problems.,True
@sachinumrao5844,2017-02-24T09:44:02Z,0,"When the same program as shown in video is run using different batch_size values, the accuracy is effected. Here are some values I found: Batch_size: 100, Accuracy: 95.17 ,,,, Batch_size:1000 , Accuracy:91.23 ,,,,,, Batch_size: 10, Accuracy: 96.66. Why is this happening?",True
@DILLIPKUMARSAHOOIITM,2017-02-23T10:05:21Z,67,"Fully working code on Tensorflow version 1.0 with the help of ""https://www.tensorflow.org/install/migration"" :  import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""tmp/data/"", one_hot=True)  n_nodes_hl1 = 500 n_nodes_hl2 = 500 n_nodes_hl3 = 500  n_classes = 10 batch_size = 100  # input feature size = 28x28 pixels = 784 x = tf.placeholder('float', [None, 784]) y = tf.placeholder('float')  def neural_network_model(data):  # input_data * weights + biases  hidden_l1 = {'weights': tf.Variable(tf.random_normal([784, n_nodes_hl1])),     'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}   hidden_l2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),     'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}   hidden_l3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),     'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}   output_l = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),     'biases': tf.Variable(tf.random_normal([n_classes]))}   l1 = tf.add(tf.matmul(data, hidden_l1['weights']), hidden_l1['biases'])  l1 = tf.nn.relu(l1)   l2 = tf.add(tf.matmul(l1, hidden_l2['weights']), hidden_l2['biases'])  l2 = tf.nn.relu(l2)   l3 = tf.add(tf.matmul(l2, hidden_l3['weights']), hidden_l3['biases'])  l3 = tf.nn.relu(l3)   output = tf.add(tf.matmul(l3, output_l['weights']), output_l['biases'])  return output  def train_neural_network(x):  prediction = neural_network_model(x)  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))   # v1.0 changes   # optimizer value = 0.001, Adam similar to SGD  optimizer = tf.train.AdamOptimizer().minimize(cost)  epochs_no = 10    with tf.Session() as sess:   sess.run(tf.global_variables_initializer())   # v1.0 changes      # training   for epoch in range(epochs_no):    epoch_loss = 0    for _ in range(int(mnist.train.num_examples/batch_size)):     epoch_x, epoch_y = mnist.train.next_batch(batch_size)     _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})       # code that optimizes the weights & biases     epoch_loss += c    print('Epoch', epoch, 'completed out of', epochs_no, 'loss:', epoch_loss)      # testing   correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))   accuracy = tf.reduce_mean(tf.cast(correct, 'float'))   print('Accuracy:', accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))  train_neural_network(x)",True
@RobertoFrobs,2017-02-19T00:51:28Z,40,"How the hell is tensorflow doing all that? It's cool, but on the other hand it feels like the user doesn't really know what's going on. I feel a little bit lost working with this. How is the optimiser accessing values of a dictionary within another function? I need a drink.",True
@djkj9028,2017-02-18T04:51:43Z,0,"hi, i am getting this error - how can this be fixed using the code in the video File ""mnist-try.py"", line 55, in train_neural_network     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y)) UnboundLocalError: local variable 'y' referenced before assignment  I am using tensorflow (0.12.1) on Py3",True
@sentdex,2017-02-17T13:45:31Z,227,"Here are some updates to the code to support TensorFlow version 1.0:  def train_neural_network(x):     prediction = neural_network_model(x)     # OLD VERSION:     #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )     # NEW:     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )     optimizer = tf.train.AdamOptimizer().minimize(cost)          hm_epochs = 10     with tf.Session() as sess:         # OLD:         #sess.run(tf.initialize_all_variables())         # NEW:         sess.run(tf.global_variables_initializer())",True
@Aptpitfall,2017-02-17T00:25:26Z,9,"I keep getting: ValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...) Any advise? thnx",True
@roywoodhead1827,2017-02-13T17:28:52Z,0,"Great video !!!  What does the loss number actually  mean?  For example, I have: "" Epoch  9 Completed out of  10 Loss:  18065.8154933"" and I got an accuracy of 0.951 so does that mean the loss is the ""1 -0.951"" it got wrong, the flip of accuracy? in other words it got ~18 thousand wrong out of ~360 thousand trials?",True
@wunderlust7252,2017-02-09T00:43:59Z,23,Eventually worked on raspberry pi.Took almost all CPU and memory resources.The pi feels like it has just been abused,True
@allanng78,2017-02-08T00:12:52Z,0,"Hi, I am running the code on Alienware i7 with 4Gig of ram. And it is very slow. Is it normal?",True
@mickboe1,2017-02-02T08:27:24Z,0,"Don't know if anyone is gonna read this but im running into a problem calculating the cost, Could be that im using python 2.7 but im getting the following error :   ValueError: Shapes must be equal rank, but are 2 and 1  From merging shape 0 with other shapes. for 'packed' (op: 'Pack') with input shapes: [?,10], [10]. anyone  knows where im going wrong ?",True
@jyotikumbhoje6983,2017-01-30T05:27:43Z,0,"My dataset consists of low resolution astronomical images for object detection.Will CNN be useful for it, because i m going to try with it..",True
@jyotikumbhoje6983,2017-01-30T05:26:02Z,0,what is the helper function to create 'next batch' for our own dataset,True
@leberkassemmel,2017-01-28T11:09:43Z,0,"Is there a download for your Program?  I can't find my mistake... I would love to continue with this, but i can't, if i can't even run the most basic program...",True
@petarking66,2017-01-22T15:53:03Z,0,"Getting 0,9565 accuracy just by redistributing the nodes to 1200, 200 and 100 :)",True
@wessidemd,2017-01-21T21:02:00Z,0,"Hi +sentdex.  Great videos.  I'm learning a lot, but I need a better explanation than ""magic"" @8:34 when explaining this      '_, c = sess.run([optimizer, cost], feed_dict = {x: x, y: y})'  Do you have other references, videos, or literature so we can get smart on this?  Thanks!",True
@zaneaussie,2017-01-20T14:01:11Z,0,Brilliant tutorials mate!,True
@Adultballetla,2017-01-20T07:24:41Z,0,"Last hurdle: I keep getting a eval() error message - any help would be appreciated.  Traceback (most recent call last):   File ""networkmodel.py"", line 76, in <module>     train_neural_network(x)   File ""networkmodel.py"", line 73, in train_neural_network     print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))   File ""/home/phillip/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 575, in eval     return _eval_using_default_session(self, feed_dict, self.graph, session)   File ""/home/phillip/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3619, in _eval_using_default_session     raise ValueError(""Cannot evaluate tensor using `eval()`: No default "" ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`  CODE:  with tf.Session() as sess:   sess.run(tf.global_variables_initializer())   #instead of tf.initialize_all_variables()   for epoch in range (hm_epochs):    epoch_loss = 0    for _ in range(int(mnist.train.num_examples/batch_size)):     epoch_x, epoch_y = mnist.train.next_batch(batch_size)     _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})     epoch_loss += c    print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)   correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))  accuracy = tf.reduce_mean(tf.cast(correct,'float'))  print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))   train_neural_network(x)",True
@Adultballetla,2017-01-14T08:12:59Z,0,"I been working on this for two days with no luck : (  If anyone can shine light on the following error I'd appreciate it.  Traceback (most recent call last):   File ""networkmodel.py"", line 76, in <module>     train_neural_network(x)   File ""networkmodel.py"", line 55, in train_neural_network     prediction = neural_network_model(x)   File ""networkmodel.py"", line 42, in neural_network_model     l2 = tf.add(tf.matmul(data, hidden_2_layer['weights']), hidden_2_layer['biases'])   File ""/home/phillip/anaconda3/envs/tensorflow/lib/python3.5/site-   line 76  train_neural_network(x) line 55 prediction = neural_network_model(x) line 42 l2 = tf.add(tf.matmul(data, hidden_2_layer['weights']), hidden_2_layer['biases'])  l2 = tf.nn.relu(l2) # activation function 2",True
@jakebeau2790,2017-01-12T20:34:37Z,0,"On your website, where you're defining output_layer, there is a unexpected comma - second to the last character.  https://pythonprogramming.net/tensorflow-deep-neural-network-machine-learning-tutorial/?completed=/tensorflow-introduction-machine-learning-tutorial/",True
@narveeryadav6689,2017-01-09T19:57:16Z,0,in the code  prediction = neural_network_model(x)  we are calling the function neural_network_model function without feeding any value to x.i can't get it please someone help me. i shall be vary thankful...please,True
@agent01x,2017-01-08T08:05:26Z,0,"In order to get the predicted values I used this function: def use_neural_network(newX,newY):     prediction = neural_network_model(x)     with tf.Session() as sess:         sess.run(tf.global_variables_initializer())         result = (sess.run(tf.argmax(prediction.eval(feed_dict={x: newX}), 1)))         return result  //Call the function: X, Y = mnist.train.next_batch(20) myResult = use_neural_network(X, Y)  print(myResult)  # [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4] Y = np.argmax(Y,1) print(Y) #[0, 1, 1, 3, 4, 7, 7, 8, 1, 0, 0, 4, 0, 5, 0, 8, 8, 0, 1, 0]  But I get an accuracy of 98% on my tests. Why are all the calculated values are output as [4 4 4 4 4 .......]??????",True
@kvishnudev,2016-12-29T20:50:42Z,1,"You have to do y = tf.placeholder('float') instead of y = tf.placeholder('float', [None, 784]) to make the program run",True
@elilichtblau3193,2016-12-28T23:46:15Z,0,"Where do you upload the code, I ran into an error and can't seem to figure it out, so I was wondering if you post the source code anywhere. Also, just incase someone here can figure it out.   The errors:  Extracting MNIST_data/train-images-idx3-ubyte.gz Extracting MNIST_data/train-labels-idx1-ubyte.gz Extracting MNIST_data/t10k-images-idx3-ubyte.gz Extracting MNIST_data/t10k-labels-idx1-ubyte.gz Traceback (most recent call last):   File ""MNIST-sentdex.py"", line 72, in <module>     train_neural_network(x)   File ""MNIST-sentdex.py"", line 56, in train_neural_network     with tf.Session as sess: AttributeError: __exit__  The code:   import tensorflow as tf  from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  n_nodes_hl1 = 500 n_nodes_hl2 = 500 n_nodes_hl3 = 500  n_classes = 10  batch_size = 100  #height x width x = tf.placeholder('float', [None, 784]) y = tf.placeholder('float')   def neural_network_model(data):    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([784, n_nodes_hl1])), 'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}    hidden_2_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), 'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}    hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), 'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}   output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), 'biases':tf.Variable(tf.random_normal([n_classes]))}   #input_data * weights + biases   l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])  l1 = tf.nn.relu(l1)   l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])  l2 = tf.nn.relu(l2)   l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])  l3 = tf.nn.relu(l3)   output = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])    return output   def train_neural_network(x):  prediction = neural_network_model(x)  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))   optmizer = tf.train.AdamOptimizer().minimize(cost)    hm_epochs = 10   with tf.Session as sess:   sess.run(tf.inialize_all_variables())    for epoch in range(hm_epochs):    epoch_loss = 0    for _ in range(int(mnist.train.num_examples/batch_size)):     epoch_x, epoch_y = mnist.train.next_batch(batch_size)     _, c = sess.run([optmizer, cost], feed_dict={x: epoch_x, y: epoch_y})     epoch_loss +=c     print('Epoch ',epoch,'completed out of ',hm_epochs,'loss ',epoch_loss)   correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))   print('Accuracy: ', accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))   train_neural_network(x)",True
@agent01x,2016-12-27T11:08:25Z,0,"Hi, How can we work with our own images, rather than taking data from MNIST. I am not good dealing with images and pixels.       img = cv2.imread('myImage.jpg',cv2.IMREAD_GRAYSCALE)       #512*512 pixels of image.       img = cv2.pyrDown(img)       img = cv2.pyrDown(img)                    #reduced to an array of 128 by 128. But the dimensions come out to be 128*128, but we need a row vector of N features. How is it possible to convert image data to the MNIST form of data.  Thanks a lot. :)",True
@raisarasul6105,2016-12-22T08:24:49Z,0,"I ran the this code and got ""RecursionError: maximum recursion depth exceeded"". I have anaconda 3.5... How to solve this?",True
@harshitagarwal5188,2016-12-21T12:34:23Z,0,can you explain a bit more on how did we get correctness by just comparing the maximum respective values in y_predicted and y_actual  rather than comparing the values in line 64,True
@varunyadav5181,2016-12-19T18:56:30Z,0,"http://ideone.com/TwcISO This is showing error,, i will be very glad if u can help me with it,, btw thx for all the editorials :)",True
@Timvoortaal,2016-12-18T22:38:49Z,0,Anyway! :),True
@RK9090909,2016-12-17T12:00:20Z,0,"Thank you so much for your tutorial! They are very helpful! In the next tensorflow project, do you mind explain the data structure a little bit?",True
@harborned,2016-12-17T04:27:19Z,0,Thanks for the tutorial. Very useful!  I'm sure by now you've been sent a lot of solutions that create the layers in loops but just in case: https://github.com/HarborneD/DanTFDeepNet,True
@Trvgn,2016-12-15T22:26:44Z,0,"Hey, great series, I really enjoyed the videos so far! I got two questions:  1. is some of this stuff (I am referring to the whole series) useful to do some curve/surface/parameter fitting or is it only used for data classification? Could you point out something I could read about that?  2. any comment on how the methods you covered might apply to small datasets of, say, tens to hundreds of points?  Thanks again for the hard work you put in explaining all of this!",True
@javiermatias-cabrera5267,2016-12-15T03:38:38Z,0,Excellent tutorial!,True
@ArcanePhalanx,2016-12-13T07:04:17Z,1,Accuracy varies between 10% and 88% each time I run...  Thoughts?,True
@muhammadharris4470,2016-12-12T17:35:09Z,102,Someone needs to tell tensorflow dev team on how to name their functions -_-,True
@MrOmniblast,2016-12-09T20:07:25Z,0,"Me using PyCharm:  All my fuckups are clearly shown to me in real time. <_<. Maybe use PyCharm? Lol You can even click your ""problem"" and be taken directly to it? Pretty cooool. #sponsored lol jk.",True
@Undeworld667,2016-12-04T16:21:34Z,3,"Is there a website listing of ""best models"" in Tensor Flow for any given problem area? I.e. the best model for Face Recognition, a best model for stock market prediction?",True
@navjotsingh1,2016-11-27T10:11:26Z,0,@sentdex : idea a deep neural network that outputs graphs,True
@edmundguzman7813,2016-11-26T12:27:47Z,0,Awesome! Better than some of the paid-for online classes on TensorFlow!,True
@muhammadhannan6701,2016-11-10T21:35:35Z,0,"Hi, thanks for sharing. Tensorflow doc is very difficult to understand for a noob like me. Great tutorials. I am able to achieve 96.6% accuracy. (Y)",True
@MoeElsadig,2016-11-06T10:02:53Z,0,my accuracy is 13.2%....:(,True
@kagrenac7191,2016-10-30T12:46:58Z,0,"Hi, thanks for the great tutorial. What do you think about mnist tutorial at tensorflow's website? Is there are much difference between yours and theirs?",True
@user-oh4vm2kv5r,2016-10-29T02:02:03Z,0,"Hi, where is the source code for the video?  thank you for the work!",True
@utkarshsrivastava7700,2016-10-24T20:07:41Z,0,"Hi,  can you share the code that you wrote in the session.",True
@andremeIIo,2016-10-22T15:09:58Z,22,"Change the weights initialization to tf.truncated_normal(shape, stddev=0.1) and bias to tf.constant(0.1, shape=shape). Got 98% accuracy. :D",True
@scarletovergods,2016-10-06T20:23:15Z,1,"I don't quite understand one thing. We have our final model ready and we pass it an image. Let's say at the last hidden layer we have only 4 nodes that after applying relu evaluate to 0,1,1,1 respectively. To reach to the output we multiply these by some final weights, add bias and end up with 23, 102, 0, 2. My question is how do these values get translated into 1-hot encoded number, e.g. [0,0,0,0,1,0,0,0,0,0] ?  The last layer that could have an arbitrary nr of nodes gets somehow translated to the 1-hot encoded label that could be of any arbitrary length as well. How does it happen?",True
@Jojo-hr7il,2016-10-05T19:50:49Z,0,"I got an accuracy of 0.9521, I think this because my tf 0.11 v , also I will try it for more pixels",True
@CarlSaptarshi,2016-10-01T22:03:49Z,1,"Hey! brilliant tutorial. I was wondering if you could clarify something for me.  where you have on line 59 epoch_x,  epoch_y, why did you have to do that? secondly, what is the point of the _ in the for loop? is it just an arbitrary value like i, j, k etc? and finally, I didnt really understand why you assigned _,c for the optimiser? like what made you do that? i didnt follow, thanks!",True
@champnaman,2016-09-30T15:27:05Z,0,A great video! But I'm confused about the function train_neural_network( x ).   Why are we not also passing y  as an argument as we are accessing it inside the above function? Or to rephrase how does is the above-mentioned function accessing y as it clearly seems to be working?   I tried it without passing any arguments and it still works. How is that even possible?   Thanks in advance!,True
@roshankarwalkar7102,2016-09-30T07:33:55Z,0,"Hello I am facing problem in import input_data statement File ""tensorflowTest.py"", line 3, in <module>     from tensorflow.examples.tutorials.mnist import input_data ImportError: No module named examples.tutorials.mnist  please help me",True
@herp_derpingson,2016-09-30T06:21:06Z,7,sudo apt-get install idle3,True
@qzorn4440,2016-09-29T04:46:11Z,0,now calculate some Neural stem cells. thanks.,True
@MikeDrones,2016-09-26T20:56:24Z,0,strange i got Accuracy: 0.9524,True
@emanuelzobev1931,2016-09-24T20:22:15Z,0,"Hi, I did exactly the same steps as you, however, my accuracy was 0.9522. Why did we get different accuracies, since we did the  same thing? Than you in advance.",True
@schoneschone,2016-09-20T19:27:13Z,1,"All of your tensor flow series are awesome tutorials, thank you! - Question... is there any flaw in using the prediction variable as you demonstrate it without running it thru a softmax filter, while the cost function passes the output thru a softmax filter and then calculates the loss on each item?",True
@daclarooftw,2016-08-28T00:03:41Z,0,"this was so funyny with the error lol , I 'm glad that you didn't script this before and made the code froms cratch , this debugging was really great and beneficial to us ,thank you.",True
@nicolastutuianu3175,2016-08-27T14:36:40Z,0,Thanks for making these videos they're very helpful. I'm wondering how  would one go about inputting a single image of their own and having the  program determine what class it is?,True
@hunarahmad,2016-08-21T19:29:13Z,0,awesome videos,True
@steveb12,2016-08-21T13:19:22Z,0,Would this work with say 10 second sound files?,True
@MahdiZouch,2016-08-17T14:46:12Z,0,"Thanks for the tutorial , could you please make a tutorial on tensorflow serving ?",True
@cjrage2401,2016-08-16T19:49:32Z,0,"sorry for being a pain guys but if you have any spare time and a fairly good knowledge of kivy, can you check out this problem I have with my program at http://stackoverflow.com/questions/38979876/using-two-kv-files-in-one-program-error . Any help is much appreciated",True
@notia2000,2016-08-13T21:05:11Z,0,Thank you! great tutorial! When is the next one happening?,True
@ProjectSuperSport,2016-08-13T10:44:33Z,0,We miss you sentdex :(,True
@_dmenin,2016-08-12T08:36:00Z,0,"Hey thanks for that, great tutorial. quick question, how can I train the model once, save it, and then use it latter on to test on different images?",True
@marinepower,2016-08-12T03:42:06Z,1,"just found out this channel from two minute papers and ran through your tensor flow tut.  Great content so far, and I'm looking forward to more.  Subscribed!",True
@furkankaynar6491,2016-08-11T23:54:40Z,0,ADAMIN DİBİ! HELAL OLSUN.   You are such a man. Thank you!,True
@nikithajv6460,2016-08-09T06:28:24Z,0,"Hi thanks so much for the video. For starters your videos will surely help. I am facing an error in the last line train_neural_network(x) and in the cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) ). Can you please tell em wheat could be went wrong?? in the video you also encountered the same error, you added epoch to variables x and y. I did the same. but still i am getting the same error again. Thanks in advance.",True
@Kreisquadratur,2016-08-08T22:49:50Z,0,"@sentdex Could you help out: I don't see where I pass an empty list rather then the `shape must be a vector of {int32,int64}, got shape []`.  https://gist.github.com/djui/5dd4f140ee44c3f0f1d753bd6b759ca1",True
@RaoufGnda,2016-08-08T15:33:05Z,3,"Thanks for this amazing tutorials, could you build a deep-neural network on the top of word2vec model then apply it on text dataset to find similarities among the words. Best",True
@343clement,2016-08-07T15:54:38Z,0,"on line 42, shouldn't it be: output = tf.nn.softmax(tf.matmul(l3, output....))??",True
@chuckaschultzz,2016-08-03T18:51:51Z,0,"Really great content! Would be interesting to see a tutorial on how to prepare your own data. Would not have to be image recognition - but could be more of time series data or very granular event data (i.e. IoT). How might we prep the data, build the model, train and test. Heck even predict - using new data.",True
@parthshah9898,2016-07-31T08:02:43Z,0,"Traceback (most recent call last): File ""deep-think.py"", line 73, in <module> train_neural_network(x) File ""deep-think.py"", line 49, in train_neural_network prediction = neural_network_model(x) File ""deep-think.py"",  line 33, in neural_network_model l1 = tf.add(tf.addmul(data, hidden_1_layer['weights']),hidden_1_layer['biases']) AttributeError: module 'tensorflow' has no attribute 'addmul'",True
@rubencontesti221,2016-07-29T16:02:52Z,0,This is a great tutorial. I'm getting a MemoryError. I wonder if there is a way to solve it or whether I need to get a new pc.,True
@soumilmandal7644,2016-07-26T20:32:14Z,0,"Great video. Loved it. Can you please provide a flowchart for the steps you're implementing, like a link to it maybe somehow in the description. It'll be better to follow since there's a lot of complicated things going on here. Thanks. :)",True
@AntoanMilkov,2016-07-26T17:46:07Z,2,"Hey man very good video very nice job!  Can you make a video not for prediction but for anomaly detection? Everybody is doing predictions those days, so let's try something different.",True
@Neura1net,2016-07-26T12:49:21Z,17,Thank you for sharing,True
@ProjectSuperSport,2016-07-26T09:11:14Z,1,Great video... when will we get more?,True
@khanlee9369,2016-07-26T08:09:08Z,1,best teacher ever (y),True
@radixvinni,2016-07-24T12:15:15Z,0,"When I saw 95% accuracy a was like ""what?"". Than I realized that you were testing accuracy on the training dataset...",True
,2016-07-23T19:15:16Z,21,"Dude! I follow you for a long time and you make awesome videos. I tried to study for my own tensorflow also but I get to the point where I have a nice accuracy, great, but how implement that in a real image, you now try to multiscale the weight and convolute over a image so we can detect numbers from 1 to 10 that are in a image, could you do a video about that. Thanks!",True
@nareshnagabushan4588,2016-07-22T20:08:42Z,14,"Hey , I'd suggest you to use PyCharm IDE instead , it's much easier to debug.",True
@nickmcneely5601,2016-07-22T19:31:43Z,0,"How would you suggest creating a dynamic diagram of the network that has the nodes colored according to the magnitude of their output? (analogous to the pretty fMRI videos, I guess, if we're still using the brain analogy)",True
@zeyadobaia4831,2016-07-22T13:18:52Z,0,"Hey man, great work !  Is there any tutorial coming up about scrapy ?  if not, can you recommend me some material ?",True
@netmanco1,2016-07-21T23:08:18Z,0,"Very nice series, thank you! Now if I could just get an accurate prediction on the winning Powerball numbers! :)",True
@foxxxyben,2016-07-21T22:07:03Z,1,"Thank you so much for this series. I've seen too many videos, papers, etc. that gloss over the internals as ""passing around tensors"" between the hidden layers, but actually seeing the weights/biases as simple arrays finally made it simple to grasp it as data being transformed **through** multidimensional arrays between each layer, as well as these weight/bias tensors being the persistent portion of the model. If this is indeed the case, I would love to see a few things:  1) How to pickle the model.  2) When it comes to other ML tasks such as regression, clustering, etc. do you just feed the output layer as input into a linear model? Can you feed a hidden layer the same way for multi-task learning?  3) How to inspect the weight matrices. I'm working with data where feature positions are stable, and building masks for most relevant regions will help reduce noise and data collection costs.  Thanks again for the videos, keep up the great work! -Ben",True
@onepunchsaitma1413,2016-07-21T18:14:02Z,0,"Is it possible to have multiple outputs. Training a neural network with images of several different animals. Then have it essentially be give output of , the animal name,  is it a mammal or not, is it bipedal or not, is it spotted or not. That sort of thing.",True
@johncusack3646,2016-07-21T16:59:16Z,0,Are you going to do pre-trained models as well? I tried it but I am having trouble getting it working. Below is a link to (I think) the weights of  Resnet 152 converted from caffe to TensorFlow.  https://github.com/ry/tensorflow-resnet,True
@panagiotispetridis7961,2016-07-21T14:12:55Z,1,Really liking this Neural Network with tensorFlow part of the series. Here are some suggestions:  A Neural Network to predict time series (let's say Stock Prices) would be nice. Or maybe writing a reinforcement learning NN to write articles or make music.,True
@levyroth,2016-07-21T14:10:15Z,0,Is it possible to represent regression type problems (predictions) using neural networks? Or are they best suited for classification tasks?,True
@MrGussemuss,2016-07-21T13:41:39Z,0,Nice! Been sitting here waiting (first),True
