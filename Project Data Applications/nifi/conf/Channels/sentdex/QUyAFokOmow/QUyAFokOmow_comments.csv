author,updated_at,like_count,text,public
@jammybravo,2023-07-21T21:53:09Z,0,This ML series is phenomenal...still.,True
@bakuninRogers,2022-09-20T08:51:26Z,0,"R-squared of 0.58 means only 58% of the variation of y is explained by the model, so in statistics this would be considered unacceptable.",True
@jairajsahgal5062,2021-09-12T10:07:41Z,0,Thank you,True
@muhammadqasim2344,2021-03-16T09:12:05Z,0,Where can i learn mathematics for machine learning?  Plz suggest some resources i really need that .,True
@skillmonks.intern235,2020-09-16T14:03:12Z,0,Major Skills You Need to Master Machine Learning and Deep Learning. Read Full Blog Article https://blog.skillmonks.com/deep-tech/major-skills-you-need-to-master-machine-learning-and-deep-learning/,True
@sodiq_5,2020-07-20T10:55:05Z,0,"unsupported operand type(s) for -: 'generator' and 'float' Please I get the error when I try to run the r_squared, how do I fix this please?",True
@ajibademotunrayo8453,2020-07-11T16:00:23Z,0,Nice,True
@b5nj1m9n,2020-05-30T06:46:15Z,5,"You could have written your 2 functions in 4 lines of code:  def determination_coefficient(): 	sst = np.sum(np.power(regression_line - ys, 2)) 	sse = np.sum(np.power(ys- np.mean(ys ), 2)) 	return 1 -sst/sse",True
@5firehawk,2020-05-09T15:39:09Z,0,ys_orig and ys_line haven’t been defined beneath the function. Or are these carried over from the numpy array ys ?,True
@thecros1076,2020-04-06T20:20:16Z,0,"as seen in simple linear regression that value of ma and b are calculated by the fourmula m=((mean(xs)*mean(ys))-mean(xs*ys))/((mean(xs)**2)-(mean(xs**2))) b=mean(ys)-(m*mean(xs)) this fourmula is derived my making the equation derivative equal to zero which is as follows e(m,b)=(y-m*x+b)^^2 the derivative of m with respect to ma and b is made zero and equations are solved to get value of ma and b   but in gradient desent we do not equate the darivative of cost fuction with respect to the feature equal to zero why it is so........................please please do answer my question i need answers",True
@twofacesa,2020-04-05T16:43:12Z,0,"You need more maths bro, you didn't even define SEy^ or SEy",True
@dhananjaykansal8097,2019-09-02T06:17:44Z,0,Lovely,True
@priteshborad1504,2019-05-11T07:49:07Z,1,Shouldn't it like  mean(y) for y in ys_orig????,True
@megamodz5326,2018-11-30T12:29:42Z,1,rsquared = -0.050000000000000000000004  can anyone explain please @sentdex,True
@jimiyu8537,2018-11-29T05:12:06Z,0,"What happened if squared_error_y_mean == 0 ? If we run some data those just select from a straight line which means the squared error y mean equal 0, will this script crash?",True
@10trends57,2018-11-04T19:16:58Z,20,*This is the perfect pace. Don't hear those who say that you're slow. It think you're doing perfect. Lots of Love*,True
@sbannik,2018-09-24T02:29:02Z,0,"Hey sentdex, for squared_error_y_mean = squared_error(ys_orig, y_mean_line) I am getting, Type 'list' doesn't have expected attribute '_sub_' and I am getting that same message for the r_squared equation for regression_line, r_squared = coefficent_of_determination(ys, regression_line), why is this occurring, I am typing exactly what you typed in your example",True
@mohammednagdy6661,2018-08-17T00:14:14Z,0,I can't wait to see a tutorial on AI by you. You're the best sentdex!,True
@mohammadsaqib1400,2018-07-05T07:16:50Z,0,"whats mistake in our program I get -59.00558730158717 import matplotlib.pyplot as plt import math from statistics import mean import numpy as np from matplotlib import style style.use(""fivethirtyeight"") xs=np.array([1,2,3,4,5,6],dtype=np.float64) ys=np.array([5,4,6,5,6,7],dtype=np.float64) def best_fit_slope_intercept(xs,ys):     m=((mean(xs)*mean(ys)-mean(xs*ys))/((mean(xs)*mean(xs))-mean(ys*ys)))     b=mean(ys)-m*mean(xs)     return m,b  def squared_error(ys_orig,ys_line):     return sum((ys_line-ys_orig)**2)  def determenation_sqr_error(ys_orig,ys_line):     y_mean_line=[mean(ys_orig) for y in (ys_orig)]     sqr_er_re=squared_error(ys_orig,ys_line)     sqr_er_ymean=squared_error(y_mean_line,ys_line)     return 1-(sqr_er_re/sqr_er_ymean) m,b=best_fit_slope_intercept(xs,ys)   regressionline=np.array([(m*x+b) for x in (xs)]) r_squared_error=determenation_sqr_error(ys,regressionline) print(r_squared_error) predict_x=8 predict_y=(m*predict_x)+b  plt.scatter(xs,ys) plt.scatter(predict_x,predict_y,color='g') #plt.show() plt.plot(xs,regressionline) plt.show()",True
@hrithikraj4277,2018-06-08T15:42:35Z,0,"'numpy.float64' object is not iterable  smbdy help me out",True
@asepsaepulrohman9787,2018-05-24T18:51:31Z,0,"name ys_line is not defined, but ys_orig that work. why??",True
@suhirdsingh92,2018-05-14T14:51:29Z,0,Though I am 2 yrs late but I have some queries. Here your are calculating the coefficient of determination. Is it different from root mean square error? and Is it important to calculate both or just One method will do?,True
@kilfoofan,2018-04-30T04:48:46Z,0,Thank you for these videos. For some reason it's so much easier for me to understand the math of the statistics via the code than via the mathematical equations.,True
@ansiruddin6719,2018-04-19T20:02:07Z,0,where was the ys_orig and ys_line variable defined in order to pass the other function,True
@michaelperez5426,2018-04-06T00:40:41Z,0,"Yo use esta forma para hallar el coeficiente de determinacion def coeficiente_determinacion(yPredecido,yReal):     promedio = mean(yReal)     scr = [(item-promedio)**2 for item in yPredecido]     sct = [(item-promedio)**2 for item in yReal]     coe = sum(scr)/sum(sct)         return coe",True
@fredericknusetor1254,2018-03-15T21:58:58Z,0,I run the same code and i had the error message below NameError: name 'y_orig' is not defined.  Can you please help me out.,True
@nicobohlinger7077,2018-03-14T21:40:08Z,6,"On the coefficient function you can just use: ""y_mean_line = mean(ys_orig)"" the for loop is not needed",True
@DaLoler1,2018-03-13T22:55:24Z,0,y_mean_line = [mean(ys_orig) for y in ys_orig]  Write this as y_mean_line = [mean(ys_orig)] * len(ys_orig) 10 times more easy to understand,True
@deaddead698,2018-03-02T01:16:48Z,0,My r_squared is -38.711688311688334? Is that supposed to happen?,True
@ChEkOv,2018-02-04T20:54:39Z,0,"Hi sir, Do you know something about programming with R? I heard is a better choice for a very heavy data sets. What are your thoughts about it? If you have any experience, I propose you to build some tutorials covering the basics of it applied to yahoo's free data of stock market datasets for example to work with something practical :)   Is very similar to python but I think it could became a powerful competitor at statistics vs python's panda and numpy.  Btw, congrats for the channel. Very good lessons you do.  I've subscribed myself not long ago :)",True
@ZeroDebtInvestments,2018-01-29T21:53:23Z,0,function coefficient_of_determination at 0x00000162C3DC3D08   ????,True
@dumpert999,2018-01-08T12:26:48Z,3,"In statistics SE stands for standard error and has a total different meaning than SSR (Sum of Squared Residual).  Although the Python programming is great, I wouldn´t recommend learning about linear regression here.",True
@rafid1998,2017-12-24T14:21:26Z,0,0:30 ..,True
@aurobindomondal13,2017-11-29T14:45:04Z,0,"Hello sentdex, I have a small doubt. What if the X = [1, 2, 3, 4, 5] and Y = [5, 5, 5, 5, 5]. In this case the best fit line would be y = 5, and mean_y would also be 5. So the R squared would turn out to be zero. But the best fit line is a very good predictor which is opposite to what you say that (more the R_squared value better it is). Please explain. Thanks in advance",True
@mavriksc,2017-11-22T23:51:57Z,5,"def r_sq(regression_line, ys):     se_y_hat = sum((ys - regression_line)**2)     ys_mean = mean(ys)     se_y_bar = sum((ys - ys_mean)**2)     return 1 - (se_y_hat / se_y_bar)",True
@timharris72,2017-10-20T19:04:41Z,0,These are really good explanations for a subject that is one of the more difficult topics to understand in computer programming or math.,True
@faisalel-shabani8550,2017-09-14T02:14:27Z,0,"Love your videos sentdex ! Just wondering if you ever get to reducing the dimensional space. When do you start taking that into account ? That would be useful for the Stock market example, as some features might not be relevant.",True
@PandemicGameplay,2017-09-01T22:17:43Z,45,When Edward Snowden be teaching Python analysis in Russia....,True
@rancheng5037,2017-06-29T15:09:25Z,0,"Hey, guys.  I try the same method but get the different answer, because I did not set the data type in np.array  when I add the data type, I got the same answer as the video.  Can anyone please tell me why, thanks",True
@g0rth0rTBL,2017-06-20T21:25:41Z,3,Isn't that for loop ([mean(ys_orig) for y in ys_orig]) not needed?   [mean(ys_orig)] * len(ys_orig)  would seem less taxing and return the same.,True
@huanyumao5530,2017-06-15T10:07:41Z,3,i meet the error:   sum((ys_line - ys_orig) ** 2)  unsupported operand type(s) for -: 'generator' and 'float',True
@hoanganhkhoil,2017-06-08T20:55:27Z,1,"Hi Sentdex, first of all thank you for all the tutorials you've made. They're just wonderful. However, I think there is a misunderstood here. I think the bigger the R_square is, the more accurate the regression is. Let's say, if the predicted_output and the real_output is not much different, so the square_error of y_regression is nearly zero. You divide it to the square_error of y_mean you would get something close to zero.  Finally, R_square = 1 - (something_close_to_zero) = 0.999999....  Just saying.",True
@misreadincognito2454,2017-06-01T07:21:10Z,0,i like your teaching style..Its fun,True
@bendhimanish,2017-05-25T02:21:35Z,0,i felt it kind of hard can you suggest me any statistics class or any basic stats stuff that would help me,True
@sparshkumar69,2017-05-11T10:10:58Z,2,For people getting a TypeError. Just change the regression_line variable to an np.array like this  regression_line = np.array([(m*x)+b for x in xs]),True
@KshitizRimal,2017-04-27T11:11:42Z,0,"Great tutorial. I was wondering why haven't you talked about gradient descent, cost functions and other ml terminologies and techniques?",True
@cyl5207,2017-02-13T08:38:22Z,0,I love you too sendtex!,True
@chenchunwu3674,2017-01-19T18:41:03Z,0,"Thanks for the video, Sentdex. I met an error like this ""TypeError: unsupported operand type(s) for -: 'generator' and 'float' "" And it's seems happen when ys_line - ys_orig? Is this because different type between the variables of regression_line(list) and ys (numpy.array). However, I tried to align the type to numpy.array and use np.subtract but it still won't work. Could anyone help to check it? Thanks",True
@CarsonJamesCook,2016-12-20T08:42:15Z,27,"For anybody else who was initially confused, in squared_error he can use:  sum((ys_line - ys_orig) ** 2)  because ys_orig is a numpy array. If you attempt to do this with 2 list objects, python will throw an error. Numpy arrays perform vector subtraction ----> [4, 5, 6] - [1, 2, 3] = [3, 3, 3].",True
@yashagrawal6850,2016-11-03T18:10:52Z,0,"in the squared_error() function i am unable to understand that on increasing the exponent from 2 to 4, 6, 8 why it is that r^2 is increasing from 0.5 to .8,.99 at last ? i mean does this mean that raising power increases efficiency? please explain",True
@Darthdeedee91,2016-10-07T04:28:15Z,0,"Hey Sentdex, im not sure why this did not work but my sqr_error function kept failing where it took ys and regression_line arrays. however I do the same thing without a function and it wokrs ad gives me the sum sqr_error. absolutely drives me nuts. my error is unsupported operand type(s) for -: 'generator' and 'float' seems like some. the wierd part is i dont use the function and hard code the exact same thing using same operand and arrays without calling a function and it works.",True
@ankitSharma0808,2016-09-19T06:28:12Z,0,Why have you written that for loop in  [mean(ys_orig) for y in ys_orig]??,True
@ashishkarn068,2016-08-13T18:44:23Z,0,"+sentdex In the first statement of the function coefficient_of_determination, I think it should be just  y_mean_line = mean(ys_orig)  instead of  y_mean_line = [mean(ys_orig) for y in ys_orig]    Please correct me if I am wrong",True
@xVbM1,2016-04-26T19:08:12Z,0,"Hi, I was just looking at you csv videos on python, they are very useful however i dont think you cover how to read a particular column from your csv file into python. Could you possibly comment back on how to do so?I have an assignment due friday and this will really help. Thanks",True
@vVesleyPavan,2016-04-26T01:33:18Z,0,"Hello , Sentdex. How many episodes this series will have? How often do you plan to release the videos? Thanks in advance! (I've learned a lot from your videos on python! They are very didactic and objective in the presentation of the content!)",True
@pakdhenu7990,2016-04-25T14:28:42Z,1,"Hi, Sentdex. I just watched Ron Bekkerman's Linkedin presentation on Machine Learning. Were you there on the presentation? Because somehow I heard your voice. :)",True
@protubeGT,2016-04-24T21:01:26Z,1,You should teach more with pyopengl,True
@oliveredholm4284,2016-04-24T13:06:36Z,27,I love you sendtex! I'm 13 years old and I've pretty much learned everything I can about python from you.,True
@levyroth,2016-04-24T11:35:56Z,1,"This explains really well my original instinct of saying that the R^2 value is not that relevant unless you actually look at residuals too. And the value itself is not that informative, in social sciences anything above 20% is considered good, whereas when predicting credit risk, an equivalent 80% is considered a low value. http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit",True
