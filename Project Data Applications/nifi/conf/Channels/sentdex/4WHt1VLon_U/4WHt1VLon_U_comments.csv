author,updated_at,like_count,text,public
@elbozo5723,2023-03-14T22:33:06Z,0,"19:25 I assume you already tried adding the newline token in special_tokens when training the tokenizer. Did you try adding the newline token to GPT2Tokenizer as well using tokenizer.add_tokens([""<N>""])",True
@motintv9232,2021-06-19T12:00:10Z,0,"how to solved this issue  self._tokenizer.train(files, trainer=trainer) Exception: stream did not contain valid UTF-8",True
@PepeTostado,2021-05-18T18:17:00Z,0,"If I want to make an app that is kind of like binance, it would be for trading crytpo and stuff, what do I need to learn django and mysql?",True
@PepeTostado,2021-05-17T06:12:41Z,0,What did you study to learn all this?,True
@manlypie,2021-05-17T05:43:09Z,1,lmao  that moan in the intro,True
@markojozic3944,2021-05-17T00:25:32Z,1,I did not realized that the book is already finished :O I bought the E-Book version  immediately and I'm so happy with it :D,True
@mxd8,2021-05-16T22:19:06Z,0,im so behind on sentdex....,True
@user-hk2jx5mj6z,2021-05-16T19:28:13Z,2,The character ƒ† is a prefix to indicate that the token is preceded by a whitespace character.,True
@fast_harmonic_psychedelic,2021-05-16T11:23:54Z,1,"use CLIP to tokenize.  IT can also encode text and encode image into tensors of the same shape, 1, 512",True
@guialmachado,2021-05-16T04:15:17Z,0,"Sorry, this is too complex for me, what does this video helps to solve?",True
@EtienneCharlier,2021-05-16T02:45:44Z,6,"Code to load your saved tokenizer from the json is:  from transformers import PreTrainedTokenizerFast  tokenizer = PreTrainedTokenizerFast(tokenizer_file=""tokenizer.json"")",True
@ruchi993,2021-05-16T02:16:46Z,1,Can you please tell how I can start coding if I'm really really scared of coding and I just want to overcome my phobia of coding and want to build my future in Data science.,True
@ramtinnazeryan,2021-05-16T02:11:10Z,0,I want my university to buy your book for me. they will accept only from Amazon. could you make it available there?,True
@nro337,2021-05-16T01:01:00Z,0,Thanks!!,True
@mangaart3366,2021-05-15T22:43:41Z,0,I was waiting for this! So happy to see another upload!,True
@Xaelum,2021-05-15T21:02:39Z,13,"If you want to tokenize '\n' as a special token you can simply do:  newline_token = tokenizers.AddedToken(""\n"", normalized=False) my_tokenizer.add_tokens(newline_token)",True
@theohallenius8882,2021-05-15T18:35:28Z,0,"I wonder, will it be able to tokenize any language, and not just python?",True
@HT79,2021-05-15T18:26:23Z,8,"Man, I never skip the ads from your sponsor XD",True
@Ashesoftheliving,2021-05-15T17:58:28Z,0,About to watch the video! Do you work on pdf extractor with image and table data on top of text data? Need some ideas!!,True
@usamatahir7091,2021-05-15T17:50:23Z,0,Yoo!!,True
@d33w,2021-05-15T17:48:29Z,1,can't wait for the model training video,True
@furkank5614,2021-05-15T17:41:38Z,0,Is there text based version available of this code?,True
@ruchi993,2021-05-15T17:20:42Z,1,You're videos are really cool and intresting.,True
@user-xl8uo9gp9p,2021-05-15T17:20:27Z,3,"print(""second"")",True
@_boris,2021-05-15T17:20:16Z,11,Notification squad üôã‚Äç‚ôÇÔ∏è,True
@Stinosko,2021-05-14T16:17:33Z,0,Howdy ü§†üëª,True
