author,updated_at,like_count,text,public
@Sqwan2,2023-10-30T12:33:39Z,0,Why do we create a feature list with 3000 words for each review?,True
@cesarenriquevillarguerra8270,2022-09-07T05:44:36Z,0,"I have a question: what do you pretend to do with:  featuresets=[(find_features(rev),category) for (rev,category) in documents] ?",True
@joxa6119,2022-04-25T01:53:25Z,0,I can't understand from which line that the code separate between positive and negative words?,True
@JoeGaz,2021-02-25T01:43:25Z,1,Disabled my ad blocker just for you Sentdex. Amazing work and thank you so much for clear and concise explanations!  :) <3,True
@SurajKumar-bw9oi,2020-12-14T07:29:41Z,3,For beginners: There is already a function present in sci-kit learn to get the feature words Use this chunk of code-  from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer(analyzer = ..... ) print(vectorizer.get_feature_names())  It also performs the count vectorization of the sentences.,True
@harshvivek14,2020-06-22T11:09:07Z,1,When I printed word_feature I got output as same as all_words[:3000] Because most frequent 3000 words are not stored in. To do so we have to write:  common = list(all_words.most_common(3000)) word_features=[ ] for i in common:     word_features.append(i[0]),True
@aravindsivalingam3091,2019-04-27T11:36:22Z,1,"At 1:22, 'all_words.keys()' doesn't return the keys sorted from highest frequency to lowest, so we are not getting the top 3000 words but instead the first 3000 unique words from the dataset. To get the list of top 3000 words we can use: 'word_features = [tupl[0] for tupl in all_words.most_common(3000)]' since all 'all_words.most_common()' returns a list of tuples (each tuple containing the word and its count)",True
@GelsYT,2019-04-09T17:50:56Z,0,what happened to the documents variable?,True
@palbhanazwale7747,2019-01-29T10:11:13Z,0,Anyone got an error like list indices must be integer .. hence i tried creating dictionary instead,True
@SurajKumar-ny1lx,2019-01-17T09:11:44Z,1,"featuresets = [(findfeatures(rev),category) for (rev, category) in documents]  what this code is doing here, can you explain please??",True
@sidharthgoutamkarji6563,2018-11-28T10:17:39Z,0,There's a squeezed text error being displayed. Can you please tell me how to fix it,True
@physicsgurukul8738,2018-10-26T08:44:31Z,2,"This line : featuresets = [(find_features(rev), category) for (rev, category) in documents] can be re written as ->  featuresets = [ ]     for d in documents:           dtuple = (find_features(d[0]), d[1])         featuresets.append(dtuple)    featuresets = [ ] : Creating an empty list for d in documents: = Accessing every element of documents. Please remember that documents is a list of tuples where each tuple contain 2 elements i.e. first element is list of words of every review & second element is its category (pos/neg) dtuple = It is temporary tuple which will contain 2 element i.e a list returned by function find_features & its category(pos/neg) d[0] = first element of each tuple in document d[1] =  Second element of each element in document featuresets.append(dtupple) = Appending every temporary dtuple to featuresets",True
@piyushraj9561,2018-09-16T05:02:03Z,0,last one liner how does it work?,True
@aliakbarsiddiqui2823,2018-07-16T08:55:13Z,1,Thankyou for all the hard work you are doing. It really helped me alot. God Bless.,True
@rohinmahesh1735,2018-07-12T19:39:41Z,3,"Could you possible explain the logic behind this line of code? I understand up until this point:   featuresets = [(find_features(rev), category) for (rev, category) in documents]",True
@zakk4al1,2018-06-26T10:20:12Z,0,why are the true or false(presence or absence) of words in feature required. where are they used and what purpose do they provide??,True
@sanyamgupta5571,2018-06-26T02:50:29Z,0,Correct me if I'm wrong.  all_words contain a list of words from all the 2000 movie reviews with their frequency.We built find_features on the basis of this. In the following videos we use the same method to built up a classifier by training it on first 1900 samples. But already the words in test samples were used in all_words so does this means that we have trained the classifier over test samples also.   I tried to run it over a Shawshank's review and it is giving it a <neg>. Something's really wrong.,True
@andrewdennis6976,2018-06-19T15:10:11Z,0,If a term is used less frequency but it is super important for the classification will do any of the algos in your next video take that into account?,True
@prashantarora752,2018-06-10T08:17:55Z,0,"on running the code , it gives an error of       'CategorizedPlaintextCorpusReader' object is not callable  ........ what should i have to do with it to get it removed???",True
@sdsunjay,2018-03-03T07:53:25Z,0,There is a mistake in the code. It should be features[w] = (w in words) . NOT features[w] = {w in words},True
@davidsimmonds4182,2017-11-26T18:21:21Z,10,"Love these tutorials. They are fantastic!! But I went through this one 3 times (rewinding a lot each time) and I don't understand about 1/4 of it. A diagram or flow-chart might help a lot, I think. Even though I do plan to run it with lots of print statements. I haven't done that yet - my bad. I'm sure seeing the contents of the data-structures that are created might help a lot.",True
@jenniferklemisch585,2017-10-02T18:53:23Z,0,"Hi - do you have or know of a active debugger for python? I'd love to go to a web address, type in some dummy data, paste in my code and step through the code to see whats happening so I can trouble shoot.  I'm trying to follow along with your NLP videos, but instead of using documents I have a pandas dataframe. I've got a column in my data that I've tokenized, but i can't sort out how to do the find_features function, when feeding it a row of data.",True
@siddharthuniyal1615,2017-09-11T08:52:22Z,1,"Hi,  can anyone clear the concept of using 3000 topmost words only in ""list(all_words.keys())[:3000]"" why not all words present in the all_words",True
@codeerrors4168,2017-08-23T01:38:46Z,0,"required to create a model that can discriminate between English, Afrikaans, and Dutch phrases. A labelled dataset of phrases is provided in a csv file. how can i better solve this problem?",True
@simonchan2394,2017-08-07T20:20:30Z,11,"When Harrison uses this piece of code: list(all_words.keys())[:3000] , he is actually extracting the 3000 least commonly used words in the movie reviews - when the nltk.FreqDist(all_words) code was run, it sorted the words from the least common to most common. He did it to exclude the most common words, like ',' or 'the'. I think the reason why he did it is because unique words with fewer frequencies are a better indicator of whether the review is positive or negative because having 'the' 76529 times in movie reviews shows us absolutely nothing about the sentiment of the review but having the word 'miscasting' 4 times might be a stronger indicator that the review is negative. Plus unique and infrequent words would assist in training the machine better as to what is a positive or neg review.  You can run this code to find the most common words used in movie reviews:  commons = all_words.most_common(15) my_ids = [word for word, count in commons] print(my_ids)",True
@ishanpatil6892,2017-07-22T19:36:31Z,6,M not getting the actual logic behind this !!! can anyone explain in detail about this ...,True
@zhuliang4283,2017-07-21T04:09:09Z,3,"list(all_words.keys())[:3000]   doesn't give us the most common 3000 words, instead it gives the first 3000 words in its original order. We should instead use:   [w(0) for w in all_words.most_common(3000)]",True
@bellasun3820,2017-07-07T07:51:29Z,1,"please, save me : what's the meaning of ""rev""?",True
@Catatafish0,2017-05-08T18:01:31Z,9,"Why can't we remove the punctuation and the stopwords? That way we will get only the ""important"" words.",True
@manankalra7978,2017-01-14T05:46:46Z,0,"- How can I test the same for a different .txt file? - As mentioned, movie-reviews is a sub-directory of corpus and it contains some textual data. Where do the functions like '.words(), .categories(), .fileids()' used in this tutorial come from?",True
@omaral-janabi9186,2016-10-19T11:58:21Z,0,"your tutorial very helpful, thanks indeed, from my heart.   i'm wondering if any one can explain me, """"'why i cod't use my own file instead of ""movie_reviews"""""""" thanks",True
@StoicMindAIShorts,2016-09-13T20:20:57Z,0,"Hi sentdex! Thank you for your tuts! They are awesome! I'm get this error: Traceback (most recent call last):   File ""sent.py"", line 31, in <module>     print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))   File ""sent.py"", line 27, in find_features     features[w] = (w in words) TypeError: list indices must be integers, not unicode  Any hint?",True
@Cherry-jr2kq,2016-08-09T03:08:32Z,0,"I get only one word in the output, and with each run, the word printed out varies. here is one example: {'dovey': False} my code is:import nltk from nltk.corpus import movie_reviews import randomdocs=[(list(movie_reviews.words(fileid)),category)       for category in movie_reviews.categories()       for fileid in movie_reviews.fileids(category)]   random.shuffle(docs)allwords=[] for w in movie_reviews.words():     allwords.append(w.lower()) allwords=nltk.FreqDist(allwords)wordfeatures=list(allwords.keys())[:3000] def findfeatures(doc):     words=set(doc)     features={}     for w in wordfeatures:         features[w]=(w in words)         return features print((findfeatures(movie_reviews.words('neg/cv000_29416.txt')))) what is wrong with the code? as I continue with the next video,  I get output as:                beristain = False             neg : pos    =      1.0 : 1.0 Most Informative FeaturesNaive Bayes Algo accuracy percent: 49.0{'beristain': False}",True
@dikshabadaya6378,2016-07-19T12:26:17Z,0,how can we classify the words as function words for gender prediction  within a document ?,True
@gabiayako,2016-07-05T19:06:58Z,3,"Harrison, thank you so much for your videos, I am learning a lot with them! :)  I am a beginner and I am trying to determine who won a lawsuit based on some sentences. So I am importing data from excel and I followed the steps from your tutorial, but I couldn't adapt this line:  featuresets = [(find_features(rev), category) for (rev, category) in documents]  how can I access data from some sort of ""rev"" and ""category"" from excel?",True
@ezzanadeem6779,2016-06-25T16:23:06Z,1,you have used a file neg/cv000_29416.txt ... what is this for ?,True
@mega6699,2016-03-09T11:40:59Z,0,"Why did you choose yo use 3000 words? Where does this number come from?  Why do we need some number of most frequent words for this task? It is not obvious. In fact, it was mentioned that the most frequent words are useless for sentiment analysis. So there is chance that by taking only the 3000 most frequent ones we are leaving out the important words for sentiment analysis.",True
@Arslanqadri,2016-02-10T19:58:32Z,0,"why do we want to give training examples with a Boolean value, can we not simply train the model using word+category data set? {[ word1, True word 2, False word 3, False ] positive,}",True
@lenofire586543,2015-12-26T15:06:29Z,0,"I am getting the following error: Traceback (most recent call last):   File ""python.py"", line 22, in <module>     print((find_features(movie_reviews.words('neg/cv000_29416.text'))))   File ""/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/plaintext.py"", line 163, in words     self, self._resolve(fileids, categories))   File ""/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/plaintext.py"", line 84, in words     in self.abspaths(fileids, True, True)])   File ""/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/api.py"", line 190, in abspaths     paths = [self._root.join(f) for f in fileids]   File ""/usr/local/lib/python2.7/dist-packages/nltk/data.py"", line 322, in join     return FileSystemPathPointer(_path)   File ""/usr/local/lib/python2.7/dist-packages/nltk/compat.py"", line 564, in _decorator     return init_func(*args, **kwargs)   File ""/usr/local/lib/python2.7/dist-packages/nltk/data.py"", line 300, in __init__     raise IOError('No such file or directory: %r' % _path) IOError: No such file or directory: u'/home/lenofire/nltk_data/corpora/movie_reviews/neg/cv000_29416.text'  i use ubuntu 14.04 ,the path is different from windows? what should i do?",True
@linsongchu9944,2015-12-05T04:48:06Z,35,"for your ""word_features = list(all_words.keys())[:3000]""   I don't think this give you the top 3000 features since list(all_words.keys()) does not have any order inside, yes? Maybe we can use something like ""word_features = [w[0] for w in sorted(all_words.items(), key=lambda (k,v):v, reverse=True)[:3000]]""",True
@NiharikaGujela,2015-09-01T18:33:05Z,0,features[w] = (w in words) is generating a TypeError : 'tuple' object does not support item assignment.  Why so?,True
@mohammedabujayyab6146,2015-08-26T06:57:30Z,0,"Just a question please: what should I do if I would like to build a strong text classifier by taking into account the lemmatization  instead of stemming or taking both; any suggestion :) Thanks and please continue giving more useful tutorial like this, I like all of your tutorial,, it is great!  Bests, Mohammed",True
@anuragsrivastava1282,2015-08-11T16:15:24Z,0,"I am getting the following error: ""IOError: [Errno 24] Too many open files: u'C:\\Users\\AppData\\Roaming\\nltk_data\\corpora\\movie_reviews\\pos\\cv380_7574.txt'. Basically, not all the elements in the ""documents"" list are accessible. What should I do?",True
@Amit-pf2ri,2015-07-10T09:09:17Z,6,"I am not able to under stand some portion of it.  while doing  all_words = nltk.FreqDist(all_words)  word_features = all_words.keys()[0:3000]  in 1st step we get a dictionary of words but words are NOT arranged by their frequency count. So all_words.keys()[0:3000] may contain useless words like ',', '.', '-' etc.  To get a better feature set we can do something like this  stpwrd = dict((sw,True) for sw in stopwords.words('english')) all_words = [w.lower() for w in movie_reviews.words()] if len(w) > 3 and not stpwrd.get(w)]  In this way we not only remove any word shorter than 3 character but also the stopwords. Hopefully you will do something like this in later videos. please comment if I am doing something wrong.",True
@Amit-pf2ri,2015-07-10T08:45:13Z,6,"A pythonic version of find_features():   def find_features(document):     words = set(document)     return dict((w,True if w in words else False) for w in word_features)",True
@ajlu5955,2015-06-28T08:51:16Z,0,Thanks a lot man for all the tutorials!  Why does word_features = list(all_words.keys())[:3000] return different 3000 words every time? It is supposed to return the 3000 most frequent words right? what is .keys() for? Thanks :D,True
@RutgerdeKnijf,2015-05-19T18:04:49Z,0,sentdex Why does nltk.FreqDist() return the words in a different order every time I run it (with the same dataset)? And thanks a lot for these tutorials btw!,True
