author,updated_at,like_count,text,public
@hetal.priyadarshi,2021-08-21T17:24:01Z,0,"According to new update on Github, the score is capped to 200. To test your model further, add:  ""env._max_episode_steps = 500""  after ""env.reset()"" in the for loop:  ""for each_game in range(10): """,True
@soominkim1660,2021-04-23T07:30:25Z,0,"Hi! Thanks for the video. I am running the .py script via pycharm, and I would like to save the model with model.save('modelname_.model') command, but I cannot really do it in the Run section in Pycharm. Does anyone know how to perform model.save command after running the python script, instead of putting that command in the python script itself?",True
@jeffwads,2021-01-09T05:15:50Z,0,Too bad no one has used this for something like Magic the Gathering.  Unknown data type of scenarios.,True
@Joffrerap,2020-09-10T17:01:43Z,0,"at 3:40 what's the difference between ""prev_obs.reshape(-1,4,1)[0]"" and ""prev_obs.reshape(-1,4)"" ? if there is none, why do the first one? is that a habit from something else?",True
@manavdahra207,2020-03-30T11:58:46Z,1,My accuracy is above 60%. But my avg score is 9. Any idea what am I doing wrong ? I am using keras instead of tflearn,True
@joelhaftel5239,2020-03-23T20:18:12Z,0,Mine had an average score of 500,True
@neatpolygons8500,2020-02-29T13:06:31Z,0,When i play your videos at .75 speed you sound very normal actuallly,True
@paolopagani8285,2019-09-05T12:53:39Z,0,Awesome tutorial! Just one question. Which kind of machine learning method is it? Like generating training data by interacting with the environment. It looks like somthing inbetween reinforcement learning (environment interaction) and supervised learning (training with labeled data).,True
@KaziAbidAzad,2019-08-17T19:29:05Z,0,"Hi, big fan here. I changed  the goal_step to 5000 and total games to 10000. I am getting an average score of 1479 !!! With many scores upto 5000! Uploaded my model here: https://www.dropbox.com/sh/1o8ic24qhoamc9o/AADMvdTkk2lYNvfYPBhjOwwBa?dl=0 I am doing it wrong?",True
@user-cv9uh6mb8c,2019-07-04T23:15:53Z,0,"It seems like there is no difference between adding or not adding that ""[0]"" at the end model.predict method. Can anyone explain this to me?",True
@daniel89123,2019-07-01T14:51:01Z,0,"shouldnt you have 3 possible actions instead of just 2. left,right, or nothing, so if its good your shit doesnt fuck itself up",True
@liangyumin9405,2019-05-27T04:49:40Z,0,I want the requirements.txt,True
@b3armonk,2019-05-01T06:34:22Z,0,Post your scores here and lets see who's king.,True
@VojtechMach,2019-03-18T22:50:19Z,0,Can someone  please explain how is the network able to learn anything from initially random data? I mean in the beginning he performed some random actions and trained the network on them. This shouldn't be learning anything in my opinion.,True
@aritromukherjee9862,2019-03-06T03:10:09Z,0,"When i run the sample code form https://pythonprogramming.net/openai-cartpole-neural-network-example-machine-learning-tutorial/ , For the line print(model.predict(prev_obs.reshape(-1, len(prev_obs), 1))) ,  I am facing this error : AttributeError: 'NoneType' object has no attribute 'predict'. Any suggestions, as in how to resolve this issue?",True
@qtmomo,2019-03-01T05:32:58Z,0,at 4:00 Lord Sentdex said that we take the 0th element of the list because we are only predicting on one frame. Anyone knows how to make predictions based on multiple frames ?,True
@omidasadi2264,2019-02-04T15:49:21Z,0,"It was great.... But about the accuracy, what should I do to improve it? just by adding some hidden layers?",True
@VojtechMach,2019-02-02T17:51:04Z,0,"Correct me if Im wrong, but this isnt a reinforced learning in practice is it? He created a labeled dataset from which he trains a network in a supervised way.",True
@InsertWaffle,2019-01-02T11:23:34Z,0,"i get this : Traceback (most recent call last):   File ""learn.py"", line 153, in <module>     action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0]) AttributeError: 'NoneType' object has no attribute 'predict'  any solution?",True
@ankitrokde4907,2018-12-28T04:57:36Z,0,How does data generated from random actions help here? Shouldn't we have some good policy for taking actions and generating training data? Also after each run the results are vastly different. In real world this may not be desirable. How do you generate reproducible results with your current design?,True
@snowboyyuhui,2018-12-22T18:00:52Z,1,"when I run the train_model function more than once in jupyter, it throws this error: ""You must feed a value for placeholder tensor 'input_1/X' with dtype float and shape [?,4,1]   [[Node: input_1/X = Placeholder[dtype=DT_FLOAT, shape=[?,4,1], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]"" why is that? it always works the first time I start the notebook but not after that",True
@Richard-by2zn,2018-11-03T02:49:35Z,0,how can i run my saved models?,True
@maxitube30,2018-10-13T00:29:36Z,0,love this video.... i saw tha gym is used with universe(not avable on windows). can someone tell me the difference between this libraries?,True
@rankoradovanovic6298,2018-09-25T09:08:55Z,0,"I have a problem, every time I try to run the game through the environment it crashes (not responding), any ideas?",True
@jtdyalEngineer,2018-09-20T02:36:06Z,0,"Thank You so much @sentex !  after applying the .env fix, then playing with: initial_games, number of layers, layer size, n_epoch watching the model play is fascinating and human-like with certain settings.   For others trying this:     change model = tflearn.DNN(network, tensorboard_verbose=3)    then  run:   c:\tensorboard --logdir C:\tmp\tflearn_logs\openai_learning    it will let you see your changes afecting the neuralnet    reducing the relu layers and size and then increasing the n_epoch makes for more human-like reactions    Use (TF 1.11, CUDA 9.0, cuDNN 7.0 and Python 3.6.6) not the latest or your waste hrs.",True
@gabrielwilliams7360,2018-09-18T18:40:58Z,0,"I am trying to use this code for pong but I am getting an error at this phase : if data[1]: output ect... because it has more than 2 outputs, how do I fix this?",True
@TristanTradgedy,2018-08-27T14:19:08Z,3,"My best after a bit of tweaking and a few games is 462.3 - I can't believe that this time last week I didn't know the first thing about python and while I won't pretend to understand all of this, I understand the statements, functions, variables etc and to now to have my own AI learning to play a game... it's mind blowing!",True
@szymonkosakowski4373,2018-08-23T07:30:46Z,0,"Hello thank for the tutorial! I converted the code to learn to play snake, so it has few more inputs and outputs, but im getting error in reshaping training data array, it says ""setting an array element with a sequence"" when passed to model.fit(). Could you please help me?",True
@vaporware7572,2018-08-18T13:50:04Z,0,Mine keeps resetting half why is this? The pole is not even falling over? Any help or advice of how to fix?,True
@muratsunbul7346,2018-08-06T04:11:19Z,0,very scary it learns ._.,True
@j3r3miasmg,2018-06-29T04:33:17Z,0,"If you don't use the games on the fly to improve your model, why raise the number of games?",True
@zim6212,2018-06-25T22:27:58Z,0,"assert self.action_space.contains(action), ""%r (%s) invalid""%(action, type(action)) AssertionError: 2 (<class 'int'>) invalid  please help",True
@harshpanchal2618,2018-06-14T17:18:59Z,0,AttributeError: 'Viewer' object has no attribute 'window' evn.render() not working pls help,True
@TheGamesProjecttv,2018-04-30T05:20:23Z,0,"when trying to retrain based on the previous game i get this error:  tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1/X' with dtype float and shape [?,4,1][[Node: input_1/X = Placeholder[dtype=DT_FLOAT, shape=[?,4,1], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]] The code that its talking about is when it is defining a new model",True
@Ashwin.M.S,2018-04-24T16:12:25Z,0,Can someone pls help. This model for me seems to be capping to a max score of 200 per episode for me . The only possible conclusion i could come to was that game ran for 200 frames and the pole lasted throughout. But i see you get score>300....and now im confused ....Any suggestions as to why this may be happening/how i can fix it?  Thank you in advance :),True
@Ashwin.M.S,2018-04-10T16:32:20Z,0,Great video set!! LoveD it,True
@d1gBR,2018-03-16T01:17:14Z,0,"got this error>     action = np.argmax(model.predict(prev_obs.reshape(-1, len(prev_obs), 1))[0]) AttributeError: 'NoneType' object has no attribute 'predict'   how to solve it ?",True
@princeofexcess,2018-03-11T00:54:38Z,0,can you find the entire file to copy paste?? I have a file but its not training that well and i want to figure out if i made some simple logic error. The website seems to have code snippets but not an entire file. Just a small annoyance..,True
@plopking2882,2018-03-04T16:02:37Z,0,Is it normal to have a score of 500?,True
@MoneyStudyLeeJongSung,2018-02-23T18:42:17Z,0,good!,True
@funkupgamer5364,2018-02-17T12:18:59Z,0,i got AS : 330,True
@johnbyun98,2018-01-28T15:22:41Z,0,"Can we make this machine significantly better by allowing machine to observe the future_observation when making decision(action)? it seems like it doesn't know if the stick is tilting to the right, the bottom part should move much more faster to the right to tilt the stick left..",True
@ryanmccauley211,2018-01-25T20:44:31Z,0,"I struggled to get a model above 100. When I did it got a perfect score everytime it moved left, but performed poorly everytime to the right. What mechanisms cause the difference in performance between the direction it moves?",True
@vichnaiev,2018-01-13T21:39:48Z,2,"Actually, the threshold for winning the version 1 of Cart is a score of 450 over 100 consec games, so it's not quite there yet.",True
@aidanventer4747,2018-01-12T10:00:19Z,0,"how do I load a previously saved model, if you don't feel like explaining it, please show me the part where he explains it",True
@Burn1nSun,2018-01-02T23:46:09Z,0,I think the title should say that this is part 4,True
@biggergez,2017-12-27T03:23:46Z,0,what happen to part 3 ?,True
@williamtan3857,2017-10-31T06:07:28Z,0,"same code, 197 at first time. why?",True
@father_michael_pkk,2017-10-30T16:49:13Z,0,"Nice, but it has nothing to do with reinforcement learning, which is Open AI gym is about – ""A toolkit for developing and comparing reinforcement learning algorithms.""",True
@DaekTwentri,2017-10-01T07:54:55Z,5,Sentdex! I didn't quite understand when you were explaining how to load a model. Could you show an example how it looks like?,True
@emregeylani,2017-09-24T19:31:01Z,0,epoch != epic,True
@weratebikes6406,2017-09-20T02:29:44Z,0,"before you ran the 386 final result, why did it give such a poor result (did it run an 'untrained' model), thanks and great video!",True
@mrrealpx3189,2017-09-16T16:02:06Z,0,"good job, you could solve this with cybernetics in one video, BUT lets use NN for everything",True
@8o8inSquares,2017-09-14T13:30:59Z,0,I didn't change anything and it gave me 197.6,True
@zachgleason8301,2017-08-18T23:09:41Z,6,Could you show us how to update our neural net from this episode rather than training on a random sampling?,True
@badnam3189,2017-08-08T20:59:50Z,0,"Why isn't each of the 10 games the same, considering you use the same model. Are there random movements in the enviorment or am I missing something?",True
@alexcostello2911,2017-08-08T12:06:08Z,0,i cant seem to get an average score above 10 - 20 and im not sure why,True
@vulnerablegrowth3774,2017-07-14T01:11:20Z,0,"Hey Sentdex, what do you mean by 'headless' at 4:50? What type of example is headless and what type isn't?",True
@hyoungjunkwon2253,2017-07-11T00:11:50Z,0,"thanks for your video, it's really easy to understand  cart-pole",True
@dLoLe,2017-07-09T19:58:12Z,0,"Hey, you can use observation = env.reset() instead of having to write a special condition for the first step  Also, you don't really need a separate variable for prev_observation. You can just structure the code so that before reassignment observation is considered prev and after reassignment it's considered the current one",True
@aakupsp,2017-07-06T03:08:53Z,0,"Hello Harrison Kinsley sir, I am trying to solve Open Ai's Lunar Lander challenge using this algorithm. I am not getting the desired output, my neural network just predicts one action out of 4 available actions. Can you take a look at my algorithm and tell me where I am wrong. Thank you.  Link to algorithm: https://github.com/akuchotrani/MyOpenAIGym/blob/master/LunarLanderBox2D.py",True
@DoYouBelieveInMiracles,2017-06-29T08:19:56Z,0,"Hey sentdex, thank you for your amazing courses! Everything has been right for me up to this video. While my syntax is correct, the choices I get in the end are both 0%. Why is that? Also I am getting quite different results from you, is it because of my mid-level CPU? Thanks. :)",True
@EinaiMoriTsipoura,2017-06-26T11:23:19Z,0,Hi Sandex - I think one of the biggest problem in this games is to create the training dataset Do you think that a good solution would be to create a simple model first and then use it to create training data to fit a another model? Is this a good approach?,True
@Akshatgiri,2017-06-08T20:18:14Z,0,"Hey Sentdex,  I saved the data and output from the trained neural network. I thought I could simply use that data to train the model again and it will become better. But the model keeps on overfitting and gives me a accuracy of 90%+. I have look at the data and it is pretty spread out. I don't understand what might be the issue. Could you point me in the right direction?  My code is super messy, but if it helps i'll put it here. Run the code once to save the dataset from the trained neural net. When you run the code again just change the is_saved_data to True.  and comment out the part towards the bottom, I have the left the comments there. You will be able to replicate the overfititng.   import gym import random import numpy as np import tflearn  from tflearn.layers.core import input_data, dropout, fully_connected from tflearn.layers.estimator import regression from statistics import mean, median from collections import Counter  LR = 1e-3 env = gym.make('CartPole-v0') env.reset() goal_steps = 500 score_requirement = 50 initial_games = 10000 epochs = 3  # I don't need this (just for testing purposes) def some_random_games_first():     for episode in range(5):         env.reset()         for t in range(goal_steps):             env.render()             action = env.action_space.sample()             observation, reward, done, info = env.step(action)             if done:                 break              #some_random_games_first()              def initial_population(is_saved_data=False, saved_data=''):     # Making the random dataset        training_data=[]     if not is_saved_data:             training_data = []         scores = []         accepted_scores = []         for _ in range(initial_games):             score = 0             game_memory = []             prev_observation = []             for _ in range(goal_steps):                 action = random.randrange(0, 2)                 observation, reward, done, info = env.step(action)                                  if len(prev_observation) > 0:                     game_memory.append([prev_observation, action])                                      prev_observation = observation                                  score += reward                 if done:                     break                          if score >= score_requirement:                 accepted_scores.append(score)                 for data in game_memory:                     if data[1] == 1:                         output = [0, 1]                                              elif data[1] ==0:                         output = [1, 0]                                              training_data.append([data[0], output])                          env.reset()             scores.append(score)                      training_data_save = np.array(training_data)         #  this saves the data         np.save('saved.npy', training_data_save)                   print('average accepted score:', mean(accepted_scores))         print('median accepted score:', median(accepted_scores))         print(Counter(accepted_scores))     #Loading the dataset     else:         training_data = np.load(saved_data)         print(training_data)     return training_data   def neural_network_model(input_size):     network = input_data(shape=[None, input_size, 1], name='input')          network = fully_connected(network, 128, activation = 'relu')   # network, 128 layers, rely = rectified linear     network = dropout(network, 0.8)  #  0.8 is the keep rate which is odd          network = fully_connected(network, 256, activation = 'relu')     network = dropout(network, 0.8)      network = fully_connected(network, 512, activation = 'relu')     network = dropout(network, 0.8)      network = fully_connected(network, 256, activation = 'relu')     network = dropout(network, 0.8)      network = fully_connected(network, 128, activation = 'relu')     network = dropout(network, 0.8)           network = fully_connected(network, 2, activation='softmax')     network = regression(network, optimizer='adam', learning_rate=LR,                          loss='categorical_crossentropy', name = 'targets')     model = tflearn.DNN(network, tensorboard_dir='log')          return model      def train_model(training_data, saved_model=False):     X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]), 1)     y = [i[1] for i in training_data]          model = neural_network_model(input_size = len(X[0]))             if not saved_model:         model.fit({'input':X}, {'targets':y}, n_epoch=epochs, snapshot_step=500, show_metric=True,                   run_id='openaistuff')     else:         #Loading the old model(only loads the weights)         model.load('200.0.model')                   return model    # Running the script  #Change is_saved_data to True after running the code once training_data = initial_population(is_saved_data=False, saved_data='TFsaved3.npy') model = train_model(training_data, saved_model=False)         scores = [] choices = [] accepted_scores =[] training_data = [] for each_game in range(100):     if each_game%100==0: print(each_game)     score = 0     game_memory = []     prev_obs = []     env.reset()     for _ in range(goal_steps):         #env.render()         if len(prev_obs) == 0:             action = random.randrange(0,2)         else:             action = np.argmax(model.predict(prev_obs.reshape(-1, len(prev_obs), 1))[0])         choices.append(action)                  new_observation, reward, done, info = env.step(action)         prev_obs = new_observation         game_memory.append([new_observation, action])         score+= reward         if done:             break              scores.append(score) #You can comment out this part after it save the dataset #Saving the training dataset      if score >= 100:         accepted_scores.append(score)         for data in game_memory:             if data[1] ==1:                 output = [0, 1]                          elif data[1]==0:                 output = [1, 0]                          training_data.append([data[0], output])  training_data_save = np.array(training_data) #  this saves the dataset np.save('TFsaved3.npy', training_data_save)  print(training_data_save) print('average accepted score:', mean(accepted_scores)) print('median accepted score:', median(accepted_scores)) print(Counter(accepted_scores))  # Saving the dataset ends here     #Please comment the part above  average_score = sum(scores)/len(scores) print('Average Score', average_score) print('Choices 1: {}, Choice 2: {}'.format(choices.count(1)/len(choices), choices.count(0)/len(choices)))      #Saving the model #if average_score > 195: #    model.save(str(average_score)+'.model')",True
@zakcodes3507,2017-05-13T20:42:09Z,0,When you reset the game the function env.reset() returns the observations so you don't have to add all of the other code you had. Also the CartPole problem is extremely easy and you can solve it easily with only one hidden layer since I've done it with absolutely no hidden nodes and getting an average of 500 using my NEAT api,True
@rohithgm1,2017-05-11T09:06:15Z,2,"Hi,  Its been a pleasure to follow your tutorials. Really appreciate the whole machine learning series. I started of with 0 knowledge of python and almost 0 knowledge of machine learning, Reached this point in less than a month.  Just a quick question:- (I ran this model which got trained for random data for 1000 games and took all the data above 100 score and retrained) * 5, with each time adding the data to the original set and shuffling and retraining.   When I tested the model it gets score 200 and it returns done as true. Is this the environment itself when the score reaches 200 it stops?  Cause I saw a different comment stating  ""other times the game just ends while the pole seems to be quite straight, might just be too many fps to catch it falling over quickly sometimes..."" The same is happening..pole is very straight and the cart balances well but still the game stops In my case I ran the model 10 times and these are my scores:- 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 Average Score: 200.0 Choice1: 0.4985, Choice2: 0.5015  So I am pretty sure that the game is stopping once the score reaches 200. Is this the case with you also, cause in the tutorial you reached scores of 300",True
@vithushanjegatheeswaran5317,2017-05-09T21:34:26Z,0,please make a go bot !! :),True
@juleswombat5309,2017-05-01T11:37:07Z,0,"Yeah well that was pretty awesome result. cudos. I got an average score fo 200.0 (194.6 over 25 games) !  So I guess this is really a classic control theory problem (balancing a broomstick), and what we have used batch Neural network to discover and optimise the control laws State => Action, a sort of direct Policy model, directly in a large batch NN model. Which is pretty neat, but its not the same approach as iterative Q(S,A) learning typically suggested in  Game path finding via DQN.   It would be nide to see if this approach works with the Open Gym Walking biped scenarios, or whether we have to use a Q learner, with perhaps a NN Q Approximator (aka DQN) instead.",True
@ryanshrott9622,2017-04-25T20:35:16Z,11,"Is this a correct general summary?  1. Let the machine play 10,000 games. 2. Gather input feature space (previous observation) and output data (action) only from the subset of games which performed 'well'.  3. Train a neural network to predict an action based on an input feature space.  4. Play the game by taking actions based on the current input feature space.",True
@Sauerbier_,2017-04-21T16:22:56Z,2,ok wait i think i missunderstood the whole training concept. Isn't the goal to achive the highest accuracy as possible without overfitting? So how did you know your network was already overfitted with only aprox. 60% accuracy? And how do i know when i should stop training?,True
@finalfantasy7820,2017-03-26T12:01:29Z,0,This guy rocks!,True
@AntikLee_HAMBURG,2017-03-25T08:49:25Z,0,thank you so much. and where can I find a playlist for this tutorial? I do like to share on google + .,True
@GustyBroadcast,2017-03-20T19:09:13Z,0,You should make a live tutorial or a live Q&A,True
@JossWhittle,2017-03-19T18:02:52Z,13,Thanks for this series :) Would you consider continuing to a Q Learning series?,True
@cmatthew91,2017-03-18T19:34:48Z,0,"I reimplemented some of it in Keras, my max score is always 200 ( I reach that 4 times from 5), can someone tell me why is that? Is 200 a special nr somehow to this game?",True
@jiexun9679,2017-03-17T08:49:56Z,1,Thanks for this! Would you be able to do a tutorial for a reinforcement learning solution to this game?,True
@grigoriikushnir4927,2017-03-15T21:56:08Z,0,Much fun!,True
@willkoehrsen934,2017-03-15T13:46:55Z,0,"After you have saved a model, how do you load it back in and use it?",True
@edisoner97,2017-03-14T22:13:20Z,0,"What does ""epoch"" mean?. Isn't that like a single pass of the data or just an iteration?",True
@Ashmoreinc,2017-03-14T21:26:25Z,0,"Im not too sure on how the code works, this is likely because im not too sure how tensor flow and all the other modules are used, so ill have to do some revision on that, but how do we teach the game that falling more than 15 degrees is bad and how can we teach it to try to avoid that?",True
@beckettman42,2017-03-14T19:30:12Z,15,Great series.   Next Stop:  Dark Souls 3,True
@jialiangchen9984,2017-03-14T17:55:28Z,1,This series is awesome! Great work!,True
@georgevjose,2017-03-14T13:56:13Z,0,"Hi, ive used the same concept that you've used for a CNN and i get an error ""Exception: Feed dict asks for variable named 'input' but no such variable is known to exist "" when fitting the model. Pls Help",True
@tor7295,2017-03-14T04:54:33Z,0,"if u wanna there is a ""pycharm"" programme whit that u can write easier than before",True
@dippatel1739,2017-03-14T03:11:24Z,0,can you make tutorial on how to submit our results on openai ?,True
@firespark804,2017-03-13T19:02:54Z,0,"Hey sentdex, can you please help me with my code? The problem is with local and global variables, I watched your vid about those but can find a solution.. :(  import pygame, time, random  pygame.init()  size = width, heigth = 800, 600 screen = pygame.display.set_mode(size) black = (0, 0, 0)  white = (255, 255, 255) red = (255, 0, 0) green = (0, 155, 0)  blockSize = 20 FPS = 60  clock = pygame.time.Clock()  xMove = 0 yMove = 0 xObject = 0 yObject = 0  gameExit = false  def gameloopQuit(): #to quit pygame     for event in pygame.event.get():         if event.type == pygame.QUIT:             pygame.quit()             quit()         iff event.type == pygame.KEYDOWN:             if event.key == pygame.K_ESCAPE:             pygame.quit()             quit()    def move():     for event in pygame.event.get():         if event.type == pygame.KEYDOWN:             if event.key == pygame.K_RIGHT:                 xMove += 1             elif event.key == pygame.K_DOWN:                 yMove += 1             elif event.key == pygame.K_LEFT:                 xMove -= 1             elif event.key == pygame.K_UP:                 yMove -= 1                          if event.type == pygame.KEYUP:             if event.key == pygame.K_RIGHT:                 xMove = 0             elif event.key == pygame.K_DOWN:                 yMove = 0             elif event.key == pygame.K_LEFT:                 xMove = 0             elif event.key == pygame.K_UP:                 yMove = 0                          xObject += xMove #I think here is my problem with local variables..         yObject += yMove #The rect does not move even it should, i think at least                          #But I really need help with those local variables changing, but                          #not changing the coords of the rect, cause there i try to use my xObject                          #and yObject variables, that are global... PLEASE HELP :D                          #comment below for solutions please, thank you in advance :)!!!              def screenUpdate():     screen.fill(white) #here below my variables i change local in move() but not global... so ot does not move :(     pygame.draw.rect(screen, red, [xObject, yObject, blockSize, blockSize])     pygame.display.update()     clock.tick(FPS)  def gameloop():     gameloopQuit()     move()     screenUpdate()    while not gameExit:     gameloop()",True
@hayhayos,2017-03-13T17:22:51Z,0,"Im new to setting up neural networks, why wouldn't you want the training data to be as good as possible. Why didn't you up the accepted score in the test data?",True
@ramilijaonampoina3862,2017-03-13T16:03:23Z,0,Thank you very much (y) (y),True
@fuba44,2017-03-13T15:22:51Z,0,"Thanks for doing these videos, they made my day!",True
@pothny3371,2017-03-13T15:15:08Z,1,Just went through these tutorials. This is pretty cool!  I really like this type of tutorial where tensorflow is used to train a neural network to play games.,True
@kevinkevinpham,2017-03-13T15:00:04Z,0,Do you typically plan your codes and videos before hand? Or are all of these videos on the fly?,True
@lm5050,2017-03-13T13:48:34Z,43,"Wow the first time I was here on your channel I was learning simple pygame, these days diving into tensorflow, kaggle and AI its amazing where python can take you. Keep up the great vids!",True
