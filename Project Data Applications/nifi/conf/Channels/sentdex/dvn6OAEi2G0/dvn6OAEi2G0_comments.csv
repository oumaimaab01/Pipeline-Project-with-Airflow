author,updated_at,like_count,text,public
@Amir4v,2024-05-31T08:21:10Z,1,"2024, still Aaaaaaaaaaaaaaaaaaaawesome :)",True
@radimkozl3270,2023-03-14T18:30:44Z,0,"Hi, can I ask  you. Why you use the Scikit-Learn for preprocesing? Is it there some solution from Keras and TensorFlow?",True
@muhammadhadi1854,2023-01-08T16:22:12Z,1,bro you look like Justin Gaethje of Computer Science hahaha and I don't mean to disrespect :D thanks dude this is really helpful,True
@dominiquem1869,2022-07-22T17:06:03Z,0,Maybe I'm missing something. But why is there only one target column. Wouldn't there be different targets for each type of bitcoin?,True
@manivenkat777,2022-06-03T05:32:25Z,0,"Hi sir, I'm having my project in recurrent neural networks. Could you help me in doing the project?",True
@dayiozkan,2022-04-02T18:47:23Z,0,"Thank you for the complete solution. I tired Normalizing part, but while running the code it gives infinity error :( . To handle the error I tried following additions to fix the problem.  def preprocess_df(df):     df = df.drop('future', axis = 1)     for col in df.columns:         if col != 'target':             df[col] = df[col].pct_change()             df.dropna(inplace = True)             df[col] = df[col].replace((np.inf, -np.inf, np.nan), 0)  #  <---- This part It replace all infinite values             df[col] = preprocessing.scale(df[col].values)  Is there any other suggestion ?",True
@nityanandk5522,2022-01-30T15:23:27Z,0,why random.shuffle seq data,True
@fuzzywuzzy318,2022-01-15T04:03:16Z,0,everytime he change a new mug,True
@joegriffith1683,2021-12-30T23:28:55Z,1,"Might have found a minor bug with preprocess_df(). when iterating through columns one row gets deleted for each column where col != 'target'.  each time the first df.dropna(inplace=True) is called, one row is deleted. this is because the first row for each column will have a pct_change() of NaN, thus the row gets removed. However, as we treat each column iteratively, we end up deleting a row for each column, finding NaNs in a staircaselike fashion: NaN ...     ...    ... 0.2 NAN  ...    ... 0.5 0.1   NAN ... 0.1 0.2 -0.2 NAN  can be fixed by only dropping the NaNs after each column has been converted to pct_change() however, the preprocessing.scale() must be run on each column after the NaNs have been dropped.  Although we're losing minimal data with this bug, it does make it a little more painful to debug, when the head() isn't what you expect. Hope this made sense! LOVED the series, was so helpful!  peace",True
@vithaln7646,2021-12-23T16:26:15Z,0,"i am addicted to his videos , put him in jail or else i will loose my job",True
@alexandremondaini,2021-11-16T14:49:02Z,0,"since your dataset is sorted from oldest to most recent timestamp, what you are actually getting with sequential data is the 60 minutes from the oldest timestamp and not the last (most recent) timestamp when you iterate through the numpy arrays (rows) of the dataset",True
@ChrisNilsson,2021-09-02T08:43:36Z,0,"If you use ""df.dropna(inplace=True)"" inside the loop ""for col in df.columns:"" then the dataset will remove at least one row (the first row) for every loop. Wouldn't it be better to use two loops and drop all NaN values in between them instead. Otherwise, if you have a lot of columns, you will loose a lot of data.",True
@dwightledet5857,2021-05-23T09:35:34Z,0,"I agree these are great, but I do wish you would slow down and try to explain what you are doing and why. These are wonderful if you already know what is going on, but not too helpful if you don't. Maybe less coffee???",True
@Andrew6James,2021-04-24T14:41:35Z,0,What was the need for randomly shuffling here?,True
@Andrew6James,2021-04-24T14:34:09Z,0,"What is the need for scaling if we compute the daily returns? In all likelihood, those returns will already be between 0 and 1.",True
@AMVSAGOs,2021-03-02T10:46:49Z,0,I am not able to download the dataset from that website dude. can you please give me some other link?,True
@alexd7466,2021-02-05T10:15:41Z,0,"pct_change() only looks at the previous row, but would it not make more sense to use a percentage change compared to some average of last N rows ?",True
@samcherry1000,2020-12-21T22:27:52Z,0,"I don't totally understand what ""targets"" are",True
@shahihtv2582,2020-11-17T05:14:57Z,0,I want to classify anomaly detection using RNN keras.tf but I have a problem where the accuracy value increases but the val_accuracy value does not change and just remains constant at 50%. this is my complete code available on google colab https://colab.research.google.com/drive/1saoNuCxj08JCxZ_7taIjhp8sJEIV-T5U?usp=sharing     //,True
@user-tj4ut8ox9r,2020-11-10T15:47:22Z,0,But why are you scaling across main_df? At the time of real use you won't be loading years of data so that you can get your 60 minutes scaled. Every sample and the future sequence should be scaled together and separately for each example.,True
@alsanabe5241,2020-10-29T13:26:23Z,0,"Interesting Tutorial bro, but I have a question on data normalization like after having ur own dataset how u can normalize it with RGB images?",True
@adamhendry945,2020-09-09T01:08:52Z,0,Use `scikit-learn` instead. Pypi `sklearn` states to use `scikit-learn` instead.,True
@ramtinnazeryan,2020-08-19T11:18:16Z,0,for scaling: (Value-min)/Max         this will always give you a number between 0 to one,True
@puma8185,2020-08-02T16:44:52Z,2,"deque is pronounced ""deck"" lol. at least that's how my professor pronounced it.",True
@99ansh,2020-06-18T14:50:09Z,3,"Why is the target based only on  ""LTC-USD""?",True
@youssefbakadir2625,2020-06-15T23:57:11Z,0,"Thanks a lot for what you are doing, you'r my number one reference.  I have a question plz :  why didn't you try to predict price of the coin instead of whether it's going up or down? Again thanks a lot for your efforts.",True
@zmladen1,2020-06-11T07:14:48Z,0,"In 17:28 you are defining the sequential_data list as sequential_data.append([np,array(prev_days), i[-1]). Which is a list of sequences of SEQ_LEN length. For the sequence you use a single target which is the last value in the sequence of datapoints. Why only last one? What is has to do with all other points in the sequence of SEQ_LEN points? Would the max of 1 or 0 appearances be more appropriate for the final target?",True
@AlphatheRock,2020-05-22T14:09:53Z,1,"Got some question regrading the preprocessing. If I got it right, after the preprocessing, the values should be between 0 and 1. However, by using preprocessing.scale(), I got the values less than 0 and more than 1. Should I go for MinMaxScaler() instead?",True
@jjbetofonseca,2020-05-01T11:41:08Z,1,"amazing video man, thank you very much!   Just a small question, why do you use just one coin to calculate a target? Shouldnt it have multiple targets? I mean, one for each coin? Normally this coins follows BTC, but this is not always true, mainly on minute charts! I also dont understand (maybe conceptually) why we should shuffle data for a series-based train, if you can tell a bit about it, it would be amazing!  Again, the series is amazing and Im looking forward to end the entire website =D",True
@fahimebabaee8834,2020-04-20T12:21:20Z,2,"Thanks. really helpful  tutorials. just one question. you wrote df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1. but as we can see, you have negative value after running the code.  As I searched, preprocessing.scale just used to center to the mean and component wise scale to unit variance but it dose not mean we scale between 0 and 1. Am I right?",True
@zombietechz8361,2020-03-08T21:03:26Z,0,"Amazing video! I know this is a stupid question, but I did the exact same thing but somehow the preprocess_df function is not working ( i copied it word for word) where i cannot see neither the normalization nor the future column being removed, is there anything i am missing out on here? Please let me know. Thanks",True
@jumpthecagemma4987,2020-01-09T21:14:58Z,0,validation_main_df_2 = main_df[main_df.index.get_loc(last_5_pct):]  a little cleaner way to get the slice of a DF instead of having an operator,True
@Kinsella-yt,2019-12-30T09:55:24Z,0,Thank you !,True
@manishnayak1000,2019-11-23T07:53:43Z,4,Just how much coffee do you drink and how many weird-shaped cups do you have?  :P,True
@BrettClimb,2019-10-31T19:29:00Z,0,"So each column of sequential_data[i][0] is a time series for a different attribute for the past 60 time steps, and sequential_data[i][1] is the trend of the time series from the last time step to 3 time steps in the future?",True
@user-bg2ct4dy5k,2019-09-02T19:31:32Z,0,"FYI deque is pronounced ""DECK"" not ""day-q"".",True
@foronlineinternetuse3335,2019-09-02T12:17:26Z,7,"Let's collect all the reviews of the tutorials made by Harrison, and by applying LSTM based sequential time-series analysis, detect whether his next tutorial might be another excellent one or not!",True
@pushkardey,2019-08-27T11:08:23Z,3,"Thanks for the Tutorials, they are amazing and easy to understand ....",True
@tejapolisetty3806,2019-08-15T08:26:49Z,0,"why use pct_change, when you can normalize using preprocessing.scale",True
@local_minima162,2019-08-13T16:21:03Z,1,This is the most beneficial youtube video I've ever seen. Thank you!,True
@markd964,2019-08-02T11:38:10Z,4,"Sentdex_Wonderful series! But problem here in the def preprocess_df function. The first dropna needs to be removed as, after each pct_change() action on a single column which generates a NaN in first position, dropna deletes entire row of real data from subsequent columns on each iteration. Next iteration creates a new Nan on a new row, that row is then deleted, and so on..Each iteration means the columns then get shorter and shorter. Commenting out first dropna increases final accuracy by c. 5%. Not easy to spot as no error is generated here. Error only with different geometry featuresets (my own '000's of columns). I've added the print(df[col]) to show the difference in the columns being  pct_change's. the dropna outside the loop catches all NaNs, but keeps all other data. I'll take a 5% increase in accuracy... Thx, great series!   def preprocess_df(df):   # balance data (between currencies, scale data to between -1,0,1)     df = df.drop('future', 1)     for col in df.columns:         if col != 'target':             df[col] = df[col].pct_change()   # this normalises all columns except target #             df.dropna(inplace=True)        # PROBLEM-as pct_change will generate at least 1 nan             print(df[col])                                # verify each column             df[col] = preprocessing.scale(df[col].values)   # scales data between -1,0,1 range     df.dropna(inplace=True)",True
@bordignonjunior,2019-07-19T02:18:57Z,0,where is the order of the videos?????????????,True
@MrLumberer,2019-07-11T18:36:04Z,11,"You have lookahead bias, when you scale with regards to future prices, you need to scale it within sequences. Your accuracy will drop as a result of it",True
@silentroar2873,2019-07-10T03:15:05Z,0,"Great video, many thanks! Can the target be a list of future 3 minutes of price, i.e. I want to predict the whole future 3 minutes of price instead just a binary value?",True
@ryanlevin6348,2019-07-01T08:51:12Z,0,Why wouldn't you just chop the full arrays by array length *0.95 instead of doing the whole timestamp thing?,True
@samardeepsinghsarna8091,2019-06-20T19:58:12Z,2,"I am trying to implement RNN for batch control. I have 1 input and 4 outputs.  All of them are in 50 batches each of length 600 each.  So input A (temperature) has data of 50 batches with 600 values for each of the 50 batches. The outputs B,C,D also have the same dimension.  Could you tell me how do I go about preparing appropriate shape/structure of this dataset to implement RNN?",True
@srinivasabanna2967,2019-06-01T03:04:32Z,0,"i am not sure about the deque. for example, does the first degive you the first 60 numbers starting from the first number and the second 60 numbers starting from the 61st number or does the first degive you the first 60 numbers starting from the first number and the second 60 numbers starting from the secnd number",True
@viniciusvalente312,2019-04-19T15:03:13Z,1,Why to use preprocessing.scale? Can I use MinMaxScaler instead?,True
@Leo-zj2qp,2019-04-18T04:29:12Z,0,"Code doesn't work, you can't under preprocessing_df() you have to return something, you cannot leave it blank.",True
@oladojaabiose3227,2019-04-09T22:44:12Z,0,"Hi Sentdex, another great video. Im trying to apply this to stock data but i keep getting ValueError: Input contains infinity or a value too large for dtype('float64'). when trying to preprocess my df. Any pointers?",True
@ronit8067,2019-02-09T16:41:50Z,1,"i might be wrong but isnt the ""target' col just based on LTC values? dont we need to make 3 more ""Targets"" for all others?",True
@sgrouge,2019-01-28T00:13:24Z,0,Why did you shuffle your sequence?,True
@DanielWeikert,2019-01-12T16:50:58Z,2,"Thanks a lot Sentdex. Could you please do more videos about preprocessing data with pandas, numpy and other ML / Python packages. Cleaning up messy data and shaping it to then fit it into the model is the hardest part so I would really love to see more here. It would be a great series on youtube. Thanks again. Awesome Channel, I really appreciate your work here",True
@DrPisspot,2019-01-08T01:20:38Z,0,Is there any way of using the GPU to speed up the sequential data building process? Mine took forever to build...,True
@vaibhavsingh1049,2019-01-02T23:51:07Z,1,"Sentdex, can you please explain a little about the ""sequential_data "". Why did we take 60 values from the dataframe row-wise and assigned the target value for these sequences to be the same as the 's target value of the first row in ""prev_days""? I get what computation is taking place here, but I can't understand the reason behind it. And wouldn't some of the target values will go unutilized when the last pair of ""prev_days"" get constructed?  What am I missing?",True
@krishnabharadwaj4715,2019-01-01T08:35:25Z,1,"When you say ""out of sample"" data, you mean ""test"" data, right? Splitting data into train and test data and feeding train data to the NN which in turn splits it into train and validation data. We test the performance of the NN and tune the parameters based on validation data (because it is definitely going to fit train data). Later we test the model on completely unseen data which is test data or you may call it ""out of sample"" data.  EDIT: I watched the entire video. You don't have test data. You have only train and validation data. If you change any parameters based on the results you get when you run your RNN, you are actually trying to fit the validation data, which can be misleading and can actually perform worse on completely unseen. You should always have three datasets - train, validation & test.",True
@ritikakumari7701,2018-12-18T08:04:48Z,7,"In the function preprocess_df you need to return some value here otherwise it will give error ""Nontype object is not iterable""",True
@zyliu3517,2018-10-18T09:15:10Z,1,"for the normalization step, may i use the natural log of ratio instead of pct_change, because it ensured that the percentage change was consistent from both up/down.",True
@vincentliao9251,2018-09-26T04:04:06Z,0,"An alternative way to create the out-of-bound data is to use sklearn's train_test_split with with shuffle turned off. I find it easier to do it that way. You just need to sort the df, then split it.",True
@coolervik1994,2018-09-22T08:42:36Z,6,"A few questions/suggestions: 1. Isn't it better to split the data into 3 streams: train, valid and test. Use the validation set to tune the hyperparameters and finally test the final trained model on the test stream. 2. Doesn't df.values return a numpy array? So why use list comprehension instead of numpy addressing? (Eg : prev_days.append(i[:-1])) 3. Also, we can create a sliding window function instead of deque as we can define how many strides(overlap) we need between sequences: def sliding_window(dataset, seq_len , stride):     n_samples, d = dataset.shape     data_slide = np.zeros((int((n_samples-seq_len)/stride)+1,seq_len,d))     k=0          for i in range(0, n_samples-seq_len, stride):          data_slide[k,:,:] = dataset[i:i+seq_len,:]         k=k+1",True
@christopherdomingo9793,2018-09-20T01:10:17Z,0,Why did you shuffle the sequence? Wouldn't this make the timed sequential data out of order?,True
@cmatthew91,2018-09-19T20:23:36Z,1,"why not just prevDays.append(i[:-1]), also more readable  'for row in df.values:' Great tutorial",True
@Filly643,2018-09-19T19:09:38Z,0,will you make videos about reinforcement learning?,True
@artemis9511,2018-09-19T15:26:15Z,0,Just to confirm will this model make predictions based on the last 60 timestamps. If the model wasn't a list of lists and just a list would the LSTM make predictions with just 1 timestamp of data per prediction when I fit my LSTM model?,True
@stephan663,2018-09-19T07:11:11Z,6,"""deque() is a fancy little dude"" :)",True
@alemazzuca,2018-09-18T20:31:15Z,0,I think it's worth analyzing in the preprocessing step the correlation between the symbols. Dropping the low correlated maybe gives a better prediction. Check this video for better a better Pearson's correlation in Pandas. https://www.youtube.com/watch?v=LoiVuDKxXBs,True
@DamienLaRocque,2018-09-18T01:46:23Z,3,16:01 [n for n in i[:-1]] is the same as i[:-1],True
@polares8187,2018-09-17T22:04:32Z,0,Sentdex you will not have the min max normalization data while doing inference. You should use fit_transform on your training data and use only transform on your validation data. You have to use the parameters you obtained from the training data.,True
@polares8187,2018-09-17T21:50:18Z,0,pandas has a sort_values function.,True
@marc-alexandrepaquet7696,2018-09-17T21:13:52Z,0,"Hello sentdex. I used a grouped shuffle to avoid overfitting while trainning on weather data and traffic data to build a traffic predicting model.  Here was my code using pandas : groups = [df for x, df in df.groupby('year_days_since_2015')] random.shuffle(groups) df = pd.concat(groups).reset_index(drop=True)  and then I could simply use the last x% of my dataset in Keras as normal",True
@benjaminhofmann7665,2018-09-17T20:10:33Z,1,"Awesome tutorial, really love them! One question tho wouldn’t it be smarter just dropping the nans at the end otherwise you loose one observation per feature especially when doing this with a lot more features later on?",True
@senhalil,2018-09-17T17:52:09Z,0,"Hey mate I believe it is pronounced ""deck"" because (c/p) From The Art of Computer Programming, volume 1, section 2.2.1 ""Stacks, Queues, and Deques"":  A deque (""double-ended queue"") is a linear list for which all insertions and deletions (and usually all accesses) are made at the ends of the list. A deque is therefore more general than a stack or a queue; it has some properties in common with a deck of cards, and it is pronounced the same way.",True
@Alex4n3r,2018-09-17T17:44:14Z,0,Nice idea to consider the percentage change. That will even attribute to the upwards trend of crypto currencies. BTW: I wish you'd make a video on how to make a good semantic embedding of semi-structured (e.g. json with binary events like 'is weekend') time-series data.,True
@mcan543,2018-09-17T15:03:39Z,0,Your loops over loops over loops stucked Google Colab.,True
@oliverli9630,2018-09-17T14:31:52Z,1,"can you show us what ""deque"" does? thanks man.",True
@marintakanov3554,2018-09-17T12:07:07Z,0,1. Install Python 2. Copy/paste from these tutorials 3. Get stream of data from all major exchanges 4. ??? 5. Profit,True
@MrRitmu,2018-09-17T11:57:42Z,0,"Is this a thing? Can you make money by buying, when your program tells you to?",True
@PrometheusZer00,2018-09-17T07:03:11Z,4,Great video! Why do `[n for n in i[:-1]]`instead of just `i[:-1]` ?,True
@Annunaki_0517,2018-09-17T06:02:37Z,0,"I think that perhaps some of the data you dropped from the dataset might've been useful in terms of predictive power.  I'm mainly speaking of the 'high' and 'low' values.  I think the relative volatility of returns in each time period might be significant in terms of predicting price out into the very near term.  Maybe after you finish the series you might add those cols back into the df  (or some transformation of them, like 'high-low' maybe) and see if those 'features' add any significant predictive power over just 'close' and 'volume'? Just a thought.",True
@gowtham_mission,2018-09-17T03:45:02Z,0,Optimise the data,True
@gowtham_mission,2018-09-17T02:57:59Z,0,you missed summarising it is confusing.,True
@AbhishekKumar-mq1tt,2018-09-17T02:32:37Z,0,Thank u for this awesome video and series,True
@abdulhasibuddin3317,2018-09-17T01:20:38Z,0,"Hi Harrison, I regularly follow your tutorials. I am currently working on ""Depression Analysis using Deep Recurrent Network in Bengali"". Problem is, my dataset is small as I have to create my own dataset. Also I have to go with Deep Learning as feature extraction for Machine Learning models for depression analysis is very very hard. So, I was thinking about using GRU. Can you please make a tutorial covering sentiment analysis on texts, like Twitter data in Keras for relatively small dataset, e.g. using GRU?",True
@andrewczeizler5528,2018-09-16T23:25:16Z,0,Hey mate! Love the videos! I am wondering if this could be expanded to add the catalyst backtesting platform. https://enigma.co/catalyst/beginner-tutorial.html,True
@mattimhiyasmith,2018-09-16T22:29:21Z,0,Did your voice get deeper?..,True
@typicalhog,2018-09-16T20:58:35Z,0,"I think using open, low and high (potentially even number of trades) would result in more accurate predictions. You probably didn't do that for the sake of simplicity, correct?",True
@ThomasPlaysTheGames,2018-09-16T20:40:32Z,0,"I'm curious, how possible would it be to scrape the data from google trends/news sources for info like how often 'positive' or 'negative'  adjectives were used in relation with the crypto and could those predict the severity of a drop or increase?  I was thinking about doing a similar things with stocks a few months ago but didn't get too far because I have little experience with webscraping in any organized manner.",True
@rezas2626,2018-09-16T20:18:23Z,4,"I believe there is something wrong with the normalization process as implemented here. You should normalize the train and validation with the same exact parameters and not individually. Suppose for one of the columns the numbers are in the range of 0 to 100 while the same column in validation has numbers in the range 0 to 10, this creates a mismatch of what the model thinks of the normalized values  between train and validation. I know all the columns are percent changes and hence in theory between -100 to 100, but the validation data might happen to be in a tighter range than the training data.",True
@bkrazor3,2018-09-16T19:20:33Z,1,"Out of curiosity, do you read off a script you wrote before making the video?  Because you're really eloquent and have a steady flow of words haha",True
@shaankhosla2236,2018-09-16T19:02:04Z,1,Won't the target be different for the different crypto's? Why is there only one target column? Thank you!,True
@MrDraykeL,2018-09-16T18:56:24Z,1,"I did this instead of scale because it didn't work for me, it gave me numbers outside 0 and 1 and it works pretty well. I also fixed the dropna because if you drop one column by one column it will make more NaNs so by the last dropna the data will be shifted. def preprocess_df(df):     df.drop(columns=['future'], inplace=True)     scaler = preprocessing.MinMaxScaler()     for col in df.columns:         #Target is already done, there is nothing else to do to it         if col != 'target':             df[col] = df[col].pct_change()     df.dropna(inplace=True)     colsToScale = [n for n in df.columns[:-1]]     df[colsToScale] = scaler.fit_transform(df[colsToScale])     print(df.head())  Thanks for the video!",True
@tkinter3160,2018-09-16T18:07:27Z,0,"when i run  in jupyter notebook  it  show error text   FileNotFoundError: File b'training_datas/BTC-USD.csv' does not exist , but i write  code follow you all step .",True
@sagaracharya1856,2018-09-16T17:59:00Z,0,Functional programming only makes sense. OOP creates stupid objects and methods and what not even for simple HelloWorld. I wonder if the guy who invented it was even sane!,True
@TheRaghavboyz,2018-09-16T17:52:05Z,45,"Who knows, maybe Edward Snowden changed his id, and is making these amazing tutorials.",True
@greis790,2018-09-16T16:04:57Z,2,great video once again! only one observation if you can make more code lines to appear so that we have a better idea of the whole project. keep up the good work!,True
@Sebpv2006,2018-09-16T15:51:31Z,0,I think it's easier to add columns which are just shifts of the original column than using a deque ...,True
@accentor713,2018-09-16T15:51:10Z,0,hey what ide is this,True
@ggpopa1319,2018-09-16T15:33:23Z,12,I did not understand the end part where you shuffle the sequence data. Here the order matters right? Why would you want to break the order?,True
@NicolajTopp,2018-09-16T15:22:00Z,6,"Why ""prev_days""? shouldn't it be ""prev_hours""? Or what did I miss?",True
@rrgghuhgfvjhdrgfzejjg,2018-09-16T15:19:28Z,0,When is the next part ?,True
@fbmemar,2018-09-16T15:14:50Z,0,"Can you make a video showing how to receive a text /telegram message signal? Example: 1. When stock price gets to a certain price. 2. When an item on Amazon changes price, receive a text etc. Thank you for the great work. Really appreciate all the efforts you put into this channel.",True
@quanghuyngo7556,2018-09-16T15:13:45Z,0,There's nothing faster than clicking Harrison's Deep Learning tutorials to hit the thumps up!,True
@ilyasmax4778,2018-09-16T15:08:53Z,0,i guess my method word lol 5 min and take a break and another 5 min until next part release lol speed up your engine and don't forgot the drone thing lol,True
@desi.programmer,2018-09-16T15:02:34Z,0,Totally off the topic but can you make a tutorial or tutorials on creating applications that uses gossip protocol?,True
@glorytoarstotzka330,2018-09-16T14:34:41Z,16,how many episodes left till we all get milionare ?,True
@petr.g,2018-09-16T14:15:06Z,63,"Omg, i dont know how but your tutorials are always better than from other youtubers. Great video!, keep it up.",True
@anupamsps,2018-09-16T14:07:47Z,0,Thank you Thank you <3,True
@starfightlp5124,2018-09-16T13:39:43Z,0,"Since I know how much you like to be corrected on your pronunciation here is an excerpt from the ""collections"" documentation:  Deques are a generalization of stacks and queues (the name is pronounced “deck” and is short for “double-ended queue”). (https://docs.python.org/3/library/collections.html#collections.deque)  BTW love your videos no matter how you pronounce epochs and I can't wait for the next one :D",True
@mahomedcassim9329,2018-09-16T13:27:35Z,0,Hey can we maybe include some forecast errors ?,True
@slavenya001,2018-09-16T13:21:44Z,0,THANKS MAN,True
@danweecc,2018-09-16T13:08:18Z,8,"Instead of binary targets, why not just use the differential (pct_change) as the target? This way the network gets an additional dimension to predict.",True
@MrMatheus195,2018-09-16T13:04:42Z,0,"Guys i've found a free e-book called ""A programmers guide to data mining"", and i want to know if its worth it to read it now in 2018?(sorry for my bad english)",True
@IradAharoni,2018-09-16T13:04:30Z,2,Can you please do a sports predictions using neural networks project?  like gathering teams data over time and predicting scores or even league standings!,True
@cjw1994cool,2018-09-16T12:56:33Z,5,I just saw the last video an hour ago and you dropped this!! Thanks all the time!!,True
@lokesh9322,2018-09-16T12:52:39Z,5,Waiting for GANs,True
@neiltsakatsa,2018-09-16T12:51:59Z,1,Hie There Sentdex ! Great Tutorial Thanks :-),True
@mockingbird3809,2018-09-13T06:45:06Z,37,Please Make a video  on HOW TO DEPLOY MACHINE LEARNING MODEL INTO WEB USING FLASK start with a simpler one like Linear Regression and Convolutional Neural Network because it is a Video I have been waiting for so long and It will help lot of  us,True
@fuba44,2018-09-12T19:29:46Z,3,Why not preprocess the entire main_df and then cut the 5 percent.. then you won't run into normalization issues..,True
