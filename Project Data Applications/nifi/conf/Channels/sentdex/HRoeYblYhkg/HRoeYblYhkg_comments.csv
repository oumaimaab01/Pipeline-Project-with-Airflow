author,updated_at,like_count,text,public
@daivd1991,2022-04-25T10:24:00Z,0,where Can i get text based tutorial of this video?,True
@abhinavsinghtawar9157,2021-10-11T17:18:19Z,0,What is  color = colors[classification] ??? I am getting NameError : name 'colors' is not defined.,True
@jairajsahgal5062,2021-09-23T21:50:56Z,0,Thank you,True
@uyentran5994,2021-06-28T13:13:37Z,0,Hey just comment to let u know that ur video is really helpful with me as a beginner-level in ML even in 2021. I really appreciate for what u did <3,True
@decode0126,2020-08-29T11:07:24Z,0,"Is 70% good or bad ,  cause u said in the last tutorial that 70% means 30% .  I'm a bit confused.",True
@godschoicesamuelugare7439,2020-08-07T10:20:24Z,1,Can i run the following codes on Jupyter notebook?,True
@antonburenko2852,2019-07-24T12:09:19Z,0,I am wondering why we are getting a different value for accuracy each time?  I can't see anywhere that our program shuffles data to get different accuracy.  Or is it because the KMeans from sklearn drops some data(for optimization) before running?,True
@patmtc477,2019-05-10T19:26:43Z,0,Thank you. The code breaks for anything above k =2,True
@hajranoor4160,2019-04-25T01:32:24Z,0,"in half of the code, you are using data as your index... which does not make sense.. Shouldnt it be distances=[....... for centroid in range(len(centroids)) ?",True
@juanandresrodriguezpedreir8922,2019-01-09T05:33:28Z,0,"My plot with unknowns, predicts all data in one group, showing them in red besides being next to the other centroid. Did anybody gad the same problem?",True
@johnruwanthaperera5223,2018-08-15T08:20:47Z,1,"What distance metric did you ues??? Euclidean, cosine, manhattan?",True
@MathiasCiliberto,2018-07-12T08:59:12Z,4,"Thank you for this very useful tutorial!!! May I just suggest to put an abs() around the np.sum(....) in the optimization step? I found myself working with negative numbers in the feature vectors, and without the absolute value, the algorithm was always stopping at the first iteration.",True
@michiokaku101,2018-06-13T21:51:58Z,0,where i can find your full code?,True
@michiokaku101,2018-06-13T21:45:55Z,0,Why i got this error? K_Means object has no attribute clasifications?,True
@MrIlikecoolstuff,2018-04-29T13:54:31Z,0,"I was just wondering, whats the name of the objective function you used to optimise the kfold classifier? For reference the code I am referring to is the line: ""if sum((current_centroid - original_centroid)/original_centroid*100) > tol:"". I am new to machine learning and after searching for its name have come up with nothing! Thanks.",True
@maramkamil8327,2018-03-25T18:33:24Z,1,"I followed your steps and use my own data, but an error showd up to me and i couldn't solve it      distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]  TypeError: ufunc 'subtract' did not contain a loop with signature matching types dtype('<U5') dtype('<U5') dtype('<U5')",True
@MrXterminate,2018-01-10T15:10:45Z,2,"shouln't you use the absolut value of the differnce in in range? if the differnce is negative, it might think its optimized while its not.",True
@yorickbolster782,2017-11-22T18:52:54Z,1,"To show the first 5 frames of your centroids changing and getting the percentage change for that as well you can do the following:  for i in range(5):  clf = K_means(max_iter = i+1)  clf.fit(x)   for centroid in clf.centroids:   plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1], marker = 'o', color = 'k', s = 50, linewidths = 5)   for classification in clf.classifications:   color = colors[classification]   for featureset in clf.classifications[classification]:    plt.scatter(featureset[0], featureset[1], marker = 'x', color = color, s = 50, linewidths = 5)  plt.show()  And change the part where you check if the change is lower than the tolance to this:  if np.sum((current_centroid- original_centroid)/original_centroid * 100.0) > self.tol:  if i == self.max_iter -1:   print(""percent change: "", np.sum((current_centroid- original_centroid)/original_centroid * 100.0))  optimized = False  This will of course make the algorithm repeat from the start every iteration, but it's just for looking at",True
@darkskydrake,2017-11-15T22:49:27Z,0,Thank you for this.  What do you do when K = 3?  I tried with the same code and it always fails when K is great than 2.,True
@zipeiwei4702,2017-10-03T14:41:39Z,8,"Hi, Thank you so much for this tutorial. One thing I am getting error of when passing color as ""g."", this gives me an error of :Â ValueError: 'color' kwarg must be an mpl color spec or sequence of color specs. Instead using ""g"" will solve this exception.",True
@dude2260,2017-08-16T19:45:32Z,0,"Hi Harrison , when u are checking for tolerance u r only checking if either one of the centroid is within the tolerance limit, shouldn't we check if both our centroids are in tolerance limit , i worte a code for that please see if its ok,  for i in self.centroids:                 self.tolerance = []                 current_centroid = self.centroids[i]                 prev_centroid = prev_centroids[i]                 self.tolerance.append(np.sum((current_centroid - prev_centroid) / prev_centroid * 100.0))  optimized = False  if sum(self.tolerance) <= self.k*self.tol:                 optimized = True   if optimized:                 break",True
@dude2260,2017-08-15T17:57:38Z,0,"whats this [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids] , shouldnt we have [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in range(len(self.k))]",True
@user-og7po3ro5n,2017-08-06T17:12:42Z,0,how I can put sklearn  package in python directory ?,True
@allanng5522,2017-06-20T04:58:07Z,2,"Hi, Can we plot a graph using the feature set of the titanic?",True
@heri_prieto,2017-06-14T01:06:57Z,0,"I'd imagine that checking to make sure our initial centroids aren't the same would be extremely important? Else, theyd follow the same 'trail' wouldnt they?",True
@FranciscoLopez-ox5pe,2017-06-13T19:03:47Z,0,May be possible your third chart in your web isn't correct? I'm talking about the plot when you add the last data to the original.  Thanks for all,True
@Frank-ch3fe,2017-05-20T18:47:20Z,0,"""the final centroid"" xD",True
@hellopoop8888,2017-04-12T19:00:06Z,0,"Awesome videos! Couple questions on this video: (1) The line : np.sum((current_centroid - original_centroid)/original_centroid*100) > self.tol ...My understanding is this first gets the % increase of each element in the np arrays current_centroid from original_centroid. Then I sum up these %'s and compare it to self.tol. Is this correct? (Little unsure why adding up each % change would get you the 'total' change. Would think you'd need to do some norm calculation to get % moved or just individually compare each dimensions change with self.tol) (2) In your predict / fit function - you just use the index of the minimum distance as the classification, doesn't that assume that you are looping through self.centroids in ascending key order (ie the index of distances will have the same value as the centroid)? And I thought that isn't guranteed with python dicts.  Thanks again! (P.s. Quite the awesome first name you got)",True
@naveenaelamaran3449,2017-03-29T01:57:12Z,0,+sentdex - I see that you have shown the K means code by taking a default K value as 2. I would like to know if there is an algorithm to find the best K for for a particular dataset.  Thank you for your help!,True
@Ai_Monk,2017-02-26T11:53:53Z,1,Great Tutorials Man...Learning a lot.. Probably adding the np.abs would help while calculating the percentage change in centroids...coz it can be negative too.,True
@bf2825,2017-02-08T06:54:01Z,0,For people that get a graph only at the first iteration because of using python 2.7  use the following line  if float(np.sum((current_centroid - original_centroid)/ original_centroid * 100.0)) > self.tol:,True
@rrusk6414,2016-06-26T10:52:03Z,34,"As far as I'm concerned, your channel is probably the best use of youtube space I've come across to date. Is that because I have a huge appreciation for machine learning? Maybe yes. Thanks for all your hard work and dedication.",True
