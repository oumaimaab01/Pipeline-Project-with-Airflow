author,updated_at,like_count,text,public
@hainguyenthe3737,2023-12-03T13:30:31Z,0,every day was changed by every new type of teacup ðŸ˜†,True
@mahmoodkashmiri,2023-12-01T05:28:26Z,0,You have telgu video suggestions .. haha,True
@johnkianja456,2023-07-05T20:59:16Z,0,The graph for 0 and the graph for 2 at 22:13 seem like complements of each other.,True
@vinben007,2023-06-30T10:41:29Z,0,"I love the tutorials man, very helpful but you are not alone in your head ahhahaha, you made me laugh so much",True
@utkarshsharma6434,2023-05-28T08:10:44Z,0,the function for discrete step is not working,True
@krzysztofdymanowski8759,2023-05-06T19:18:52Z,0,"Is anyone else getting an out of bounds error? I copied and pasted the code from the site but everytime the second action in the new_discrete_state is something like -60 or -70, so it is out of bounds of the q_table.",True
@vanongle9648,2023-04-15T14:37:36Z,0,thanks for the lesson,True
@ulisesbussi,2023-02-25T15:29:06Z,0,maybe a matshow for the qtables?,True
@fridmanjohnny,2021-08-27T17:19:29Z,0,"Great vids! how would u be able to improve if u never reach the top? lets call it infinite mountain, how would u score rewards so he would keep on improving? I tried keeping the highest point i have reached in this run, but it doesn't sound right, we need some award for each step",True
@pushkarparanjpe,2021-08-22T12:45:24Z,0,"If I fix the initial state and freeze a trained Q table : should'nt I expect to the agent to solve the MountainCar task in every episode deterministically with the same total reward ? I tried fixing initial state for each episode but the agent behaviour still appears to be stochastic (sometimes it fails, sometimes it succeesds)... why this stochasticity ?",True
@hariprasadv5431,2021-08-14T14:58:19Z,0,"In some cases, the agent doesnt reach the goal. Then how does the loop get ""done""?",True
@stephenb2477,2021-07-01T14:46:59Z,0,22:33 youtube video advises are terrific,True
@kontra_21,2021-06-09T19:25:14Z,0,"For anyone (like me) where the loop through your tables takes forever: I'd recommend changing the plot from scatter to just plot. It's 10 times faster  For example, generating and saving a figure from a 20x20 qtable took me 10 seconds using the code mentioned in the video, however changing  ax1.scatter(x, y, c=get_q_color(y_vals[0], y_vals)[0], marker='o', alpha=get_q_color(y_vals[0], y_vals)[1]) ax2.scatter(x, y, c=get_q_color(y_vals[1], y_vals)[0], marker='o', alpha=get_q_color(y_vals[1], y_vals)[1]) ax3.scatter(x, y, c=get_q_color(y_vals[2], y_vals)[0], marker='o', alpha=get_q_color(y_vals[2], y_vals)[1])  to  ax1.plot(x, y, c=get_q_color(y_vals[0], y_vals)[0], marker='o', alpha=get_q_color(y_vals[0], y_vals)[1]) ax2.plot(x, y, c=get_q_color(y_vals[1], y_vals)[0], marker='o', alpha=get_q_color(y_vals[1], y_vals)[1]) ax3.plot(x, y, c=get_q_color(y_vals[2], y_vals)[0], marker='o', alpha=get_q_color(y_vals[2], y_vals)[1])    makes it take ~1 second per",True
@raconteurhermit1533,2021-05-25T07:35:01Z,0,at 13:36 What kind of videos he is watching in his you tube,True
@prashantsharmastunning,2021-05-03T09:29:40Z,0,PermissionError: [Errno 13] Permission denied: 'cartPole_qtables/0-qtable.npy'  anybody knows how to fix this permission error?,True
@martinsosmucnieks8515,2021-01-04T21:54:51Z,1,great video man! Gonna continue with the series tomorrow,True
@Deshammanideep,2021-01-01T14:50:49Z,0,Can't believe that you are watching videos of our beloved CM YS Jagan at 13:33,True
@grizzle8911,2020-12-06T11:57:26Z,0,"@sentdex - I can't take all the credit for this one, my linter suggested that the (if not episode % SHOW_EVERY....) could be replaced by a var = bool(test).  render = bool(episode % EPISODE_SHOW == 0)  That line would replace the whole if/else block with one line and it's a little easier to read I think too.",True
@HarshRaj-sr4gb,2020-11-19T10:08:16Z,0,What's up with your recommendation? Amazing video though. Thanks for your content.,True
@prafulmaka7710,2020-11-05T10:03:55Z,0,How would you clean a mug like that? :P,True
@jcmachicao,2020-10-13T15:42:36Z,0,"I was thinking you can save the q_table when there are interesting parameters max, min, etc",True
@boongbaang482,2020-10-11T04:08:24Z,0,"Around 13:21, why did he say the minimum should be -200 ? Is one episode terminated after taking 200 actions unless the goal is achieved ?",True
@charleswarren2089,2020-09-15T17:35:02Z,0,Your coffee mugs are amazing!,True
@decode0126,2020-09-12T14:25:22Z,0,u are really cool !!!,True
@beomseokpark1500,2020-08-03T14:53:08Z,0,I have no idea how does it break the infinite while loop without 'break' function.... Can someone explain?,True
@YeshwanthReddy,2020-07-04T15:09:24Z,8,I'm so curious as to why there are recommendations of Telugu videos at 13:33 (I speak Telugu),True
@satyamedh,2020-06-02T04:21:28Z,0,"some kota student!, u were fetured on sentdex's channel",True
@satyamedh,2020-06-02T04:12:05Z,0,"y are u being recommended telgu news stuff?, I don't think u r in india",True
@arshshah1871,2020-05-31T18:59:29Z,0,"Man, making the graphs takes forever, for the second part it took more than a day(every 10 epochs)",True
@Mikey-lj2kq,2020-05-29T00:25:19Z,2,"there's a reserved word called all in python, and any is another reserved word.",True
@dhinesh534,2020-05-19T12:33:51Z,0,How to use q learning over an android game,True
@5pellcast3r,2020-04-22T13:49:24Z,5,WHY DOES HE HAVE VIDEO RECOMMENDATIONS RELATED TO KOTA STUDENTS lollll,True
@andreamassacci7942,2020-04-01T08:28:29Z,0,There is only one question to this video: why the hell at minute @22.33 you have an Italian commentator in suggested and an old ass Italian song???????,True
@ahbarahad3203,2020-03-14T21:09:56Z,3,"at 21:31 is a legendary moment for an engineering student out of Kota watching this video, i hope i am not the not one to notice it.",True
@sudheeshpg,2020-02-27T05:32:09Z,0,"Can you make a video on how to edit an existing environment. For example taxi.py is there, how to run a new taxi environment. it will be very useful",True
@VascoCC95,2019-12-24T00:40:52Z,9,For visualizing data I scaled the q_table to values from 0 to 1 values and made an RGB image (remember the q_table is a 3 channel 20*20 matrix :) ),True
@carlc7508,2019-12-12T12:19:33Z,1,thanks for the series! :),True
@ashithshyam1842,2019-12-02T15:02:24Z,3,"Great tutorial. I'm learning ML. Just a question that there are many times the mountain car climbed up and reached the flag. And once, it took the minimum backward movement and still reached the flag. I guess that would be the optimal policy. How do you extract that from the Q tables?",True
@vinniehat,2019-09-02T09:57:22Z,0,How would we be able to fix the pausing between every x episodes where the game just freezes?,True
@lingchen8849,2019-08-28T07:05:26Z,0,"I evaluated the results:   Without the trained q table: Results after 100 episodes: The highest point is: -0.1566973277228189 Average timesteps per episode: 200.0 Average reward per episode: -200.0  With the resulting q table: Results after 100 episodes: The highest point is: -0.1491031420361987 Average timesteps per episode: 200.0 Average reward per episode: -200.0   I don't know the reason. Seems like no improvement after q learning. Any method to improve the results?",True
@mohammadkhan5430,2019-08-09T14:41:04Z,0,that voice @6:59 lol,True
@RASKARZ34,2019-06-22T13:07:16Z,0,You're the best honestly,True
@ivandrofly,2019-06-11T15:18:22Z,0,why don't you use visual studio code?,True
@RK-kp9xs,2019-06-10T13:02:36Z,2,"Excellent work on this (as usual). It was very interesting to see the video of the q-table, and how the q-values adjusted as the model converged.",True
@blueman333,2019-06-08T10:42:48Z,0,Watch Kota Factory @Sentdex ðŸ˜›,True
@paologinefra4980,2019-06-06T15:04:37Z,2,I'm italian but why does your youtube advise you some italian videos?,True
@yanfengliu,2019-06-06T04:22:08Z,2,Nice work! I think reinforcement learning will be a great addition to your GTA self-driving car series as well :),True
@davidg421,2019-06-06T04:02:17Z,2,this series is amazing,True
@spsharan2000,2019-06-06T03:20:48Z,0,"Hey Harrison! How do you think I can go about plotting the graph for every PLOT_EVERY? Doing the plt.plot(xs, ys) creates new figures everytime. What I mean is how to add points to an already existing plot? I want it to dynamically update as progress is made",True
@lurayy,2019-06-05T21:13:38Z,7,"Just before you started this series, I had to learned Deep-Q for my college project, it was fun but a awful mess. Your videos always clear things up. Thanks keep making awesome stuffs. And Yes, more ML series please.",True
@asdf-ef8if,2019-06-05T20:51:40Z,0,This is really cool. And do you plan to like do any geodajngo or geolocation website tuts?,True
@StuartHolliday,2019-06-05T18:44:13Z,0,A 2d chart with 3 colours is better than 3 2d charts with 2 colours for showing best action.,True
@connerknox5645,2019-06-05T18:06:44Z,0,"Here's one for ""every"".  Uses a custom context manager to skip the code so as nice as a built-in but seems a little more readable. Pieced together from a stack overflow on skipping code with a context manager: https://stackoverflow.com/questions/12594148/skipping-execution-of-with-block   import sys  class Skip(Exception):     pass  # could lower case this against pep 8 class Every():     def __init__(self, interval, count):         self.run = count % interval == 0      def __enter__(self):         if not self.run:             sys.settrace(lambda *args, **kwargs: None)             frame = sys._getframe(1)             frame.f_trace = self.trace      def trace(self, frame, event, arg):         raise Skip()          def __exit__(self, type, value, traceback):         if type is None:             return         if issubclass(type, Skip):             return True  for i in range(1000):     with Every(100, i):         print(i)",True
@froop0,2019-06-05T17:42:53Z,0,Instead of plotting the min and max could it be interesting to show the volatility over the epiodes? :),True
@lucaslopesf,2019-06-05T16:59:53Z,2,"Just in time, thank you very much!",True
@beoleo,2019-06-05T16:30:36Z,0,"STOKED FOR NEXT VID =) is the best way to implement your own env via gym, essentially with a pip structure, so you can use open ai baselines agents?",True
@neighborsj,2019-06-05T16:17:50Z,4,"I followed along your 'Practical Go' and 'Sockets with Python 3' playlists while patiently waiting for this video, I've learned a ton!",True
@cod-newbie9166,2019-06-05T16:01:32Z,0,Don't know machine learning just watching the series and getting overwhelmed.,True
@MachineLearningwithPhil,2019-06-05T15:54:32Z,2,"Thanks for the series, really love your explanations. Any plans on doing some deep reinforcement learning tutorials?",True
@ThomasPlaysTheGames,2019-06-05T15:52:25Z,0,"Is there any way to easily get around the problem that the Qtables scale at o*(g^i) where g=granularity, o=outputs and i =inputs? It seems like this is quite a large limiting factor because having more than around 14 inputs quickly makes a matrix larger than what most numpy can handle (let alone how much RAM this would take to store in memory).",True
@DromaEditing,2019-06-05T15:37:51Z,20,What about an implementation of DEEP Q Learning now ? :) Because Q Learning is very cool for simple games but let's say for a complicated game like poker the Q Table will be too big Patiently waiting the part 4 :D,True
@lucav4045,2019-06-05T15:10:48Z,2,"Could you maybe produce a series about different applications of machine learning? Like either your own ideas or open source projects explained. Of course only after this series, this is way more important atm",True
@imkronos_me,2019-06-05T15:09:55Z,0,205th,True
@amitdutta121,2019-06-05T15:06:44Z,0,make a tutorial on tf_agents or tensorforce,True
@brandoncardillo5359,2019-06-05T15:05:09Z,67,"can you teach us Deep Q learning for finance soon, please <3",True
@brandoncardillo5359,2019-06-05T14:59:54Z,0,I love you!,True
@MoinKhan-qe3iw,2019-06-05T14:59:18Z,0,4th,True
@shahzerbaig1397,2019-06-05T14:57:22Z,0,First ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ˜‚,True
