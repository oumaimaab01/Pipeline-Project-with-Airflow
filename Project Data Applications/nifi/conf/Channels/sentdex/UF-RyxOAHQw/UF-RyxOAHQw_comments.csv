author,updated_at,like_count,text,public
@rmt3589,2021-07-19T07:23:38Z,0,PICKLE EVERYTHING!!!  Reminds me of my plan to pickle onions and bananas.,True
@GoredGored,2021-05-21T12:18:50Z,0,"This is a good example why you should use a Jupyter notebook instead, there you can test a bunch of codes before moving to the next.  Just saying.",True
@abhishekshinde5514,2021-04-11T17:52:03Z,0,You're using the pickled classifier and not training it so you get the bad accuracy maybe,True
@peschebichsu,2021-03-19T21:03:00Z,6,"Just in case someone else has an issue with reading the .txt files with the code shown. (Error ""UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 4645: invalid continuation byte""): It's probably because of letters like ""ó"". --> Try this instead: short_pos = open(""positive.txt"",""r"", encoding='latin-1').read() short_neg = open(""negative.txt"",""r"", encoding='latin-1').read()",True
@blackypurkait196,2020-09-04T15:42:02Z,0,While training I am getting the Memory Error. There is no problem in loading the dataset but at the time of training it is showing this error,True
@83vbond,2020-07-25T13:43:08Z,0,16:14 Pickle them all!,True
@Rajivrocks-Ltd.,2020-05-24T13:31:20Z,0,"Just wondering, but if I'd want to use a CSV file with around 27k+ tweets how would I go about pre-processing the data so NLTK can work with it?",True
@patricialouise9140,2020-04-27T14:24:21Z,0,"Hi.  It has an error - raise ValueError(""Sample sequence X is empty."") ValueError: Sample sequence X is empty.  How can this one be solved? (Does anyone have the same problem?) Thanks so much!",True
@elhamrahmani2246,2019-05-17T19:59:19Z,0,"Great..Thanks a lot. I have a question that I think it is very similar to this topic but I need to be sure to follow the true approach. What is the best technique to detect acronyms in a text? There are many specific acronyms which are widely used in many PDF documents. Their full meaning is however given at the place where it is first used. What is the best approach to recognize such words wherever they occur in the document? Would you please send me any specif link or tutorial to do that? I think I should create my own training data set. But II don't know How should I all parts of this task?",True
@GelsYT,2019-04-14T16:11:21Z,0,what is the value of rev? is it the documents variable?,True
@GelsYT,2019-04-14T16:06:53Z,0,"hey can someone please answer me? sentdex, so the one that we've erased is just the same to the new one we've created? I'm talking about on getting the value of documents variable, sorry my English is so terrible hope you understand",True
@GelsYT,2019-04-14T15:44:51Z,0,is it really necessary to be a tuple?,True
@dustinpianalto2729,2019-03-20T01:14:10Z,1,"Thanks for the great tutorials, btw I was able to get the whole process down to ~3.5 minutes using multiprocessing and using 8 processor cores to train all 7 classifiers at the same time... Although it does use about 36GB of RAM/Swap... So the trade off is time vs RAM.",True
@RandomUser-ls9ez,2019-02-09T19:56:34Z,1,"When training my own data, I am confused how I put it in. The training and testing set are shuffled feature_sets, which are the words and the Boolean value saying whether it is present in the the most common features. But where is the actual negative and positive part of the training. How do the algos now how to classify it as one or the other?",True
@rajatsharma126,2019-02-07T09:29:16Z,1,how I can load  this data into Google Colab and perform the same tasks,True
@suharthganju,2018-11-28T17:05:26Z,0,Amazing tutorial!!  However would this code work for a single file which  is not divided into positive and negative datasets?,True
@mikesiefert1309,2018-08-10T18:35:43Z,0,"Hey man, really great and helpful videos. Good job! Thanks a lot and best greets from Germany",True
@luvsuneja,2018-07-06T15:28:29Z,0,Anyone getting Segmentation fault: 11,True
@skjha98,2018-07-04T13:50:44Z,0,"I am getting ""StatisticsError: no unique mode; found 2 equally common values. What should I do?",True
@gauravkataria3388,2018-06-28T06:56:06Z,0,"Do we need to remove comma , fullstop , hyphen type characters in a sentence from the training data , will that help us any way ??",True
@MistaT44,2018-06-13T12:32:01Z,1,I'm getting memory error :( any idea how to resolve?,True
@user-zi3qq9ef1g,2018-06-06T09:59:59Z,0,What is another directory?,True
@sagniksarkar506,2018-05-11T18:16:46Z,0,I think the pickle object is having the instance of the previous data set rather than having the new short data set. Maybe that is the reason why it has low accuracy...,True
@ltmagic542,2018-01-09T21:54:14Z,1,"I got this error:  Traceback (most recent call last):   File ""E:\ub\final project\script\sentiment analysis\text classification.py"", line 123, in <module>     print (""MNB_classifier accuracy percent"", (nltk.classify.accuracy (MNB_classifier, testing_set))*100)   File ""C:\python36\lib\site-packages\nltk\classify\util.py"", line 87, in accuracy     results = classifier.classify_many([fs for (fs, l) in gold])   File ""C:\python36\lib\site-packages\nltk\classify\scikitlearn.py"", line 85, in classify_many     X = self._vectorizer.transform(featuresets)   File ""C:\python36\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 291, in transform     return self._transform(X, fitting=False)   File ""C:\python36\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 183, in _transform     raise ValueError(""Sample sequence X is empty."") ValueError: Sample sequence X is empty.   Anybody know how I can solve this problem? Thank you.",True
@sajaal-dabet148,2017-12-21T09:54:29Z,0,Please can you tell me what is the source for the dataset and if is have a benchmark or not?,True
@anasoren1632,2017-12-13T02:38:54Z,0,I saved two files positive.txt and negative .txt in a folder short_reviews in the same directory where Scripts is saved. Still I am getting the same error FileNotFoundError:No such file file or directory:'short_reviews/positive.txt'  Will you pls help me with the saving path,True
@sofianfadli7910,2017-11-14T03:29:31Z,1,"Hey, sentdex. I want to ask you. When i try to run your code, i got this error :  Traceback (most recent call last):   File ""C:\Users\Sofian\Documents\NLTK Python\Better Training Data.py"", line 92, in <module>     #GaussianNB_classifier = SklearnClassifier(GaussianNB())   File ""C:\Users\Sofian\AppData\Local\Programs\Python\Python36-32\lib\site-packages\nltk\classify\scikitlearn.py"", line 117, in train     X = self._vectorizer.fit_transform(X)   File ""C:\Users\Sofian\AppData\Local\Programs\Python\Python36-32\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 230, in fit_transform     return self._transform(X, fitting=True)   File ""C:\Users\Sofian\AppData\Local\Programs\Python\Python36-32\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 172, in _transform     values.append(dtype(v)) MemoryError  What is the meaning of this error and how to fix it?Thank you for any help :)",True
@fiveyearold,2017-10-31T03:34:37Z,0,So thats that.  On to the next video,True
@rishabgupta9926,2017-10-26T08:18:44Z,0,Shouldn't we be shuffling the documents here instead of the featureset?  Taking first 5000 of all_words would rather mean mostly the positive words since we appended short_pos_words first.,True
@gunjanaggarwal1301,2017-10-20T16:35:08Z,1,"My accuracy is coming out to be 50-55 %, no matter how many times i run it. I am using imdb dataset. How to improve the accuracy? Moreover nusvc is also not working. The shell restarts after some time whenever i run it. What to do?",True
@darkhan8713,2017-10-15T06:29:47Z,0,"Good day, could anyone please help me. I run into an error, I am using spyder on mac os and whenever  I try to compile the code you have provided, it shows that   runfile('/Users/Darkhan/python', wdir='/Users/Darkhan') Traceback (most recent call last): File ""<ipython-input-28-c514b7b79643>"", line 1, in <module>  runfile('/Users/Darkhan/python', wdir='/Users/Darkhan') File ""/Users/Darkhan/anaconda/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 880, in runfile  execfile(filename, namespace) File ""/Users/Darkhan/anaconda/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 102, in execfile  exec(compile(f.read(), filename, 'exec'), namespace) File ""/Users/Darkhan/python"", line 53, in <module>  readMe = open('text.txt','r').read() File ""/Users/Darkhan/anaconda/lib/python3.6/encodings/ascii.py"", line 26, in decode  return codecs.ascii_decode(input, self.errors)[0]  UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3118: ordinal not in range(128)  I am doing everything as you did but this new data set can`t be read by python, I think it is due to the size of .txt because when i try to enter smaller set of data it reads, so what would you suggest to do. I searched net and could not find any proper solution. thanks!",True
@dude2260,2017-09-25T08:27:38Z,0,"for me most informative features are 4 , ? , a , ; , & , ) ,  and many more like this any idea why this is happening",True
@simonchan2394,2017-08-15T08:34:53Z,0,"Hi Harrison, can I please ask you where you got the positive.txt and negative.txt files from? Was it downloaded from twitter?",True
@simonchan2394,2017-08-15T08:34:13Z,0,"I think we should also categorize the data by lowercasing all the words:   for r in short_pos.split('\n'):     documents.append((r.lower(),""pos""))      for r in short_neg.split('\n'):     documents.append((r.lower(),""neg""))",True
@AIandtheworld,2017-08-04T03:47:05Z,0,"Hey there! I'd like to do this stuff with Japanese tweets, do you have any advice for me? Thanks! :)",True
@aldosanjoto8296,2017-07-31T07:51:08Z,0,should be: word_features = [w[0] for w in all_words.most_common(5000)]  for the top 5000 most common,True
@ujjawalgupta4513,2017-07-07T08:26:59Z,1,there is memory error occuring in my code after the calculation of accuracy of the naive bayes algorithm,True
@highharii,2017-06-21T04:20:28Z,0,"this is the error im getting,any clue why?  File ""C:/Users/Sree Hari/Desktop/New folder/nlp/algosent.py"", line 94, in <module>     MNB_classifier.train(training_set)   File ""C:\python3\lib\site-packages\nltk\classify\scikitlearn.py"", line 117, in train     X = self._vectorizer.fit_transform(X)   File ""C:\python3\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 231, in fit_transform     return self._transform(X, fitting=True)   File ""C:\python3\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 173, in _transform     values.append(dtype(v)) MemoryError",True
@syamjalla8158,2017-06-07T13:23:54Z,0,i am getting error stating AttributeError: 'NoneType' object has no attribute 'append'  when creating document file and all_words variable. could you please help me rectify this error,True
@nimishjindal9271,2017-05-29T11:14:30Z,0,shouldn't the all_words frequency distribution list be sorted manually? i noticed that it just contains the first 3000 words if we do [:3000] without sorting.,True
@jenla5736,2017-05-01T03:43:58Z,0,This is a great video series teaching how to use NLTK with supervised learning. Do you have any videos on using NLTK with unsupervised learning?  Ex: text mining newspaper articles and clustering articles based on their similarities as a way to recommend a bunch of news articles covering a person's topic of interest.,True
@taechikalat5056,2017-04-13T10:53:55Z,0,"how can i fix this error ""'charmap' codec can't decode byte 0xfc in position 31999: character maps to <undefined>"" ??",True
@IonicCascade,2017-02-25T19:44:47Z,0,"Just tried implementing the code copied and files downloaded and get the following error: Traceback (most recent call last):   File ""/Users/imcnabb/Updated/Data Science/PYTHON/Intermediate Python/training_data.py"", line 37, in <module>     for r in short_pos.split('\n'): TypeError: a bytes-like object is required, not 'str'  How can I fix this?",True
@agraneebanerjee305,2017-01-31T16:28:03Z,1,"I have this error..please help me..   File ""C:\Users\Jeet\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>     exec(compile(f.read(), filename, 'exec'), namespace)   File ""D:/PROJECTS/Sentiment Analysis/nltkvid18.py"", line 96, in <module>     print(""MNB_classifier accuracy percent:"", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)   File ""C:\Users\Jeet\Anaconda3\lib\site-packages\nltk\classify\util.py"", line 87, in accuracy     execfile(filename, namespace)   File ""C:\Users\Jeet\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile     results = classifier.classify_many([fs for (fs, l) in gold])   File ""C:\Users\Jeet\Anaconda3\lib\site-packages\nltk\classify\scikitlearn.py"", line 83, in classify_many     X = self._vectorizer.transform(featuresets)   File ""C:\Users\Jeet\Anaconda3\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 293, in transform     raise ValueError(""Sample sequence X is empty."") ValueError: Sample sequence X is empty. >>>     return self._transform(X, fitting=False)   File ""C:\Users\Jeet\Anaconda3\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 184, in _transform  I",True
@theCanadian808,2016-12-07T19:17:45Z,0,I found a very nice dataset case you wan to compare the algorithms https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/tree/master/data/opinion-lexicon-English,True
@shrikant23jituri,2016-11-21T10:55:29Z,6,I am having a UnicodeDecodeError while using word_tokensize with  the postive.txt and negative.txt. I have used the exact same code as yours. Can you tell me where am I going wrong ?,True
@g00dvibes47,2016-11-15T19:31:35Z,0,"from nltk.corpus import PlaintextCorpusReader corpusdir = 'short_reviews/' short_reviews = PlaintextCorpusReader(corpusdir, '.*')  short_pos_words = word_tokenize(short_pos.decode(""latin-1"")) short_neg_words = word_tokenize(short_neg.decode(""latin-1""))  worked for me for solving the encryption 0fx3 error",True
@anonymosthucker5120,2016-10-27T12:03:25Z,0,"I have a error please help me  Traceback (most recent call last):   File ""C:\Python\sentiment18.py"", line 97, in <module>     MNB_Classifier.train(training_set)   File ""C:\Users\jhon anthony\AppData\Local\Programs\Python\Python35-32\lib\site-packages\nltk\classify\scikitlearn.py"", line 115, in train     X = self._vectorizer.fit_transform(X)   File ""C:\Users\jhon anthony\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 231, in fit_transform     return self._transform(X, fitting=True)   File ""C:\Users\jhon anthony\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 173, in _transform     values.append(dtype(v)) MemoryError",True
@MrDanielphillis,2016-10-16T03:48:45Z,0,"wow - my laptop is now up to 10 mins without pickling, and pickling brings it down to <edit> 3 mins - phew ! on python 2.7, 4GB of RAM",True
@RaoufGnda,2016-08-24T22:28:24Z,1,UnicodeDecodeError: 'ascii' codec can't decode byte 0xed in position 6: ordinal not in range(128),True
@jgarciabu,2016-07-29T13:33:20Z,0,"Anyone else getting the following error:  Traceback (most recent call last):   File ""/Users/jgarcia/Desktop/Python Tutorial Scripts/Text Classification.py"", line 102, in <module>     MNB_classifier.train(training_set)   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/classify/scikitlearn.py"", line 114, in train     X, y = list(compat.izip(*labeled_featuresets)) ValueError: need more than 0 values to unpack",True
@user-tq4fw1wb9g,2016-07-29T10:41:36Z,0,has no attrbute 'split'??? why??,True
@ottawareviews5616,2016-05-22T20:46:38Z,0,"Traceback (most recent call last):   File ""/Users/Tarundeep/Desktop/nltk/nltk_better_training_data.py"", line 38, in <module>     short_pos = open(""short_reviews/positive.txt"", ""r"").read()   File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/codecs.py"", line 321, in decode     (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 4645: invalid continuation byte  Process finished with exit code 1",True
@sriramchandramouli9793,2016-04-18T17:14:14Z,0,"Here, rev in featuresets has dictionary like values with words and polarity as value. Consider, I have a test dataset, which has only reviews, How to train the model using training dataset and ask the model to predict category for test dataset? If I train this feature set along with category, it is not accepting since rev column has dictionary like value. Your example shows only accuracy. How to use predict function following the same procedure?",True
@jibintk9872,2016-04-02T15:37:12Z,0,"When i executed the code in this video.i got an error. _____________________________________________________________________________________________________________________   MNB_classifier.train(training_set)   File ""/usr/local/lib/python3.4/dist-packages/nltk/classify/scikitlearn.py"", line 115, in train     X = self._vectorizer.fit_transform(X)   File ""/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/dict_vectorizer.py"", line 226, in fit_transform     return self._transform(X, fitting=True)   File ""/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/dict_vectorizer.py"", line 195, in _transform     result_matrix = result_matrix[:, map_index]   File ""/usr/local/lib/python3.4/dist-packages/scipy/sparse/csr.py"", line 292, in __getitem__     return sliced * P   File ""/usr/local/lib/python3.4/dist-packages/scipy/sparse/base.py"", line 319, in __mul__     return self._mul_sparse_matrix(other)   File ""/usr/local/lib/python3.4/dist-packages/scipy/sparse/compressed.py"", line 499, in _mul_sparse_matrix     data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype)) MemoryError  __________________________________________________________________________________________________________________________  anyone please help me",True
@SomethingStrange910,2016-03-26T03:51:21Z,0,my laptop took about 10 min before it started showing results. for a moment I thought it froze on me...,True
@gianlucataccarello6928,2016-03-05T10:34:36Z,1,"I don't understand why it gives me error: Traceback (most recent call last):   File ""C:\Python27\NLTK\NLTK BETTER TRAINING DATA.py"", line 38, in <module>     short_pos = open(""short_reviews/positive.txt"",""r"").read() IOError: [Errno 2] No such file or directory: 'short_reviews/positive.txt'I saved both files as positive.txt and negative.txt on desktop but it doesn't find them!",True
@jibintk9872,2016-02-29T11:38:15Z,0,"I have everything the same as you do just for some reason I am experiencing this error which is given below  ************************************************************************************************************************ Traceback (most recent call last):   File ""new.py"", line 53, in <module>     short_pos_words = word_tokenize(short_pos)   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/__init__.py"", line 104, in word_tokenize     return [token for sent in sent_tokenize(text, language)   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/__init__.py"", line 89, in sent_tokenize     return tokenizer.tokenize(text)   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1226, in tokenize     return list(self.sentences_from_text(text, realign_boundaries))   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1274, in sentences_from_text     return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1265, in span_tokenize     return [(sl.start, sl.stop) for sl in slices]   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1304, in _realign_boundaries     for sl1, sl2 in _pair_iter(slices):   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 311, in _pair_iter     for el in it:   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1280, in _slices_from_text     if self.text_contains_sentbreak(context):   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1325, in text_contains_sentbreak     for t in self._annotate_tokens(self._tokenize_words(text)):   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1460, in _annotate_second_pass     for t1, t2 in _pair_iter(tokens):   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 310, in _pair_iter     prev = next(it)   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 577, in _annotate_first_pass     for aug_tok in tokens:   File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 542, in _tokenize_words     for line in plaintext.split('\n'): UnicodeDecodeError: 'ascii' codec can't decode byte 0xed in position 6: ordinal not in range(128)  ******************************************************************************************************************** plz help me to solve this error",True
@Arslanqadri,2016-02-13T02:53:50Z,4,"The place you found the word_tokenize error, i spent an hour trying to convince myself it is correct, earlier :|",True
@fleimiux,2016-02-11T20:59:17Z,0,"Hi,  I am using python 3.4 on ubuntu 14.04.  I have everything the same as you do just for some reason I am experiencing this error which is asking for the pickled file(or at least I think that is the case) even when i have commented out the lines of code which use the pickled file.  here is the error: /usr/bin/python3.4 /home/ignuz/Desktop/sentiment/test.py Traceback (most recent call last):   File ""/home/ignuz/Desktop/sentiment/test.py"", line 48, in <module>     short_positive_words = word_tokenize(short_positive)   File ""/usr/local/lib/python3.4/dist-packages/nltk/tokenize/__init__.py"", line 104, in word_tokenize     return [token for sent in sent_tokenize(text, language)   File ""/usr/local/lib/python3.4/dist-packages/nltk/tokenize/__init__.py"", line 88, in sent_tokenize     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))   File ""/usr/local/lib/python3.4/dist-packages/nltk/data.py"", line 796, in load     opened_resource = _open(resource_url)   File ""/usr/local/lib/python3.4/dist-packages/nltk/data.py"", line 914, in _open     return find(path_, path + ['']).open()   File ""/usr/local/lib/python3.4/dist-packages/nltk/data.py"", line 636, in find     raise LookupError(resource_not_found) LookupError:  **********************************************************************   Resource 'tokenizers/punkt/PY3/english.pickle' not found.   Please use the NLTK Downloader to obtain the resource:  >>>   nltk.download()   Searched in:     - '/home/ignuz/nltk_data'     - '/usr/share/nltk_data'     - '/usr/local/share/nltk_data'     - '/usr/lib/nltk_data'     - '/usr/local/lib/nltk_data'     - '' **********************************************************************  If you could let me know what is wrong, I would be grateful!  Thanks",True
@rmalshe,2015-12-07T01:09:06Z,0,"Hi there! I tried to run this code but I continued to run into errors. Thereafter, I reduced the size of files by deleting majority of the text. My file sizes were merely ~ 27 kb. And word count was merely ~ 5000 words compared to original dataset of over ~ 125,000 words. I could run it, but with word_features = .... [:500], and training_set = ...[:200], testing_set = ...[200:].I am not able to figure out why it crashes with bigger datasets.  Does anyone have any ideas on how to resolve this? I would like to run it on full datasets.",True
@elliottcooper7767,2015-12-01T15:25:16Z,8,this is really great data to train from thanks! and thank you for the awesome video series! may i ask where you got this data from? or did you mine it yourself?,True
@muhammedahmet9615,2015-11-12T13:34:24Z,0,"hı tray to run but I get this error and thaks your video j Traceback (most recent call last):   File ""C:/Python34/Scripts/twitterapi.py"", line 7, in <module>     from sklearn.naive_bayes import MultinomialNB, BernoulliNB   File ""C:\Python34\lib\site-packages\sklearn\__init__.py"", line 56, in <module>     from . import __check_build ImportError: cannot import name '__check_build'",True
@shivrajacharjee6945,2015-06-28T15:07:21Z,0,"I keep having this error. What do i do?  Traceback (most recent call last):   File ""D:/Workspace v1/nltk/sentiment analysis.py"", line 93, in <module>     MNB_classifier.train(training_set)   File ""C:\Python34\lib\site-packages\nltk\classify\scikitlearn.py"", line 115, in train     X = self._vectorizer.fit_transform(X)   File ""C:\Python34\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 224, in fit_transform     return self._transform(X, fitting=True)   File ""C:\Python34\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 165, in _transform     indices.append(vocab[f]) MemoryError",True
@arjipoo,2015-06-17T20:52:41Z,0,"Hello sentdex,   Wonder if you could help me out with an error in the *MNB_classifier.train(training_set)* line. The error is:    File ""c:\Python32\lib\site-packages\nltk\classify\scikitlearn.py"", line 115, in train     X = self._vectorizer.fit_transform(X)   File ""c:\Python32\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 224, in fit_transform     return self._transform(X, fitting=True)   File ""c:\Python32\lib\site-packages\sklearn\feature_extraction\dict_vectorizer.py"", line 166, in _transform     values.append(dtype(v)) MemoryError   ###################################################### Edit:  So after a long few hours I finally figured out that my computer is giving these MemoryError's for everything! I tried to stop them by pickling but even the process of picking caused them!    I went on your github and tried to find your source code but you didn't commit any of the pickle files. Seeing as I cannot run the code, could you push the pickled files to the github page? That way I can actually get a working demo!    Thanks!",True
@jamielunn5869,2015-06-06T21:11:06Z,3,Anyone having trouble importing the short reviews? I'm getting this error: UnicodeDecodeError: 'ascii' codec can't decode byte 0xf3 in position 4645: ordinal not in range(128)  ..Any ideas?,True
@ManteChannel,2015-06-03T15:05:44Z,3,"Thanks for your awesome tuts there. I've a question, why you don't you do the  set (word_tokenize(document))  any reason behind it? since it'll help to get rid duplicate words.",True
@Karzacan,2015-05-18T16:40:02Z,0,Great video!,True
