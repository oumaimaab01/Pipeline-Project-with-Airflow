author,updated_at,like_count,text,public
@karaniii,2024-05-02T07:24:57Z,0,"Thanks. Best ML channel ever. I learned a lot, from building neural networks from scratch, image classification in cnn, GTA series(loved every moment of it) to this.  Keep up the awesome work.",True
@chakradharreddy6437,2024-01-22T17:14:21Z,0,"You told that x shouldnt be <0 but initialized (x1,y1),(x2,y2) from -9 to 10 please explain please",True
@ManuelMuller-ux2sf,2024-01-20T21:50:03Z,0,"Why BGA? Because it is openCV. RGB is arbitrary but at some point, everyone agreed on it... but openCV is older than that and noone adapted. Silly, but I guess it is the actual reason :)",True
@lylerodericks,2023-09-03T12:56:11Z,0,"Learning from a cussy Walter white + Jessy Pinkman is literally the only way to learn AI now 😅😅 LOVE THE VIDEO SERIES!! you mean transpose the output (view) environment, not rotate :) Best channel!❤",True
@Zelmann1,2023-07-06T23:40:21Z,0,This is the best tut on reinforcement learning I have come across...literally from the ground up. Step wise and very logical. Amazing and very inspiring learning content!!,True
@SkShahidAliNawaz,2023-05-04T13:50:42Z,0,"any content on youtube/any other source which explains how to use q learning on optimization algorithms, if anybody knows please reply to this comment, it will be of great help for my thesis",True
@jayjhaveri1906,2023-02-26T06:53:02Z,0,"Acc to me, Although PIL (IMAGE) reads it as an RGB file. You are passing it into openCV, which interprets any color codes as BGR by default lol...  So, changing the default of openCv read should fix it ig.  GREAT TUTORIAL BTW OP",True
@praveenmurali9357,2023-02-21T11:53:31Z,0,"Hey man thanks for your videos they are absolutely amazing!!! I have a question though, when I try to resize my 10x10 grid by img.resize, my blue/green/red blocks are blurred. How do I fix that? I tried upscaling the img matrix first and then convert it into img, didnt work while rendering. My guess is when you resize any image to a higher resolution it gets blurred. Thanks for your time!!!",True
@nature_through_my_lens,2022-10-12T10:31:17Z,0,"OMG! I did a terrible mistake by not updating the q_value with the new_q. Even though everything worked fine, my mean wasn't reducing below 200. I was knocking my head off! Wonderful crash course so far! Keep it up...",True
@ianik,2022-09-16T12:51:57Z,8,"Words cant describe the appreciation I have for your channel and you in person. The energy you put to offer us such quality education for free. Some of us come from really challenged societies and as a result cant afford such education. Idk if you a believer but still my prayer is that the Lord Almighty continue to bless you, your work ..and your people at large",True
@krvijay2161,2022-09-11T11:41:59Z,0,"For the example at 2:11, can anyone provide some reference study material to implement such a complex model",True
@bharghavak,2022-06-17T03:04:49Z,0,Had helluva great time watching bro. Learnt stuff too You rock,True
@jimmyloluolajide7777,2022-05-23T06:51:29Z,0,"You are amazing. Please, how do I create an environment for a tabular data to find optimal policy for mortality rate. Your feedback would be highly appreciated.",True
@venusshah4476,2022-03-27T17:08:29Z,0,"amazing video, however I am not getting that how we can code the boxes size, because whenI am coding by myself, I am just getting the dots with different colours.",True
@rifahaziz7539,2022-03-18T03:15:23Z,0,"Great video! This was very helpful indeed! One question I had is, if there are multiple enemies, how would the 'obs' change? would the q-table size change? Thanks!",True
@juhotuho10,2022-01-01T16:23:55Z,0,"I tried to expand this to having 2 enemies, but i can't because the q_table is hogging all my RAM :(  seriously, it's taking like 11 GB and more",True
@maximeg3178,2021-12-17T01:42:03Z,0,"Bonjour monsieur,  J'aime beaucoup  le for dans un for dans un for dans un for dans un for  dans un for dans un for , c'est tres tres fort !",True
@DbitStudents-xd2zo,2021-11-22T21:32:02Z,0,you are a vibe,True
@eastwoodsamuel4,2021-10-15T20:25:50Z,0,Why are we taking coordinates from -SIZE+1 ?  Aren't all the coordinates positive? Shouldn't it be from 0 to SIZE?,True
@aashnavaid6918,2021-10-07T14:10:31Z,0,you are AWSOME! dust keep doin this ! thank you !!!,True
@michaelhenderson65,2021-09-14T15:25:24Z,0,"Thoughts on learning about the wall:  randint includes both end points (unlike lists, etc) so your random moves are -1,0,1 and 2;  so the player will drift quite quickly when epsilon is high. Thinking of grid like a chess board with light and dark squares when food and player are on opposite colors the agent needs to hit the wall then do a search. So its a two step problem: hit the wall once and then search for food. But the first step is happening quickly because of the drift.  This will help learning. The q-values will be significantly higher for states when food and player are same color squares so I imagine it gets there quickly and because of the value differences. You could also think of the state space as being 50 states where player and food are on same colour and 50 states where player and food on different colors.  In other words, there is one state space where there is a food item and another state space where there isn't any food.  The drift term ensures frequent transitions between these two state spaces so I imagine it will learn quite quickly the food state is more valuable than the non-food state and you'll have a positive slope for q-values towards the state with the food item.    I",True
@g73_easonpeng29,2021-07-10T04:15:11Z,0,You have so many mugs,True
@aishwaryakumar256,2021-07-02T12:57:29Z,0,Can we like do some changes for making the environment continuous rather than episodic?,True
@atithi8,2021-06-18T17:45:51Z,0,Fabulous!! 21:05,True
@1ZEGA,2021-05-17T10:27:18Z,0,"Nice, but I probably still don't understand how the q-learning actually works after these videos.  How comes that the agent learns to improve the q-table, if he only gets the reward once he hits the target? That means the first few hundred times it is not getting any reward until it randomly hits it, and only after that this information back-propagates into the q-table? What if he never stumbles upon the target in the first place though??",True
@tcgvsocg1458,2021-03-21T13:51:10Z,0,Can you explain how to learn python with you channel? The path to become good in python,True
@kaishang6406,2021-03-17T03:47:42Z,0,"if im thrown into this environment, only know the distance to food and enemy and four door. and get my memory erased after entering a new room. i don't think i can get the food.",True
@neuron8186,2021-02-24T06:29:40Z,0,Ayo my man Snowden how you doin?,True
@timmdunker8507,2021-01-18T21:13:36Z,0,"Was getting a ""Segmentation Fault: 11"" error on MacOS when trying to display the image with cv2.imshow. Uninstalling opencv-python (""pip3 uninstall opencv-python"") and ""pip3 install -Iv opencv-python==4.4.0.42"" did the trick for me :)",True
@rajghugare6161,2020-12-28T14:07:13Z,0,Is anyone facing the problem that the image gets blurry after resizing?,True
@tayler6000,2020-12-25T07:31:06Z,39,"For anyone that's having blurry boxes displayed, changed your code to img = img.resize((300, 300), resample=Image.BOX)",True
@GreenLeafSoo,2020-11-26T13:42:13Z,0,"Hi sir, why (-SIZE + 1)? What do you mean by ""we need to shift""? Thanks for the awesome tutorial :)",True
@schwarofmonte,2020-11-18T21:12:57Z,0,abi harikasın,True
@lahaale5840,2020-11-06T18:03:48Z,0,"The content is cool,  only people may not really code in this way, write from the first line to the last line of the code.  For example, we may import the libs only we realize it's needed, no import a lot of them in before we start to code. Similarly, we may add variables and functions when they needed and test it every time we added something, we may want to test it before adding new things.",True
@tielessin,2020-11-05T16:33:23Z,2,When I just can't find the cause of an error: 19:18,True
@mzareer2376,2020-11-03T01:47:14Z,0,"i tried the same code but the array image is always blurry, do you have any idea why?",True
@soulfrench,2020-11-02T17:01:34Z,1,How can I change your codes with open AI style? I mean I want to inherit Open AI's gym environment to create the environment of your RL codes.,True
@boongbaang482,2020-10-11T15:31:50Z,0,why isn't the currently running episode terminated once you hit the enemy or food ?,True
@techtutorials8812,2020-09-15T21:09:51Z,0,"100% agree about the line limit. ""I know what I'm doing, PEP8,  so just be quiet will you?!""",True
@svegaldde,2020-09-12T13:26:37Z,0,awesome!,True
@adamhendry945,2020-09-11T08:02:56Z,0,"@sentdex `opencv` is BGR, which is why you're seeing what your seeing. Matplotlib and Pillow are RGB. The reason for BGR is because C arrays (pyhon is written in C) is row major NCHW (number of images, channels, height width). Many vector operations packages (BLAS, etc.) are optimized for row major arrays. Since `opencv` is an image processing library, and we want it to perform image operations fast, it is BGR. For the same reasons, numpy is row major. Hence, if you read the shape tuple of a numpy array, the outermost array (i.e. ""the container"") is the left-most dimension. Dimensions go right to left as columns, height, depth, etc. In image space, the origin is the upper left corner and y points down, so you'll have to take that into consideration as well. Also, you access numpy arrays as ""A[x,y,z, etc.]"", not ""A[x][y][z]"". That latter is used for lists. One last note is you could have overloaded ""__eq__"" as well to determine when two Blobjects are equal since you went to the trouble of creating a class.",True
@qkloh6804,2020-08-14T06:49:21Z,0,"Regarding to the wall problem, it works because he learned what to do in the state. It doesn't need recognise a wall to utilise that.",True
@liwaiyip1769,2020-08-05T05:54:36Z,0,What if the enemy is also an agent?,True
@sargamyadav1069,2020-08-03T10:36:10Z,0,Hey. Great videos! Is there any way to create a dialogue system in openAI gym?,True
@LionelMessi-fu6wn,2020-06-28T15:33:39Z,0,"Could you please do the autonomous car with kivy example too? None of the available tutorials cover it in detail; they just show the code and wipe their hands. Love the tutorials btw! Also, it's bgr because you're using the tensorflow backend. For rgb, use theano.",True
@user-or7ji5hv8y,2020-06-27T11:29:39Z,0,Does anybody understand how np.convolve() works? Can you refer to a resource online that explains this well.,True
@yashodeepchikte433,2020-06-16T15:11:35Z,0,this guy is so talented but his  constant laughs are so damn annoying   That's the fucking reason you cant watch 1 hour of him bursting out,True
@yashodeepchikte433,2020-06-16T15:01:00Z,0,why the hell does he laugh in between words; so fucking irritating  Just speak like a normal person not like a little baby,True
@tarat.techhh,2020-06-15T19:44:13Z,1,I just watched a 1 hour long video can't believe. You r magic dude!,True
@KodandocomFaria,2020-05-16T04:46:26Z,0,"Sentdex what about use it instead of create an enviroment, I use it on an external site as enviroment for instance... there are some sites where I have the api so I have all information about the enviroment and this site give me an option to train on this environment. I can performe some actions based on information from this environment I if I do right action I receive reward and if I do incorrect action I receive negative reward. I after I train I can do it on real world on site the same way. my doubht is how can I create an gym enviroment like openai gym to use those algoritms ?",True
@phamdinhhoang1998,2020-05-12T09:29:46Z,0,"my ""blob"" move so fast. any way to slow him down?",True
@hamza-325,2020-04-19T13:03:00Z,0,"26:00 Excuse me, but your initialization of q_table is very messy, as a beginner has done it! Why not simply use 5D np.random.uniform, and end all this story with a single line of (optimized) code?",True
@volt897,2020-04-14T12:32:02Z,0,why don't you make a video with this setup but with multiagent q learning? like in this program you can add that also the enemy wants food and the 2 agents must avoid each other and find food. i think it would be cool!,True
@Mani-jo5lq,2020-04-11T22:34:14Z,0,"Hey, I have to create an environment for a computer science assignment. Is it okay if I ask you for help on the assignment?",True
@abraristiakakib1925,2020-04-11T17:25:33Z,0,Hey! Did anyone try keeping enemy dynamic and food static? Result was so much nail biting .,True
@nathantsang5806,2020-04-06T19:59:01Z,0,My game graphic is super fuzzy. Does anyone else have that too?,True
@madigor9264,2020-03-22T18:16:06Z,0,"Does x1,y1  and x2,y2 belong to player and enemy?",True
@williammeitzen321,2020-03-14T21:46:08Z,0,"Hi Sentdex, I'm new to ML and RL, and I am enjoying your videos a lot!  Q: It would seem that the environment has finished learning when the moving_average becomes less than the distance between opposite corners * move_penalty + food_reward.  Thoughts?",True
@shashwatrathod1342,2020-02-15T14:32:08Z,0,You've been more helpful than any of my university professors. Thank you!,True
@chickensalad1369,2020-01-30T16:58:18Z,1,31:58 ahh this is why my agent has stockholm syndrome...,True
@markd964,2020-01-20T05:11:47Z,0,"Why are qtable values initialised with random values between -5 and 0 ? ( np.random.uniform(-5, 0))?",True
@imrankhanissm,2020-01-15T17:26:21Z,1,23:20 trump wants to know your location,True
@soorajh9480,2020-01-07T16:38:19Z,0,"I am still confused about np.random.random() being compared to epsilon, I mean its a random fraction right what is the point of comparing?",True
@dipanjanabiswas,2020-01-03T18:46:50Z,0,"Hi Sir, I want to learn how to define a non deterministic environment. I have a transition probability matrix of the states for each action and the reward matrixes for all the actions. I want to apply RL and my environment is stochastic body area network.",True
@erwinkonink,2020-01-01T20:39:35Z,0,"It doesn't know about a wall, but it has learned to move away from the food in order to change the reachable spots. So it moves away when (diff_x, diff_y) is equivalent either (odd, even) or (even, odd) and moves towards when (diff_x, diff_y) is equivalent to either (even, even) or (odd, odd).",True
@AcapellaAjax,2019-12-27T06:53:48Z,0,This helped a lot...Can you make some videos on Capsule Nets using tf and keras?,True
@VascoCC95,2019-12-26T02:49:02Z,0,"It's not that weird that the blob learns to use the wall. I made a 360x360 q_table with the mountain car and printed the q_table as an RGB image (as it's a 2D 3 channel matrix ;) ). You can clearly see that he learns the left boundary of the ""world"" as a bad thing because it forms a black diagonal line on the top of the image marking the car hitting the wall with speed and loosing it.",True
@EricsShitposting,2019-12-22T23:37:45Z,0,very impressive,True
@a.b.c.d.e...,2019-12-08T21:17:04Z,0,Beautiful tutorial! Shouldnt enemy.move() and food.move() happen before the observation is made / the qtable choice gets evaluated? Like this it leaves the AI in the ugly position that it has to guess where its enemy / food goes if it gets close to them.,True
@maxneumann,2019-11-26T08:39:18Z,0,"I don't think its using the wall randomly. I ""trained"" it with food and enemy movement on and when I turned the movement off the Blob had no idea how to get to the food half of the time. Great videos by the way!",True
@galgilor3241,2019-11-18T17:42:45Z,0,"Thank you for an amazing video series, extremely helpful! about the BGR instead of RGB, it's because of OpenCV. Using Matplotlib imshow could fix it (plt.imshow), or changing the order  of the array ([:, :, ::-1])  Thanks again!",True
@stellanlange1406,2019-11-17T16:19:42Z,0,"Hello Everyone!    For the following code line:  obs = (player-food, player-enemy) I get the error:   TypeError: unsupported operand type(s) for -: 'Blob' and 'Blob'    If anyone else came across this error for the blob environment then please share if you came up with any workarounds!    The code line is from the section after the blob class is finished.",True
@SahilDawka,2019-10-31T02:11:42Z,0,"in the action dunder, can we write just 'self.move(1,1)' etc?",True
@pratibhalaturiya5969,2019-10-23T17:52:31Z,0,Please provide imitation learning approach.,True
@bgokhale,2019-10-10T08:57:30Z,0,"Why is the observation taken at (player-food, player-enemy) and not at player?",True
@wktodd,2019-09-21T13:32:44Z,0,"the wall isn't there as far as the blob is concerned, it simply sees a continuum",True
@lsp2987,2019-09-20T06:48:06Z,0,Hi! Could you plaese recommend some reading on q learning basics to better understand all the sense of what you are doing including building environments logic?,True
@TheInternet81,2019-09-16T09:32:20Z,0,"bro, still not get the keypoint of data that should input to RL environment. I mean, I need clue about the key point create custom RL environtment, to develop it. example: Android Game.... we can load the picture of moving picture to a browser (streaming image). we can control it by many tools like uiautomator 2. the only problem is how to make a 'bridge ' between the 'data' from streaming picture movement of game or we can call it video, and make the AI control it.  what data should I suplly in that condition.  Q-Learning like gym (openai), is good progress in teach computer. and can solve many problem in our jobs...  may I ask, how about create environtment in that condition?",True
@dominikrafa6262,2019-09-15T12:29:40Z,0,Can I build this way an env that will be able to operate in an endless loop? Or at least a one that will go as long as reward is over some specified threshold?,True
@BakaliMampers,2019-09-15T05:25:23Z,0,"Thank you so much for this video! One question, how would I integrate this into a game I made using pygame?",True
@johnnewlands946,2019-09-06T00:07:53Z,0,"I added in the extra 4 move options, left, right, up and down. It reduces the time it takes to learn. Quite interesting for a novice like me.",True
@jackhuang468,2019-09-03T12:15:22Z,0,"This tutorial is so cool.  Thank you so much, Sentdex.",True
@deekshaaggarwal7981,2019-08-29T16:22:56Z,0,"Hi Sentdex, Can you make a tutorial on solving basic dynamic programming problems such as Edit Distance, with RL? I will be a great help in understanding the non-game environment problems. Thanks.",True
@neuromanta,2019-08-27T20:13:10Z,0,"I read all the comments, and I'm actually surprised I'm the first to mention this, but... all of this simply doesn't add up. The AI doesn't actually get better at finding the food (and avoiding the enemy), because the reward doesn't tell it how far the agent is from the food (or the enemy). Thing is, it would be able to learn and get better at it, if the environment wouldn't get randomized at every episode... but it does. So, you should either implement a better reward calculation (for example by calculating the agent's manhattan distance from the food and the enemy, weighting them accordingly (making so being closer to the food and farther from the enemy counts as better), and summing this value for all steps taken), or not randomize the environment. Also, the agent being able to only move diagonally is weird, to put it mildly.  @sentdex, do you plan to fix this? I think it would worth a followup video. I can help you by writing the code for the reward calculation, if you need.",True
@vandortamas2010,2019-08-24T15:53:03Z,5,"The learning environment is interesting because (1) the agent can move only diagonally and therefore half of the fields are inaccessible for it and (2) the ""out of bounds, fix"" part of the code is causing that if the agent slides through the wall the fixing pull back to the game field, but just the wrong direction movement. For example, if the agent in the upper left corner and moves up+right, the fixing cancels the up part of the movement, but keeps the right part and the agent is on the adjacent, originally inaccessible field next to the upper left corner. So it really ""uses the wall"" to get to the originally inaccessible fields.   The regular implementation of keeping the agent on the field would be if you check both directions before the movement and if one of them would be out of boundary after calculation than the whole movement is canceled. In that case, unfortunately, the agent cannot access all fields.",True
@souradiproy690,2019-08-18T15:52:14Z,0,I am thinking of using reinforcement learning on iris dataset. I m thinking to remove the label column and use the rest of the dataset as a reinforcement learning data. And then use q-learning algorithm and build an agent if he can detect values. Any help how shall I do it is really appreciated. How shall I approach this problem. or what things i shall consider while creating the environment.,True
@martinzthabo5341,2019-08-02T19:15:35Z,0,This series is interesting from this point onwards. It seems like everyone who does RL tutorials uses gym. But now things are interesting. Have you thought of combining LSTM and RL?,True
@seifsedky9131,2019-07-25T16:45:48Z,0,"If there are multiple agents, do you use one q table for each agent, or just one q table for everything? eg. If the player and enemy are both competing to get to the food. Also, how do you draw one graph of cumulative reward for each individual agent on the y axis and the episode no. on the x-axis? Thanks in advance !",True
@FuZZbaLLbee,2019-07-21T04:40:55Z,2,"“I don’t think it should be able to learn that”. The last phrase a scientist spoke before the AI took over.  BTW Awesome tutorial, thanks for making it.",True
@herbertrafael8001,2019-07-06T19:29:56Z,2,"Im getting  ""KeyError: ((0, -10), (5, -4))"" at line: max_future_q = np.max(q_table[new_obstacles   Does anyone know whats wrong?",True
@bharathvarma,2019-06-29T10:38:18Z,1,Great video series Mr.Sentdex. Making an environment was the first thing I did when I started learning reinforcement learning. I am a great fan of yours. Hope to see more of reinforcement learning algorithms here,True
@XxGabberlordxX,2019-06-29T10:36:43Z,0,"Hey sentdex. I really enjoy your tutorial about reinforcement learning so far but I have a short question. Could you pls make a short video, how to get these information about the current state, reward etc. from a game where you don't have a direct access to the in- and outputs. I try atm to implement Deep Q-Learning on a steam game called Super Hexagon. It's atm 2 Euros/Dollars on steam. Maybe you could extend your Reinforcement Learning series to a topic like this.  I really appreciate an answer.   Have a nice day :)",True
@BryceChudomelka,2019-06-26T01:26:26Z,0,"Thank you for your videos @sentdex. This was a good one. How can I implement multiple enemies though? I am getting an errror.   wall_1 = Blob((coord_x, coord_y)) wall_2 = Blob((coord_x, coord_y))   obs = (player - food, player - wall_1, player - wall_2)   There is an error in action=np.argmax(q_table(obs]). Any ideas?",True
@xixappon,2019-06-21T20:03:30Z,0,@9:32 Curious the info about BGR,True
@gaboboy,2019-06-19T17:46:02Z,0,What is the method to create a simply wall in pygame? Like a maze game,True
@spencerlockhart9684,2019-06-19T15:40:36Z,1,"It isn't really complete yet, but here's a project I based on this tut: https://github.com/s-p-n/reinforce-squared",True
@Ahmed.Shaikh,2019-06-19T14:02:38Z,0,"Hey, can you do a tutorial on PyTorch? I am trying to learn it and your tutorials would be the best!",True
@haj5856,2019-06-18T23:43:11Z,0,next episode when? :),True
@girish7914,2019-06-18T14:06:48Z,0,"your text based version is already out there , waiting for videos because DQN has potential to solve lots of problems of humanity!!!!",True
@giancarlosanchez4171,2019-06-18T01:06:32Z,0,actor critic thoooo,True
@andreamassacci7942,2019-06-17T07:30:16Z,2,"Sentdex could you please have in program to use gym for financial application.   How to set up and env using a DataFrame and how to apply various RL algorithms to it.   There is not such a tutorial on the web yet, It would be really revolutionary. Even thought the result might not be of any interest, it would be very nice to have something to build on top or expand.  The cascade of RL is coming very fast...  I'm an Algo trader, being very much in out of the world profit %, but I would like to know what RL has to offer.   (I wrote you like two / three years ago for a job about solving poker LoL. I have being studying ML non stop for the last 1.5 year).  Got pretty good skill for anything that is LSTM for financial applications, but RL really is hard for me to grasp.   Do something about it. The world needs you, I need you. :D",True
@kks2105,2019-06-17T06:48:55Z,0,Excellent. Thank you so much for the tutorial. I am eagerly waiting for the next set of videos. Could you please let me know when can I expect tutorial on Deep Q Network?,True
@shreyashpandey7934,2019-06-16T13:31:34Z,0,"Sir, you asked y bgr . I think because opencv read image in bgr instead of rgb",True
@raheelsiddiqi8285,2019-06-16T08:04:57Z,0,Thanks for doing such a good work. Please make a detailed deep Q-Learning series.,True
@aadityarane3464,2019-06-14T21:51:06Z,0,Can u please make something about deep q learning where we use neural network model in q learning. I had already went through many articles and videos but still not able to understand it.,True
@girish7914,2019-06-14T15:09:14Z,1,"when the next tutorial is  coming ,because we r waiting eagerly !!",True
@mannycalavera121,2019-06-13T08:39:26Z,0,"Ffs your channel has to much damn information, going take me months to go through",True
@coolervik1994,2019-06-13T05:44:45Z,2,Umm.. I would appreciate if someone explains line 82. Why choose '-5'? I did watch the first video but I still don't seem to understand the intuition behind initializing the q_table with small values between 0 & 5,True
@urvishpatel6230,2019-06-12T10:50:15Z,1,Sentdex are you planning to make one more video on creating a gym environment and not just only a simple environment like the one in this video? Gym already provides some simple guide but it is not detailed and hard to understand. If you will make such video then it will be great for learners and who knows if we can create some PRs for OpenAI gym with some new environments.,True
@girish7914,2019-06-12T03:48:30Z,0,yes this is great series and we want see more from you. You are doing great work...,True
@mpricop,2019-06-11T11:29:22Z,0,OMG! This is great. Would love to see more videos like this. Keep up the good work.,True
@codekhor6263,2019-06-10T12:05:51Z,0,"Really like your videos man, very inspiring. I will start watching all of your videos after start my university in 2 month. Love from Bangladesh ❤❤❤",True
@nikhiljagtap6587,2019-06-10T07:17:04Z,0,Will you do tutorials for DQN as well?,True
@baruchba7503,2019-06-09T15:50:59Z,0,"I know you've used Sublime Text for a long time and very comfortable with it, but it doesn't suit my needs.  I'm using PyCharm and was wondering do you have any plans/desire to transition to PyCharm?",True
@Drugio24,2019-06-08T12:25:09Z,0,Can you check if the values of the cells around the wall have a high value? This could indicate if it learned to use the wall or not,True
@chrisray1567,2019-06-08T02:58:24Z,2,"The agent is kind of like a bishop since it can only move diagonally.  You should try letting the agent move like a knight, L-shaped moves.  It would require 8 actions instead of 4.",True
@thirdreplicator,2019-06-07T18:01:58Z,0,That was awesome. It would be nice to see how this compares to training a convnet or say a 4 layer fully connected network.,True
@yashdwivedi2037,2019-06-07T12:54:46Z,2,Bro...this is mindblowing that we can do this...Amazing as usual  -Your Huge Fan,True
@chrisherring8733,2019-06-07T11:26:46Z,21,"opencv uses BGR. You create an rgb image using PIL, but then you convert it to an np array and pass it to opencv. So opencv interprets that np array as BGR.",True
@tusharrajput8206,2019-06-07T10:50:29Z,0,Why don't you use Unity3d environment?,True
@ahmedhany5037,2019-06-07T09:01:24Z,0,Awesome tutorial ! I just have a really small question that I would be happy if you answer. Can we use the negative distance between the player and the blob as reward instead of using the -1 move penalty ? So that the player will minimize the distance to increase the reward till the  distance becomes zero and it gets a reward of negative 0 which is basically 0. So will this work or is it not the best way ?,True
@TheAppleCrisps,2019-06-07T07:06:17Z,0,"OpenCV uses BGR colour convention. Just use cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for conversion, or img[:, :, -1].",True
@tntarang3,2019-06-07T07:04:53Z,0,Sir can you please make a video on the current configuration required to run ML code smoothly in a laptop/desktop. Like a description video on the machine which can be used for programming.,True
@andreamassacci7942,2019-06-07T05:48:54Z,0,"That change in tone of voice you are doing lately is the Sh*t man...  Smart and funny, a rare combination...",True
@photovideoman8189,2019-06-07T03:37:39Z,2,"Saw this commented below, but it bears repeating... Pillow's images are stored in the order you specify (RGB in this case), but OpenCV displays assuming the order is BGR (no matter what you do). You could fix it with cv.cvtColor(img, cv.COLOR_RGB2BGR) if you cared enough, or you could have Pillow store the image in BGR format. Great tutorial! Cheers!",True
@gautamj7450,2019-06-07T01:20:15Z,0,Oh! Was really hoping the usage of Pygame for making the environment. ;),True
@JamesGaither11,2019-06-06T22:18:21Z,0,Blobjects.....amazing lol,True
@Smikay,2019-06-06T20:44:35Z,0,Know of any way to make those quadruple nested for loops look a bit better?,True
@metaliumtux,2019-06-06T20:39:41Z,4,"Dude, thank you so much!!! The most expected video I was waiting for. Hope, finally I'll be able to implement my thoughts in code.",True
@johnli264,2019-06-06T20:18:03Z,0,"OHOHOH,THANK YOU SOO MUCHH!! NOW I HAVE AN IDEA... ))))",True
@levrence8888,2019-06-06T19:43:59Z,0,dope,True
@RabeezRiaz,2019-06-06T19:13:56Z,1,44:55 line 161 The plt.plot function infers the x vector if you just give it a y so you can just have plt.plot(moving_avg ),True
@5-minknowledge188,2019-06-06T18:55:58Z,0,Can We create ChatBot using RL,True
@wi1h,2019-06-06T18:42:39Z,0,"when opencv was created, bgr was a popular format for cameras and in windows itself ( https://docs.microsoft.com/en-us/windows/desktop/gdi/colorref ). one of those things that for historical reasons, it stuck. unfortunately there isn't really a way to use rgb values natively with opencv, but if you want to convert the image you can use cv2.cvtcolor (https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor). the best you can do if you want non-native support is to create wrappers to convert back and forth",True
@indivarmishra6119,2019-06-06T18:28:14Z,0,Thanks a lot sir,True
@connerknox5645,2019-06-06T18:17:11Z,1,"Hi Harrison, pretty sure the image you're making is RGB but opencv assumes channels are BGR when you display it.  Seen it trip a ton of people up, but they've used BGR so long they aren't gonna change it.  Cool stuff though",True
@sirynka,2019-06-06T17:58:20Z,7,"why noone doesnt blame him because of that? [np.random.uniform(-5, 0) for i in range(4)]  its ugly. you coud simply use np.random.uniform(-5, 0, size = 4)",True
@Garrett_C,2019-06-06T17:56:40Z,23,GTA Q-Learning series? =D,True
@user-dq1nk3uo2t,2019-06-06T17:55:34Z,0,Thanks for part 4.  I'd like to see a part with numpy.,True
@saisritejakuppa9856,2019-06-06T17:12:06Z,86,Mr.sentdex this is why I follow you..what's makes you special from remaining ML blogs and youtubers is at a certain point you start creating everything from scratch... you r something like Jesus reborn for me..keep going...Thanks a lot..,True
@RounderSkillZ,2019-06-06T16:45:24Z,1,Can you make a couple videos about RL where you use VizDoom ??,True
@k4it4n,2019-06-06T16:34:10Z,1,It would be interesting to see what would happen if the food and enemy could learn as well,True
@alexanderpohl1949,2019-06-06T16:24:25Z,3,I am in the process of creating my own Environment aswell. Perfect Timing on the video,True
@igorespindola6759,2019-06-06T16:08:44Z,0,"Please. Make a serie about Server and Clients XML, WSDL, SOAP using Python 3.",True
@brandoncardillo5359,2019-06-06T15:55:39Z,0,Thank you great video! can you please show us how to make it using the gym library tho? <3,True
@liangyumin9405,2019-06-06T15:52:00Z,0,Great!,True
@elhamaryanpur,2019-06-06T15:27:56Z,12,"FINALLY! believe it or not, this is the first tutorial on custom version I found yet! YESSSS",True
@shmarvdogg69420,2019-06-06T15:16:24Z,0,"Oo, long video",True
@pythoning5284,2019-06-06T15:14:47Z,0,5th,True
@manan5,2019-06-06T15:13:48Z,1,Yup that's awesome kindly make one on convolution network also please,True
@shahzerbaig1397,2019-06-06T15:08:13Z,0,Second 😂😂😂😂🔥,True
@Stinosko,2019-06-04T17:45:46Z,3,Thank you for.making this awesome video! Very cool you did this for us 🥰,True
