author,updated_at,like_count,text,public
@mrfrozen97-despicable,2020-12-22T10:18:55Z,0,Yeah!!!! Compiled after maybe 22nd attempt Thanks as always))),True
@kerolesmonsef4179,2020-08-03T20:46:57Z,0,ok you explain very well but you have a problem with variables names,True
@andreww406,2019-07-19T07:47:29Z,0,please tell me why my w_counts has the lenght = 30???,True
@khalidal-reemi3361,2019-06-18T08:57:40Z,0,Enjoyment of Learning,True
@kashifinam135,2019-02-19T15:26:28Z,0,Can't understand the logic behind: features[index_value] += 1 Someone please explain,True
@tsaideepak8762,2019-02-03T19:54:40Z,0,my pickle file size is 13kb. -.-,True
@adityask277,2018-12-18T03:29:55Z,0,I got 500 MB of pickle file. Is it normal??,True
@heinzguderian9980,2018-12-14T04:01:32Z,2,The word_tokenize function will actually make distinctions between words that have capitalized/lowercase letters so  you were right to put lower() in. I just tried it.,True
@anilchaudhry804,2018-11-02T10:57:24Z,0,"Hey,sentdex make a video on indian coin classifer using convolution neural networks and keras",True
@nomesa7374,2018-10-18T13:49:44Z,0,"Hello, Thanks a lot for the presentations! They are very efficient and well explained! ;) Do you also have a video on how to preprocess Image datasets? That would be very beneficial too! Kind regards",True
@zahidulislam9812,2018-09-21T18:15:49Z,0,hey sentdex will you please make a video on speech recognition and chatbot manually,True
@alignedbyprinciple,2018-08-27T01:08:59Z,0,What design pattern do you recommend for seq2seq model?,True
@AndersWOlsen-we3nw,2018-07-20T10:17:57Z,2,"Lol, pro tip: in create_lexicon(pos,neg)...... Remember to return l2, not lexicon. I just spent an hour figuring out why my program crashed with MemoryError when I have 128gb ram on the server I use to execute this.",True
@confusedLittleShitter,2018-07-13T12:11:00Z,0,"Hi i love your tutorials, but im having a problem with the train_x = list(features[:,0][:-testing_size]) line, the error code says there are too many indices for array. I even tried copypasting the entire code, but it still didnt work.  Thanks!",True
@michiokaku101,2018-06-30T06:09:26Z,0,"is running the code, but doesnt print l2 and also the pickle is not created. why?",True
@unnatural3083,2018-06-28T13:50:19Z,1,Laptop gave up ðŸ˜‚,True
@gautamj7450,2018-04-29T12:02:02Z,0,I have my data (pos.txt and neg.txt) in a separate folder... How can I use them without copying it to the directory of my python script???  Any ideas???,True
@akshayjagadeesh5209,2018-04-16T02:32:35Z,0,Sentdex you have saved me man.,True
@ranickpatra6013,2018-03-26T19:05:58Z,1,"I'am getting error in line 60 said ""train_x = list(features[:,0][:-testing_size]) IndexError: too many indices for array""",True
@benjabenja1972,2018-03-14T22:52:17Z,2,'pos.txt' and 'neg.txt' file names are hardcoded in create_feature_sets_and_labels() . they should not be,True
@XxGummib3arxX,2018-03-13T22:16:03Z,0,"Getting this error:  File ""/Users/kevinlin/Documents/create_sentiment_featuresets.py"", line 16, in create_lexicon     contents=f.readlines()   File ""/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/encodings/ascii.py"", line 26, in decode     return codecs.ascii_decode(input, self.errors)[0] UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 4645: ordinal not in range(128)",True
@tedp9146,2018-02-27T19:48:32Z,0,"worked for me changing np.array to np.matrix, why ever",True
@arsalan2780,2018-02-18T16:58:13Z,0,why u didnt filter the stop words,True
@adamstrike1,2018-02-17T18:33:55Z,1,Should be called a 'some-hot array'. :P,True
@lowmax4431,2018-02-06T01:30:48Z,0,"File ""sentiment.py"", line 74, in <module>     train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')   File ""sentiment.py"", line 54, in create_feature_sets_and_labels     features += sample_handling('pos.txt', lexicon, [1,0]) TypeError: 'NoneType' object is not iterable  Help me please",True
@gesemgudinomejia7886,2018-01-24T17:25:58Z,0,"Thank you for all the help, could you do a tutorial like this but with images instead of text ? i know that you have the dogs vs cats tutorial, but that data set was already in form.",True
@MohitPratapSingh95,2018-01-03T14:39:39Z,0,why are we doing word.lower() in line 41.. because i think we hav already lowered the words in line 37?,True
@aadishgoel,2017-12-18T04:50:28Z,3,"When watching this tutorial it is pretty scary , But When you just start following side by side, everything just melts down. Thanks a lot. You're the best. I recommend your channel to everyone. (Python and Sentdex Deadly combo )  if you just change the rare condition to 10 instead of 50. pickle size will grow to 637 MB.",True
@ahmadfitri6035,2017-10-15T07:36:28Z,0,"i am trying to use the model created here for other application, which is to train my robot to predict the direction to move based on the input of 4 distance sensor.   compare to this application,  my       feed_dict {x: inputData[i], y: outputData[i]} , they are a 1-dimensional inputs instead of a batch .  however, i got this error:   File ""D:/ecte 451 thesis/neural network/tensorflow_neuralnet.py"", line 297, in <module>     train(x)    File ""D:/ecte 451 thesis/neural network/tensorflow_neuralnet.py"", line 278, in train     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))    File ""C:\Users\user-\AppData\Local\conda\conda\envs\omni35\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1558, in softmax_cross_entropy_with_logits     labels, logits)    File ""C:\Users\user-\AppData\Local\conda\conda\envs\omni35\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1512, in _ensure_xent_args     ""named arguments (labels=..., logits=..., ...)"" % name)  ValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",True
@Technovacca,2017-10-05T11:21:20Z,0,I'm getting (AttributeError: module 'create_sentiment_feauturesets' has no attribute '__path__') Any idea why?,True
@saichandramadduri8365,2017-09-27T06:53:19Z,1,how to use the trained neural network in another computer??,True
@chinmayphadnis9906,2017-09-22T13:09:18Z,1,"My code compilation does not show any error, but a pickle file is not generated anywhere, neither is the print statement executed. The first time it was running I accidentally closed the terminal. Is the problem related to it?",True
@renzbay123,2017-09-10T07:39:14Z,2,TypeError: lemmatize() missing 1 required positional argument: 'word',True
@dude2260,2017-08-26T08:19:50Z,0,"my pickle size us 580 mb and i only used hm_lines = 100000 , also in ur code i used  features += sample_handling(pos, lexicon, [1, 0]) features += sample_handling(neg, lexicon, [0, 1]) instead of 'pos.txt' ans 'neg.txt' is that wrong ??",True
@darianharrison4836,2017-08-19T20:05:04Z,0,thanks for all of the videos Bro !! you are tha Ultimate Teacher,True
@deepakmahapatra1185,2017-08-17T04:06:17Z,0,In the create lexicon function inside the for loop you can create the dictionary and update it to avoid using a large chunk of memory. As in your case lexicon is the list of all the words before filtering on the basis of frequency.,True
@chandra_83,2017-08-15T19:10:33Z,0,Hi Harry ! Can you prepare a tutorial on Tensor Flow Android?,True
@poornachandra1603,2017-07-23T06:22:24Z,0,"423 Traceback (most recent call last):   File ""create_sentiment_featuresets.py"", line 77, in <module>     train_x, train,y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt') ValueError: not enough values to unpack (expected 5, got 4)  Anybody with this error fix?",True
@revanttiwari4669,2017-07-22T18:48:15Z,0,"Sir, will it be wrong if we use *lexicon += all_words* insted of lexicon += list(all_words)  because according to the documentation(if i am not wrong) a string passed in word_tokenize converts the string in list format?? or maybe i am losing something please reply",True
@JordanShackelford,2017-07-17T11:54:54Z,0,"Cool I can use this. I thought of scraping job ads, weeding out the common words, and finding out what skills/programming languages are most sought after",True
@mohitchaturvedi3002,2017-06-23T04:12:56Z,0,"I was trying to pickle it rather than take import it from previous module  I got it in list successfully using :  objects = [] with (open(""sentiment_set.pickle"", ""rb"")) as openfile:     while True:         try:             objects.append(pickle.load(openfile))         except EOFError:             break  But not sure how can I unpack it into train_x, train_y , test_x,test_y     bcz its list of list i think ...did anyone tried that out ?",True
@ryancarr6491,2017-06-22T02:02:07Z,0,"note: python 2.7 on Mac osx with tensorflow installed inside virtualenv.  I was getting  """""" ValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...) """""" I had to roll back to previous version tensorflow==0.12. All good now",True
@KROKC007,2017-05-31T08:26:41Z,0,My code runs without any error but gives the result 0(zero) :( help plz?,True
@shitteljanjan,2017-05-31T06:31:46Z,0,"Hi sentdex, Thank you so much for your wonderful tutorials.  I have problems to feed own image data set to CNN using TensorFlow. I watched this video, but I don't know how can I load, label my images with TF. I am a beginner in Python and TensorFlow. Can you please guide me how I can do that??  Or can you tell me if there is any other tutorial that feeds own ""image"" data set to CNN with TensorFlow?? Thank you so much.",True
@leloss,2017-05-30T21:56:50Z,0,"Quick fix In 12:45,: pos = ""pos.txt"" and neg = ""neg.txt"". You should use the variables, not hard code their values. Thanks for the videos!",True
@user-jh4sc1vh4q,2017-05-29T11:41:15Z,0,"Hi, Thanks for the tutorial sentdex Could you please tell me which word to vector model are we following in this tutorial?",True
@matissos208,2017-05-26T13:23:22Z,0,"My code made a 1.3GB large file instead of couple of MB in python2.7. What could be the reason for this? Also when I do the next tutorial and run this pickle file i get ValueError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [423], [423,500].",True
@shikhirkalia6828,2017-05-20T03:39:13Z,0,Hey do you have tutorials on recommander systems using machine/deep learning? Good tutorial bdw,True
@bhargavpanth9954,2017-05-03T09:15:36Z,0,"Awesome series @sentdex :) I do have one question though. Wouldn't it make sense to have a list of stopwords and filter those out instead of giving a range? Although lists will slow down the performance since we're checking all the time. For datasets that are larger than the ones you've shown us, wouldn't a list of stopwords make more sense?",True
@CariagaXIII,2017-04-19T09:45:43Z,2,import random   and if you see the error Resource 'tokenizers/punkt/english.pickle' not found. import nltk nltk.download('all'),True
@V1nc3nt20,2017-04-18T15:01:29Z,0,So are you creating the lexicon based on all the samples you have? Shouldn't you just create a lexicon based on the training set and use this lexicon to build the feature vectors of your test set? I'm a bit confused here :s,True
@moritzmakowski9422,2017-04-14T14:28:46Z,0,Great series of videos!,True
@yaidontknowwhattoput,2017-04-12T03:13:44Z,0,This is fantastic. (positive sentiment),True
@ryanshrott9622,2017-03-21T15:37:03Z,0,"Hi Harrison. I don't understand the explanation you gave at 14:54 for shuffling the data. Why would those numbers be so big? Shouldn't the algo. always output either [1, 0] or [0,1]? I don't see anything written about it in the text-based version of the tutorial either, could you help me understand?",True
@halfbakedc00kie,2017-03-12T20:41:12Z,3,"training_x = list(dataset_training[:,0]) TypeError: list indices must be integers, not tuple  Oh dear... Any thoughts?",True
@lucmissoum5721,2017-03-04T22:38:08Z,0,"Can anyone explain to me why we are splitting the data by subsetting the features (features[:,0][:-testing_size]) and not the dataset? There's a concept I can't catch here.",True
@roecigreview,2017-02-26T18:39:48Z,2,"How can I optimize this process to not store all the data in the memory ? If you are processing around 1 million lines, all the arrays will be in the memory",True
@Aptpitfall,2017-02-26T03:29:45Z,0,"Great tutorial, I was wondering where I can find help with using custom image data (large data set). Thanks",True
@yashagrawal6850,2017-02-24T17:42:47Z,0,how much time does it require to complete ?  I mean after printing 423 it took almost 10 mins still the execution wasnt complete in the terminal as well as on interruption also pickle file made was 0f 290 mbs. Please do tell what is happening?,True
@abolfazlmeyarian7454,2017-02-22T20:14:45Z,0,i love your teaching .,True
@abolfazlmeyarian7454,2017-02-22T20:14:16Z,1,i love your teaching .,True
@hamza-325,2017-02-19T16:05:11Z,0,too much lower()s in your code ! LOL I mean you are lower()ing words that are already lowercase,True
@luckysaghani1100,2017-02-13T13:11:51Z,0,How Can I input my own txt to check the accuracy ?,True
@bf2825,2017-02-13T08:38:40Z,16,"For python 2.7 user, UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 2, my solution is import io  and then with io.open(pos, 'r', encoding='cp437') as f: repeat it for neg and sample",True
@zkowie,2017-01-20T16:13:47Z,0,"Excellent tutorials! As such a big part of the open source community, consider to get yourself a Sublime license ;-) If you were to put up some kind of donation system, I'd be more than happy to chip in to keep these tutorials going.",True
@yitingzou4478,2017-01-14T13:28:01Z,0,"Thanks for these videos. But in this part ,I can't get the final print. Because the scale of the ASCII,I don't know how to deal with this problem. What I use for programming is python2.7.",True
@PWOcarlos,2017-01-13T04:47:01Z,1,Thank you for these videos! You are awesome!  I ran into an issue I was hoping you can help me. UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 2: ordinal not in range(128)  https://github.com/510carlos/deep-neural-network/blob/master/sentiment.py,True
@laledakhokha,2017-01-09T05:04:54Z,0,Is there any resource out there how to build own  image dataset from google images  like cifar 10 fro running cnn in tensorflow?,True
@niteshkumarn55,2016-12-20T07:06:10Z,0,"hi there,  What is for loop with two vraibles do.... In the above example for fi in [pos,neg], how will this iterate? If pos is 0, 1, 2 and neg is 0, 1, 2 will it iterate like this 1> 0 0 , 1 1 , 2 2. or 2> 0 0, 0 1, 0 2, 1 0, 1 1, 1 2, 2 0, 2 1, 2 2. please explain what is zip and iteratetool as well.",True
@aaronshed,2016-12-18T02:35:41Z,0,"Thanks for the tutorials, they're great!",True
@ozanyilmaz2134,2016-12-08T14:03:58Z,8,Wouldn't it be safer to just remove stopwords with the help of nltk? I think some words that could be common could still represent some sentimental value for our classification.  Otherwise nice tutorial :),True
@AkshayAradhya,2016-12-04T15:56:45Z,0,Weird. My pickle file size is only 14 MB,True
@LatestTips101,2016-11-11T03:00:37Z,7,"Hey Sentdex, why do you use a np array for features, but then just make test_x, test_y, etc a regular list?",True
@adewolekayode6148,2016-10-31T11:49:31Z,1,I really enjoy your tutorials. God bless you.,True
@bismayan87,2016-10-19T06:49:17Z,0,"Thank you so much for the video. I find your video series incredibly helpful and this is one of the best tutorials on Tensorflow I have come across. Just a quick note- Theoretically speaking,  I am pretty sure ( feel free to correct me ) it is a bad practice to include your testing data when creating your lexicon. Technically the test data is supposed to be stuff that your algorithm hasn't seen so your dictionary should just not include any new words that only occur in the test set. I understand that this wont really matter because while training, any words not in the training set will not play a role in tuning our weights anyway. But you probably still should not do this , in case somebody pedantic comes along (this might also result in a small speedup in the actual count vectorization process).  I am sure in actual real world applications you would use something like the sklearn Countvectorizer anyway to do the vectorization, but it was still really cool to do all of this by hand and see nltk in action.",True
@ThatExBow,2016-09-29T23:23:37Z,1,"I'm using Python2 and have this error:  Traceback (most recent call last):   File ""create_sentiment_featuresets.py"", line 70, in <module>     train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')   File ""create_sentiment_featuresets.py"", line 50, in create_feature_sets_and_labels     lexicon = create_lexicon(pos,neg)   File ""create_sentiment_featuresets.py"", line 18, in create_lexicon     all_words = word_tokenize(l.lower())   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/__init__.py"", line 106, in word_tokenize     return [token for sent in sent_tokenize(text, language)   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/__init__.py"", line 91, in sent_tokenize     return tokenizer.tokenize(text)   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1226, in tokenize     return list(self.sentences_from_text(text, realign_boundaries))   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1274, in sentences_from_text     return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1265, in span_tokenize     return [(sl.start, sl.stop) for sl in slices]   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1304, in _realign_boundaries     for sl1, sl2 in _pair_iter(slices):   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 310, in _pair_iter     prev = next(it)   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1280, in _slices_from_text     if self.text_contains_sentbreak(context):   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1325, in text_contains_sentbreak     for t in self._annotate_tokens(self._tokenize_words(text)):   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1460, in _annotate_second_pass     for t1, t2 in _pair_iter(tokens):   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 310, in _pair_iter     prev = next(it)   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 577, in _annotate_first_pass     for aug_tok in tokens:   File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 542, in _tokenize_words     for line in plaintext.split('\n'): UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 2: ordinal not in range(128)",True
@MrAnujchopra,2016-09-26T10:38:53Z,2,See countvectorizer of sklearn.,True
@danielravina130,2016-09-25T23:14:21Z,1,"how would I go about doing a multilabel classification? in your example, pos is [1,0] and neg is [0,1]. how would it look like with three or four labels?",True
@radixvinni,2016-09-06T05:27:18Z,3,"You are lemmatizing wrong. You do: lexicon = [lemmatizer.lemmatize(i) for i in lexicon]  and that works only for nouns. You should do something like: lexicon = [lemmatizer.lemmatize(i, pos[0].lower()) for i,pos in nltk.pos_tag(lexicon)]  test: lemmatizer.lemmatize('went') #=> went lemmatizer.lemmatize('went','v') #=> go",True
@1991niks,2016-08-31T14:50:01Z,0,I got not in range(128) whats that??,True
@stefanoberleitner1489,2016-08-25T09:34:44Z,1,"I get a UnicodeDecodeError when I run the code. Does anyone have a solution for this? Btw thanks for the great tutorials, sentdex!",True
@migano2023,2016-08-23T16:04:27Z,28,Thank you for all the effort you put into this!,True
@ridhojunaidi7829,2016-08-23T13:48:01Z,0,Share Python please :),True
