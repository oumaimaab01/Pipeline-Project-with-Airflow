author,updated_at,like_count,text,public
@zoahmed8923,2023-12-28T08:32:39Z,0,Love your videos! What do you use to keep track of papers? You mentioned you have a tool that summarises research papers for you,True
@ander300,2023-09-16T17:47:11Z,0,"Part 10 of Neural Net from Scratch, about analytical derivatives??? Please bring the series back!",True
@akam9919,2023-09-04T22:22:01Z,0,"I solved it.  What we need is to do, is make another AI that simplifies all the context info and then makes a TikTok-style video for the actual model to process and use in generating an actually good answer.",True
@keco185,2023-08-15T12:02:14Z,1,Context almost needs to be stored in a tree architecture instead of a single 1-D line,True
@sandraviknander7898,2023-08-08T16:48:30Z,0,I kind of got big bird flashbacks reading this paper.,True
@notfaang4702,2023-08-01T07:22:15Z,0,Thanks for video and explaining why it's difficult to scale context size,True
@dawre3124,2023-07-28T23:48:42Z,0,"Half way through this video and I feel like I'm watching a healtjy gamer video on how attention and adhd works, not an video about ai   I think with massivly improved hardware the only solution is to have something like memory and information source for the ai to work wirh (I guess something like the paper said, but I didn't get it since I'm not a sience guy). Like a human solving a problem the ai needs to work with the data to brake down the task into chunks it can hold in memory. Split that beginnimg and end into many more beginnings and ends like a human working on a todo list involving many research-, understanding- and execution- steps. For this to work the process would need to move away from running of memory alone to memory+source aswell as creating a specialised checkpoint of that model just for that task",True
@opusdei1151,2023-07-27T09:53:58Z,0,"Do you think that liquid neural networks is a marketing move. It seems to be so amazing, but there is almost no github repositories on it. There are some paper here and there. But if its so revolutionizing why not everybody jumping on it?",True
@opusdei1151,2023-07-27T09:50:44Z,0,"This is a good video, thank you very much.",True
@goldenbananas1389,2023-07-24T15:47:37Z,1,are we going be getting anymore videos on the neural networks from scratch series?,True
@gandoffboss197,2023-07-20T12:46:22Z,1,"These issues remind me of problems from Operating Systems design. Maybe a few concepts from OS Design might be thought provoking. Swap space is an obvious memory management technique that might be useful for the limited RAM but when a need for larger amounts of memory exists. In the vain of how long does it run, maybe thinking about how OS design uses context switching could be useful. Just throwing out some food for thought. Got to get those creative juices flowing!",True
@CitizenWarwick,2023-07-17T19:54:34Z,0,"There is a gpt 4 32k model, Claude has 100k, larger context is coming!",True
@cw9249,2023-07-17T11:38:10Z,0,Have you seen the new paper about long convolutions and toeplitz matrices? I didnâ€™t quite get the toeplitz matrix thing but it sounded interesting,True
@MKBergins,2023-07-17T10:46:42Z,0,"Love your content, and got your Machine Learning PDF - awesome stuff good sir. Do you have any local LLM recommendation to help with programming, or a cost-effective way of doing it via something like ChatGPT?",True
@CarlosWong54,2023-07-16T15:31:51Z,0,Thanks for the video! Your finals comments about compressing information reminded me of this talk: https://www.youtube.com/live/dO4TPJkeaaU?feature=share,True
@isajoha9962,2023-07-16T13:34:16Z,1,"Hmm, bottleneck of AGI? ðŸ¤”",True
@_XoR_,2023-07-16T11:25:57Z,0,"I really think we need to emulate attention at the hardware level. And by this I don't mean an accelerator that operates at the instruction level, but at the architecture level. I don't think there is any other workaround and what I don't understand is why bigger companies haven't invested in the development of this sooner..",True
@davidpan5471,2023-07-16T10:20:15Z,0,Train a Neural Network that Train a Neural Network that can tell a LLM where in the context should it pay more attention XDDD,True
@hewhointheearthlydomainsee1272,2023-07-16T08:14:29Z,0,"It will possibly be human solution. A group of people read a million tokens of text and the ones with the best comprehension and fastest times could be queried about their technique. I think the Wheel of Time is a good example to try with, with 4.4 million words. The great dictionaries are another with up 60 million words, but humans could never read it all, apparently.",True
@crackwitz,2023-07-14T09:28:14Z,0,Give it the document to read TWICE. Or give it a table of contents first.,True
@siddharthaudayakumar9444,2023-07-14T08:26:59Z,0,Im unable to find the code interpreter in my gpt 4 im from India why is this issue,True
@MaxGuides,2023-07-13T23:26:46Z,0,Each segment has its own middle in dilated attention.  Just a way of knowing which attention to reference as far as Iâ€™m aware.,True
@smicha15,2023-07-13T21:26:45Z,0,"You can upload scientific documents to code interpreter. The document size limit is 100MB. I downloaded a whole book into it, and it was bale to answer questions for me.",True
@lucasa8710,2023-07-13T10:35:06Z,0,"Well, you can always introduce another model to summarize the entire context window into 8k-ish tokens for the primary model",True
@lincolt,2023-07-13T08:17:45Z,0,Looks like an issue that was with the image data in before the convolutions arrived.,True
@CharlesVanNoland,2023-07-13T06:03:27Z,0,"The biggest problem facing modern ""AI"" is the fact that everything relies on backpropagation. Also, has anyone considered using smaller tokens, like pairs of characters - similar to what simhashing does. It seems like you could have a lot less tokens which are able to represent a lot more words. You'd need some kind of hierarchical token representation and it might end up being the same size as just tokenizing words.",True
@TiagoTiagoT,2023-07-13T04:05:12Z,0,"Could something sorta like a ""mipmap"" of the context, with varying levels of ""convolution"" (ideally some sort of semantic compression if that's a possibility), combined with streaming from disk to read individual details at full resolution when needed, perhaps something sorta analog to Unreal Engine 5's Nanite, perhaps be a possibility?",True
@mordokai597,2023-07-13T02:25:32Z,0,"wow, youre REAL far outta sync... almost every popular model has has had SuperHot merge and runs at 8k, numerous models coming out now are 8k base ,16k high context high parameter models like Falcon 40B can run in ~12gb of vram if you do all the extra steps.... the Landmark models have a context of 75k.... the truncate_prompt slider in textgen goes up to 16k for a reason now...",True
@vikranthkanumuru8900,2023-07-13T02:00:19Z,0,"To all the papers mentioned, can we have the links",True
@jannikmeissner,2023-07-12T22:23:06Z,1,"I agree with a lot here; I spent a lot of my working time in the past four years on researching extending the context window. In the end, our solution (I am co-founder at a startup called Neuralfinity) is indeed a redesigned attention mechanism. I sadly can't reveal how we did it, but we will release a paper end of the year/ beginning of next, when our next generation LLM is ready.",True
@labeardod,2023-07-12T20:57:43Z,1,"Had no idea about the U-shaped attention problem, but I've definitely come across it. That valley is where GPT's hallucinations live, and thrive.",True
@ChaseFreedomMusician,2023-07-12T20:31:30Z,0,"I think longnet should actually do better with this middle out problem (silicone valley) Because it's not just doing the additional computations in parallel, it's also the layering they show a pretty interesting mathematical proof that the layers required for 100% coverage are logarithmic. But I think the more interesting part is that the attention heads themselves can attend to different segments of the graph independently which should actually solve that middle problem.",True
@siddharthkhodke4219,2023-07-12T20:19:52Z,0,Why did you stop neural network series?!!!ðŸ˜¢ðŸ˜­ðŸ˜–,True
@Linck192,2023-07-12T20:04:15Z,0,Splitting attention into segments doesn't make much sense to me. What if in the second segment you needed the context from the first segment to comprehend it?,True
@pisoiorfan,2023-07-12T17:56:48Z,0,"Yup, this is a problem. I think a good attempt is to do what we humans do: incrementally drop irrelevant (=not worth attention) tokens. If you split a 2k span window in a series of 8x256 token segments, feeding in each segment 1/2 of tokens coming out of the previous segment, the ""virtual"" attention span expands to 256 + 512 + 1024 ... =~ 64k tokens.",True
@Rivali0us,2023-07-12T17:54:19Z,1,"Attention scales quadratically, not exponentially. Other than that, great video!",True
@thisisnotramansmusic1045,2023-07-12T17:48:17Z,0,Smaller models hyper-tuned to specific tasks might actually solve this problem.,True
@erfanzarechavoshi909,2023-07-12T16:17:13Z,0,i think multy query works fine if you trying larger ctx but yes the current attention needs to change,True
@fpsmeter,2023-07-12T13:46:52Z,0,"Wait, so there's O(N^2) complexity when those models process text prompts? Why is so much hype about chat GPT4 but nobody talks about this fact? It's a huge constraint, seriously limiting the capatibilities and possible use cases.",True
@accountnumber1234567,2023-07-12T13:17:47Z,1,"Computational cost is quadratic in sequence length, not exponential.",True
@Embassy_of_Jupiter,2023-07-12T12:04:41Z,1,"I think you might need to develop a model that has a separate prompt and a separate context. Like a mic of this diluted attention and retrieved models. Then the model doesn't necessarily need to condense the entire context, but just learn to condense the relevant parts of the context. Maybe that's easier to train. I could imagine that the prompt easily gets lost in diluted attention.  But perhaps it can learn to emulate such a structure automatically, so who knows.",True
@go_better,2023-07-12T11:24:49Z,0,"Dude, is it correct to assume that a newbie is desperately behind these days? Is there any hope to learn all these things?",True
@thorchristoffersen6424,2023-07-12T11:08:57Z,0,Do you have any sources/links to further research the topic of attention's U shaped graph?,True
@NevelWong,2023-07-12T11:03:18Z,0,"I think the easiest way to improve the context length situation for now would be a compression of the input tokens. Eg to solve most math problems, you will NOT need the prose or syntax related information of the question. That's just baggage to the AI. So ideally we could design a maximally dense language, which purely contains factual statements, no fluff words, no articles, no superfluous prepositions, etc. We could then convert a user's input into this intermediate language, process and generate output in that intermediate language, and then convert it back to English. Sure, it would sound dry and boring, but we don't need author level prose in say our meta analysis of papers. This way we could easily double our context length, and likely improve accuracy along the way.",True
@lostpianist,2023-07-12T10:19:27Z,0,"What I'm realising the last few months is that there is ultimately only so much you can do with LLMs. They are very useful and will become even more useful, but in isolation they will always have some limitations. In future (or indeed already) we will have networks of LLMs that work together and networks of LLMs that decide which LLM to call. The human brain works with symbols, even at the deepest levels of meaning, emotion, its all symbolic representation of information. Look at savantism/savants. It's almost like they are less finely and/or more finely tuned tuned LLMs. Interesting times...",True
@freedom_aint_free,2023-07-12T09:44:57Z,1,"I seems to me that it would be surprising if it was not like that: Since the very inception of AI field, (e.g. Rosenblatt's perceptron) the system have been modeled after human nervous system, and have been trained in human generated data, it seems pretty natural that the system at least in a high level view, would display human psychology like phenomena.",True
@sanjaymarison5629,2023-07-12T09:42:53Z,0,your audio volume is a little low,True
@8eck,2023-07-12T09:16:16Z,0,"Exactly, i also think that gathering some useful information after all those distributed attention mechanisms is kinda hard or impossible. How model will know which attentions were the most important... I think generalize it will be super hard. Possibly if there would be some better pre-processing and possibly even some models before this big model, which would separate semantics of the input and distribute by semantic. Then delegate input by semantic into a specific attention responsible for that semantic, then that would possibly lead to some generalization of the model in the end.",True
@8eck,2023-07-12T09:01:25Z,0,Can't models use distributed GPU for inference? I thought that this is already implemented in some frameworks...,True
@8eck,2023-07-12T08:49:39Z,0,"Yeah, i think that those summarization techniques are not a real use case for something like code or something that is sensitive to data loss.",True
@TheAero,2023-07-12T08:45:13Z,0,"We are starting to reach a peak in performance. The  differences will  start to be 1% - 2% per year moving forwards, till entirely something new comes. Maybe fusion models and transformer mix.. New datasets, more data, better compute units, deeper models, larger models. Thats gonna be the game, till the technology saturates.",True
@minimal3734,2023-07-12T08:20:39Z,1,"Obviously bits of information have to be dropped to fit the data into sparser representations. The dropped data might be crucial for the understanding of the whole context. I wonder if the model will be able to direct the attention to the ""ground level"" when necessary, to obtain and process all relevant details.",True
@Kram1032,2023-07-12T06:48:46Z,15,"I think something that *might* work is if you took a Mixture Of Experts approach, where each expert has a different attention dilation map. Probably not ideal for computer architecture (which wants powers of 2) but at least in principle, it might make sense to choose each expert with a dilation factor that's a prime number, so you get nice phase coverage across a wide area of tokens.  Of course that also means you need more memory for each such expert.  But if you have like 8k Tokens for each expert, where one sees every single one of the first 8k tokens, one sees the first 4k and then 2k worth of every second token and 1k worth of every fourth and so on, and another agent that dilates in steps of three, and five, and seven - you probably get a fairly dense coverage even at fairly high numbers.  Or alternatively, you could just stick to powers of 2 but add a ""phase shift"" between experts so they see ""the other half"" or ""the other three quarters"" etc.",True
@BinarySplit,2023-07-12T06:30:03Z,0,"LongNet's evaluation section is terribly lacking. Extraordinary claims require extraordinary proof, but they didn't even give an inkling of an indication that it could scale even up to SOTA context sizes (84k for MPT, 100k for Claude), let alone up to 1B. That paper is just clickbait written in LaTeX. Please don't contribute to its spread.",True
@aidantilgner,2023-07-12T06:28:49Z,4,"Might be a dumb question, but could we use an LLM to recursively summarize the conversation context over the course of the conversation, and use that summary as the context for a given prompt? Basically just as the conversation progresses, a background process would create a summarized, and therefore a sort of lossy compressed version of the conversation. Obviously might not be the most efficient but maybe a cool idea.",True
@DevWSJ,2023-07-12T06:26:37Z,1,Are you going to continue the neural network from scratch series ? :(,True
@phobosmoon4643,2023-07-12T05:43:42Z,0,"what i envision is large foundation models spinning up volatile sub-llms, generating a training regimen and abstracted fitness function as well as a goal and a directive to execute p amount of system power and t amount of time on RLHF (not human, but you know) and to return the results of those fine-tuned models.",True
@hanskraut2018,2023-07-12T05:24:00Z,0,"Very smart very important indeed, here are some leads how to do it -*smart forgetting*: (GPT4 seems to have no controll over forgetting it can even tell what info is more important but loses it even when empty text is added if the important info is on the edge of its old token context window. Forgetting to the least importat tokens should theoretically lead to a density of relevant and important tokens to increase in effect i creasing the relevant token context lenght, freezing the use of untelated tokens aka reducibg the weight depending on task also could help 2: Sorting and compressing to different degrees of data loss. For rerrad/regaining of context based on a multitude of sorted context memorys in different ways for different purposes (ad RL on top and u have a selfimprovibg system a mix of hardcode and lerning can i crease stability as well as the ability to choose what to use based on selfobservation 3: Dynmaic weighing of token importance by attention depending on guesses pf importance and muslitble systems that do that by different metrics and methods and metasystems that choode the % of each method depending on result and past result experience (small decider lerning networks) 4: (simple diy) just have multible modeÃ¶s that each save some context and the. Reconstruct by talking to each other (hey do you know about x? Did the user talk about that?) Mayve a finetuned â€žmemorize factsâ€œ network a â€žmemorize Xâ€œ network. 5) layered categorisation with zooming in on info continually being sprted. etc. Depends on the usecase understanding the model and what bottleneck is likeÃ¶ not to change soon or payed too little attention to then should helps in deciding where you can add value. Bonus: delfreminders or reminders baded on context might be able to repromt thibgs ouside of context window the LLM could use it as a â€žunversifyed pluginâ€œ inside of chatGPT for example, weaviate is trying to fevelop such a plugin which is in alpha right now maybe they value contributors since there method in isolation could use help from creative systems in symbiosis that compliment each i other i thi k personally guessing ad to what is under there hood",True
@HoriaCristescu,2023-07-12T04:29:44Z,1,"it's quadratic (n^2), not exponential (a^n)",True
@vene,2023-07-12T04:23:38Z,77,"The parallels to human cognition are really interesting. The ""lost in the middle problem"" is very much a feature of human memory - you always remember the beginning and end of a sequence the best (e.g the beginning of the book you're reading and the part you've read last, but things in the middle are fuzzier).",True
@FuZZbaLLbee,2023-07-12T04:22:42Z,0,"Instead of taking every Nth word, maybe some way of only focusing on meaningful words could help  The above would become:  â€œ instead every Nth focus meaningful â€œ  Although that is still 5 tokens",True
@scottmiller2591,2023-07-12T04:18:25Z,0,"""I dilate down to the 1st and last token, so I can do 10^10000 tokens now; it just takes longer than the heat death of the universe to read them in.""  Is this really useful?",True
@FREELEARNING,2023-07-12T04:17:29Z,2,Maybe Larger Context is all we need for Even Better (LLMs). I was thinking that maybe integrating the RNN layer withing the Transformer architecture could help in achieving this. For example if the input is split into 8k chunks and each one passes the first layer of the attention then the output is concatenated and passed through the RNN then doing this again and again until the end where everything is passed to the dense layer. So in this case we have the performance of the full attention for each chunk and we have the performance of the RNN in processing the verly long output representation. What do you think?,True
@scottmiller2591,2023-07-12T04:12:31Z,0,"If LongNet were being honest, they'd use a log y scale.",True
@steve_jabz,2023-07-12T03:47:09Z,0,Recurrent Memory Transformers and RWKV got up to 1 million tokens. Magic LTM-1 manages 5 million. They had some pretty interesting optimizations for getting around some of these problems too,True
@KodandocomFaria,2023-07-12T03:02:52Z,0,"I don't know if it is already possible but I think it is time to start using quantum computing for those kinds of things.  Another alternative is to maybe use different architectures like RMT(recurrent memory transformers - paper propose 1M Tokens), or gnn (maybe can be better but will also consume a lot of resources), longNet (1 billion tokens). But independent of architecture, i notice most models are not well optimized to use gpu, I saw many models with the same amount of params but with different memory usage. So I believe for starting there are 3 options that can help better:  1 - Improve model to better resource utilization 2 - maybe migrate to another language faster and that uses less resources like c++ or rust or even go. 3 - to not be necessary to migrate to another language, community could go together and help to improve python performance.",True
@TheManinBlack9054,2023-07-12T02:53:35Z,0,What about Claude 100k?,True
@sgramstrup,2023-07-12T01:18:57Z,1,"They need attention on the level of sentences and sections in a text. It's ridiculous that the whole context is prioritized using only token attention. If we have attention on several layers, we no longer need a big context and could even reduce context size to < 1K for speedier inference. Longer context is NOT the answer.",True
@JJBoi8708,2023-07-12T01:06:17Z,0,Try Claude 2.0 with 100k context,True
@calcs001,2023-07-12T00:53:04Z,0,OpenAI has a gpt-4 32K model.,True
@Jay-kb7if,2023-07-12T00:30:49Z,1,"to add to this, and I hope you read this because I think about this as much as you do; Hinton is big on negative data, and cross entropy is also not just looking at what is high in attention but gets the confidence for low attention. IF they do not assess what has low attention because they simply do not bother to evaluate all tokens in a context, then it's not going to appropraitely strtify tokens within a context.",True
@JohnVandivier,2023-07-12T00:29:56Z,0,Easy fix: middle-out compression,True
@Jay-kb7if,2023-07-12T00:16:26Z,1,"all the research is crappy trying to do 1 million context length, it just removes so much variability and sparsly evaluates tokens within a context or not at all.",True
@Jackson_Zheng,2023-07-11T23:56:22Z,1,"They also used post-norm instead of pre-norm for the attention which is the same implementation as the original transformer architecture design, but not what state of the art gpt's use (which is pre-norm). This can affect performance since post norm models will need to be trained for longer than pre-norm models before they reach similar accuracy. Because they didn't reveal the exact time they trained the models for, this may not be quite reflective of real world use.",True
@MouldySoul,2023-07-11T23:48:29Z,0,"Probably has to be the first video where I'm not even slightly annoyed by the ad at the end. Neural nets from scratch for the win, I'll definitely have a dig there thank you!!",True
@MakeDataUseful,2023-07-11T23:37:21Z,10,ChatGPT blows my mind and infuriates at the same time when it spits out completely whack responses,True
@talis1063,2023-07-11T23:27:45Z,31,"My intuition is that LLMs need to be stateful. That might allow them to pick out relevant information from the input and compress it to their internal representation. Trying to fight the O(N^2) curve for both training and inference isn't gonna lead to much progress. That state could be separable from the core LLM just like the prompt, but the LLM needs to be able to manage it. Kind of like memory module that you'd pass along with the prompt, but unlike the prompt it isn't converted back to tokens and LLM modifies it. Much closer to how humans are allowed to process entire books worth of data 'at once'. First internalize and then query. Training something like this would be really hard though.",True
@jahcane3711,2023-07-11T23:18:56Z,0,The perceiver model is not a potentially viable solution?,True
@NickWindham,2023-07-11T23:13:52Z,0,Claude is 100k tokens already,True
@ronnyrockel5293,2023-07-11T23:12:34Z,0,Funny you are making this video when Claude 2 is released with 100k context ^^,True
@ramzes2241,2023-07-11T23:11:15Z,0,"Have you checkout out  Claude by Anthropic ? they claim to have 100k tokens and by playing with it, it seems smart, gpt3.5 smart, or even somewhere between 3.5 and 4. It's also pretty quick. It's worth checking out.",True
@nathank5140,2023-07-11T23:10:31Z,0,Am I missing something. The perplexity score goes down with increasing context size when the batch size is 16â€¦ if it continues to go down for larger contexts doesnâ€™t that give us very large context windows without performance drop off? 12:39,True
@CitizensCommunity,2023-07-11T22:58:39Z,2,"It is like working with a person who is not thinking very hard but is very smart, asking them about detail can result in small errors like numbers or just be wrong if they put little thought into it. So you need to ask it to consider the answer it is giving. We do a lot on auto pilot from system one that is similar to chat, so we should be able to give it larger amounts of context if we reduce the detail except on what you want it to do, and force it to give it the consideration needed on what we are looking for.",True
@4crafters597,2023-07-11T22:33:41Z,0,"Oh, i thought this was ADHD tips, sad",True
@jaimejaime9800,2023-07-11T22:32:44Z,1,"Nice informative summary! I've been doing lately structured data mining from chemistry papers with llms, and I am not unhappy with the map-reduce hackery with openai 4k chatgpt. In fact I tried to feed the full paper with the 16k models, and the results were far worse. I found a sweet spot of the chunk I fed to the model to get the best extraction around 3k. Some recurrent hybridation and a differentiably trained retrieval index to automate all this hackery of the map reduce and the next neighbour embeddings, looks like the low hanging fruit of improvement for me",True
@EdanMeyer,2023-07-11T22:10:42Z,29,"I donâ€™t think larger context lengths is necessarily what people want. People want to process longer sequences, and there are other ways to do that, namely memory. Current memory methods leave a lot to be desired, but they are linear in time, and humans manage to do it.",True
@diadetediotedio6918,2023-07-11T21:56:02Z,0,How does Claude work with the 100k context window?,True
@Veptis,2023-07-11T21:55:03Z,2,My second comment was about the overall architecture of the whole model. Do we need the full width of the context length all the way up? Or can you simply make higher layers narrower. Somewhat like a pyramid scheme? The one output that matters is either a single CLS token in the front or a probability of next token near the end. maybe you just have small transformers and then chain them with LSTMs or something.,True
@TTTrouble,2023-07-11T21:53:21Z,1,"I had the exact same question/concern about longnet. A youtuber named Gabriel Mongaras made a really good video reviewing the paper and in the comments I asked him about how Longnet gets away with just arbitrarily not calculating every n-th attention score and he gives a really good explanation. It still feels like it shouldn't work, but I guess if the attention matrix is inherently composed of a lot of redundant data, then I think the idea is during training, it's still able to calculate weights in a way to more efficiently transmit that data through the layer hops of one token to the next.  I don't think youtube links work in comments, but you can just google Longnet and his video is one of the first that come up.  https://www.youtube.com/watch?v=0kdETHUhjAA&t=1542s",True
@Veptis,2023-07-11T21:52:43Z,0,"I lost my previous comment, so I will split it up.   I am working on a code generation evaluation benchmark that will support multiple tasks. And a difficult decision for me is what to allow as model context. And also do I write a variant that works for instruction finetuned models...",True
@michaelbasher,2023-07-11T21:33:01Z,0,Top bloke lol,True
@CapsAdmin,2023-07-11T21:30:07Z,1,"It's kinda crazy that to produce one token, it must pay attention to all of its previous context. If you need to compress information we might as well do finetuning with the context?",True
@CEOofTheHood,2023-07-11T21:29:30Z,17,I will never forget the abandoned neural network from scratch project. Some of the best content on this channel but never finished.,True
@adempc,2023-07-11T21:28:11Z,1,"I'm not gonna pretend to understand any of this. But it sounds like we are pushing up against the limits of processing information without it being compressed first.. is that right?  I know we aren't computers, and computers aren't us - but we have various levels at which we process information, top level all the way down to the unconscious level.  Are we missing the equivalent with our current tools?",True
@JazevoAudiosurf,2023-07-11T21:23:58Z,0,"I'm thinking if pretraining is long term memory, if you could store all the information of a data set in the weights, and had a perfect memory, it would not be necessary to have long context. instead you would just ""fine tune"" the pretrained model with your 100 page document from your prompt and it would perfectly know the document. in other words, if we would overfit the model perfectly during training, and every prompt would be a perfectly overfitted fine tuning, it would solve the problem of short term memory. the trade-off would then be its reasoning abilities because of overfitting. but if you have vast amounts of data, that could potentially be solved. perhaps this solution would require more than double precision weights. I think it is possible with enough data and compute, without altering transformers, to solve AGI. it probably won't happen this way, but it shows that there are many ways to reach it",True
@MenkoDany,2023-07-11T21:14:15Z,0,"How about binary tree attn? Like don't do it every nth token, do it halfway through each segment, and fractal your way down. Either way, pardon my sleepy bactery infection sick brain, I think this is one of those fundamental laws of the universe. It's alway been cheaper to gather facts, than to find relationships between the facts. The number of possible connections scales with dimensions, which are not always orthogonal to each other and don't have many correlations. While the number of real connections is <<< smaller. AI is just compression, and while current LLMs brought us closer to the kholmogorov complexity limit, I don't think there are easy wins here. Either the compression will be even more lossy (maybe we can ""focus"" on the important parts, have less compression there? I think in a way, that's what ""expert models"" are today), or we will need wins in computation we have not seen since the invention of the transistor. Or maybe quantum/hyper computing somehow finds a sneaky way to compute what we need :D",True
@YEASTY_COMMIE,2023-07-11T21:06:17Z,1,"I had this simple idea a while ago to improve attention, just take a normal transformer, with like a relatively small context, and apply it to your whole large context like you would with a convolution filter in a CNN, and either by changing the stride or with max pooling or something, reduce the size of your input context. Do that over multiple layers, and you can in theory compress your context, divide its size by two or four at every step, until it fits in that 2048 window. I wonder if something like this has been tried",True
@countofst.germain6417,2023-07-11T21:05:23Z,0,He said the thing!,True
@yorailevi6747,2023-07-11T21:04:12Z,2,"Is it so complicated the make attention iterative though? Like how humans do, they're aware that something exists not specifically with all the detail and if needed the parse it again with higher level of detail. It's really not that complicated if you make the system dynamic.  But then ofc it's rnn's all over again",True
@adi331,2023-07-11T20:58:11Z,0,"Haven't read the research paper regarding remembering information in the middle . But could it be that the stuff in the middle is a lot of ""filler"" information and therefore not worth remebering ?  Is it just an inherent property of text that the stuff in the middle is less important than the beginning and end ? Not sure",True
@serta5727,2023-07-11T20:57:55Z,0,Very interesting development,True
@serta5727,2023-07-11T20:57:42Z,1,I am happy that as you explained your thoughts on the new attention mechanism they were similar to my thoughts. So I feel reassured that my understanding of it is not total nonsense.,True
@JebBradwell,2023-07-11T20:57:09Z,0,FoT focused transformer has shown better training with larger context length by using positive and negative examples to help with this issue. Check it out and let me know what you think.,True
@IronCandyNotes,2023-07-11T20:56:55Z,1,Damn... if only I paid attention to what the video was about prply something awesome with python.,True
@kevinmaillet8017,2023-07-11T20:54:56Z,43,"Whatâ€™s interesting is that if you go for an interview they say either be the first or the last want to interview. By doing that you were going to be remembered the best itâ€™s weird that this curve happens at the beginning and the end of the context, it makes you wonder how close we are to real human thought",True
@MaJetiGizzle,2023-07-11T20:51:56Z,4,"Okay, I just finished the video and these are my thoughts.   Yeah, the O(n^2) nature of attention in transformers is really whatâ€™s holding the tech back at this point. If we could somehow get that into even a linear complexity that would open up so many doors of possibilities for LLMs, context length, etc.  I see a lot of people trading space in the form of vector db embeddings as a way to offset the problem without completely addressing it, which works to some extent for long term use cases, but ultimately doesnâ€™t make the problem go away. At the end of the day weâ€™re all essentially needing to chunk things at some level of the LLM pipeline.  Ultimately, I do think a breakthrough with the architecture is possible, especially if we go down the route of trying to scale these models horizontally instead of vertically with techniques like MoE from OpenAI.  I think once we get to the point where we have tiny LLMs automated together with a Kubernetes-like software for managing tiny neural networks weâ€™ll be in better shape.",True
@Bencurlis,2023-07-11T20:49:17Z,6,"I think we should just ditch Attention as the main information processing feature entirely. Attention will always require to have all tokens available in memory, so the memory required will always scale linearly with the context size, even if we bring down the time complexity of Attention to O(n) (and that will always imply missing some pairwise token relations or simply some of the tokens). A smarter replacement would be to use Attention with a smaller window, but let the model ""place"" the window anywhere it wants in the context, as needed, and the model will only need this subset of tokens in memory. Of course this would require to get back to RNNs in order to let the model update the location of the Attention window in the context, and that would increase computation times quite a bit.",True
@DeepTylerDurden,2023-07-11T20:46:05Z,26,"I really don't like RNN's, but my intuition says that to solve the context problems we will need to go back to some sort of RNN. It just looks crazy to me that we are feeding those models entire books in one shot and expecting them to be able to answer all questions about it with 100% accuracy.",True
@wktodd,2023-07-11T20:44:58Z,3,"Maybe the answer is to tokenize a whole concept . Ie when I listen to you , I'm not storing every word in my head, I'm filtering for facts and context to form a concept of what you are talking about.  So,  once you have defined the subject, I store that as a concept and recall it when necessary, not the long waffle getting there. If that whole  waffle can be condensed to a single token , you have a vast space opening up.   E.G I only have to say 'Lap-time' for you to be triggered into racing car mode . Am I right? 8â -â )",True
@bigphab7205,2023-07-11T20:42:52Z,0,"Why would longnet go public if it didn't address those points? Does the sagging attention curve have anything to do with the data? More specifically, what is it empirically related to? If it's the model itself and the calculations that's one thing if it's simply a product of the data and the format that's different. One thing I have noticed is that the ""good"" data all has a common theme/format. It seems very likely to me that the curve was a learned shortcut. I'm even more convinced of this by the simple inclusion of RLHF. There is a very specific way most people choose to communicate, especially in writing, and that curve that you mentioned matches it perfectly.  But that is not how educational books or scientific papers are written.",True
@adempc,2023-07-11T20:38:17Z,2,Better attention is all I need.. ain't that the truth!,True
@americanbagel,2023-07-11T20:34:35Z,0,Where can I get some better attention?,True
@timstevens3361,2023-07-11T20:34:23Z,0,ya n somebody wants you to PAY for it !,True
@MaJetiGizzle,2023-07-11T20:28:05Z,0,You had me at â€œBetter Attentionâ€ my dude.,True
@Artorias920,2023-07-11T20:25:54Z,0,awesome vid! Loving NNFS as well :D,True
@timmygilbert4102,2023-07-11T20:25:17Z,0,Nth first ðŸ˜‚,True
@BetinhoSM,2023-07-11T20:23:05Z,0,"I do wonder if or when Attention will surpass CNNs in medical imaging though. It is still not very clear that has already happened, and definitely not from my experience",True
@gunrage,2023-07-11T20:15:47Z,1,"Do you think Tesla's dojo will enable building much larger models? Maybe not initially, because it will be used just for Tesla needs, but in general.",True
@bobfish4404,2023-07-11T20:09:31Z,0,Misleading title,True
@MrGeordiejon,2023-07-11T20:07:50Z,0,@04:30 Bidens' auto prompt? I was thinking of extending nnfsip to wrap each attention and plug them into the context(s)? ...,True
@Stinosko,2023-07-11T20:04:37Z,0,Hello ðŸŽ‰,True
@hummuswithpitta,2023-07-11T19:56:56Z,157,"Interestingly (or not), every one of the authors of the original Attention Is All You Need paper have since left Google.",True
@princemars6746,2023-07-11T19:55:48Z,0,Seventh,True
@60hit99,2023-07-11T19:54:45Z,2,Good to see you back with amazing content <3,True
@dgbvideos,2023-07-11T19:53:20Z,61,You have my attention.,True
@antoniozhang6055,2023-07-11T19:53:05Z,0,First,True
@Liteship,2023-07-11T19:52:54Z,0,First â¤,True
@SaurabhKumar-mc1is,2023-07-11T19:52:54Z,0,Firts view,True
@MrSameerMalik1,2023-07-11T19:52:51Z,0,First?!,True
