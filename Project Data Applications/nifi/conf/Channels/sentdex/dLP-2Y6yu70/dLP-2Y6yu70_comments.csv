author,updated_at,like_count,text,public
@AntoninRaffin2,2022-02-28T17:58:55Z,51,"Hello again, Stable-Baselines (SB3) maintainer here ;) Nice video showing the variability of results in RL. For what you are doing, you should take a look at SB3 callbacks, especially CheckpointCallback and EvalCallback to save the best model automatically. And to have a more fair comparison, you can always check the tuned hyperparameters from the RL Zoo.",True
@MegaBd23,2024-03-22T14:37:19Z,0,"When i do this, i don't get the rollout/ep_len_mean or rollout/ep_rew_mean graphs, but instead a bunch of time graphs...",True
@KaikeCastroCarvalho,2024-03-20T19:38:45Z,0,"Hello, Is possible to use DQN model in tensorboard?",True
@satyamedh,2024-02-18T19:58:03Z,0,sb3 makes it so easy that a video about saving the model is longer than the inital intro and first steps,True
@walterwang5996,2023-12-26T14:38:08Z,0,"I used the save code similar to this, but it gave me several curves in tensorboard. and the reward curves are not even close, they have big gaps. I am wondering why, and my training cost time, can I train it and save the model after some episodes? Thanks.",True
@RafaParkoureiro,2023-12-10T00:35:47Z,0,"I wonder, how they play so good atari games if they cant land on moon properly",True
@ApexArtistX,2023-11-24T18:56:29Z,1,Can you load and continue training instead of starting from scratch again,True
@bluebox6307,2023-07-29T12:20:16Z,2,"For some reason, this is not only helpful but actually entertaining. Usually, I barely comment, but this is some good stuff :)",True
@andhikaindra5427,2023-07-14T06:03:55Z,1,"Hii can I ask you something : import os import gymnasium as gym import gym.envs.registration import pybullet_envs import rl_zoo3.gym_patches  from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize from stable_baselines3 import PPO  apply_api_compatibility=True # Note: pybullet is not compatible yet with Gymnasium # you might need to use `import rl_zoo3.gym_patches` # and use gym (not Gymnasium) to instantiate the env # Alternatively, you can use the MuJoCo equivalent ""HalfCheetah-v4"" vec_env = DummyVecEnv([lambda: gym.make(""HalfCheetah-v4"")]) # Automatically normalize the input features and reward vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True,                    clip_obs=10.)  model = PPO(""MlpPolicy"", vec_env) model.learn(total_timesteps=2000)  # Don't forget to save the VecNormalize statistics when saving the agent log_dir = ""/tmp/"" model.save(log_dir + ""ppo_halfcheetah"") stats_path = os.path.join(log_dir, ""vec_normalize.pkl"") env.save(stats_path)  # To demonstrate loading del model, vec_env  # Load the saved statistics vec_env = DummyVecEnv([lambda: gym.make(""HalfCheetah-v4"")]) vec_env = VecNormalize.load(stats_path, vec_env) #  do not update them at test time vec_env.training = False # reward normalization is not needed at test time vec_env.norm_reward = False  # Load the agent model = PPO.load(log_dir + ""ppo_halfcheetah"", env=vec_env)  And I got this : Traceback (most recent call last):   File ""d:\download baru\import gymnasium as gym 2.py"", line 27, in <module>     env.save(stats_path)     ^^^ NameError: name 'env' is not defined  What should I do? Thanks",True
@carlanjackson1490,2023-01-13T14:32:35Z,2,Say you partially train a model for say 50000 steps. Is it possible to once its finished you wish to reload that same trained model and continue training it for an additional say 20000 steps. I have a partially trained DQN but its not performing as well as it should and would like to continue the training but I am not sure if it is possible or will I just have to train an entirely new model,True
@scudice,2022-12-28T21:01:10Z,0,"I have an issue after leading a trained agent, the model.predict(obs) outputs always the same action, even though the agent was not doing that at all during learning.",True
@niklasdamm6900,2022-12-22T13:20:45Z,0,22.12.22  15:00,True
@marcin.sobocinski,2022-12-08T18:18:28Z,0,ü§öCan I have a question: is there going to be ML Agents RL tutorial as well‚ùìIt could be a nice sequel to SB3 series üòÄ,True
@MuazRazaq,2022-10-12T02:22:26Z,0,"Hey thankyou so much for such a good explanation. I wanted to ask how you are using GPU? Because whenever I run the same code it gives ""Using CPU Device"", I have a NVIDIA GeForce GTX 1650 Card",True
@sarc007,2022-08-05T06:19:47Z,1,"How to save the most optimized value of the model. I understood that this follow value ""ep_rew_mean tag: rollout/ep_rew_mean"" should be highest possible and this ""value_loss tag: train/value_loss"" should be lowest possible , so how to get or save the best model when this happens, any idea?",True
@vaizerdgrey,2022-07-25T10:25:22Z,1,Can we a tutorial on custom policy,True
@mehranzand2873,2022-07-15T16:45:03Z,0,thanks,True
@karthikbharadhwaj9488,2022-07-06T12:59:34Z,0,"Hey Sentdex, Actually in env.step() method you have passed the env.action_space.sample() instead of model.predict() !!!!!",True
@arjunkrishna5790,2022-06-17T19:05:36Z,0,Great video!,True
@Nerdimo,2022-05-23T20:25:03Z,0,"Probably should have commented last video, but I keep on getting this error on Mac OS: 2022-05-23 16:22:46.979 python[10741:91952] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f869dce60c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.  I tried to resolve using some stuff on stack overflow but the same thing happens. This error, I believe prevents me for running more than 1 episode. It will run one episode in the environment then crash :/",True
@sarvagyagupta1744,2022-05-08T22:18:56Z,0,"Hey, thanks for the video. I was wondering if I can ask some questions around loading a model here or would you prefer somewhere else?",True
@konkistadorr,2022-02-24T12:59:01Z,0,"Hey, great vid√©os as always :) Shouldn't you use predict_step instead of predict for faster execution ?",True
@ytsks,2022-02-11T00:38:54Z,0,"What you said about the jumping and rolling robot dog got me thinking - do you have a way to force minimum exertion policy? This is a guiding principle in most living organisms, only use the minimal effort (energy) that will produce the desired result. It purpose While you likely don't care about the energy conservation in that way, it could filter out the behavior patterns like the one you mentioned.",True
@PerfectNight123,2022-02-08T11:27:24Z,6,"Quick question, how are you training on CUDA device? I have a GPU installed but im training on cpu device?",True
@ruidian8157,2022-02-07T15:30:06Z,0,"Another side note: When dealing with files and directory, basically anything related to path, it is recommended to use pathlib instead of os.",True
@yawar58,2022-02-07T14:32:23Z,0,"Can't wait for the custom environment video. If I may suggest, please do a trading environment as an example. Thanks.",True
@unknown3.3.34,2022-02-07T10:28:25Z,2,Bro Plz help me. I would like to learn reinforcement learning. I'm good at machine learning( supervised and unsupervised) and deep learning. I would like to learn reinforcement learning but don't know where to start it. Plz guide me through my journey bro.  Where should I start,True
@raven9057,2022-02-07T02:04:23Z,0,im having real good results with TRPO under sb3-contrib,True
@anasalrifai2217,2022-02-07T00:07:13Z,1,Thanks for the content. Can you show us how to customize actor and critic network architecture?,True
@raven9057,2022-02-06T23:54:51Z,1,"""nvidia-smi"" nice flex :P",True
@georgebassemfouad,2022-02-06T23:21:14Z,1,Looking forward for the custom environment.,True
@giacomocarfi5015,2022-02-06T22:38:27Z,2,great video sentdex!! will you also talk about multi agent reinforcement learning?,True
@serta5727,2022-02-06T19:48:34Z,0,Stable Baselines 3 is very helpful!,True
@rohitchan007,2022-02-06T18:14:58Z,0,Thank you so much for this,True
@nnpy,2022-02-06T17:57:34Z,0,Awesome move forward ‚ö°,True
@sudhansubaladas2322,2022-02-06T17:36:07Z,0,"Well explained...please make some video on machine translation from scratch like loading huge data , training and testing etc...",True
@renkeludwig7405,2022-02-06T16:17:26Z,3,Only P.2 and I already love this series so much! Thanks for all your great content man!,True
@enriquesnetwork,2022-02-06T16:17:12Z,0,Thank you!,True
@harrivayrynen,2022-02-06T16:10:11Z,3,"I like this series a lot.  I have tried to learn To use ml-agents with Unity, but getting started is quite hard. Yes, there is official examples in Mlagents repo. But to get something new to work is hard to me. Hopefully there will be new book for this scene.  There are some, but those are quite old.",True
@jyothishmohan5613,2022-02-06T15:50:11Z,1,Wow...one video per day ...that's super cool üòé üëå üëç,True
@HT79,2022-02-06T09:18:17Z,24,"Side note: To create those folders, instead of an if clause, you can simply use makedirs with exist_ok = True",True
@fuba44,2022-02-05T19:03:29Z,7,"This is cool, looking forward to this series.",True
@Stinosko,2022-02-04T21:02:16Z,0,Hello again ,True
