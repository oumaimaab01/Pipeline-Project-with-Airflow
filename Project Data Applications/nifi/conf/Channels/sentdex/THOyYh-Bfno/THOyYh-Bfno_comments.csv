author,updated_at,like_count,text,public
@viniciuscaetano7267,2022-02-06T17:11:44Z,0,"Hi guys, I'm having problems with the graphics. I'm using Matplotlib 3.4.3, I tried to use 1.5 but it conflicts with other libraries",True
@fmj.mytube8846,2021-10-31T01:45:15Z,0,so obsolete..,True
@jamesdavidson8911,2020-05-17T10:06:33Z,1,"for anyone getting volatile 'difference' lines which don't start from 0, check your TotalDebtEquitymrq.csv file for datetime order. I found that I had to use... ""for file in sorted(each_file):"". Also, thanks for the video @sentdex! :)",True
@georgeg2558,2020-02-15T14:23:54Z,0,There's only pathetic amount of machine learning material up to this point. EXCEPTION: These are just USEFUL as hell.,True
@khan5265,2019-11-20T03:46:37Z,0,is there any way to use machine learning with sql queries ?,True
@Demodude123,2019-10-19T19:13:49Z,0,"I think with your problem at the end, you were printing the exception even though you were handling it properly, that's why you got really stumped!",True
@palashjyotiborah9888,2019-03-03T10:15:14Z,0,Last part of the video that u could  not find the issue . I think that tablehead1 not tabledata1 for the exception that u were incurring in the final journey.,True
@manitbhusal1554,2019-03-01T02:45:57Z,0,"In place of  stock_price = (source.split('<span class=""time_rtq_ticker"">')[1].split('</span>')[0]) stock_price = re.search(r'(\d{1,8}\.\d{1,8})', stock_price) stock_price = float(stock_price.group(1))  I replaced the code with stock_price = source.split('<span class=""time_rtq_ticker"">')[1].split('</span>')[0].split('"">')[1]  I got the same stock price, but I got a different graph and the graph seems broken, Why is that happening?  Thanks in advance.",True
@fanel1900toamna,2018-08-21T20:03:26Z,7,Yo sentdex I heard you like exceptions so I put an exception to the exception so that the exception would no longer be an exception! :),True
@SamuelMuiruri,2018-07-23T15:52:29Z,0,beautiful soup is something I would have used or pandas but I get it you're trying to keep it simple because some guys are pretty new to all this and they might be scared of with how many tools they might need to use just to get it working.,True
@skansar5411,2018-04-27T17:57:21Z,0,bfmovie,True
@vishalthengane637,2018-03-19T16:46:17Z,0,"after using this gather +  ':</td>\n<td class=""yfnc_tabledata1"">'  I am getting only three plots",True
@OTTO2794MD,2018-03-10T08:22:47Z,0,"I can't see clearly the word ""ticker"" on my monitor so every time I read it as ""Licker"".",True
@tanaykarmarkar6099,2017-12-11T02:18:01Z,0,"you could have just added one code  value = source.replace('\n','').split..... , this would have saved you everything but thanks a lot for this project. I can shine my resume now, just kiddin",True
@AtulKakrana,2017-12-10T02:49:57Z,2,@sentdex - You could have provided the links to CSV files in your description and started the tutorial with loading the data and doing ML.,True
@parthjoshiastron,2017-11-24T02:18:16Z,0,"I am not getting the plot, any guidance?",True
@alickzhang8597,2017-11-18T08:39:04Z,0,you forgot to escape \n,True
@vmikeyboi323,2017-10-20T00:17:13Z,0,My .csv is not properly being saved and I get no pyplot coming up at all,True
@Lee-ie1xg,2017-08-27T08:41:31Z,0,"(Big fan from Singapore ) Hi Harrison, Wells I have been stuck trying to get the info for the 2006 Appl stocks where the source code has been cleaned up and the value has been moved down to the next line. I am wondering with your expertise in 2017 how would you parse the codes now? :D  Also how will I be able to fix the following code to get the values..  This is the code: value = float(source.split(gather+':</td>\n<td class=""yfnc_tabledata1"">')[1].split('</td>')[0])  This is the error: can only concatenate list (not ""str"") to list aapl 20060217150101.html  This is the HTML source code: <td class=""yfnc_tablehead1"" width=""75%"">Total Debt/Equity (mrq):</td> <td class=""yfnc_tabledata1"">0</td>  Thank you!",True
@DjDryIcee,2017-04-15T12:37:43Z,0,Hey sentdex. Thanks again for your videos!! I've been following this set of tutorials and coming to this one i wanted to make sure all the data is legit. Seeking online for the closing stock price for Agilent Technologies Inc. for example on 14/05/2013 resulted in a different number ($31.44 ). So as other stocks in the dataset that fall under the same exception.  Any idea why that is? I searched a few different sites and they all resulted in $31.44 for that example..,True
@NPatel-vk5sk,2016-12-10T23:45:22Z,1,"I was having some issues with your exact code. I am working on Ubuntu and the ""os"" did not read the files in a sorted manner. So, all the dates were scrambled - leading to difference initializing at random time. Anyways, the solution was to simply add the ""sorted"" command in the each_file. This works because, thankfully, the name of the html files corresponds to the date and time. each_file = sorted(os.listdir(each_dir))",True
@bb5242,2016-10-24T01:57:59Z,0,"The solution to the data problem in this video is to use one of the many Python HTML parsing libraries!  Doing string matching is a tortured, error-prone approach that is totally unnecessary.  There are libraries that allow the programmer to use a jQuery like syntax and thus query for the desired fields.  Since I'm handing out advice:  The nested try blocks are also a big no-no.  They made it much harder to debug the problem and indicate that there is a design flaw in his code--separate those concerns!  Write functions!  Obtain and IDE with a debugger and use it.",True
@borgestheborg,2016-09-18T14:24:55Z,0,"value = float(source.split(gather+':</td>\n<td class=""yfnc_tabledata1"">')[1].split('</td>')[0])  All you had to do was concatenate \n as a separate string instead of writing it as part of the whole string  So you should have written  value = float(source.split(gather+':</td>','\n','<td class=""yfnc_tabledata1"">')[1].split('</td>')[0])  Problem solved...",True
@guigao282,2016-03-21T00:13:18Z,0,"hi @sentdex, great videos mate.   I understand you were going real fast with the vids and trying to get done with the data handling part asap to move on to the cool ML stuff but your method of retrieving data is pretty bad imo.  You are basically looking for every possible header from the HTML file. BeautifulSoup or scrapy would have been way better and more concise options imo for future work or reference.  Apart from that, great work so far overall.",True
@s0234799,2016-02-12T07:48:32Z,5,"I am aware you mentioned in the video about not arguing about the way of calculating % price change. But this question has bothered from this video to the end of this series, so I have to ask: the method used in the script to calcalute %price change means price/SP500 value change is based on how much they change from the begining year (i.e. year 1), right? So if a stock has already beat the SP500 in year n by 80% (i.e. stock_p_change - sp500_p_change = 80%), the chance of stock_p_change > sp500_p_change in year n+1 is significant even though from year n to n+1 it underperformed (e.g. stock_p_change-sp500_p_change drops to 70% in year n+1, so it would still suggest the stock is a good investment for the future. What's the point to use this strategy to select stock or what details am I missing here? Thanks.",True
@rufusgao6159,2015-07-30T14:06:22Z,2,"'source.split(gather+':</td>\n<td class=""yfnc_tabledata1"">')[1].split('</td>')[0]' didn't solve the problem...",True
@deanhu3410,2015-06-19T23:29:13Z,0,"Hey sentdex,  when i am trying to plot the diagram, I got all the labels as Difference...what could be the reason? Cheers!",True
@fabianbuentello202,2015-06-18T08:43:45Z,4,"Hey sentdex, been following this tutorial. I got stuck here for the longest time because our graphs just did not match. I think I might've figured out the issue you were trying to fix towards the end of the video. I feel you don't need to type cast `value` as a float. Doing that made my graph much more detailed. I'm going to link the GIST to the possible fix and the code incase you want to rewind back to this version of the code.   https://gist.github.com/fbuentello/33163ccadde059c8f07f",True
@anthonystevens2156,2015-03-24T06:17:56Z,1,Great series.  Seems like you got lost in data acquisition.  Too bad as I was really interested in what comes next.  Would love to hear you thoughts on actually analyzing the data.    Perhaps you could start with datasets from Google Finance like this instead:  http://www.google.com/finance/historical?q=NASDAQ%3AAAPL&startdate=Jan+1%2C+1990&output=csv,True
@TheMraptor,2015-02-04T05:33:27Z,6,You should have used XPath-like module that parses the HTML for you and then just browse the DOM... that would have saved you alot of trouble...  Otherwise commend you on doing those tutorials.. ;),True
@ognyanmoore146,2014-12-31T18:33:36Z,0,"Thanks for the video, glad to see even the best get stumped.  Would there not be a way to exclude 'N/A' exceptions explicitly?  For example in the try/except case for value, could it not be something on the order of...  if value = 'N/A':     pass  Or perhaps since we're anticipating the N/A error, would it not be worthwhile to consider writing our own user defined exceptions as per: https://docs.python.org/3/tutorial/errors.html#user-defined-exceptions  Thanks for the awesome video again!",True
@mamazu1995,2014-12-30T23:00:30Z,0,Just a quick question why haven't you tried printing out the string that you want to get the float of? This would have saved you a lot of headache. That's the way I deal with problems like this.,True
@sentdex,2014-12-30T14:42:16Z,0,"I don't seem to have a reply option to your comment +이재호 The quadcopter got put on the back burner after a bunch of issues kept coming up with it. I still plan to eventually do it. I have two of them now, since one was bound to meet its demise. I definitely want to continue the quad series, but will probably scratch the auto-pilot / stabilization board entirely and do everything with the pis and sensors myself or with the tk1 entirely... which will be messy and probably dangerous for anyone within a 1 mile radius :P  ... hopefully soon.",True
@Thehero394,2014-12-30T14:38:18Z,1,I give u a like without even see ur video! Because i know how awsome u r.,True
@kahisawheel,2014-12-30T02:58:50Z,3,These are amazing. Please make a thousand more.,True
