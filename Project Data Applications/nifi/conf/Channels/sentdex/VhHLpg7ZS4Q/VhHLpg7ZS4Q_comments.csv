author,updated_at,like_count,text,public
@gbagba81,2022-07-28T01:16:24Z,3,"I will be honest. I still have no idea what the math part was talking about 3 videos previously, I kinda followed this coding part though. How much time has it taken you for understanding the things you provide us in 4 videos?  edit: well I kinda got it. I actually saw this MIT talk, loved the proffessor, it's interesting that this Vapnik only had this made visible from 1992 and on from there after having worked on it for 25 years in a public building's soviet computer for testing what he made at home and then emigrating to United States. So no one should be ashamed of being lost, specially if you're not a maths student in Massachusetts.  How on Earth you came up with actually making a code out of the Vapnik, Lagrange and MIT thing? I mean dude come on. I mean... I mean dude come on! I MEAAAAAAAAAAAAAAAAAAAAAAAN...",True
@raconteurhermit1533,2021-08-28T14:28:21Z,0,"At 10:28 he has query about operations between a vector and scalar value , numpy supports operations on arrays of different sizes/dimensions using a concept called broadcasting Reference https://youtu.be/EEUXKG97YRw?t=447",True
@laternenpfahle.1249,2021-03-08T23:43:32Z,1,"For those wondering:  The transformations that are used here (4:16) are not by any means covering all possible directions for the vector w. Every decision-boundary calculated with this SVM will have 45° to the x-axis by defenition (!!). I think that was not quite clear in the video, but this is a very important point, since a data-set like {-1:[[-1,1],[1,1]], 1:[[-1,-1],[1,-1]]}, which is clearly seperated by the x-axis, cannot be porperly classified with this implementation. What I did to make it a bit more general and which really helps in understanding the maths behind the code is the following: I used this  transforms = np.array([[1, 0]])          for alpha in np.arange(np.pi/pi_frac, 2*np.pi, np.pi/pi_frac):             my_trf = np.array([[np.cos(alpha), np.sin(alpha)]])             transforms = np.append(transforms, my_trf, axis=0)  for the transformations and passed pi_frac to the fit-method. This way w will be tested for the number of angles to the x-axis you specify with pi_frac. Ofc this will make the algorithm a bit slower, but if you use pi_frac = 2 it should give you exactly the result you can see in the video. Dunno if this will actually help anybody, but I found it quite interesting to find such things in given code.",True
@visibletoanyone1508,2021-02-18T12:02:31Z,0,What is found_option here?,True
@zesvyaayvesez2849,2021-02-14T15:54:40Z,0,"Hey! Yes, indeed, mathematically you would have to subtract two objects of the same size. That would mean subtracting two vectors with the same dimensions, or two scalars. There is no such operation as subtracting a scalar to a vector, but Python is weird like that ><",True
@uujiguu4896,2020-11-17T06:00:52Z,0,"not working on data like   apples = [     [72, 5.85],     [73, 5.68],     [70, 5.56],     [75, 5.11],     [74, 5.36],     [73, 5.79],     [70, 5.47],     [74, 5.53],     [74, 5.22], ] oranges = [     [69, 4.39],     [69, 4.21],     [65, 4.09],     [67, 4.7],     [65, 4.27],     [68, 4.47], ]  data_dict = {-1: np.array(apples),              1: np.array(oranges)}  i play around a little bit and found out ""b_range_multiple = 5"" was the reason i change it to greater number like 8 and it worked but it not generating a perfect hyper plane. how to optimize and make more efficient?  and what if we didn't find any option here?                         if found_option:                             opt_dict[np.linalg.norm(w_t)] = [w_t, b]",True
@cj2074,2020-09-25T10:07:36Z,1,we are checking w[0] cause  the minute the w gets to negative aftwr stepping down ...it will be transformed while transformation and the same values will be generated so ..its no point in double.cheking,True
@akshatjain8052,2020-08-25T14:50:39Z,14,didn't anyone find this optimization exercise extremely difficult !!?!,True
@ajibademotunrayo8453,2020-08-01T15:18:21Z,0,This is cool,True
@aswinosbalaji4224,2020-07-11T07:43:31Z,1,"hey, each of your point (x) has only 2 features, what if there are 100 features. how w.x will work?  . Same binary image classification problem but features in each point is more than 2. should we create w_transform for all combinations of 100??",True
@tuhinmukherjee8141,2020-06-29T07:22:37Z,0,"Hey, I'm having a little trouble understanding that w[0]<0 condition, because shouldn't we be checking if my current_weight>last_weight which means we have skipped the global minimum",True
@mathiasyeremiaaryadi9097,2020-03-12T03:51:42Z,0,I have some error    can't multiply sequence by non-int of type 'numpy.float64'   in line where w_t = w * transformation,True
@darrenchan9951,2020-01-14T07:08:30Z,2,Can someone explain latest_optimum = opt_choice[0][0]+step*2 ?,True
@Arik1989,2019-12-21T11:46:20Z,0,"Thanks, great videos, nice to see an implementation in code",True
@abhaygoyal7263,2019-10-25T23:51:31Z,0,how do you calculate the number of Support vectors here?,True
@kulothungans6193,2019-09-18T15:29:36Z,1,how did we arrive at the value of 5 in this statement b_range_multiple = 5         #           b_multiple = 5,True
@NhatTanDuong,2019-09-02T05:39:11Z,0,next: https://www.youtube.com/watch?v=yrnhziJk-z8,True
@NhatTanDuong,2019-09-02T05:27:48Z,0,you could use for else: http://book.pythontips.com/en/latest/for_-_else.html,True
@ahbarahad3203,2019-08-14T18:41:54Z,0,"yeah, this is big brain time",True
@prakharsankrityayan1300,2019-06-26T07:00:10Z,2,I'll just use the SVM library man..... SIGH!!,True
@focker0000,2019-03-27T20:23:49Z,1,12:41 you should use ordrereddict,True
@luctiber,2018-10-13T16:38:50Z,1,just a short comment : effort to share --> great ; SVM --> Aaaaahhhhhhhh ;-),True
@rahul.vpoojari6553,2018-08-03T19:06:46Z,3,"kindly somebody explain why are we comparing w[0]<0 , i am stuck at this part ,please help",True
@kazakiewicz,2018-07-28T22:31:55Z,0,It seems as a wrong solution. components of w does not have to be the same. Lec 32 seems much more reasonable,True
@EbiSadeghi,2018-06-19T04:05:48Z,0,I'm having trouble understanding opt_dict. What is it and how does it work?,True
@bipulkalita5780,2018-05-05T21:54:59Z,5,we are not using lagrangian optimization ????????,True
@amanma12,2018-03-31T08:51:40Z,0,"in 13:51 is better take absolute value of  ""latest_optimum"" or not? because w_t can be negative",True
@samkumargupta2536,2018-03-09T13:20:20Z,1,"Hello sir, I am beginner to ML, I am a student and I took a simple project in ML(school presentation).I want to show the output of project along with the code in python using ML. I Hv no Idea how to do the project..I have collected some of data related to the project..I don't know data may be adequate or not...  please sir, can you write a machine learning program in python that will display list of food items(best recommendation for sugar patient for consumption)  based on the input of user(sugar level(1 to 9)) from the given list of food items.  Diet recommendation for sugar patient. #####################################################  user input: 	sugar level of patient(1 to 9). #####################################################  OUTPUT : food_items		sugar_level(in teaspoon) ----------		------------------ Corn Flakes,		1.93 Cheerios,		0.88 Rice Krispies,		2 Special K		2.57 Wheaties,		3.08 Rice Chex, 		1.62 Wheat Chex, 		2.09  Corn Chex,		2.25  Shredded Wheat, 	0 Lemons,			0.5  Kiwi fruit,		1.82 Apricots,		1.87  Strawberries,		0.99 Raspberries,		0.9 Blueberries,		2.02 Cranberries,		0.87 Tomatoes,		0.53  ###################################################### DATA SET : food_items		sugar_level(in teaspoon) ---------------		-----------------------	 Snickers bar,		5.83 Milky Way bar,		7.02 3 Musketeers bar,	8.14 Butterfinger bar,	5.58 Dove chocolate bar,	4.16 Milk Chocolate bar,	4.87 Coca-Cola,		7.25 Red Bull,		5.35 Sprite,			7.61 Jamaica Ginger Beer,	10.18 Alpen,			4.05 Cheerios,		0.88 Corn Flakes,		1.93 Cocoa Krispies,		7.83 Froot Loops,		8.46 Raisin Bran		6.35 Frosted Flakes,		7.12 Honey Smacks,		11.4 Rice Krispies,		2 Special K		2.57 Wheaties,		3.08 Trix,			6.49 Lucky Charms,		7.33 Rice Chex, 		1.62  Wheat Chex, 		2.09  Corn Chex,		2.25  Honey Nut Cheerios, 	6.67 Reese's Puffs, 		6.3 Golden Grahams, 	7.1 Cocoa Puffs, 		7.55 Cookie Crisp, 		7.06 Shredded Wheat, 	0 Cocoa Pebbles, 		7.26 Banana Nut Crunch, 	3.55 Mangos, 		2.77 Bananas, 		2.48 Apple,			2.11 Pineapples,		2 Grapes,			3.14 Lemons,			0.5  Kiwi fruit,		1.82 Apricots,		1.87  Strawberries,		0.99 Raspberries,		0.9 Blueberries,		2.02 Cranberries,		0.87 Tomatoes,		0.53",True
@bhuveshgupta8985,2018-02-16T06:24:03Z,3,Hello +sentdex  thanks for your great tutorials. I have some doubts regarding SVM: 1.) what is basically w ? Wasn't is supposed to be the perpendicular line to the decision boundary. 2.)why are you comparing w[0]<0,True
@JohanSebastianCorn,2017-12-22T00:51:34Z,0,"I have looked over the web and sudo coded everything even after video 28 but I still don't understand why the following has be greater than equal to 1.  yi*(np.dot(w_t,xi)+b) >= 1  You can change the 1 to any number you like like  yi*(np.dot(w_t,xi)+b) >= 7777  as long as you change the data_set's class value (yi).  I'm not not wrapping my head around why it has to be >= 1. The decision line is 0 right? I thought anything below 0 is negative and anything above is positive so why are we saying >= 1.",True
@isalvage1,2017-09-15T09:29:21Z,0,"I get this message when I run it ================================================================= C:\Python27\python.exe C:/Users/Owner/PycharmProjects/untitled1/ml1j.py optimized a step. Traceback (most recent call last):   File ""C:/Users/Owner/PycharmProjects/untitled1/ml1j.py"", line 126, in <module>     svm.fit(data=data_dict)   File ""C:/Users/Owner/PycharmProjects/untitled1/ml1j.py"", line 71, in fit     opt_choice = opt_dict[norms[0]] IndexError: list index out of range  Process finished with exit code 1 ================================================================ what am I doing wrong?",True
@benbenjamin5,2017-09-08T09:35:42Z,6,"Hey man great work, really appreciate it! Just one question, how come we have to maximise b? I didn't get that from the previous explanation videos either",True
@persus2001,2017-09-05T16:10:25Z,0,I don't get the math... Can someone teach me how SVM works like you were teaching a 5 year old. Please,True
@zhenruichen5192,2017-08-26T13:15:27Z,1,"""w - step""  numpy broadcast",True
@AmanSharma-nn3pp,2017-08-20T09:10:05Z,6,How and from where do you study yourself ? you are good but how did u reach this level? tips & tricks ?,True
@aashkaran2006,2017-08-20T04:47:10Z,2,Pretty confusing to absorb it. A flow chart would have been very convenient. Even having studied optimization I am lost understanding that loop.,True
@dude2260,2017-08-14T11:04:33Z,5,"what is w[0] < 0 , also is is compulsory that vector w will always be [a,a] i mean both terms be same",True
@vincentsung5972,2017-06-23T14:30:14Z,0,"In 2 dimension, the hyperpane is wx+b = 0, does w mean the slope here?? I can't get how w is affecting the hyperpane",True
@shivankpathak4367,2017-06-19T03:20:12Z,1,"what is opt_choice[0][0]? opt choice is something like { ||w||: [w,b] }. So opt_choice[0] is w .I am not getting what is opt_choice[0][0]. Please explain.",True
@winterhand9394,2017-06-02T22:43:03Z,2,"Thanks for the vid! Gotta question though - For w, why do we only need to consider the vectors under the transforms and not every single vector with that same magnitude?",True
@josephrejive4081,2017-05-25T13:35:15Z,1,"Why is vector w a (x,x) vector? What if we have data such that one class is directly above another class? How can a vector such as this classify that dataset properly?",True
@florentinalexandruiftimie8214,2017-03-25T00:29:24Z,6,"i think this implementation is not very good. it can find only decision boundries that are 45% degrees inclined because w = [ value, value] and it does not maximize the width of the ""street"" for some datasets. i guess that for better results, there should be 3 for loops, one for b one for w[0] and one for w[1], and remove the tranformations. but that increases the search space. any other solutions available? thanks",True
@raygeem1665,2017-03-22T01:56:19Z,2,"thanks for the whole python tutorials.  one thing that I don't understand is why 'w' being (latest_optimum,latest_optimum). My guess is, w is customized for this specific training data? I've learned a lot from your amazing python tutorials, thx.",True
@pratik6447,2017-01-28T07:40:07Z,4,"Hi sentdex,  the code became too messy. It would have been better if you created separate function for each,explaining each one.  Do you agree?",True
@lovethesmallthings2009,2016-10-01T08:02:03Z,1,why are we taking the initial value of latest_optimum as self.max_feature_value*10 Also what is b_multiple variable doing Also what is the intuition behind finding max valued feature,True
@talicaigoogle5836,2016-07-15T01:59:17Z,11,"thanks for sharing, but i eager to know what the meaning of this:  if w[0] < 0:     optimized = True     print('Optimized a step.')  thank you!",True
@leepat9226,2016-07-04T10:41:41Z,1,Is adding step*2 at for the latest_optimum at the end of the steps for loop equals to the metaphor on going back 2 steps on the ball rolling back in the bowl?,True
@narayana1043,2016-06-25T04:15:51Z,1,if w[0] < 0:    optimized = True   Please explain it a little briefly,True
@ahmedallali409,2016-05-27T10:51:28Z,1,Thanks.,True
@hoogstraten4271,2016-05-26T17:49:40Z,2,"What's the use of this machine learning I'm seeing a lot of machine learning, what can I do with this???",True
@sabahabdulbari5521,2016-05-26T13:14:22Z,2,"Hi there, thank you so much for your videos , I really enjoy your videos and I'm considering them as my reference .... I would kindly asking if you can make video to explain how leave one out cross validation be used in classification for different classifier like SVM, KNN,...etc",True
@MrIkariaman,2016-05-26T13:08:13Z,2,https://docs.python.org/2/library/functions.html#range default python range has step too :P,True
