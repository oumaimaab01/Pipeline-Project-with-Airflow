author,updated_at,like_count,text,public
@shivankprajapati3858,2022-04-29T11:25:15Z,0,how to use confidence for sklearn??,True
@jairajsahgal5062,2021-09-16T11:42:54Z,0,Thank you,True
@siddheshbalsaraf1776,2021-02-04T16:56:06Z,0,Thank you sentdex,True
@boenglover2593,2020-12-21T20:26:32Z,1,my accuracy littaraly just went up to 100% :/,True
@hiteshchoudhary9871,2020-07-12T05:48:31Z,0,why the accuracy chages for the same dataset when dealing with KNN  i mean model don't use any random variables which  can influence our result,True
@alexmattheis,2020-06-28T08:32:20Z,0,Studied CS but learning from your tutorials even more.,True
@mahvid123,2020-06-25T17:06:50Z,1,Thanks for the video. How can we calculate the confidence you did manually using sklearn?,True
@ambarishkapil8004,2020-05-25T12:27:27Z,22,"Hi Sentdex, I just completed watching all the knearest  part and I just wanted to say thank you for making these videos and you are super talented.❤",True
@VascoCC95,2019-12-01T21:37:43Z,0,Hi Sentdex! Love your work and I'm learning a lot from you! One thing I noticed is that your method of making '?' or NaN outliers (-9999) is not as good as just sacrificing them... For me it made a difference of 3% in accuracy and I'm getting consistent values around 98%. It was just to let you know.,True
@ElCanalDeBubu,2019-09-06T14:33:27Z,0,"that's for prediction within the sample, right?",True
@huseyincansimsek9675,2019-08-26T11:43:53Z,0,Where did you download CSV file? I am taking KeyError nan .I researched miscellanous sites but I couldn't find a solution.,True
@huseyincansimsek9675,2019-08-26T11:34:22Z,0,"Key error NAN why is that? i got it on train_set[i[-1]].append(i[:1]) ?",True
@keyo3945,2019-05-27T09:43:32Z,1,"you  teach me how to think  print(""thank"" , ""yo"" + 20*'u' , 's' + 20*'o' ,  ""much"") 😍😍🤗🤗😘😘☕☕",True
@Rishi-nv7bp,2019-05-26T05:09:48Z,0,can someone pls explain how the Counter(votes).most_common(1)[0][1] or Counter(votes).most_common(1)[0][0]  function works,True
@foriwill7384,2019-05-20T09:21:49Z,0,looks like k=4 is the best accuracy,True
@drjoriv,2019-01-14T21:12:32Z,0,You forgot to shuffle for sklearn,True
@maxteer2800,2018-12-19T20:22:06Z,2,"Thanks so much for the videos! It's really helping me with some research in kinesiology at the moment. I had a question, however: Would it be possible to produce a third class that is neither of the classes of interest?  My problem is that I don't want to train the algorithm with features and labels that are not in either of the two classes of interest, since this third class can vary wildly. I was hoping for some higher level way of saying: If (most of?) the k nearest neighbors of the test point are sufficiently far away, then the test point cannot be placed in either class.  My best guess involves looking at the standard deviations away from the mean distances and deciding whether a point is an outlier by using that.",True
@veggiet2009,2018-11-03T13:42:23Z,0,"Is it possible to give certain nodes a greater weight?  My problem is planning new people in small groups based on the location on the map of the people in the group as well as ages and gender, that data tends to have a lot of noise as not everyone who as chosen a group goes to a group they live closest to, and so when placing a new person in a small group I'd like to give a weight to the actual group location, and if they are a group that caters to a specific age group, or gender group I'd like to place that group node in an a more ideal place in those dimensions too",True
@sladealex308,2018-08-22T16:50:08Z,0,"correct: 0, thus Accuracy: 0 ..............please help",True
@liangyumin9405,2018-08-16T16:18:42Z,0,"Nice tutorial, I will follow that!!! Thank you sentdex",True
@dimitriskarampistis7605,2018-08-11T12:24:39Z,0,HAHAHAHAHA! It takes about 10 sec on my computer to print the accuracy,True
@progerzua,2018-07-11T07:08:18Z,0,"Thanks for the tutorial!  Dataset is pretty obvious, but it would be great to visualize it(and maybe look at the wrong predicted points)... So I`ve done it) For everyone in 2k18+. I`ve used PCA to visualize it, results you can see here https://github.com/progerzua/ML-tools/blob/master/PracticalTutorial/Classification/Knn_with_visualisation.ipynb   Its kinda shitty code, but I hope it can be useful.",True
@shaguftahenna7422,2018-07-08T06:27:42Z,6,Most effective learning way. Thanks,True
@EranM,2018-06-12T13:01:48Z,1,5:47 And now.. Comp-HARRISON!,True
@michiokaku101,2018-06-08T15:25:43Z,0,Key error NAN why is that? i got it on train_set[i[-1]].append(i[:1]) ?,True
@mushahidkhan7472,2018-05-05T17:43:41Z,3,why does the accuracy keep changing every time you re run the program with the same value for k?,True
@Adaministrator,2018-03-23T00:53:37Z,1,that was excellent man! Thank you so much!,True
@nicobohlinger7077,2018-03-18T10:15:11Z,4,Keep in mind the split for the own KNN was 60:40 the one with scikit was 80:20,True
@uhskn9753,2018-02-16T18:14:59Z,1,Using n_jobs=-1 Taken 2.912843942642212 seconds Using n_jobs=1 Taken 0.20581483840942383 seconds  why is it slower and not faster? for the sklearn version,True
@RayCarrender,2017-12-23T19:53:33Z,0,Elapsed time for self written: 21.904s Acc: 0.9697841726618706 Elapsed time for sklearn: 2.827s Acc: 0.968285714286  Just slightly faster. :),True
@jaferyshah,2017-12-14T11:22:59Z,0,Great tutorial's not this one but all of them found on python programming i want you to upload a tutorial on cuda programming in python. Thanks,True
@JoseLopez-oz5tn,2017-11-05T05:38:41Z,1,How would I add my own data points and see which one it would classify?,True
@akramsystems,2017-10-07T22:01:38Z,5,if anyone is getting confidence of 1s and 0s try to make a variable in the confidence a float like   confidence = Counter(votes).most_common(1)[0][1] / float(k)  :) thanks for the vids sentdex!,True
@aj35lightning,2017-08-04T02:15:43Z,0,Is there a way to get the confidence level with the sklearn library like you were able to in the knn you wrote out yourself?,True
@bellasun3820,2017-07-01T14:44:59Z,1,"hi sentdex~ I found that the results of ""accuracy method"" and ""accuracies method"" are totally the same.  so why should we calculate accuracies again after calculated accuracy?",True
@snowsel6686,2017-06-30T14:01:48Z,0,i have a question please. may I ask please?,True
@mayukh_,2017-06-27T10:18:00Z,0,"Hi Sendex,   I like you approach a lot but there is a concern that need to solve. In scikit Learn, there are two steps for KNN. One is fit and another is Predict. The fuinction that you wrote seems to be doing all the things togather. Can you please tell me upto which part in your code is run fitting and which part is for prediction. I ask this just to correlate with actual scikit learn.",True
@porlando12,2017-06-24T06:27:53Z,0,The comparison between sklearn and your custom function really helped demystify knn. Looking forward to the SVM stuff!,True
@lodashnotebook5390,2017-06-12T19:52:58Z,3,"Gosh, how do u manage to write the program without errors and make the video on-the-fly? If you're improvising, amazing stuff man!",True
@GeneralWolffang07,2017-04-23T16:46:16Z,0,"Is it possible to apply this same methodology to classify images, for example classify tomatoes? What would change?",True
@smailouttas1614,2017-04-19T21:26:34Z,0,I wont to classified a tweet to a sport tweet or cooking tweet or ...  with  the algorithme of knn  How can I do it ??,True
@zhorky,2017-02-25T13:35:22Z,0,"You don't use 80% of the data to test, only the last 20%. I guess test_data= full_data[(-int((1-test_size)*len(full_data))):] is what you wanted to do.",True
@tariqmehmood.97,2017-02-23T04:48:30Z,0,Does knn supports mix type of features? or we need them to be numerical?,True
@kgehmlich,2017-02-22T12:04:36Z,0,"For classifying using regression lines, wouldn't the squared error give the wrong result in your example? As I understand, squared error looks along the y-axis, which would make your point ""closer"" to the wrong line. You would have to use the formula for the distance between a point and a line.",True
@cyl5207,2017-02-13T14:08:55Z,0,awesome videos!,True
@YuvalFatal,2017-01-23T19:14:50Z,6,"I am afraid your comparison between your algorithm and the sklearn algorithm is not exactly right, because in your algorithm you tested 40 percent of the data instead of just 20 percent you tested in the sklearn algorithm.",True
@christophelimbree7878,2016-12-25T19:55:34Z,0,Where can we get the source code of your tutorials?,True
@andrzejzielezinski1489,2016-12-05T20:21:11Z,9,"Great tutorial! One thing,  in your comparison of accuracies between your own implementation of K-Nearest Neighbors and sklearn's one, the test_size is 0.4 (your implementation) and 0.2 (in sklearn).",True
@MariyanZarev,2016-11-19T10:22:46Z,4,"Changing the size of K doesn't affect my accuracy for some reason, as long as k>0",True
@gutoa.764,2016-11-02T01:47:50Z,0,Great video! Could you do a tutorial about Neural network models (supervised)?,True
@RealMcDudu,2016-06-14T09:16:42Z,1,"A nice thing to get out of a predictive model is what are the important features? In the linear regression we can get these from the parameters (the slope's, in case we have more than one feature). If the slope is very big, we know - ok this feature is very important, if it's small - we know, ok this one is not so important. *Unfortunately, I don't think we can do that in KNN* since we're basically treating every feature the same, in calculating the euclidean distance... What we can get still is the average ""point""/""co-ordinate"" for each class, which will tell us what feature we expect to see for each class.",True
@PkFrTricker,2016-06-14T05:30:00Z,0,"Hi Harrison, When you compare the sklearn's KNN to the one you wrote, the test_size for written code was 0.4 instead of 0.2.  I did two tests for the written code, each 25 trials at test_size = 0.4 and 0.2.  The 0.2 had a run-time ~33% faster, but the accuracy was practically the same.  I doubt that'd make any significant difference to the point you were making! Just pointing out the difference in values in that comparison :3",True
@nishance,2016-06-12T09:43:58Z,0,"Hello man, I found your video series of machine learning is very interesting and nice. But I have requirement where I'm using Scikit-learn KNN using Python but with the large dataset it is taking hours to train the model and generate prediction. So I want to leverage Apache Spark distribution power with my  Scikit-learn KNN.  I'm using my code as below,  knn = NearestNeighbors(n_neighbors=13, algorithm='ball_tree') knn.fit(features)  distances, indices = knn.kneighbors(testset)  Do you know how can I distribute using Spark and make my Scikit-learn KNN faster?",True
