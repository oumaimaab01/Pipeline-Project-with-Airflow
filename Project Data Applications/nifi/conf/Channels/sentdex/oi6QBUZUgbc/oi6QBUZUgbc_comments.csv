author,updated_at,like_count,text,public
@Lucas-nz1xp,2020-05-30T19:32:44Z,0,"I appreciated this tutorial, but you really messed up all code at the middle of this tutorial, a lot of confusion to decide a way to keep doing haha Do you will record more of this stuff? <3",True
@jaredmt,2020-05-27T03:29:00Z,0,"Is it possible to build a NN to try to mimick the strategy of your earlier model (with only 4 options)? You can convert the original 4 decisions into the new set of decisions. This way you can have a good starting point (which is capable of winning a lot) but still have many actions to take. After your NN is trained, try using a stochastic policy to slightly deviate and generate the new data.",True
@alexanderyau6347,2020-05-17T12:16:31Z,0,Good videos. I learned a lot and it is much easier for me to learn SC2 AI now,True
@StevenBarwell01,2020-05-03T02:59:20Z,0,"My two pence.  Watched whole series.  Was this ever solved.  My idea Iterate and converge.  I would create a very very very easy custom AI Opponent.  (Eg. 1 base, collects resource, builds 5 simple combat units, waits x time (20 mins say then attacks, over and over till it wins)) This gives your random AI time to try and get a build order to win.  metric: win time.  Run DL to get an AI that can at least build an army that wins in eg <20mins.  Collect data.  Then up the custom opponent AI difficulty to attack say after 18 mins.  Use your DL AI  from very very very easy AI opponent but code a random choice 50% / 33% / 10% of the time, other time it picks the DL AI choice.  (DL-Deep Learning). :)",True
@fuba44,2020-03-22T21:46:24Z,7,"damn shame this series ended, was really interesting.",True
@tarushsingh1108,2019-06-29T19:13:53Z,0,"One recommendation Harrison try using learning rate decay algorithm. We use one single learning rate and wen you train it using that, it learns very slow. What we need is the learning rate to drop exponentially so that when it initially starts training it will take bigger steps to minima of the cost function and as it gets closes, those steps will drop allowing for more accuracy in the end. It'll be like a fast in slow out kinda thing. Have used this idea on Alexnet and Inception models to make imitation bot from your GTA plays tutorial and the learning rate after every epoch increased significantly.  Hope it helps again thanks for such amazing tutorials man.",True
@bjornrenshaijanson1657,2019-03-16T05:54:32Z,0,"With the introduction of AlphaStar by Deepmind, do you have any new ideas about using maybe a Deep Learning Algorithm like A2C or a Q-Network to work with that you have to this point? Maybe using Reinforcement Learning?",True
@Dangazm,2019-02-12T22:24:32Z,4,"I'm not smart enough to keep going on my own, Mr. Dex. Plz do moar starcraft.",True
@kezgaming4906,2019-01-12T10:46:47Z,0,Is their going to be an update to this at all? 😁,True
@catwolf256,2019-01-01T14:57:24Z,0,does AI can be play in user custom maps? like arcade games.,True
@naveenadhikari5953,2018-10-17T14:27:58Z,0,Can you plzz make a video on binary search🤨🤔🤔🤔,True
@Mokodokococo,2018-10-05T00:22:19Z,0,Why are you obstinate about having your input being a 2d projection of the game ? Why not just take as input the coordinate of the buildings and units ?,True
@Mokodokococo,2018-10-05T00:12:09Z,0,is there a way to move camera ingame with a bot ?,True
@DrJohnnyStalker,2018-09-26T10:37:09Z,0,New Paper for SC2 Reinforced Learning. They try a Hierarchical Architecture with a CNN   https://arxiv.org/abs/1809.09095,True
@edvinnorling6331,2018-09-01T06:53:29Z,0,"First of all great series, it's the first time for me looking how to build your own ai and always fun to see it in a game.  I just want to give some feedback on the code you don't seem to love pep8 which I can understand. But if you want your code to be more readable I would suggest using black. It will help out to reformat your code so it looks nicer. pip install black black --py36 . https://github.com/ambv/black",True
@dxamphetamin,2018-08-19T13:53:55Z,0,"Don't know if this is too late or if this gets read by anyone but here is something I am bothered with for a while now: This is the code I replaced around line 400. Inside the else part of the choice selection where the choice is executed):  I removed ""if self.time > self.do_something_after:"" (so you need to rearrange your code!)  Then I replaced this:  >>>try:                 await self.choices[choice]()             except Exception as e:                 print(str(e))<<<  with:  >>>if pos <= 10:                 try:                     await self.choices[final_choice]()                 except Exception as e:                     print(str(e))             elif pos > 10 and self.time > self.do_something_after:                 try:                     await self.choices[final_choice]()                 except Exception as e:                     print(str(e))<<<  and this:  >>>choice_weights = 1*[0]+zealot_weight*[1]+gateway_weight*[2]+voidray_weight*[3]+stalker_weight*[4]+worker_weight*[5]+1*                                        [6]+stargate_weight*[7]+pylon_weight*[8]+1*[9]+1*[10]+1*[11]+1*[12]+1*[13] choice = random.choice(choice_weights)<<<  with this:  >>>choice_weights = 1*[0]+zealot_weight*[1]+gateway_weight*[2]+voidray_weight*[3]+stalker_weight*[4]+worker_weight*[5]+1*                                           [6]+stargate_weight*[7]+pylon_weight*[8]+1*[9]+1*[10]+1*[11]+1*[12]+doing_nothing_weight*[13] final_choice = random.choice(choice_weights) pos = choice_weights.index(final_choice)<<<  And I added a ""doing_nothing_weight = 10"" above at the weights.  ("">>> <<<"" those are only for clarity, if anyone wonders)  It's important to note I switched the choice 10 with 13, here is why and what I want to achieve: You don't want to do nothing. never. And this check if you want to work on your economy or make an attack move. So, 1-10 are things like building buildings and units & 11-13 are attack options. So, instead of doing nothing this code lets your bot build your economy all the time and only does nothing in terms of attacking.  hope I could help anyone creating better training data.",True
@THESocialJusticeWarrior,2018-08-19T03:01:38Z,0,"Instead of just using wins as positive feedback, why not use kills?  Each enemy killed is a +1 score, each lost unit is -1 score.  Killing or losing buildings could be + or - 5 score.  Seems like it could learn faster that way.",True
@aldude9511,2018-08-16T10:53:35Z,1,"Some thoughts from my experience building a mostly rule-based AI to play SC Brood War and from many years of playing BW and SC2. 1. I'd recommend having the AI more strict in early game. As a lot of SC players will tell you, the early game is very rigid and you just kind of follow a set build order based on the strategy you are going for that game. 2. Give the NN more data. Supply used (at least early game) is probably the most important metric to gauge when to build things. In addition (and I am sure this wouldn't be easy to add in) try to differentiate which enemy units have been spotted by colour coding them. 3. Keep the colours for friendly units so the AI knows when it has a Cyber Core and doesn't try to build a Stargate without one. This is just one case of the many tiered tech trees in SC2. 4. A big game component is what is referred to as macro or economy. The theory is you always want to be producing as many basic workers as can saturate a base (aka 3 per mineral patch and 3 per vespene geyser) so allowing the AI to choose when to make workers will severely hinder it.  Hope some of this can help and I look forward to seeing the results.",True
@priyapramesi6026,2018-08-15T18:03:36Z,0,"In sc2, usually we use supply as the guide for when to build stuff, instead of using in-game timer. There are alot of Void Ray focused build orders that people have come up with that you can probably use (e.g. https://lotv.spawningtool.com/build/67673/) and all of them use supply as the guide for when to build units and structures. I think it would be better if the AI is ""guided"" by the build order in the beginning of the match because some input data that look similar probably map to contradictory commands. It's like trying to map 2 slightly different pictures of a cat to a cat and a horse, if that makes any sense at all.   The AI would probably perform better if the input data consistently maps to same commands in the beginning of the match. When it gets to the mid-game (about 50-80 supply), and the enemy starts attacking and you have vision of their base, then random commands can be used to map to the input because now there's more context (i.e. enemy units and structures). Also, you don't really need to scout until mid-game. The build order ""guide"" doesn't have to be hard coded, but instead can be different probability distributions for each supply level. This way, the AI model is generalized and won't overfit.  Another idea is to get replays from the best players in the world and see their build order. You can download replays and use https://github.com/Blizzard/s2protocol to create data sets of early game builds. Or you can do what OpenAI do with their Dota 2 bots, which is just concatenate a bunch of other data points into the network (see https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf ). In your case, probably after flatten but before dense. Maybe even incorporate LSTM in there somewhere, which really makes sense because the network will then be able to ""remember"" enemy attacks, army composition and other useful stuff.",True
@leonardoxavier1481,2018-08-13T21:53:33Z,0,"SENTDEX Do you have any suggestions on any ""AI + GAMES"" application not yet implemented that would be great to see the working?",True
@Adamcaudill,2018-08-12T23:33:03Z,2,"I just wrote a really long comment that didn't save. This one may not be as well worded but....  if I am understanding this correctly you are taking the decisions from the 'random bots' that won and using those to weight choices that are still in a random order in the 'trained' bot. But this is not an effective method of applying learning in an RTS game. Instead of weighting a random order of the decisions you should be learning optimal build order.   I am still learning programming, but I have played TONS of starcraft and followed the pro starcraft scene for several years. Build order is THE thing to learn. If your program is going 'the bot that won built workers 1/4 the time and zealot 1/3 the time etc. and just applying those ratios of decisions it is completely missing the point. It should be learning 'The bots that won built 3 workers, then a gateway, then 1 more worker, 1 zealot, then 2 pylons, then a cybercore, then 3 more zealots an a stalker, then expanded.... etc.' You shouldn't be shuffling the lists from the winning 'Random' bots. you should be choosing from those decisions in the order they were originally chosen.  Again I may not be fully understanding as some of this is beyond my python skills and I haven't had a chance to play with it myself.",True
@TOCZEKX,2018-08-12T20:39:05Z,0,"i have copied code from tutorial and it doesn't save a victorious game and i have no idea why, anybody?",True
@marcuslanvers5138,2018-08-12T12:46:09Z,0,"About the problem with the training data. I was wondering if it might be a possibility to save some data while the game is still running. For example you could check the game after the first 5 ingame minutes if you have minimum x supply used and maybe minimum 2 bases, if thats the case save the data from the first 5 minutes. While that would not work for low econemy rush games, in most of the cases it would give you at least with some usable data that is not to noisy like the data you get from you won the game.",True
@shadow2422,2018-08-12T09:21:05Z,0,"Wow sentdex, really great series till now and I'm super excited for your next contributions in this. AI/ML was always interesting to me and i heard some lectures about it. Till now i did not have an application idea of it in my free time, as i want to learn new technologies out of scope from my actual domain to find new inspiration about new applications in my field and you gave me the motivation to tackle this subject now on a practical field with RTS/SC2. You sir got on one site a rational and in the same time for me humorous way to explain what and why you're doing specific things, which is very clear and enjoyable to watch. Thanks for this high quality content, keep up that good work and hope to see new episodes of this series soon! Btw: Zerg>Protoss. 😉 But as i've read a few papers and a thesis about this subject, in my imagination it seems Protoss is the best race to enter this field, right?",True
@quaketheduck1247,2018-08-11T02:13:59Z,0,"Hey Sentdex. I've played quite a bit of Starcraft 2, I want to suggest two things. The bot that is bring programmed before it's put into the Tensor-flow for deep learning. I suggest weight based on situation and a different visual process. The weight based of the situation can start fairly simple but needs to change rather than be static. I think having a minor random variance is good for the AI learning process. ie, early game, focus on getting two bases and some army up. Early mid - Have we seen the enemy army, how big is it? If not seen expand and add more army. Stages like this for weight will help. I strongly believe that having a ""seen enemy army"" count and add 5-10% more in what we currently know they have, then 50 more Voidrays for good measure. https://pastebin.com/Bfxi1y3v . your enemy adds the random, you need to build something static that changes based on them, a static weight class that randomly changes based on the enemy. Sounds funny but it is a good place to start I think.  I believe you mentioned in one of your episodes on these tutorials that you could use an actual unit size instead of personally assigned ones. Do this and add colour. The colour for a few reasons, example, when a Forge is upgrading weapons/shield, etc that forge has visual indicators, it could be worth focusing on. The same thing could apply to a structure that is being attacked, if it's below a certain threshold it may be worth focusing on that. Mostly, kill probes 9/10 if there isn't an enemy army though.  Hope something in what I said  was helpful, cheers!",True
@dpm_07,2018-08-09T12:08:56Z,0,"sir can you make any video on ""Internet Speed test"" using python",True
@yimoawanardo,2018-08-09T11:11:10Z,0,Can I say? You look really similar to Edward Snowden in terms of your face stendex https://i.imgur.com/yGpeHN5.png Him https://i.imgur.com/9Dasmqy.png You,True
@abdullahmosibah560,2018-08-08T12:01:13Z,0,have you shanged your set up ?,True
@VonUndZuCaesar,2018-08-08T09:24:23Z,1,Did you tried a conv. lstm neural network with genetic algorithm to get a better training start? From that on you could use backpropagation to train your network further :D,True
@hermsstanley,2018-08-07T20:04:48Z,0,Anyone can provide a simple python script to list all usb device on windows machine using python 3.6?,True
@sibyjoseplathottam4828,2018-08-07T02:29:01Z,0,Have you considered using RL?,True
@sibyjoseplathottam4828,2018-08-07T02:27:49Z,0,I implemented the training loop using TensorFlow datasets. One epoch took about 40 minutes. Please check my GitHub repo. https://github.com/sibyjackgrove/SC2_CNN_bot_training_dataset_API,True
@TheNestyCZ,2018-08-06T11:56:54Z,0,Hi I like your AI videos! :) Did you hear about http://www.rlbot.org/? (AI framework for Rocket League) Its tons of fun :),True
@PROdzeikob,2018-08-04T22:39:02Z,0,"Hey, take a look at making rocket league bots, https://github.com/RLBot/RLBot they usually have tournaments with user made bots on twitch, only like every year though I think",True
@jakeforsey4308,2018-08-04T22:29:17Z,0,"Things like building probes/pylons are very low-level decisions that can be thought of as ""mechanics"".  Teaching an RL / genetic algorithm to learn mechanics will be really boring and hard because they are individually inconsequential. It also won't be particularly satisfying I don't think because you can achieve near-perfect mechanics very easily as you showed in the first few videos (you will have been building probes faster than even the best human players).  I'd focus on training a model that can select much higher level ""modes"" such as:   - Expanding economy  - Increasing access to technology  - Attacking  - Harassing  - Defending  - ... add more detailed modes for more interesting behaviours  Once the model has decided what mode its currently in I'd use your rule-based approach as perversion one.  These things are more interesting in my opinion.  Really love this video series. I hope that this set back doesn't stop you progressing it!",True
@hansmartin31,2018-08-04T16:14:33Z,0,i have a 1080 is there a way to support your training time?,True
@d.2656,2018-08-04T12:06:52Z,0,What happens if u let the AI play against itself a bunch of times. Does it get better that way or just stay random?,True
@user-fd4uq9lx9y,2018-08-03T23:36:42Z,1,"I am not sure about this, as I have only seen short snippets of this behaviour in your videos, but: When your units choose to attack enemy nexus, they seem to move to the enemy nexus and focus it, not attacking potential defenders in it's way, which leads to your units dying unless you are massively ahead. In sc2 you can move or attack-move, the former being the problematic behaviour, and the latter is ""move there, but attack anything hostile you find on your way"" I don't know if this is possible with this API, but letting your units attack-move could be an easy solution Keep up the good work :)",True
@ProjectSuperSport,2018-08-03T19:04:01Z,0,"My view about your model - I think that your attempt to represent the map as a image to match an existing infrastructure was a mistake. You can look at it this way - you are feeding your NN a much much bigger input data than needed, with no extra value for the system to produce from it. I think that you need to look a little bit more for the right ""feeding"" way for your neural network, because I believe that this model is uncapable of actually learning. maybe somthing in the lines of what carykh did here - https://www.youtube.com/watch?v=Sc7RiNgHHaE",True
@tormentedbacon4573,2018-08-03T18:11:31Z,0,"Oh, and I don't know if this will be helpful, but when I was following your tutorials using OpenAI and gym to solve those little problems, when I tried the 1st one without a tutorial it would never complete the objective randomly like was able to with balance.      Thus it was never able to get any ""good"" data to complete the task.   So what I ended up doing was creating a score for each try, then I skimmed off the top10% based on score.  Amazingly it was able to take that data and not only did it spin the bars to get it high enough, it did it really fast.   I was quite amazed with it.      But I did have to shorten those ""games"" down to a few seconds so it could find the actual action that made it happen as longer attempts simply added a bunch of bad data.      I never could get either method to work on the cart uphill, and eventually used the NEAT method to solve it, but the NEAT method would likely be a PITA with such long game times.   I drives me crazy on 20 second games with the atari games lol.  Anyway, I saw that the game gives a score at the end, but not sure how to pull it out with the python-sc2 mod.   I saw it in the pysc2 mod, so I know it exists though.       Can do the same thing, run a bunch of games and only skim off the top scores.     That said, I've seen in other such uses where the AI ends up just sitting outside the enemy base farming kills because it equals more score rather than finishing them off, but since this isn't individual unit score, I don't think that would be an issue here - but it may just want to keep building base/defending.",True
@trevorseitz502,2018-08-03T16:37:59Z,0,Is the model you're using a recurrent neural network?   Pre-training your AI prior to taking it live with data from your hardcoded AI might make a difference as well.,True
@ayushthada9544,2018-08-03T15:55:40Z,0,I have recently joined your channel & I think it is one of the best learning platforms on youtube. I'm working on a project and your GTAV series is really helping me there.  Could you please make a video on how to make GPU cluster using PySpark or something and train these model on those cluster? This will really help subscribers to follow up your video easily.,True
@idacal,2018-08-03T13:41:30Z,4,There are a lot of great ideas posted in the comments! Unfortunately i got stuck when you import cv2 - so many troubles to install it .. omg,True
@qwfp,2018-08-03T12:39:41Z,1,why not just train it against itself? thats how OpenAI trains its DotA2 bots,True
@glorytoarstotzka330,2018-08-03T09:53:15Z,0,"well i was expecting more choices not work, because it takes a lot of tries to get good samples and if you put it to make 14 stuff of course it won't work as good as 3 , because there are too many posibilities and scenarios , so controlling only attack function would be better, damn it bummer",True
@BigBadBurrow,2018-08-03T09:01:05Z,0,"Looks like you have a sparse reward problem.  Rather than the reward being purely on ""win"", to get things started you could have a couple of additional rewards of ""Enemy units killed"" and/or ""Enemy structures destroyed"" - they'd seem like worthwhile rewards. Obviously you'll need to make ""win"" reward much much higher so that the AI learns a win is better than killing lots of enemy, but allow the base to stay alive so it can regroup and then kill more enemies once they've regrouped.",True
@tormentedbacon4573,2018-08-03T08:40:02Z,3,"My opinions, hope it's helpful.  1.   Write some code to have your nexus use it's boost.    It's not that difficult, I did it tonight in a little bit of time.    I have it look to see if it's building a worker, and if it is, then it applies the boost to itself.   If it doesn't have a worker being built, then it will look out to other buildings  that have a current queue, and apply the buff to that building.     I'll happily pass you the code if you want.     The end result is a huge boost to your economy.  Your workers get built faster to start with, and that snowballs into a better overall AI.     I was able to get much faster wins with this tiny bit of code being added.  2.  Update the quality of your units.       Upgrade to the latest version on git(pip isn't upgraded sadly) and there are lots of new goodies in there.    Burny uploaded a bot that shows some easy and basic micromanaging of a unit.   I took that code and adapted it to a few of the protoss units and it has improved the quality of the units.     My units will pull back when getting attacked, try to keep maximum range if possible, etc.      Also the voidrays themselves have a buff that they can apply to themselves that increases dps.    It only increases against armor units, so I added some code that says if attacking a unit with armor, and it's available, activate the ability.   You can also make it just activate everytime it's available, but it will slow the unit down.   Not a big deal with a fire and forget method, but not ideal when wanting to micro.  They are a little sloppy still, but they act independently and sometimes do a good job of staying alive.    I'm still working on their targeting priority, which I'll bang out tomorrow hopefully if work isn't too hectic.   They are really good in a group and would work even better under a command.  3.   Add in some research/upgrades.   You are already building the cybernetics core, and with it alone you unlock 2 upgrades for the voidrays.   1 increases their DPS and the other increases their armor.    You can build other things to unlock further levels, but these 2 are like right there for the picking.     The forge has the same 2 for ground units, but then also a shield buff for all.  But it's not in your build, so a good bit of resources for just a shield buff.  I think those things would help you get the data you need to train properly.    Especially with the weights you are using for getting workers etc.     I love this series and I want to do something almost exactly the same for the AI to handle the macro strategy of the game, so if you want any of the code mentioned I'll be happy to share/post.     Also, as far as visuals go, a few things I was thinking about.  I haven't done any code other than following along this tutorial with the color one.  1.  Structures as squares like the minimap in game, units are circles.  2.  I would not use the unit radius because they aren't to scale with the map.   You could scale them I guess, but I think a better way to represent a crude way of showing strength is to make the radius = to the supply cost of the unit.   Thus a little ol zealot has a radius of 1, while the void ray has a radius of 4.   Townhall/Nexus/etc gets a set size, other structures get a smaller size.  3. With the above I think you could fill in the units with solid color and even with greyscale get plenty of detail.  Especially if you found some alpha/overlap as you hinted towards in another comment.  Hope it helps, and hope you come back with another episode soon.    I check daily for them.",True
@sizzlorox,2018-08-03T08:34:31Z,0,The bot needs to retrieve data from what units the other player is building to build counter units. Basically seeing the buildings the other player has shows possibilties of certain units so your AI can build units that will be effective against those.,True
@MrBoubource,2018-08-03T08:34:16Z,0,"I know you didn't want to do that at first but maybe, to get to the easy bot level, you can try AI vs AI so that best random wins, you get a little better ai (maybe) that might beat easy. I'm not an ai pro at all and i have no idea if this can lead to any improvement at all or if it will stay completely random without too much training.",True
@_test_test,2018-08-03T08:24:57Z,0,wanna u to tell someth about openAI dota bots. how damn hard it is to do someth like this,True
@vmokrousov,2018-08-03T07:18:25Z,0,Maybe you'll try deep Q learning?,True
@edwardhu7883,2018-08-03T07:10:30Z,0,"Also the sparse reward problem is tough. Perhaps look into some intrinsic rewards such as curiosity (pixel or generalized) or a hand shaped reward function. Anyways, this looks pretty hard. Looking forward to see what you do next!",True
@edwardhu7883,2018-08-03T07:08:41Z,0,It might be worthwhile to look into hierarchical reinforcement learning. Define and train some primitive skills and then train a meta controller that can learn to use the primitive skills as well as learn a new policy,True
@AlienService,2018-08-03T05:43:03Z,1,"I've seen some things on this approach and I wonder if it would be useful here.  Instead of just rewarding wins, give smaller rewards for milestones, or achievements along the way.  Maybe small rewards for supply (units) produced and supply destroyed.  Instead of rewarding making cake, give reward for getting eggs, flour, mixing, cooking batter.  The AI ideally should be able to reverse engineer all the steps that were favorable to winning.  We win by killing all the bases - we kill the bases with our army - we build our army from our buildings and our economy...  We lose if our bases die, our bases die when...  You also can win games for being more efficient in your fights, and drawing an enemy into a concave of fire.  But I have no clue how you'd get that granular.",True
@mhermatevosyan5315,2018-08-03T05:08:04Z,0,"Seems like Blizzard has some replay packs https://github.com/Blizzard/s2client-proto. Could you use them as training data? Also maybe there are pro players replays, which would be much better training data?",True
@jasonreviews,2018-08-03T03:59:20Z,0,do machine learning for fortnite,True
@gibsonemg,2018-08-03T03:54:58Z,0,"I think you could do something much simpler than a ConvNet + visual representation that would also work a lot better. For example, create a game state vector and use a normal DNN. The way you’re doing it now, you have bars representing some number, which the model needs to learn to extract and use. Why not just feed in the number directly as a model input?  If there are things you absolutely need the image for, then you could just concatenate the ConvNet output with the game state vector and feed that to a fully connected layer to get the best of both worlds.   Finally, you might consider reserving a color channel for each player. You’re not restricted to 3 channels for modeling, only for display.   All that said, I agree with the others that biggest concern is the training data. You’re really dealing with a sequence, and it’s not clear how the final outcome should inform the individual decisions. I would guess a better strategy is using an evolutionary algorithm to evolve a population of networks. But you need a fitness function. If you can’t get a game score (which I think sc2 provides), then you might use a proxy like time since better strategies will last longer.",True
@alexs477,2018-08-03T02:44:32Z,1,"It might work better if you break the model up into several different models.  For example, one could build units out of the nexus, one could determine where to send the attack units, one could build units for the barrack (I am not sure what they're actually called).",True
@chinmaydas4053,2018-08-03T02:33:02Z,8,"Sir please make more video on financial analysis, investment.. from the real programmer analysis. I always surprised by your knowledge base.. how one can have this much huge knowledge 🤔🤔.. only legend can do this. Lots of love and respect for you sir from India 🙏🙏🙏..",True
@JonDeibeljr,2018-08-03T02:25:47Z,2,Why not use a reinforcement learning approach? Something that might be good to try is PPO. It worked really really well for Open AI in their recent Dota 2 AI.,True
@idacal,2018-08-02T23:46:54Z,2,"One of the biggest issues i see, is the way AI attacks. 1 by 1, or just passes through enemies without attacking and dying. That way the AI will never win a battle, or a game... I just wanted to point that out. i dont know how to solve it",True
@idacal,2018-08-02T23:38:02Z,22,bummer please keep trying sentdex,True
@bibianaburela,2018-08-02T23:20:17Z,0,"I think it's a really big problem to use win/lose, since there are a lot of decisions, and some of them were bad despite it ended up winning.  Maybe adding some minor rewards could help. Like, getting resources is good. Losing units is bad. Building things is good. Killing enemies is good, if the reward is greater than building an unit, the AI would be more aggressive, if not, more defensive.  That way, a really good game that ended up being a loss, won't have the same effect than a game that was lost for a lot.  The problem here is that defining those rewards will define the general strategy of the AI, but if it can improve the general score, it's a good start.  Also, doesn't starcraft have a score system, with units, economy, etc? I think that could be useful data for the AI.",True
@kylesorg,2018-08-02T23:08:27Z,2,"Now i'm just learning this stuff through you.... but you mention a rule based system is what your trying to avoid.... isn't star craft a rule based game, even for humans.... when it always makes sense to have at least 50+ workers in the beginning, shouldn't you tell the AI that, then maybe taper the weight on things based on game time?  Zelots in the begging, and better units more weight later in the game.  or is it better to pretend this is a baby and it has to figure out how to walk without daddy holding him up?   could you explain the reasoning?  thank you so much for your videos I'm learning a ton!",True
@shreeyaksajjan1200,2018-08-02T22:20:49Z,0,"Amazing to see, as always. Please do keep up the great work!  I've been interested in working on StarCraft and have been following this series. Unfortunately, I'm caught up with other projects at the moment. I'd love to help out - I could try out some different things on my GPU for you. I have a single 1080ti. Would that help?",True
@Keirp1,2018-08-02T21:59:23Z,1,Why not use rl,True
@andrewm4894,2018-08-02T21:54:23Z,0,Could you try something like lstm cnn to try take advantage of the history and treat it more like a video?  https://machinelearningmastery.com/cnn-long-short-term-memory-networks/  Or maybe add some metadata about game history to the cnn https://stackoverflow.com/q/47819191/1919374,True
@caiodallecio,2018-08-02T21:22:22Z,7,"Hummm try playing against your own ai, that guarantees that you will have training data in the beginning, then move to in game ai's from there",True
@opokusm,2018-08-02T21:14:15Z,9,"Without color it is extremely difficult or even impossible for model to learn what kind and how much of units/buildings it already has, so it cannot make good decisions. Since enemy color is different, model has no issue to learn to defend nexus. Best option would be to use colored circles or even separate input channels for different types of units/buildings.",True
@user-ze7cb2kb1c,2018-08-02T21:04:49Z,0,Top,True
@jphoward86,2018-07-31T17:29:04Z,44,"I personally think you need to have a better loss function than win/loss, as beneficial actions are lost in the noise. Is it possible for you to extract 'hidden' data from the game like your net worth (minerals + army cost etc.) vs that of the opponents and aim to maximise that throughout the game, so individual decisions can be rewarded and penalised? This wont be used for decision-making of course, that would be cheating, but just for loss calculations.",True
@fuba44,2018-07-30T21:12:24Z,7,"How about go back and make the  episode 12 version gray scale, and see if it turns out bad, if so you know the gray scaling is bad and you  should change the 14 choice version back to color.",True
@stekim,2018-07-30T18:54:51Z,2,hang in there! looking forward to see how you're figuring it out. *edited * nm about the sounds,True
