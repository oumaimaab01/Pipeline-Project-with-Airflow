author,updated_at,like_count,text,public
@jeffrojas8535,2021-12-27T02:28:50Z,0,"Hello Sentdex, can you give a hints where i shold start to search. this is my story. I have a project in the project user can post image or text. now other user like the post and wanted to share the post to her own or him wall just like facebook. please and thank you,,",True
@THE-MNG,2021-02-09T09:49:35Z,0,wow 1M sub,True
@Dtomper,2020-11-09T12:22:08Z,0,Thank you,True
@sammraz1990,2020-07-28T12:00:28Z,0,thanks god,True
@TXfoxie,2020-05-27T18:25:13Z,0,Amazing. Love you. You make parsing so easy to understand.,True
@rahimzahi1883,2020-05-17T23:53:56Z,0,Thank you bruh â¤,True
@mistertech2536,2020-03-21T11:46:21Z,0,Great Bruh,True
@lokeshbhirud4918,2020-03-06T13:25:15Z,0,how to replace spaces using symbol in python using regular expressions,True
@big_cheese2162,2020-02-15T07:07:17Z,0,I tried this script on a different URL and got a Forbidden 403 error...do some websites block parsing via script?,True
@frenchyfred1310,2019-11-23T03:13:51Z,0,"@sentdex Hi there!! Thanx for your great tutorial! I'm a newbie on python and programming in general and I have a problem right now that's kinda like what you show here. I've extracted a table from a website (using the api) and the results come in text (csv). I get around 20 different statistics (it's sports-related) and I only need 3 of them. So I would like to eliminate all the data that I don't need and just get those 3. Would you recommend the same Library modules (re and urllib) or another module to do that? As I said, it looks to be the same kinda thing you're showing here, the difference being that I need to basically remove stats instead of text when I scrape it and just get the one I need. Thanx again for your great tutorials!!",True
@hastibozorgi9173,2019-09-02T13:50:41Z,0,"Hi, Thanks for these series of tutorials.l am new in this field and need help.I'm trying to write a code for scraping several pages of web and don't know how should I start? I tried several times but hadn't true runâ˜¹I hope u help meğŸ™",True
@suhasnm8206,2019-07-15T10:16:37Z,0,how to save that file you have extracted ?,True
@logomoniclearning6680,2019-07-10T19:15:49Z,0,"how do i get the full playlist, it's not in the  user's profile. infact it is a totally different person but I want this guy!",True
@chengyaozheng8536,2019-06-20T06:39:57Z,0,import re what was re tho? I'm trying to recall this part now and I can't remember what it is.,True
@hamahawlery7194,2019-05-10T23:17:57Z,0,"it does not print any thing in terminal i maybe know because ""eachp""",True
@braker37,2019-02-12T09:46:20Z,0,"data = urllib.parse.urlencode(values) data = data.encode('utf-8')  These two lines. You assign different values to the same variable. How does that work?",True
@Usammityduzntafraidofanythin,2019-02-07T01:37:00Z,0,So how does regex code work exactly?  Is it one after the other?  Would '.*?' yield different results than '*.?' or '?*.'?,True
@alexlasareishvili126,2019-01-12T18:54:24Z,0,"Thanks for your video.  I have one question.. instead of specifying the sample URL in the code, would it be possible to make it via input?  what I mean is, I work with web based tools that contain same data fields with different values of course.. like support tickets lets say.  I want script where I can paste my ticket URL and then to be parsed for specific fields like ticket number, customer name, etc and populate the excel table with the parsed data  I have lot of tickets to deal with sometimes and opening all the URLs in separate tabs is just not an option so I'm trying to consolidate everything in excel file (for now) to quickly see which ticket is in what state, when they are scheduled, etc.",True
@shubhamnagalwade4642,2018-11-27T06:27:20Z,0,"@sentdex  hi, when i used another url i got error. HTTP error 405: Method not allowed",True
@richardc9325,2018-11-27T02:32:56Z,0,How would i do this in django?,True
@hoodedwarrior8956,2018-09-16T06:57:30Z,1,"It may work on p but for scraping useful stuff like links  it gets tricky especially if you wanna get the href and also the value inside the tags. I did use a library for that before but now I wanna try without.  EDIT: nvm, doing a second findall on the result of the first for further filtering does it.  Also you could use those url results to traverse through all the results and filter those as well... hmm Thanks, good tutorial.",True
@niteshjaiswal9694,2018-09-08T12:10:45Z,0,please process json data using urllib and string slicing,True
@soldiergaming2722,2018-08-12T22:38:31Z,0,"content =  [] paragraphs = re.findall(r'<p>(.*?)</p>', str(respData)) for eachP in str(paragraphs):   content.append(eachP)   sentence = ''.join(content)  * This just cleans the output a little more so you are not reading in like a downwards fassion",True
@ricky1wdv1,2018-08-06T18:55:34Z,0,Thanks for the vid. Can anyone help me on how to send username and password to handle an authentication popup to automate it in chrome?,True
@nikunjparmar4256,2018-07-25T18:25:58Z,0,You are awesome!,True
@voidbeats5485,2018-07-17T21:46:05Z,0,"@sentdex  values = {'s': 'basics',           'submit': 'search'}  I have tried to put some other links but It does not work, it only works with the link that you posted",True
@jamesjemima7737,2018-07-06T22:55:28Z,0,"Instead of importing urllib.request and urllib.parse individually, is it possible to just import urllib as a whole library? In the same respect, since in the last vid you said you mostly only use re.findall() , can we just import re.findall instead of the whole re library module?",True
@TheEng1neerIPT,2018-07-05T20:53:23Z,0,This is AWESOME! Thanks a lot!,True
@andreaspapadakis8097,2018-07-04T17:29:11Z,0,"Hi, great video!  I just have a question, when you do this it doesn't save the webpage as ""Complete"" but rather as ""HTML, only"". Is there a way to do Complete using urllib?",True
@rohan1427,2018-06-24T08:02:49Z,0,"how can you do this with ""google"" i am not able to achieve this with google. it's just blank after execution.  but i'm curious to read the para data or any normal English data in the Html source code of google.",True
@EliasPaiFilho,2018-06-12T19:23:34Z,0,"Hello sentdex  I have a url generated by django and requires authentication via Cookies.  My django view is:  @ajax_login_required def list_todos (request): Â Â Â Â all = all_svc.list_todos () Â Â Â Â return JsonResponse ({'all': all})  My kivy project is this: https://gist.github.com/EliasCounter/7a4481ddbe481579f852ce59b15f881f  Even knowing that kivy is not the subject of your channel, I'm posting because I do not say where to ask for help,  Below are the views of the django project:  @csrf_exempt def login (request): Â Â Â Â username = request.POST ['username'] Â Â Â Â password = request.POST ['password'] Â Â Â Â user = auth.authenticate (username = username, password = password) Â Â Â Â user_dict = None Â Â Â Â if user is not None: Â Â Â Â Â Â Â Â if user.is_active: Â Â Â Â Â Â Â Â Â Â Â Â auth.login (request, user) Â Â Â Â Â Â Â Â Â Â Â Â log_svc.log_login (request.user) Â Â Â Â Â Â Â Â Â Â Â Â user_dict = _user2dict (user) Â Â Â Â return JsonResponse (user_dict, safe = False)   def logout (request): Â Â Â Â if request.user.is_authenticated (): Â Â Â Â Â Â Â Â log_svc.log_logout (request.user) Â Â Â Â auth.logout (request) Â Â Â Â return HttpResponse ('{}', content_type = 'application / json')   def whoami (request): Â Â Â Â i_am = { Â Â Â Â Â Â Â Â 'user': _user2dict (request.user), Â Â Â Â Â Â Â Â 'authenticated': True, Â Â Â Â } if request.user.is_authenticated () else {'authenticated': False} Â Â Â Â return JsonResponse (i_am)   @ajax_login_required def add_todo (request): Â Â Â Â all = all_svc.add_todo (request.POST ['new_task']) Â Â Â Â return JsonResponse (all)   And finally this is the projectileono gist:  https://gist.github.com/EliasCounter/7a4481ddbe481579f852ce59b15f881f",True
@tirthrajmahajan2508,2018-05-21T18:02:00Z,0,"Here you go - I have created a program so that the websites that are blocking you from getting in will not bother you anymore! Just change the url  import urllib.request import urllib.parse import re try:   url = 'https://hypixel.net'   headers = {}   headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'   req = urllib.request.Request(url, headers=headers)   resp = urllib.request.urlopen(req)   respData = resp.read()   headers1 = re.findall(r'<h1>(.*?)</h1>', str(respData))   paragraphs = re.findall(r'<p>(.*?)</p>', str(respData))  except Exception as e:   print(str(e)) #Headers for eachH in headers1:   print(eachH) print('\n') #Paragraphs for eachP in paragraphs:   print(eachP)   print('\n')",True
@helloadventureworld,2018-05-05T20:35:50Z,0,"Hi and thank you for the great tutorial. I have extracted my paragraphs as you said but inside the <p> tags there are so many <a href=""....."">some stuff in between</a> !!  I want to some how delete the <a href=""....""> junks as well . I don't know how in your work u didn't face them :D  let me know if you have any comment on this.  thanks in advance for all the great videos you have uploaded for everyone. ! :)",True
@indiansoftwareengineer4899,2018-03-26T03:30:22Z,1,"@6:10, what if if don't use regular expression ""?"", as already we are using ""*"", which says zero or all, then what is need of ""?"" ?. Please explain.",True
@Myview_Aravind,2018-03-25T19:44:32Z,2,Could you explain how to parse HTML data which has two columns and have to go via login authtentication system,True
@sauronseye294,2018-01-18T14:52:12Z,1,"Just created a Wikipedia Search ----------------------------------------------------- #CODE import urllib.request import urllib.parse import re  search_query=input(""Enter Keyword to search for: "") url='https://en.wikipedia.org/wiki/'+search_query  resp=urllib.request.urlopen(url) respData=resp.read()  parsed_data=re.findall(r'<p>(.*?)</p>',str(respData)) save_data=open('parsed.html','w') for eachP in parsed_data:     save_data.write(eachP) save_data.close() print('SUCCESS')",True
@problem4643,2017-12-24T15:47:51Z,1,Help me plz when i run the program it gives me that error   AttributeError: module 'urllib' has no attribute 'encode',True
@walkerward2291,2017-12-21T05:14:26Z,0,Awesome videos! Keep it up,True
@jagmohanyadav5629,2017-12-15T15:27:53Z,0,"recommendable contribution, appreciate your effort to teach others",True
@fepifig,2017-11-29T19:27:49Z,0,thanks!!!!,True
@whistler6318,2017-11-26T18:11:39Z,0,Thank you for taking the time to make these videos... You are a great teacher,True
@finnbuhse4775,2017-11-04T17:33:45Z,1,very good but how to integrate the fake id info so you can get into google with this?,True
@computerprogramming9947,2017-10-15T06:10:40Z,0,I'm trying to parse a page where the paragraphs don't have closing paragraph tags i.e. </p>. I keep coming up empty. Here's the page: https://www.ocf.berkeley.edu/~abhishek/chicmath.htm  I can get the parser to work for <h4>(.*?)</h4> beautifully because the header tags on the webpage are complete. But the author of the page didn't include a closing paragraph tag on any of his content.  What would be the solution to that?,True
@babylonaliking,2017-10-05T02:36:35Z,0,"Write a Python script to search for flight tickets from google flight website. We notice that google flight link is like this https://www.google.com/flights/#search;f=SAT;t=LAX;d=2017-10-07;r=2017-10-11 You can create strings by changing â€œf=SATâ€ to any five airport codes (e.g., SAT for San Antonio, ATL for  Atlanta) and changing â€œt=LAXâ€ to any destination airport codes, and send request using webbrower module. You can change departure date and return date as well.  Send at least 10 requests to the website and get the ticket information, and print out ticket information using Python.    Please solve this question for me and send it on starlov@gmail.com",True
@bharath9190,2017-08-26T10:42:20Z,0,"Usually all give intoduction on single page website, what about the website which had 100 pages in it?? Try to make tutorial on it!!!",True
@weratebikes6406,2017-08-25T03:12:03Z,0,what happens of there's no closing (</p>) tag on the page?,True
@jakeambrose4294,2017-08-19T19:27:02Z,10,been watching entire series. no clue whats going on lol. hope i can make my own tutorials one day,True
@mahfuzshahin6597,2017-08-05T18:58:53Z,0,supper boss,True
@jaideepbommidi8611,2017-07-30T16:26:56Z,0,"Hi,  Great vedio. Wonderful explanation.   I have a small doubt.  I have to copy the website url which is currently opened in a browser using a python code instead of manually copy pasting the URL.  And assign it to the URL variable.  And use the code which is given  by you in this vedio.  Please help me with the code to copy the URL using the python code.  Regards, Jaideep.",True
@pulkitgupta8575,2017-07-24T10:03:42Z,0,make detailed lectures on url,True
@RagHelen,2017-06-14T04:30:25Z,0,"What if I want to look not for paragraphs only, but also for headlines and lists?",True
@veve2348,2017-06-03T21:12:53Z,0,"i tkink with this regex you can only find paragraphs  without any arguments like class or id and it also read eventualy paragraphs without anything inside. : r""<p.*?>(.+?)</p>"" ?",True
@hailunxie5075,2017-05-31T09:37:07Z,0,"paragraphs=re.findall(r'<p>(.*?)</p>',str (respData)) TypeError: 'str' object is not callable Could you tell me what's wrong? Thank you so much",True
@zadfcer,2017-05-24T14:01:51Z,0,"Traceback (most recent call last):   File ""C:\Users\i3 1470\.thonny\projects\expressoesregulares2.py"", line 14, in <module>     paragraphs = re.findall(r'<p>(.*?)<p/>', respData)   File ""C:\Users\i3 1470\AppData\Local\Programs\Thonny\lib\re.py"", line 222, in findall     return _compile(pattern, flags).findall(string) TypeError: cannot use a string pattern on a bytes-like object",True
@personsname0,2017-05-22T09:42:00Z,0,Awesome videos dude!,True
@matthewmatthee1932,2017-05-09T09:03:38Z,0,"awesome video, many thanks. Could you possibly do a series on building APIs ?",True
@manasaradhyamattam6383,2017-04-23T19:11:37Z,0,could you please do a tutorial of decorator functions of python,True
@GreatFalls18u,2017-03-30T01:34:13Z,0,Would you consider a tutorial for parsing websites that require username/password authentication?,True
@The_Wizard_Zoo,2017-03-24T08:40:49Z,0,"Thank You for the Content! This is as close to the Matrix ""Kung Fu Download"" as learning gets.",True
@pikkantt,2017-03-08T06:29:16Z,1,What if I need a paragraph that is within a table? or If i need to read a table within an HTML data?,True
@ryanocampo7465,2017-02-09T19:20:44Z,0,what if you want to parse  one paragraph tag rather than all of them?,True
@therammync,2017-02-07T19:54:54Z,0,"Too fast, too fast.. slow down... please",True
@alishaaneja2291,2016-12-30T09:53:05Z,3,"In this regular expression, r'<p>(.*?)</p>' , if i remove the question mark(?), then also it will output the same strings. So what is the objective of adding the '?' anyway?",True
@sunilkumarreddy1,2016-12-25T15:11:46Z,0,"Hi, thanks for your tutorials, i got solutionÂ for one of my issue from your video.",True
@playnationstation4942,2016-12-07T02:09:56Z,0,with this website's api  I get b'errorcode=0\r\nsessionid=e5e55b04-88ed-456a-b9b0-1eefb0b76919\r\n' returned and I need the sessionid to post new request.  what the best way to get and use that session id in new request?,True
@hackerbf8160,2016-12-05T02:30:17Z,2,Where can I buy a clothes as yours with python icon on it?,True
@vishalverma9018,2016-11-18T13:20:28Z,0,Can you plz help me with this code  We will provided with URLS of 25 websites in a text file each URL in a separate line. Most of the URLs will be sports specific (dedicated to a particular sport) and some are not. Your program will access the websites and for each URL provide a result saying which URL is specific to which sport and if it is not sport specific the result will say NA.,True
@VeniVidiVici3587,2016-11-03T17:46:36Z,1,"I watched your URLLIB tutorial but could you explain me the thing with the values? I mean, when I change the website to scan, your values ""s"" : ""basics"" etc don't work. How should I modify these? Thank you in advance",True
@MrSollyb,2016-09-25T02:51:55Z,0,"I need to write some code to get some real estate values from the asking price vs estimated tax assessment, so two dif sites then compare to find a deal. I'm a realtor trying to help people out any help would be great!",True
@Learnin2Shuffle,2016-09-11T03:24:42Z,0,"isnt the values variable, and subsequent use of it, unnecessary for what we are doing?",True
@HelloImNoob2323,2016-09-07T09:58:05Z,0,"Thank you so much for your help! I have a question for you if you don't mind. Using your code I was finally able to get python to pull the info I needed from a html page. Now that I have it displaying a list using the 'Findall' function, I want to be able to use it to make decisions about what else to copy.  I currently have my 'FindAll' function set to a HTML tag that will always return a number from 0 to 200. My goal is to make it so any number it returns above 8 will also return the items name from a different, yet corresponding HTML tag.  I know how to set up the if/and/or clauses, but where I am getting stuck is how to tell python to choose the right HTML tag that corresponds with the correct number (instead of just giving me the first on the list). This is because each item has it's own version of the same HTML tag.  What is the best way to correlate HTML tags together? In example: each item has both <div class=""name""> and <div class=""quantity"">. How can I make it so any of the ""quantity"" tags returned above 8 will also give me the ""<div class=""name"">"" of that specific item? I only want the names of the tags with a value above 8, the rest can be skipped.   Preferrably, I'd like it to list out all items above 8 with it's corresponding name.   I hope that makes sense. Thank you! Sorry for bugging you, If you cover this in any vids please let me know and I will gladly watch. Thanks for the guides, you are a great help man!",True
@jacobrose9606,2016-08-23T13:30:50Z,0,"""sentdex.... I owe you one."" #Sentdex.Subscribers += 1",True
@allaalzoy2010a,2016-08-09T13:58:23Z,0,"Hi,  Thanks for your explanation  I have a question:   When I wanted to print the respData like you did , I didn't get the results even thoughÂ there  was  no error!",True
@eduardmart1237,2016-07-04T21:40:18Z,0,Great tutorial? Can you someday make a tutorial how to make this on c#?,True
@TEsvv,2016-05-07T20:26:07Z,0,"in russian language, please)))))",True
@LastPlayerOrigin,2016-04-03T13:21:20Z,2,How can I do this in Python2.7?,True
@BhoomikaChauhan97,2016-03-31T08:33:42Z,0,what if i want to crawl certain specific links on a webpage?? how do i use reg exp then? can u please help me with the syntax? Links within a website provided there ia a certain reg expression i use to crawl specific links & not all links,True
@maggincracking8174,2016-03-26T08:44:17Z,0,"Hey, have you thought of making a tutorial using beautiful soup for parsing?",True
@MrCrazyLinux,2016-03-17T14:17:37Z,2,I'm so curious to see your dog,True
@emadaldin,2016-02-12T07:57:36Z,0,Nice I use python 2 but you did great job cause I didn't really understand how to use the re lib in python till now thanks  Â just question do anyone really send ya btc donations :D if they do and it work I might try asking my viewers to donate lol :D,True
@igors.7515,2016-01-24T22:23:27Z,0,"Not sure if it is relevant, but what is meta search? Is it also based on scraping the data?",True
@tianfangliu8409,2016-01-11T07:24:55Z,0,"thanks a lot for the tutorial! can i ask how to show the pictures through re.findall()? instead of <p>and</p>,what should be typed in? is there anyplace where i can see the details about all of this?",True
@vishvips,2015-12-30T03:41:48Z,0,Thank you for the Python video. Can you please add a video for SOAP/REST API calls and xml parsing?,True
@Met201101M,2015-12-25T18:30:17Z,1,"Thanks for the video. It just gives me bunch of ""None"".",True
@MrNicfeller,2015-12-15T15:51:33Z,36,"Thanks for your little message! : "" Programming is a superpower.  Programming allows you to achieve and accomplish things that no ordinary human-being ever could, at amazing speed. Programming enables you to increase your production and performance nearly infinitely and it can grow exponentially. Programming allows you to extend your logic and your will out of your body and into the world in a way that only programming can achieve through the use of machines. It can become an extension of your self. Programs work while you sleep, they work while you take a vacation. They continue working while you work on something else. Programming has allowed me to start up and scale multiple businesses, almost completely without any direct help. Programming has allowed me to work for myself. I choose when I work, what I work on, and what I learn about. Programming has given me freedom. Others gave it to me. I want to give it to you. """,True
@Byggarebobo,2015-11-29T16:47:26Z,1,I got something quite different but atleast I have superpowers now :O,True
@ChameleonKodi,2015-11-07T00:33:43Z,0,"Hi great video i have a problem when i am printing out my re.findall statement     Category = re.findall(r'<Category .*?>(.*?)</Category>',str(Response))   the output i get is []  but i can print the source code any help would be much appreciated  and thanks again for some great tutorials :) Great Work",True
@universe-j,2015-10-30T09:56:46Z,0,cant i parse encrypted site??,True
@weiry6922,2015-09-27T11:43:47Z,0,"Thanks a lot for all your work. I recently took the plunge into Python and these videos have been such a huge help.  I have a question. I want to make a script that will post to a forum that I visit (nothing spam just for my own learning), but to do this I need to log in to my user and fill in a captcha. The forum also uses cookies. I was just wondering what I would look at learning and a rough idea of what I would have to do to start being able to do this sort of stuff.  Thanks a lot again sir.",True
@PeterDo1222,2015-09-23T10:53:05Z,0,"Hi, many thanks for the great tutorial. I tried to code along with you but I couldn't get the results. I m guessing because you already change the website so the link ""http://pythonprogramming.net/?s=basic&submit=Search"" doesn't link to a webpage with paragraphs anymore. I did some modifications to my code and found this webpage that actually had some texts for parsing, and I got just the expected results.  You might want to inform viewers so that they won't be disappointed when they don't get the results.  Anyway, my code is:  import urllib.request import urllib.parse import re  url = 'http://pythonprogramming.net/about/super-powers' resp = urllib.request.urlopen(url) respData = resp.read()  para = re.findall(r'<p>(.*?)</p>',str(respData))  for each in para:     print(each)  which gave me this beautiful result:  Programming is a superpower.  Programming allows you to achieve and accomplish things that no ordinary human-being ever could, at amazing speed. Programming enables you to increase your production and performance nearly infinitely and it can grow exponentially. Programming allows you to extend your logic and your will out of your body and into the world in a way that only programming can achieve through the use of machines. It can become an extension of your self. Programs work while you sleep, they work while you take a vacation. They continue working while you work on something else. Programming has allowed me to start up and scale multiple businesses, almost completely without any direct help. Programming has allowed me to work for myself. I choose when I work, what I work on, and what I learn about. Programming has given me freedom. Others gave it to me. I want to give it to you.",True
@AnkitBindal97,2015-07-11T07:29:12Z,0,"Amazing work Harrison . Although i have an issue . I followed your code on my idle (same code) , but in the end I am not getting paragraphs , just the same old HTML stuff .Â  I checked your website homepage and it was all the stuff except the footer of your website . How should I get the paragraphs ?? . I am not registered to your websiteÂ  , can that be any issue ( don't think it is but just inquiring ). Please reply asap .",True
@exoice3582,2015-07-09T02:48:26Z,1,Amazing. I was doing this in such a complicated way with list comprehensions before. thanks so much :),True
@robinfrancis9914,2015-07-01T08:47:32Z,0,great tutorial.. Just one thing.. Change that intro.. Those high frequency noises in the intro are really annoying!!,True
@cherry_blossom6172,2015-06-24T06:51:49Z,0,"Hi Harrison, can i you ask you a question. sorry post junk in your arena. I am getting lots of href. There is one or more i am interested to add to my base url. How to do use regex to fetch you all these ones. JournalEntries.dox?method=view&amp;number=JE-00000728 Â  Here are output: <a href=""JournalEntries.dox?method=view&amp;number=JE-00000728"" id="""">JE-00000728</a> <a href='javascript:downloadTansactions(""JournalEntries.dox?method=downloadTransactions&amp;number=JE-00000728"");' id="""">[download]</a> <a href=""AccountingPeriods.dox?method=view&amp;id=2c92a0f949efde7f0149f05314e640b6"" id="""">Jun 2015</a> <a href=""ChartOfAccountsSetting.do?method=edit&amp;id=2c92a0fb43812a1a0143980f213b7e34"" id="""">Accounts Receivable</a> <a href=""ChartOfAccountsSetting.do?method=edit&amp;id=2c92a0fb43812a1a0143980f21507e39"" id="""">Sales Tax Payable</a> <a href=""JournalEntries.dox?method=view&amp;number=JE-00000727"" id="""">JE-00000727</a>",True
@seanpollitt6819,2015-06-18T16:12:55Z,0,I followed your code but I keep getting the forbidden error...Do i need to add the headers from the previous video? why didnt you get the forbidden error?,True
@9aditya4,2015-06-06T10:27:14Z,0,in the above code we parsed the text paragraphs from the source. but how we can get the values of the variables used in a source code. for eg. i want to extract the temperature of my city using yahoo weather Â .,True
@kartoffelkonigreich1751,2015-05-02T23:48:36Z,1,"I keep getting an error:  Traceback (most recent call last): Â  File ""D:\Python\lib\urllib\request.py"", line 1182, in do_open Â  Â  h.request(req.get_method(), req.selector, req.data, headers) Â  File ""D:\Python\lib\http\client.py"", line 1088, in request Â  Â  self._send_request(method, url, body, headers) Â  File ""D:\Python\lib\http\client.py"", line 1126, in _send_request Â  Â  self.endheaders(body) Â  File ""D:\Python\lib\http\client.py"", line 1084, in endheaders Â  Â  self._send_output(message_body) Â  File ""D:\Python\lib\http\client.py"", line 922, in _send_output Â  Â  self.send(msg) Â  File ""D:\Python\lib\http\client.py"", line 857, in send Â  Â  self.connect() Â  File ""D:\Python\lib\http\client.py"", line 834, in connect Â  Â  self.timeout, self.source_address) Â  File ""D:\Python\lib\socket.py"", line 494, in create_connection Â  Â  for res in getaddrinfo(host, port, 0, SOCK_STREAM): Â  File ""D:\Python\lib\socket.py"", line 533, in getaddrinfo Â  Â  for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11001] getaddrinfo failed  During handling of the above exception, another exception occurred:  Traceback (most recent call last): Â  File ""D:\Programs\Python\test and dev\Test\Test2.py"", line 11, in <module> Â  Â  resp = urllib.request.urlopen(req) Â  File ""D:\Python\lib\urllib\request.py"", line 161, in urlopen Â  Â  return opener.open(url, data, timeout) Â  File ""D:\Python\lib\urllib\request.py"", line 463, in open Â  Â  response = self._open(req, data) Â  File ""D:\Python\lib\urllib\request.py"", line 481, in _open Â  Â  '_open', req) Â  File ""D:\Python\lib\urllib\request.py"", line 441, in _call_chain Â  Â  result = func(*args) Â  File ""D:\Python\lib\urllib\request.py"", line 1210, in http_open Â  Â  return self.do_open(http.client.HTTPConnection, req) Â  File ""D:\Python\lib\urllib\request.py"", line 1184, in do_open Â  Â  raise URLError(err) urllib.error.URLError: <urlopen error [Errno 11001] getaddrinfo failed>",True
@MaxPicAxe,2015-04-12T14:30:23Z,0,"Whats s:basics, submit:search?",True
@TommyRehnert,2015-02-17T20:57:48Z,2,"my output is just a bunch of regex (""input type=submit value=Register>\n\t\t\t\t\t\t</form>\n\t\t\t\t\t Â \n\t\t\t\t</div>\n\t\t\t Â </div>\n\n\t\t\t</div>\n\t\t...""). my code is exactly as above and i am running python 3.4 so was just wondering what i can do to actually get the paragraph output instead of this random source code?Â ",True
@e.wilson8958,2015-02-02T20:39:09Z,0,Quick and easy. Thanks!,True
@alemazzuca,2015-01-13T02:22:41Z,0,"Hi again. I am having some difficulties to extracting data from tags that have classes, or id names. Can you post some links with references to that issue? Thanks,",True
@EricZimerman,2015-01-04T21:00:00Z,0,"How would you take attributes into account? For example, a <p> tag could contain a style attribute, e.g. <p style=""xxx:yyy;"">. The point is you can have a pretty broad set of potential tag/attribute permutations. ",True
@Grassmpl,2014-11-22T07:58:44Z,0,"I think in this context ? Enforce lazy matching, not 0,1 reps. Ex. If the string have more than 1 p tags. The () part will not just include everything from the first open p tags to the last closing one. i.e no p tags will be matched in the middle of the re.",True
@alemazzuca,2014-10-27T17:46:00Z,0,"Between paragraph tags there are special characters as \\r\\n\\t that I don't want to include in the parsed text, can I eliminate them during the parsing process?",True
@vtcruzr,2014-08-01T16:11:35Z,0,"These are awesome tutorials! Thanks so much for sharing them. It would be great to have one on parsing data out of tables. Also, how do we get data from a table that is generated dynamically and therefore does not have HTML code produced? Thanks!",True
