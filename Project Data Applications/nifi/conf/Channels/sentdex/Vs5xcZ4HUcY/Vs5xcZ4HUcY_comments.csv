author,updated_at,like_count,text,public
@Bladermishal10,2023-12-04T13:45:18Z,0,bro pt 4,True
@drewgi7543,2022-12-04T00:16:05Z,0,"Have you considered thinking about actions instead of words? Like, imagining yourself to move your left hand vs right hand. You would be more readily accessing the motor functions of your brain.",True
@Larry321ness,2022-08-07T01:55:40Z,0,I have just bought a headset and am so glad this series exists,True
@streameant,2022-01-02T01:55:36Z,0,You know what could you do? you could set three sensors that they measure what's happening and then you just linked the signals with what you're thinking and therefore we can know what you're thinking just based on the data of your signal. And then you communicate rapidly with the computer and it gives you instant feedback,True
@DanielWilday,2021-10-27T22:25:37Z,1,"I just stumbled upon this, now slightly old, video series and it's really great.  I have a question (which may never get answered as this is so old) While I know the BCI FFT data has the shape of (16, 60)... for a linear interpolation of this wouldn't you want to swap the axes so that the new shape would be (60, 16)?    I'm new to Machine Learning so I'm likely wrong here.. but in my limited understanding a Conv1D kernel travels linearly along the x axis which would represent time (or in this case frequency), with the y axis acting like channels.  If that's the case I'd assume you'd want to travel along the waveform of a single FFT, across all channels.    Again, I'm new to this and ok with being wrong.  Any clarification on this would be super welcome.  Thanks!",True
@TroubleMakery,2021-08-18T22:19:27Z,0,Thinking about at getting one of these. Any other BCI recommendations or is the openBCI still the best?,True
@starviptv6544,2021-05-31T03:19:49Z,0,üé¨ Great Tutorials üíæüíªüìö,True
@TheRelul,2021-03-19T20:40:49Z,1,well these were 80 minutes well spent,True
@lokthar6314,2021-02-18T12:01:03Z,3,"Thanks for this video series, I'd wish you continued this and showed maybe a demo of you moving boxes with your Jedi power",True
@jond532,2020-12-29T04:01:08Z,0,Maybe instead of perfecting its ability to predict if we are thinking left or right...we should change how we actually think about left or right? Maybe if we found certain thoughts that the ai found easy to predict we could use them as a way of triggering left or right. Basically test different thoughts and see what is most easily detectable and substitute them for actually thinking left or right. Use an ai to find the most effective thoughts and label each thought.  like making a mental language.,True
@readdaily5680,2020-12-22T11:18:16Z,1,Can I follow along with 8 Channel?,True
@realityheadquarters4956,2020-12-08T00:33:27Z,0,Did you try thinking left or right in different spoken languages?,True
@LastVoyage,2020-12-02T02:18:44Z,1,Waiting for part 4,True
@Praxss,2020-11-25T17:40:19Z,1,Pls continue,True
@jtdyalEngineer,2020-11-03T14:23:38Z,0,"Think left, none, right, none, repeat... Then train the model to look for the change?",True
@iiWicked,2020-09-06T01:49:40Z,0,"I did a quick search through your videos and this one seems to be the last on the subject but I still have some thoughts on how to proceed.  Someone mentioned simplifying the data to a binary task. My immediate reaction is to do the opposite and introduce more data. Like training the feeling of not wanting to move right, not left, and not moving at all. Also doing training while being distracted or thinking left while watching the block not move or move right. I'm wondering if this might help the data differentiate between moving and not moving. I'm not an expert in the field or anything, just putting down some thoughts I had.",True
@Mohamm-ed,2020-07-23T12:41:28Z,1,Please more videos about BCI you are amazing,True
@nictanghe98,2020-05-31T13:13:45Z,0,PLz don't use for lame stuff like directions and start using it to code with your brain.,True
@maxitube30,2020-05-09T22:10:59Z,0,no more video here?,True
@julianray6802,2020-04-28T17:45:47Z,0,"Hi there Sentdex. Think all your videos are brilliant! With BCI, I was listening to a podcast (https://www.abc.net.au/radionational/programs/allinthemind/brains-old,-new,-and-augmented/12063992) where a quadraplegic patient managed to control a formula 1 car with a BCI and used a novel approach. Rather than thinking 'go left ..go left' etc, would think of eating icecream for turning left ie activating very different brain regions which would probably improve your signal to noise ratio....would be interested to see if you could replicate this technique. Thanks again for your great shows!",True
@ThisIsHatman,2020-04-16T16:31:33Z,40,Drop a like if you're still waiting for part 4.,True
@Evan-bv8bh,2020-03-30T20:25:42Z,0,Hi! Is there anyway to replay EEG data using eeglab/ bcilab as well?,True
@juusokorhonen1628,2020-02-21T08:20:02Z,0,"How are you doing the 'thinking of moving to the right/left'? Could it be that you're missing the visual response of the thing actually moving to the right? And I've been wondering could we use the EEG signaling of when you're thinking 'nope, it is going in the wrong way' as the error for the model, and then continuously train the model.",True
@ab185,2020-02-03T16:04:00Z,2,Need Part 4!,True
@rushibhatt1201,2019-12-13T18:11:24Z,0,Inspirational,True
@vladislavg7929,2019-12-01T21:29:26Z,1,https://github.com/Sentdex/BCI   Don't even thank,True
@IAmOxidised7525,2019-11-25T15:50:22Z,0,"Nice !!!! This is cool... I would get high model the behavior ,  then find the music that creates the same trip....  Can we do that ?",True
@connormilliken8347,2019-11-24T23:06:28Z,0,"Hey Sentdex,   Just wanted to say I've always really enjoyed your videos, so much so that I reference your channel in the book I just wrote. It was released recently. It helps to teach beginners and intermediate level programmers learn Python and some Analytics libraries. Feel free to check it out. You can find the shout out on page 322.   Thanks!",True
@blackadam7078,2019-11-23T09:05:55Z,4,"So, my question is, how many cups do you really have?",True
@mantasr7715,2019-11-22T11:37:17Z,0,"Sorry if you already touched on this, but wouldn't it be simpler to just train on just 'right' and 'none' as I imagine left and right would have more similarities as as opposed to 'go' and 'stop'.",True
@pranjal86able,2019-11-22T01:08:03Z,0,you are amazing!,True
@ThePinkoe,2019-11-21T22:26:18Z,0,You can do zero-phase fft if you are set on doing fft's. I worked on a project almost exactly like this but with motor imagery instead of some abstract 'left or right' and the largest problem for me was what you call the 'none' class. Try transfer learning if you have not already and definitely try wavelet transforms --> scalogram for your images.,True
@anhnguyen-ik6dj,2019-11-21T03:23:55Z,0,"hello, I learned about the Ddos attack on python3 whit module urllib.requests, I wrote a program similar to Hulk.py but using an extra Proxy, you can give me more ideas with this moudle to write the Ddos program is better than Hulk, or you can make a Ddos Video with this Module, I look forward to it. Please answer me, thank you!",True
@tanelhelmik,2019-11-21T03:15:02Z,0,"What if you took only images of left and only have objects going left or turning left to train the left data and then switch all the data type to right for the right data.  I think having something move right when you are thinking left would make you think your thought left is not the correct ""left"".",True
@gomenaros,2019-11-19T19:14:43Z,0,You do everything I wanna do... Awesome,True
@chriscatino5950,2019-11-19T16:49:58Z,26,Me: I wonder what happens if I stack a blueberry and strawberry poptart and eat them together.  Sentdex: *building a brain computer interface with python*,True
@memoai7276,2019-11-18T23:44:33Z,0,While doing some research I came across this paper which I thought might be of interest in this project. https://www.ncbi.nlm.nih.gov/pubmed/31331897,True
@alexandremarcotte7368,2019-11-18T22:47:11Z,1,I started a GUI to visualize data Live from OpenBci if someone wants to branch from it:   https://github.com/AlexandreMarcotte/PolyCortex_Gui,True
@alexandremarcil498,2019-11-17T19:34:32Z,0,"Hi Sentdex, nice project!  Moving a mouse with your hand will not contaminate the EEG signal with EMG. To capture EMG, you would need EMG electrodes on your arm to capture the electrical activity of muscles moving your arm.  This electrical activity is not recorded by the EEG (maybe only the intention or signal coming from the brain telling the arm to move, but that is not EMG).  Your heart beats regularly (I hope), yet an EEG does not capture EKG data.  To do that, you need electrodes positioned close to the heart, or on both arms.  Moving your eyes or blinking your eyelids will give rise to some EMG artifacts, as these muscles are close enough to the electrodes on your head to be picked up (and also the fact that the EMG signal is much stronger than the EEG one).  Hope this clarifies things a bit.  What you are saying about EMG doesn't make sense.  Thanks for sharing your code and data!",True
@MrBoubource,2019-11-17T15:08:25Z,0,"I was thinking that you could apply some sort of error correction code to keep only the most predicted output over a small period of time (the simplest implementation would be the most common over the last 4 guesses). That will add a little lag but the experience might be more enjoyable.   I don't know if there are many error correction codes for 3 state information unit (here left, none and right, instead of 0 and 1 for bits) since their main purpose is to correct errors in binary data..",True
@castme.z,2019-11-17T15:08:21Z,0,Hey i need a quick help,True
@kristoferkrus,2019-11-17T15:01:21Z,0,"Maybe your brain patterns are actually changing the more you use the device and do the right/left/none ""exercise,"" hence causing a ""drift"" in the patterns. You could perhaps try adding an extra input parameter for every example, which is the total time you have been using the device for, and see if that explains some of the uncertainty in your predictions.",True
@ethandickson9490,2019-11-17T14:30:39Z,1,"Consider the common spatial patterns approach, https://en.wikipedia.org/wiki/Common_spatial_pattern , is used specifically for this task and is relatively simple to implement. Using wavelets, as mentioned, is a also a good idea.",True
@rextlfung,2019-11-17T07:35:50Z,0,"Hi Sentdex!   Great video as always, what you're doing is really interesting. What would you say are the programming/math concepts I need to learn before doing stuff in this video? I am studying neuroscience and am really interested in self-learning how to interpret brain signals to create useful outputs.   Thanks!",True
@tarmiziizzuddin337,2019-11-17T03:49:58Z,3,"Actually i'm doing research on BCI specifically on decoding movement intention using deep learning, your channel is very informative, thanks for the effort!",True
@Jeacom,2019-11-17T01:52:07Z,1,"Please try driving a car with your thoughts.   I mean a game car, not a real one.",True
@willdawiz0,2019-11-17T00:28:27Z,31,"Cool mug! I have a bit of experience in EEG and ML, specifically learning to classify sleep phases -- you've chosen a super difficult but interesting problem. Not sure where you'd ultimately like to take this but a few recommendations:   1. agree with others that wavelets might be best transform 2. absolutely should be using a time window of data, you're throwing out a crucial aspect! brains are dynamic systems, how things are changing is arguably more important than their current value state, preprocessing into frequencies doesn't mean you can't take advantage of the time-series nature of the data.  3. 'Left' vs 'Right' is intuitively simple but thorny given what you're getting out of an EEG, these mental concepts are distributed all over your brain in different activity patterns, and they're actually pretty similar concepts. If you're willing to change your task but want to stick to 3 classes, you could try rest and two wildly different concepts that will evoke different brain regions and rhythms, even emotions -- MRI has way more spatial/temporal resolution and still benefits from this: ( https://www.theguardian.com/science/2010/feb/03/vegetative-state-patient-communication ) . Otherwise I saw a suggestion to play flappy bird with it on the last video, and binary event classification is way easier, as you've deduced. You can use this principle even in multi-class, training multiple binary classifiers and then using your favorite ensemble/arbiter method. 4. Definitely don't watch the computer / graphics output as you create training data unless you've really thought the experiment through, Neurofeedback does some cool stuff in a therapy setting but might corrupt your ""Left"" thought with ""the square is going right!"" thoughts 5. I'm a bit out of practice in conv nets but consider a way bigger model / param number, or generally get creative with architecture, you're trying to predict the output of a biological neural net with something like quintillions of parameters Loving the series so far, good luck ;)",True
@eitanas85,2019-11-16T22:58:05Z,0,"Hi,  Could someone please refer me to Sentdex videos, where he explains how do you choose which model to use, how many layers, what activation functions are proper for the problem etc.? It would be of great help!",True
@name1483,2019-11-16T19:13:20Z,0,Use VS code,True
@xiubinzheng7,2019-11-16T17:33:13Z,0,how do you like recording on Ubuntu vs Windows?,True
@ab185,2019-11-16T16:06:05Z,0,Is it possible you have too many input channels and that‚Äôs creating accidental noise? Maybe try a sample of some channels (e.g. sensors over Broca‚Äôs or Wernicke‚Äôs Area) and see if the ability to predict accurately improves?,True
@maxlee3838,2019-11-16T15:57:30Z,0,"You need to check out HTM, dude. It‚Äôs naturally resistant to noise.",True
@devgupta9469,2019-11-16T15:21:43Z,0,@sentdex please continue with pytorch tutorials. Concepts like RNN and LSTM,True
@wktodd,2019-11-16T14:48:56Z,0,Thinking about this (always a dangerous thing to do :-)) ... How about combining the sequential FFT sets in to a 3D cascade diagram (Freq/Time and level [colour]) this would combine the FFT sets and give temporal info to the CNN,True
@diasakishev8897,2019-11-16T12:58:48Z,4,Just imagine him watching at the monitor and all those graphs were going down. And he realizes that he is dying. Sry. 2 o'clock in the morning.,True
@allurbase,2019-11-16T11:55:16Z,0,I'd do a sequence with the hidden representation of this.,True
@teneshvignesan6227,2019-11-16T10:54:52Z,0,where can i donate for you to keep up this series?????????????????,True
@TheHackysack,2019-11-16T10:14:27Z,0,"It's about time you released another video that I'm not _quite_ ready for. Side note: Four months ago, your Python tutorial series helped to give me the head start I needed to quickly pick up programming finally. Now I'm obsessed with coding. Like, I don't do anything else anymore. I used to be miserable, damnit. Now I feel as if I need to do something productive with my life now. So, like, thanks, I guess, for giving me a purpose. Asshole.",True
@kobaltauge,2019-11-16T10:02:47Z,0,"I love this series. Thank you very much. Since the first episode I'm asking me, how do you think of ""left"" and ""right"" do you ""say"" the word. Do you visualize the word? Do you think, you move your body or head to the direction. Do you look in the direction. All this actions could create different waves. It could probably help to have the same visual reference when collecting the data, so your thought of the direction are the same. And when ""predicting"" you could mentally recall the visual reference and it probably would match better.",True
@qwisacz,2019-11-16T06:14:32Z,0,"Just some thoughts. I believe that thinking ""left"" might not be enough, how about writing a simple game where you move some character with keys and generate data from it (pressed key may be a classification result). This correlation might be useful when taking some predefined delay into account (as human reactions are not instant). Great series, keep up the good work :)",True
@DanielVeazey,2019-11-16T04:51:34Z,0,I used to have a computer.,True
@vonderart,2019-11-16T03:57:12Z,0,Very cool and great work! I did something like this a few years ago integrated with the servicenow platform and device was called Emotiv a 5 channel EEG around $300 ML was/is build in the SDK it was coded in Java nowadays you can use Red-Node https://youtu.be/fdpKAHlztZQ,True
@ahmadanis9930,2019-11-16T01:35:13Z,0,‚ù§Ô∏è,True
@masternobody1896,2019-11-16T01:25:33Z,1,Thanks for knowledge sentex,True
@danieladelodun9547,2019-11-16T01:17:24Z,2,"I wonder if it'd be helpful/possible to train the computer to know when you're thinking 'ohh yes, that is what I wanted to happen'",True
@lautarodapin,2019-11-15T20:33:06Z,0,are you filtering low frecuency fft and higher ones?,True
@x1expert1x,2019-11-15T20:24:56Z,1,"I wanted to do this shit so baaaad, I'm trying to convince my Computer Club at my university to invest into a brain-ECG reader, it would get so many more people to try out machine learning.",True
@angrymurloc7626,2019-11-15T20:14:21Z,0,"In response to your point, that this should be applicable for people with a disability: Unless fully paralyzed for a very long time, even people with a disability will have some neural connection from brain to muscle, and since you wouldn't be learning from the data at the muscle itself, that shouldn't be an issue.  I would suggest the following points, which I'm sure would make your BCI more effective, for any person using it: Try to see the BCI as some imaginary limb, for which you have to learn movement, like you would with an arm or a leg. DON'T try to equate verbal thought to movement, as that is nonsensical from a neural standpoint. Your neural representation of the word left is very complicated. For example, since it's a direction, one of the closest associations with the word 'left' will be the word 'right'. Your data will be very hard to read if you deliberately mix it. Also try to think about how to continually generate data. Like for example, you could first try to learn the difference between correct and incorrect, by simply generating data with headset keyboard and BCI output, and having a model learn, what your neural representation of failure would be. If you have that, the next step is only to set the loss of the model to the amount of failure that your brain detects, and you can have the BCI adjust to you while you use it. Obviously you'd need to stop to let the network train for a while, but I'm pretty sure that the results will be much better than what you're getting here.  All of this information is partly taken from the ted talk 'can we create new senses for humans' which is kinda the reverse of what you're doing here. It will probably be very a interesting watch",True
@atrumluminarium,2019-11-15T19:42:06Z,0,"I'm still half way in so apologies if you say something about this, are you planning to experiment with recording data from other people (i.e. a friend or family member that you can bring over to get readings over a couple of sittings)? In principle this should increase variety and make the data less anecdotal  Also maybe consider hand selecting which parts of the brain you read from if at all possible. For example the cerebellum is responsible for motor signals to the muscles so to eliminate that as much as possible one might want to experiment with not mapping it. Same goes for the other parts of the brain.",True
@xijinpooh7226,2019-11-15T19:08:48Z,2,Not related: have you ever done a series on text generation?,True
@semiclean,2019-11-15T18:13:40Z,1,"@sentdex, Is there a way to do FFT rolling averages ? in your first video you shown us that what was happening on diff√©rent time scales was totally different. We could see the curve going up or down for a specific channel once the time series was averaged.  Also maybe start recording your FFT when you are in a 'thinking about nothing' state, if your wonderland of a brain can be in such state ;-), and then think left for 5 minutes and stop recording your FFT and see if there if the general FFT superposition of all channels looks different or if there is any response from specific channels.  That's the best video series so far. I was in the middle of a computer vision notebook, I dropped everything to watch it !",True
@macarrony00,2019-11-15T17:36:06Z,0,"I am not sure how the brain network works for people without an arm for example. But I think people that lose an arm can still try to move it, the brain signals generated should be the same as before, so pressing buttons on keyboard or move a part of your body shouldn't be a problem, but a solution. Those people already got the brain control to move their new arm.",True
@darnokjarud9975,2019-11-15T17:22:01Z,105,"Hi Sentdex !   First of all, thank you for this truly amazing series !  I've just started my PhD where I will be exploring deep learning techniques in the context of creating EEG-based BCI controller. My background is in biomedical engineering so I'm quite familiar with medical signals like EEG and I want to share with you my findings in the topic of EEG-BCI. Maybe I'll start with some example:  https://www.ijcai.org/proceedings/2019/0192.pdf - here's a paper where the authors used modified GAN to create images out of EEG data. They've used some extra tricks to get some pretty amazing results (by nowadays standard) - like additional information to the loss function. They've also created the EEG MAP - it's basically and image that combines time and EEG channel data into single image which should ""represent"" the cognitive domain.   https://www.crcv.ucf.edu/papers/camera_ready_acmmm_BNI08.pdf - here's another example of exactly the same task done with both GAN and Variational Autoencoder.   And because I don't want to spam you with links - there you have a survey on different approaches at different tasks in the context of EEG-BCI - https://arxiv.org/pdf/1905.04149.pdf , maybe you'll find some of them usefull and if not at least you'll get an interesting reading ;)  In the video you've mentioned that you don't have any idea if model trained on your EEG will work on somebody else - to my knowlage of the latest studies in this topic I've only seen one constant argument, that deep learning based BCI systems have the property of: model per person (sadly) - so in this domain there's plenty of room for improvement ;)    This particular problem is actually the case of my study - I'll try to create methods that will help deep learning models generalize beyond the scope of a single person in the task of reconstruction the visual stimuly :).    Well, that was a long one. Thank you again for work sir and I wish you all the best !   Keep up the good work !",True
@thedosiusdreamtwister1546,2019-11-15T17:07:40Z,1,You might get better data if you show yourself flash cards. There's tons of good research that says the same neurons are activated in the same sequence whether the stimulus is internal (think left/right) or external (respond left/right) and it should be less affected by the fatigue you described.,True
@mathematicalninja2756,2019-11-15T17:04:33Z,0,This is unbelievable man hats off,True
@danielf3623,2019-11-15T16:54:42Z,22,Still recommend using stationary wavelet transforms on the raw data instead of FFT.  FFT doesn't give you relative phase (unless you're pulling the real/ imaginary values too) and has nonlocal artifacts since it has to do an FFT of a certain length block of time series data that isn't weighted.,True
@phillipotey9736,2019-11-15T16:34:47Z,1,Stop using FFT or increase the fft sample size. Our brains are way ahead of our reaction/conscious thinking time. I'll look at gh later but interested,True
@CrimsonTheOriginal,2019-11-15T16:23:43Z,4,"You should add a key-logger to your system that timestamps keystrokes and then aligns them with your readings. I'm an Analytics engineer, I specialize in time series data in manufacturing, but I would love to talk to you about some ideas I have around this.",True
@wktodd,2019-11-15T16:15:04Z,0,"So, there's no temporal content to your data? just instantaneous fft  . I wonder if that is missing the actual thought process , which by the nature of wet neural networks is slow. ???? And, as a matter of interest how would you present temporal / sequential data to a NN?",True
@brubrudsi,2019-11-15T16:13:29Z,0,Hey Sentdex I sent you an email idk if you saw it,True
@blitzer658,2019-11-15T16:12:32Z,3,"Yo Sentdex I'm finding that 90% of the YT ""software"" community is filled with uninspired unmotivated idiots who lack passion for what they do all they do is complain about their job whilst making unrealistic day in the life videos where they flex on you hoping their YT channel well become their new career  so, my question is what youtuber do you recomend that ACTUALLY CODE AND SHOWCASE COOL PROJECTS  and not just mediocore fromt end tutorials",True
@gm4984,2019-11-15T15:49:09Z,1,"What if you used a visual program that randomises the position of a cube (btween left,right,up,right) you follow it with your eyes and the program learns from that. Should I explain it better?",True
@raghavendrg3475,2019-11-15T15:33:27Z,15,Sentdex it won't be as simple as just plugging in fft and expecting a good output because the data itself has a heavy amount of noise. Your preprocessing must be solid. Just doing notch doesn't work. You should look at PCA and ICA to preprocess your data and then try to train a model. You have to go through artifact removal process and also the problem you have started with is not a beginner friendly one.  Also instead of FFT you should use wavelet transforms.  Can you also please upload the raw data and not just the FFT data,True
@LikeAndFavBF3,2019-11-15T15:19:37Z,105,"Please continue this video series, it is amazing and truly an inspiration. Great job!",True
@thetechegg8859,2019-11-15T14:39:29Z,0,Hiiiiiiii,True
@manukhurana483,2019-11-15T14:32:28Z,9,I was Just Watching your video and then suddenly notification pop new Video Uploaded.,True
@mr.beelzebub888,2019-11-15T14:29:52Z,0,hmmmmmmm,True
@memoai7276,2019-11-14T03:30:05Z,5,"Do you use any automated hyper parameter optimization? If not, do you  use any method for hyper parameter testing?",True
