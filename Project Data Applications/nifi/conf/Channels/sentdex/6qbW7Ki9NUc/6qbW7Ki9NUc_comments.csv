author,updated_at,like_count,text,public
@michpo1445,2024-05-08T14:59:53Z,0,please remember to include a requirements.txt file with packages and specify the exact version number of python when releasing source code. Thanks,True
@robivlahov,2024-02-24T23:11:30Z,0,"make it move legs horizontaly so front left right rear at the same time , same as bd does it with spot",True
@sefahinti5012,2024-01-23T08:23:38Z,0,I preordered my Rohorse ü§£,True
@Tamingshih,2023-10-09T11:28:29Z,0,Thank you!,True
@SoumyaMaitra2008,2023-06-24T05:30:27Z,0,Just loved it!! Can you DM me your contact details?,True
@Joe_Zajac,2023-05-14T11:21:21Z,0,DDPG algorithm on bipedal sim initially looked like John Cleese‚Äôs funny walk on Monty Python‚Äôs Flying Circus,True
@erdenkalilayev3877,2023-02-10T23:15:58Z,0,"wow so good man  but i cant understand one thing why did you use your pc as hardware as a source of power to train while we can use Google Cloud where our robot can learn walking, it seems that it's not that simple as it looks when we decide to choose cloud-based trainings, am i right?",True
@nsgoneape9899,2022-10-21T03:55:06Z,0,you are a beacon of hope and knowledge. Thank you,True
@hayoun3,2022-07-03T00:38:08Z,1,"Sentdex : And... I'll call this ""Walking training framework."" Google : What does it stands for? Sentdex :",True
@Veptis,2022-02-21T13:46:55Z,1,"I am kinda scared to look at the maths of backpropagation with multiple continuous outputs. Like what shape of data does the loss function even take?  I feel like a delta over just one step might not say enough. Perhaps you need to take a delta over a few steps,",True
@aadarshkumar2257,2022-01-20T21:28:57Z,0,"Will the Neural Networls from scratch in python series continue ? When the next video in that series will come. Please clarify this matter !",True
@HellTriX,2022-01-18T22:53:50Z,4,"I think the biggest problem with all these deep learnings, is that the reward is to basic. For example, a high reward just for reaching a goal is inadequate. Adding additional rewarding for how efficiently the servos are used, reward for standing up more straight and using a wider range of servo motion. When a kid learns to walk, they are reinforced over time for how long they can stand, how fast they can get there, and additional tasks like reaching and overcoming obstacles for treats, etc. So can we really blame the reinforcement models when we don't even reward them for how visually appealing they are, or how efficiently they use their ranges of motion?",True
@codecampbase1525,2022-01-06T01:40:34Z,0,"The beauty is how everyone is guessing as no one knows the right answer. ML and AI is such a Great topic, but it needs  ethic laws/guidelines and a moral approach to prevent our downfall in the longterm.  His book is great to grasp the math behind it, even if you are already familiar with the concept / cs related topics.",True
@joshfoulkes5327,2022-01-02T22:00:54Z,0,"How would you get a robot to walk without machine learning, are there any guidelines or would you just go for it and adapt?",True
@bennguyen1313,2021-12-17T01:46:08Z,0,Any thoughts PerceptiLabs?,True
@chris-graham,2021-12-07T00:44:05Z,4,"Few suggestions: Current public state of the art is PPO. PPO can be used with either discrete actions or continuous actions. If you choose to use discrete actions, use discrete delta actions with n buckets of modifiers. Your batch size is very small. Try something around 50k-100k environment steps & minibatches of about 4k or 8k with a rollout length of ~200-400. Actor critic networks are EXTREMELY sensitive to updates so you absolutely have to use large batches. Large batches reduce the chance of getting stuck in local minima. As a last resort, you can add an LSTM into the network since walking does have some multi frame state dependency. Your network size is appropriate. Try increasing learning rate if you haven't. 5e-4 is probably a good starting point. Use 0.01(default PPO) for entropy and decay to 0.005 (or lower) over time. I haven't checked if this is how you've built your critic, but these types of problems like one fully connected value function output to one tensor connected to last layer of your network (before action outputs) Add your previous actions as features into your observation space. There's no ""rush"" to finish the episode so perhaps add a reward that decays with time to encourage speed. There's a bajillion other things you can change, but that's the joy of reinforcement learning :)",True
@fuba44,2021-12-06T20:46:13Z,0,"I'm ""like"" number 1000, just saying... I'm pretty special.",True
@swannschilling474,2021-12-04T10:17:12Z,0,"Hey, just came back to this episode and I was thinking that reducing the available joint angles could also be something that changes the outcome quiet a bit... Also, since you did a bit with Game Engines in the past. Did you ever take a look into Unity? ML Agents and Articulation Bodys seem to be something that might be fun to explore? I would love to see you trying it out! üòä",True
@erbterb,2021-12-03T15:11:50Z,0,"I am seeing this with the thought:  Is this not what movement would look like if you have no muscle fatigue and no account of the moment arm around a joint?  I am certain we would all be scooting around like limbo dancers, if we did not have to account for lactic acid buildup.  Send me the genius prize from 311 trillion when you add these features to the model environment variables.",True
@oplavevski,2021-12-02T08:46:08Z,0,"Another route to try may be to reduce the jerkiness. When doing these micro movements, there's lots of jerk happening. Do a reward system based on the lower number of jerks per second. If that works...",True
@Nedwin,2021-12-01T17:05:49Z,0,"Wow, amazing!",True
@robomasticus,2021-12-01T16:15:05Z,0,It's the ministry of silly walks!,True
@K1RTB,2021-12-01T14:07:50Z,1,‚ÄûUpdating back to stupidity‚Äú is what I do every night.,True
@tomaslapes5722,2021-12-01T09:54:28Z,0,Maybe punish the robot when the body exceeds some tilt threshold( in order to keep the body sort of leveled),True
@elishashmalo3731,2021-12-01T09:53:24Z,0,You mentioned that you used NEAT to test your input values. Has your stance on NEAT changed since - https://youtu.be/ZC0gMhYhwW0 ? Bc in that video you seemed a little uninterested/disappointed in NEAT. Any new insights?,True
@patham9,2021-12-01T04:29:50Z,0,Cool work! On the other hand it's also interesting how RL fails to be practical. Meanwhile legged robots operate with Model Predictive Control based on first-principle engineering.,True
@aloufin,2021-11-30T15:49:43Z,0,"All these concepts are so impenetrable when the initial research papers come out, and they are briefly discussed on HN/twitter... Thanks for explaining these concepts in a well broken down manner Harrison, only when these learning videos come out can I start to really understand how it all fits together. Your unique teaching style is reaching so many people in the world, Thank you!",True
@Reza-wd3ji,2021-11-30T15:25:57Z,4,"I recommend checking out Physics-informed neural networks. It's basically a way to make the agent ""understand"" the physical environment better. Also, Normalized advantage functions might be good a substitute for DDPG in continuous controlling problems.",True
@AJMansfield1,2021-11-30T14:32:09Z,0,"Would there be any merit to a learning approach that uses NEAT to get an initial neural network that's started to bite on the problem, and then convert that network to a dense one in order to refine it by another approach? The connections NEAT makes between non-adjacent layers doesn't map _directly_ onto a dense network topology, but it should be possible to use some of the extra neurons to proxy those earlier neuron activations into later layers. (Even if you're using a sigmoid activation function, you could still make it work by using a very small 0.01√ó connection from the origin to the proxy, and then the inverse 100√ó connection from the proxy to the destination, in order to keep the proxy's value in the linear region of the activation function.)",True
@xXKM4UXx,2021-11-30T10:48:38Z,0,You should look into deep evolutionary reinforcement leaning or try with a recurrent neural net. How do you know where you are going if you doing know where you've come from?,True
@imdadood5705,2021-11-30T09:58:59Z,0,Lollll! I don‚Äôt understand even a single thing you were explaining. I know something about RL. I just kept on watching üòÖ,True
@strange_man,2021-11-30T09:48:23Z,0,import WTF,True
@easyBob100,2021-11-30T02:07:35Z,5,"The best AI controlled motion I've seen was based on learning mo-cap data.  I've always thought that was the best way to do it:  Use motion data to have basic movement, and let the AI deal with changing that motion to adapt to the environment/control signals.  That way, the model doesn't need to be big. [/thoughts]",True
@qzorn4440,2021-11-29T23:54:05Z,0,"years ago i have seen a video on the evolution of a desk lamp to hop, some just quavered and shook, then the off spring started hopping like hop along Cassidy. Mmm... üòç",True
@uskhan7353,2021-11-29T18:34:14Z,0,Will it make it more simple if I use unity ML agents library ?,True
@judedavis92,2021-11-29T17:23:59Z,0,I think you know what we‚Äôre gonna ask. Where‚Äôs the next ep of nnfs? üòÇ,True
@liquidpromo,2021-11-29T14:33:51Z,0,frogbot,True
@ethanblackthorn3533,2021-11-29T10:58:23Z,0,Fascinating stuff,True
@swannschilling474,2021-11-29T10:01:16Z,0,This is great!! Keep it up!! I am addicted already! üòÅ,True
@krembananowy,2021-11-29T08:14:45Z,3,"Often in environments that run at 60 steps per second people run their models with action repeat - they ask their model for action, say, only every 6th step, and otherwise repeat the last action. In this case it'd be even simpler to implement, i.e. try changing kit.update() from 1/60 to some other values, such as 1/10.",True
@mannycalavera121,2021-11-29T06:49:19Z,0,"Good, now teach it to update design, 3d print itself etc..",True
@ayyazulhaq7335,2021-11-29T05:57:39Z,1,"for CarePackage in range(mint6.15):     GPU(2,RTX3090)     RAM(1,TeraByte)     CPU(32,CORE)     print(""Awesome"")     print(""Can I get one "")",True
@10xXxtailedxXxdemon,2021-11-29T05:45:57Z,0,Me: sees first line of code...*likes video*,True
@ayyazulhaq7335,2021-11-29T05:44:26Z,0,"Awesome work, I wanted to create an Robotic Dog and Robotic Hand using DQN + Evolution algorithms. Give a shot to these in future work",True
@Nugget11578,2021-11-29T04:41:47Z,3,Try including energy used in the reward. Because actual creatures generally move in ways that minimize energy usage.,True
@dhruvdwivedy4192,2021-11-29T04:16:43Z,0,Hey üëã isn‚Äôt it best to do codes in c++ for better optimisation and performance ? ü§î,True
@somethingwithbryan,2021-11-29T03:51:35Z,2,"I wonder if you have seen Robert Miles' video titled ""Training AI Without Writing A Reward Function, with Reward Modelling"". Would love to see this implemented and it might help here",True
@xXReVo_LuTiOnXx,2021-11-29T02:42:52Z,7,"Just had another idea on how you might solve the current problem. Maybe it would be possible to reward the module if it keeps the back steady. Because right now it is moving the back a hell lot through all the shaky movement. But if we reward it to keep the middle part leveld, it should actually be forced to use the legs way more.",True
@MrBoubource,2021-11-28T22:26:51Z,0,I love the word bittle.,True
@ianhaylock7409,2021-11-28T21:55:42Z,4,"It seems to me that to do a correct walking gait, you need all the legs to move through a similar range. Maybe you can reward those that have the most similar leg movements.",True
@roostertechchan,2021-11-28T21:25:23Z,1,5:15 Ministry of silly walks... funny enough from Monthy Python,True
@zrebbesh,2021-11-28T20:53:40Z,0,Is this sufficiently well-documented that I could take a neural network derived in a completely different environment and write a savefile that you could import it from?,True
@SimeonRadivoev,2021-11-28T20:32:07Z,1,"Yea I still think for a functional and robust walking from machine learning you need a lot more sensors, at the very least potentiometer sampling for each joint.",True
@saminyeasararnob1810,2021-11-28T20:16:22Z,0,"A quick try would be, (1) action repeat - repeat the same action multiple times ex 2,3,4 (2) Update the network after every episode, updating after fixed episode can accumulate more bad examples in the buffer (3) try multiple actions ex: take 10 actions using the policy and then take the one that critic thinks to be the best during training only",True
@nickrazes2720,2021-11-28T19:51:46Z,0,Please zoom in the code more next time. I watched this video on my phone and it is mostly unreadable.,True
@ramtinnazeryan,2021-11-28T19:04:31Z,0,awesome video but texts were very small to read. can you punch in a little next time?,True
@wktodd,2021-11-28T18:37:42Z,23,"Taking tips from evolution, you might base your reward on energy usage. All the short gaits will use more energy to move a given distance.  You could probably estimate energy use from loop time of the gait.",True
@vell0cet517,2021-11-28T18:13:10Z,3,"Great video! Have you considered using a genetic algorithm that remaps node connections? davidrandallmiller has a video ""I programmed some creatures. They Evolved."" that goes into his approach. If you jump to 41:10 he goes into the data structure for his genome. Thought you might find this interesting.",True
@Ian-ue8iq,2021-11-28T18:06:38Z,28,"I noticed that a lot of the ""walks"" were asymmetric, (which makes sense because the model doesn't know that the robot has any summary). So what if you were to make a model that took all of the inputs but was only allowed to control the right side, then ran it again with the inputs mirrored and apply the outputs to the left side. That way lessons wouldn't have to be learned twice. Maybe.",True
@Diego0wnz,2021-11-28T17:59:06Z,1,Import WTF is a good one,True
@neuron8186,2021-11-28T17:49:22Z,4,This channel where it all started for me 2 years ago thanks because of you I got into deep learning,True
@kopasz777,2021-11-28T17:48:00Z,84,I feel like the reward function should take into account the energy required for the movement (optimizing for better distance/energy). Sudden high accelerations take more energy than slow and gradual ones. You could probably get movement closer to what natural evolution came up with rather than slowly wiggling and jittering forward.,True
@kornelillyes2848,2021-11-28T17:38:54Z,0,"Here we go, poggers",True
@louth4740,2021-11-28T17:36:20Z,3,Did you try stable baselines 3 PPO?,True
@lel7531,2021-11-28T17:28:02Z,1,Eager to see the next while not too long from now I hope,True
@spsharan2000,2021-11-28T17:06:37Z,1,Check out stable baselines-3 zoo. It would make your training work much easier.,True
@ashu-,2021-11-28T16:56:48Z,1,7:58 exploration vs exploitation,True
@batrox9219,2021-11-28T16:54:07Z,2,i've been looking forward to this video for so long :DD,True
@444haluk,2021-11-28T16:53:44Z,0,Isn't isaac sim for ROS and isaac gym for RL?,True
@AChadi-ug9pg,2021-11-28T16:51:51Z,20,One of my heros is back at it again ‚ù§Ô∏è,True
@dasoulking4190,2021-11-28T16:51:16Z,0,hmm,True
@shmarvdogg69420,2021-11-28T16:49:58Z,1,So when u gonna deploy 1000 of these in a park somewhere to spread the robot dog love?,True
@CommunistBearFighter,2021-11-28T16:48:43Z,0,first,True
@Stinosko,2021-11-26T20:06:07Z,3,"To prevent the shaking of the leg: maybe devide the end reward by the total movement of the leg.    Keep track of eveytime the servo switches direction and the end reward is divided by the total movement switches. If the robot learn the shaking movement, it wil get rewarded for shaking less with the less while it is in a good start position to learn the ""normal"" dog movement? üòä",True
@Stinosko,2021-11-26T19:52:36Z,2,"When in doubt, you can download even more ram ",True
@Stinosko,2021-11-26T19:51:59Z,1,Very NEAT video! ,True
