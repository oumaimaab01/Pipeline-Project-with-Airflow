author,updated_at,like_count,text,public
@chickenkid3242,2024-03-31T15:54:27Z,1,"Will someone help me fix this error:  File ""qLearning.py"", line 22, in <module>     dicrete_state = get_discrete_state(env.reset())   File ""qLearning.py"", line 19, in get_discrete_state     discrete_state = (state - env.observation_space.low) / discrete_os_win_size ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.  It takes place around 4:26 in the video.",True
@muhammadawais581,2024-03-14T13:30:59Z,0,Interesting to watch but  i didn't understand how its happening ðŸ˜‚,True
@JonathonCwik,2024-03-12T20:15:21Z,1,"How would someone replay a specific episode? I took the frame count of every ""winner"" and stored the fastest copy of the q_table into a variable, then reran the rendering loop (see code below) but the frames it beat it in always comes in lower than the replay frames.  The ""replay"" at the end of the program I set up:  print(""Playing best episode: "", fastest_episode[0], "" with "", fastest_episode[1], "" frames"") done = False render = True env = gym.make('MountainCar-v0', render_mode='human') discrete_state = get_discrete_state(env.reset()[0]) frame = 0 while not done:     frame += 1     env.render()     action = np.argmax(fastest_episode[2][discrete_state])     new_state, reward, done, truncated, _ = env.step(action)     done = done or truncated      if done:         break      new_discrete_state = get_discrete_state(new_state)     discrete_state = new_discrete_state  print(""final frame: "", frame) env.close()",True
@washyb,2024-02-20T21:50:54Z,0,"To anyone currently stuck, try observation,info = env.reset() and discrete_state = get_discrete_state(observation) instead of directly passing env.reset() into get_discrete_state.",True
@homataha5626,2024-02-19T03:39:44Z,0,Why is it he?,True
@ammarshahzad9627,2024-02-12T01:30:27Z,0,"If anyone is doing it in new gymnasium, in order to selectively render it you can env = gym.make(""MountainCar-v0"", render_mode=""rgb_array"") and the following to render it every 2000 episodes.  if episode%2000 == 0:                    img = cv2.cvtColor(env.render(), cv2.COLOR_RGB2BGR)             cv2.imshow(""test"", img)             cv2.waitKey(50)",True
@chakradharreddy6437,2024-01-22T08:02:04Z,0,It's taking way toooooooooo much time even with this code,True
@philipkierkegaard3408,2024-01-06T15:49:51Z,0,"At 8:55, can anyone explain to me why adding the (action, ) will access the exact q-value of that state?",True
@presente9501,2023-10-22T18:37:59Z,0,"If you are doing this in gymnasium, you need to pay attention because env.reset() return 2 arguments.",True
@Ohnomcrose,2023-09-01T09:32:02Z,4,"Did anybody got ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part and solved?",True
@abdulrehmanmohsin7893,2023-07-29T06:48:55Z,1,I have used the same code but the agent just doesn't solve it. Using the parameters in the video it doesn't even solve it once in the 20000 episodes. I experimented with the parameters and the best I got is an avg reward of around -198 after 20000 iterations. Maybe this has something to do with the version of gym I was using?,True
@HaiderAli-nm1oh,2023-07-19T18:27:33Z,0,"i dont understand , how did your loop did not break on episode 0 ,1 ,2 .. and so on ? and prompted (""we made it on episode : "" )  ,because when it completes the goal the episode ends right ? , done== True ..",True
@rahulsinghgulia6666,2023-07-05T01:44:31Z,1,"I am getting this error: discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE ValueError: operands could not be broadcast together with shapes (2,) (4,) .. ""I have gone through the comment section for some help, but I could find anything useful. Kindly suggest guys.",True
@pavfrang,2023-07-02T18:18:01Z,0,Fortunately VS code came out. Sublime is for the martyrs.,True
@nexovec,2023-06-10T13:57:09Z,2,"ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.  It doesn't work, because why should it. Why am I, again, the only person for which it doesn't work, who also thinks it shouldn't work, but it should work and does work for everyone else. And the debugger doesn't work. EDIT: I reimplemented everything myself and I will have a video about this, since this one needs a refresher.",True
@arunesh_u,2023-05-20T01:03:24Z,4,"For anyone getting the following error:  discrete_state = (np.array(state, dtype=object) - env.observation_space.low) / discrete_os_win_size                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~ TypeError: unsupported operand type(s) for -: 'dict' and 'float'  It's because 'state' is a tuple of an ndarray and a dictionary, replace this line: '(np.array(state, dtype=object) - env.observation_space.low)' with this line '(np.array(state[0], dtype=object) - env.observation_space.low)'  Here, state[0] is simply accessing the ndarray. I don't fully  understand the purpose of the dictionary yet. However, the output will not be the same as that in the video for which index you reach on your Q-table",True
@starship9874,2023-05-14T21:05:05Z,23,"For anyone doing this in the new gynmasium library, here is what you need to adapt in the code: there is no .render() method necessary anymore, you now pass the rendermode in the constructor e.g. env = gym.make(""MountainCar-v0"", render_mode='human'), so to selectively render / don't render, you need to create the environment in rendermode or not rendermode every episode, also the return type for the step function changed with it now returning truncation and termination as seperate variables, so you can call new_state, reward, termination, truncation, _ = env.step(action) and then done = termination or truncation",True
@krzysztofdymanowski8759,2023-05-06T19:18:46Z,0,"Is anyone else getting an out of bounds error? I copied and pasted the code from the site but everytime the second action in the new_discrete_state is something like -60 or -70, so it is out of bounds of the q_table.",True
@mingmingchen1925,2023-01-27T16:10:08Z,0,"The video is great and helps people to get a gist of Q-learning quickly. I just never expected I would start to laugh during the watching. I hope I can have a presentation like sentdex did one day. So relax and easy. Make audiences feel so relax and easy, too. Thanks man!",True
@sharmakartikeya,2022-12-11T08:23:27Z,0,"I think you haven't actually implemented the exploration - exploitation process while taking the action, even though you specified the epsilon value and decay.",True
@aryaashutoshpathak4849,2022-11-29T09:59:03Z,0,"in the while loop, we never set done = True. How does it ever break out of the loop to a next episode then?",True
@marcusdudebro9426,2022-11-03T09:12:24Z,0,is he implementing epsilon greedy q-learning exactly in this tutorial?,True
@gatewayadam6275,2022-10-13T01:54:43Z,5,"running gym 0.26.1. Code would not break episodes at 200 steps. The addition of the truncated parameter from env.step() changed the logic required to reproduce the model. Created a truncated = False variable under the done = False variable and used ""while not done and not truncated:"" and ""if not done and not truncated"" to replicate the 200 step behavior.",True
@nature_through_my_lens,2022-10-11T11:15:08Z,0,Why does my system take a lot of time to run it and frames are slow like everything runs in slow motion? Is it because of the library upgrade of Gym to 0.26.2? Your's look very smooth when it's running. I have a 10th Gen Core i5 machine with 8 gigs ram.,True
@konijntjeo1515,2022-10-09T03:30:03Z,12,"Following in 2022, in get_discrete_state, use discrete_state = (state[0] - env.observation_space.low) / discrete_os_win_size",True
@dahampter3844,2022-01-25T11:58:36Z,3,"I dont really understand the get discrete state function, can you please help me?  More specifically: discrete_state = (state - env.observation_space.low) / discrete_os_win_size",True
@user-fs7wc3me8o,2021-08-15T03:48:42Z,0,"My guess on why this has worked well without epsilon is that you have been punishing ""suboptimal"" choices for the agent on each state until the first random sequence led it to first succesful completion and kickstarted the learning.",True
@underlecht,2021-07-21T10:00:54Z,0,"For action selection - do we have to do argmax or sample from action probability distribution, probably softmax + sample?",True
@SameenIslam,2021-06-05T14:48:26Z,6,"I think there's an error as the epsilon term has been declared and it is decaying, but the agent does not use it for explore-exploit tradeoff. To fix this issue, I would do this in the loop:  while not done:         if np.random.uniform(low=0, high=1) < epsilon:             action = np.random.choice(env.action_space.n) # explore         else:             action = np.argmax(q_table[pos][vel]) # exploit         new_state, reward, done, _ = env.step(action)          new_pos, new_vel = get_discrete_state(new_state)         if render:             env.render()",True
@38820,2021-04-17T05:47:33Z,0,can you tell what is the logic behind choosing 25000 iterations and why cant i choose 1000 ? ..what effect it has ?,True
@alessandrorenna1222,2021-03-03T10:27:10Z,0,"If any of the state is equal to its high value, wouldn't the corresponding discrete_state be 20? If so, that's out of the size of q_table. I think discrete_os_win_size should be calculate dividing by DISCRETE_OS_SIZE - 1",True
@s.r8081,2021-01-25T01:32:56Z,0,"Thank you, man, you saved my life wkwk. You explained this tutorial very clearly!",True
@andrewdavies5722,2021-01-21T19:42:56Z,0,why do you reset the q-value to 0 if the agent reaches the goal? i don't think setting the q-value to zero indicates the reward was given,True
@sontapaa11jokulainen94,2020-11-21T17:01:51Z,0,Subscribed.,True
@Erroleldrich,2020-10-29T15:25:22Z,0,How exactly the epsilon making the agent to explore random states as in the code it is not used to take random action?,True
@elishashmalo3731,2020-10-24T14:20:19Z,0,why is mine not learning nearly as fast or consistently as his? I even downloaded his code to run on my computer(a mac air) and it didnt work. Is this hardware dependent?,True
@soupizcool,2020-10-03T12:45:11Z,5,"It took me a little bit to understand the get_discrete_state() function. Essentially what's happening is we have already decided to parse the observation values into 20 discrete buckets between the highest and lowest possible. The discrete_os_window_size is the step between each bucket value. By subtracting (state - env.observation_space.low) we get some value between the high and low. Then, by dividing out discrete_os_window_size , we get the i'th bucket the observation falls into. By returning .astype(np.int) it can be used as an index for the q_table. The index is the unique position/velocity bucket.",True
@AdrianFacchi,2020-09-17T13:17:20Z,0,"I watched up to 2:08 at the time of asking this question.  If the rewards are always -1 except 0 at the finish. How does the discount factor impact the Q-table or the agent? Aren't we only exclusively interested in the future reward, which is always the long term reward. What happens if we use 1 as discount factor?",True
@adamhendry945,2020-09-10T23:18:12Z,0,Love the tip that you can use underscores to semantically separate numbers by thousands! I deal with big numbers all the time! Great tip!,True
@adamhendry945,2020-09-10T23:15:00Z,0,"@sentdex As always, great video! However, at 2:15, discount rate is better understand opposite of how you said it. Per Wikipedia, ""it has the effect of valuing rewards received EARLIER higher than those received LATER"". This is consistent with complexity theory, where we want to get started quickly, then once we get on the right path, start delaying gratification/reward so we increase the time we take exploring so we force ourselves to look for a better result.",True
@decode0126,2020-09-10T11:12:06Z,1,watched this tutorial two to three times and understood what u were trying to convey keep up the good work!!!,True
@decode0126,2020-09-10T10:23:51Z,0,It is really nice of u that u are taking so much efforts  But i didn't really understood that what u were trying to convey It was just not clear,True
@Idlehampster,2020-08-20T19:31:44Z,2,DON'T LEAVE AROUND THE 24 MINUTE MARK. Addendum @ 26:39,True
@beomseokpark1500,2020-08-02T14:52:12Z,1,"What I understood the use of epsilon: The agent take an the optimal choice by either exploration or exploitation.  Exploitation means taking an action based on current Q table which has been experienced. Exploration means trying a random action that contributes to updating Q table. Epsilon greedy policy is a strategy to balance between exploration and exploitation.  If epsilon > random decimal number, the agent explores If epsilon < random decimal number, the agent exploits After all, the epsilon should be diminished. That's why Sentdex wrote this code below  if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:         epsilon -= epsilon_decay_value  But I'm still uncertain why epsilon should be diminished. I think in the end, the agent would exploit rather than explore because it has updated Q table enough and the agent doesn't need to explore because the reason of exploration is updating optimal Q table. This is just my guess and I'm still figuring out",True
@dominikklein2989,2020-07-19T16:46:12Z,0,"I don't understand, how does it learn to climb the hill for the first time if the reward is always -1? Is it purely random at first?",True
@Mikey-lj2kq,2020-06-12T08:15:10Z,0,"also double q learning is better, and i'm trying to understand zhiqing's solution on scoreboard (gave up on the maxqn stuff next to it...someone help....)",True
@kastin83,2020-06-05T13:33:16Z,1,"If you want to see how many ""moves"" it take to finish each time i've added these few lines to the code:  moves = 0 ##just above the line below in his code 	while not done: 		moves += 1 ##add this counter in the while loop then down where you print 			mov.append(moves) 			print(f""we made it on episode {episode} in {moves} Moves  best so far is {min(mov)} moves MA20 on moves = {sum(mov[-20:])/20}"" ) you'll also need to initialize a list  probably near the variables mov = []",True
@Mikey-lj2kq,2020-05-29T00:10:05Z,0,good videos but seriously use vim (still run python by setting up f9),True
@user-il5pt2qd4h,2020-05-20T14:16:44Z,0,lol mine got to the flag at 2000,True
@larrybird3729,2020-05-08T03:08:21Z,1,"9:57 I thought it was always this:  Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state]) - Q[state, action])",True
@renegadeMoscow,2020-05-07T23:17:17Z,6,"Thanks, dude for the tutorial. it's funny to see, that each next episode has fewer views :D",True
@jerinvarghese8545,2020-05-07T12:50:57Z,10,how is the epsilon effecting the model.  Even though we are  changing the value of epsilion but what effect is having on any of the parameters of the model,True
@redcabinstudios7248,2020-05-07T11:05:42Z,1,"24:50 ""Yes it's a He""_ Then environment must be ""She"" :)",True
@ardeshirmirbakhsh6345,2020-04-16T21:06:59Z,0,"why we have to define the ""for episode in range(EPISODES)"" condition? Normally as long as the task is not done ""while not done"" is True, the agent should keep trying to reach the ""goal position"". But it does not! why?  Thank you in advance.",True
@lulwat8523,2020-04-02T06:27:57Z,2,"12:15 you say that 0 is the reward is for completing things, ""nothing, just punishment"". So if it's the reward, then why are you using it as a Q-value?? I though that you would use it IN THE FORMULA to calculate the actual Q-value. But instead you just straight override the Q-value with the reward =0.. Could you or somebody clarify what is happening here?",True
@bobbysingh5666,2020-03-31T10:11:29Z,0,24:54 is there a way. to change the amount of time he gets,True
@bobbysingh5666,2020-03-31T09:36:13Z,0,"i ran into an error and cant figure it out, this is my code(i already started part 2): EDIT: lmao im stupid. the L in Low for me was uppercase instead of lower  import gym import numpy as np  env = gym.make(""MountainCar-v0"") env.reset()  LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 25000  DISCRETE_OS_SIZE = [20] * len(env.observation_space.high) discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE   q_table = np.random.uniform(Low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) #reward is 0 for openAI gym  def get_discrete_state(state): 	discrete_state = (state - env.observation_space.low) / discrete_os_win_size 	return tuple(discrete_state.astype(np.int))  discrete_state = get_discrete_state(env.reset())  print(discrete_state)  ''' done = False  while not done: 	action = 2 #action[0]  push car left, 1 do nothing, 2 push right 	new_state, reward, done, _ = env.step(action)  	print(reward, new_state) 	env.render()  env.close()''' this is the error: TypeError: uniform() got an unexpected keyword argument 'Low'",True
@jithinjacobbenjamin,2020-03-24T21:41:20Z,0,How do you create q table for two agents in an environment whose actions are based on a coin-flip and both have opposite goals?,True
@Youshisu,2020-03-23T19:50:16Z,0,"there is huuuge precision lost in epsilon, any way to make it work? Epsilon: 0.8, decay: 0.00016032064128256515 results in 0.76793... which is not even close real value, python lost 0.03 somewhere. Any solution to this numerical problems?",True
@amanbisht0774,2020-03-22T07:27:07Z,0,it shows me an  error  timelimit has no attribute goal_position,True
@piotrromanowski4852,2020-02-23T19:23:42Z,2,"Hey Man - great channel, thanks for sharing! We can add the epsilon (exploitation vs. exploration)  and a percentage of success to the code (sentdex does it at 27:07)  UNDER OTHER CONSTANTS IN CAPS epsilon = 0.001 # exploration vs exploitation 1.0 is random 0 is policy  counter = 1 while not done:     if np.random.rand() < epsilon: #exploration option           action = env.action_space.sample()     else: #exploitation if we are not exploring           action = np.argmax(q_table[discrete_state])             ......... all other code     elif new_state[0] >= env.goal_position:           print(f'we made it on episode {episode}')           q_table[discrete_state + (action,)] = 0           counter +=1           percentage = counter/(episode*0.01)           print('success rate: ',percentage)       if epsilon > 0.01:           epsilon = epsilon * epsilon_decay   Hope this helps others! Wish you luck.",True
@Si1veRCSGO,2020-02-21T20:34:09Z,1,Where is the epsilon value used? I see it being decremented by the decay value but it is never used in any calculations?,True
@AndJusTIceForRob,2020-02-21T17:53:23Z,1,"Might be helpful (I'm at about 8:50 right now) to periodically pull up an image of the Q update function to remind us of which part of the Q function we are working on at a given time (e.g., ""now we're working on current Q as can be seen here"" or ""this is where max future Q is""). That way, we can match the code to the function in our heads.",True
@neeru1196,2020-02-11T17:41:59Z,0,"My episodes are running really slow, compared to yours. Is there a reason that's happening? By the time I'm at 800, you've hit 2000...:/",True
@jorostuff,2020-02-10T18:46:15Z,1,My little sh*t doesn't want to learn,True
@mahdihosseinali7492,2020-02-05T20:11:17Z,0,"Great tutorial, just shouldn't there be a termination condition somewhere in the outer loop?",True
@IMWATCHING501,2020-01-26T19:57:26Z,3,"Lol, was about to ask why we never used epsilon xD",True
@HrishikeshVichore,2020-01-24T15:50:44Z,9,normies: watching the video and learning Me an intellectual: *Notices a new cup in every video* Damnnnn yet another amazing cup :),True
@alexgulewich9670,2019-12-28T18:57:10Z,1,"15:20 so, at this stage my AI is just going the wrong way :(",True
@alexgulewich9670,2019-12-28T18:51:29Z,0,"TBH I'm coding this on a laptop, so it takes an age to load 2000 episodes",True
@guaishouxiao1623,2019-12-17T00:32:13Z,1,Action 0: push cart to left Action 1: push cart to left to a very small amount (almost not moving) Action 2: push cart to right    Is there a parameter displaying the amount of force executed to the cart? Is the amount of force executed to the cart a random number? Thank you!,True
@mochilata,2019-12-11T18:15:00Z,1,"I was following the tutorial and got to 5:00 when I had different values. I was freaking out, thinking I typed in something wrong. It took me a bit until I realize they were random values.  Update: Actually, why is the discrete_state sometimes (6,10) and sometimes (7,10)?",True
@juliuskamau1975,2019-12-06T06:07:33Z,0,the ways you explain the concept was really good you made it easy for me to understand,True
@jcmachicao,2019-12-02T11:55:47Z,0,I think your agent dicovered inertia and gravity! The pattern is making the most of the intertia going to the other side!,True
@dankman7603,2019-11-21T10:20:11Z,5,"this is a lot like how ants move at random to search for food but once they find a source, they leave pheromone trails for another ants to exploit that information",True
@qugh3173,2019-11-15T11:25:37Z,0,I love your collection of mugs,True
@sungyoungkim4382,2019-10-25T14:13:11Z,0,How many coffee mugs do you have? I see a unique one per every video!,True
@ronrocks4819,2019-10-17T10:51:56Z,0,i am getting an error : AttributeError: module 'time' has no attribute 'clock' on the command env.render() plz help me out,True
@Rileyjamesc,2019-10-15T15:06:56Z,25,Mine just never learns to beat it Iâ€™m so confused,True
@sil3nt810,2019-10-08T10:20:17Z,0,My agent finished after 2000 Episodes Lol,True
@rcpinto,2019-10-04T21:33:56Z,1,"It worked so well without epsilon due to your optimistic initialization of the Q table. Values between -2 and 0 are way higher than the actual correct values, so argmax selects unexplored actions, i.e., implicit exploration.",True
@Nova-Rift,2019-09-07T02:37:49Z,0,Can we assign this to GPU? Can you show how?,True
@johnyfishborn,2019-09-04T14:50:53Z,1,"Why discrete_state = (state - env.observation_space.low) /.... Why not state / discrete_os_win_size ? State will always between high and low, i don't get it why we need subtract low bound.",True
@datascienceed3069,2019-08-25T19:33:01Z,3,I do not understand how the epsilon going to effect in the code you wrote.,True
@izxle,2019-08-10T15:17:11Z,0,Can you use the 'with' statement with the envs?,True
@g7-smart-logistic-app,2019-08-04T15:01:48Z,1,Doing Great...but maybe you could use PyCharm for AutoUpdates. I know you know about it but I don't know why you don't use it.,True
@ichisadashioko,2019-07-21T10:30:11Z,0,You should use tqdm for progress bar in future videos. It is pure Python and super useful.,True
@YouTubeChannel-jw5th,2019-07-20T15:12:43Z,15,25:54 that's the sound of a true nut if I've ever heard one,True
@FuZZbaLLbee,2019-07-20T05:59:38Z,0,"The window shows (not responding) so i thought it crashed, but help on Discord tought me i just had to wait. It only updates every 3000 episodes",True
@alagaika8515,2019-07-19T06:20:42Z,0,"Aren't you cheating by overwriting the reward if the goal position is reached, using internals of the scenario that are not part of the API? The mountain car scenario always returns reward -1, even when the goal is reached. By terminating early, reaching the goal before the timeout of 200 still yields a higher total reward. However, if I change the program accordingly so the only effect of getting to the flagpole is a max_future_q of zero, I do not seem to get it to find a good strategy. The payoff is too small I guess.               new_state, reward, done, _ = env.step(action)             new_discrete_state = get_discrete_state(new_state)              index = discrete_state + (action,)             current_q = q_table[index]             if not done:                 max_future_q = q_table[new_discrete_state].max()             else:                 max_future_q = 0              q_table[index] = (                 (1-RATE) * current_q                 + RATE * (reward + DISCOUNT * max_future_q)             )",True
@asimsan5507,2019-07-11T20:39:04Z,4,"20000 episodes and it still has not reached the top of the hill , is the code hardware dependent ?",True
@lakshyasinghal3372,2019-06-29T14:05:08Z,1,"I am getting an error ""ImportError: sys.meta_path is None, Python is likely shutting down"" after running the script, Please help",True
@aakarshan01,2019-06-22T13:55:11Z,0,epsilon to 0.93 and  discrete_os size to 10 and it completes on 62,True
@priyankrajsharma,2019-06-19T03:44:29Z,0,"the stupid question,, what is Q is it a reward",True
@priyankrajsharma,2019-06-19T02:28:16Z,0,could you please explain this line discrete_state=  (state-env.observation_space.low) /discrete_os_win_size  why we are subtracting with env.observation_space.low we can choose high as well.. why to divide by discrete_os_win_size,True
@dfaiezdfaiez1699,2019-06-06T20:23:41Z,2,dude you're so good! so helpful. thanks!,True
@jhgfdjhgfdhdjfjhd6721,2019-06-05T05:21:30Z,0,Thanks so much for your efforts,True
@TheSandCshow1,2019-06-04T17:52:49Z,2,"Was wondering whether or not you were gonna actually use epsilon for choosing actions , glad I watched until the end XD",True
@SamanwayGhatak,2019-06-03T18:32:07Z,4,"5:20 why is the 0th entry came out as max, supposed to be the 1st right?",True
@0969superman,2019-06-03T13:15:04Z,0,"Hello,  why not have the discount be 1 ? I don't really get the necessity to have a decaying reward over the back propagation",True
@vsmelo94,2019-06-03T00:46:56Z,0,Thanks for these videos. They helped me with a task for my AI class :),True
@abdullahbinjahed6900,2019-06-02T21:50:12Z,0,came here from code bullet,True
@lucaslopesf,2019-06-02T15:44:29Z,0,"It's have been 2 days since the last video!! Can't wait for the next episodes, loving this series, it came just in time <3",True
@johnlycott8538,2019-06-02T00:25:43Z,0,Oh so this isn't the reinforcement learning that involves whips and chains?,True
@Armaleet,2019-06-01T20:36:10Z,1,Would it not be a good idea to do a numpy.random.seed so you can better compare your runs when tweaking parameters?,True
@jackwiley9363,2019-06-01T18:24:42Z,0,why do you call them discrete states,True
@petermills1397,2019-06-01T15:05:15Z,3,Getting an error at : elif new_state[0] >= env.goal_position:            'TimeLimit' object has no attribute 'goal_position' â€¦ any help would be appreciated,True
@6197980,2019-06-01T14:59:13Z,0,"Why does introducing exploration make it find the first solution much later( considering that no positive reward is given until it makes it to the top for the first time)? Is it that sticking to a policy even though it is a random one (as Q table is randomly filled at the beginning) is Better than changing the policy randondly ( as it does while exploring), at least for this case? Thank You",True
@togousch,2019-06-01T12:24:42Z,12,"Don't miss to tell about impovements to DQN(double, dueling, noisy nets for exploration, prioritized experience replay and so on). Wish you luck with this tutorial, it's great",True
@PySam,2019-06-01T09:31:06Z,2,I'm just waiting until he combines q learning and neural networks,True
@jonathan-._.-,2019-06-01T02:52:54Z,3,shouldnt we be using epsilon somewhere ?,True
@tpyou1640,2019-06-01T02:52:27Z,0,"You have finally done it, Harrison.",True
@RabeezRiaz,2019-05-31T22:56:35Z,115,A tip for anyone using large numbers like at 2:09 (epochs etc) in Python 3.6+ (I think). You can write integers using underscores to separate parts just like you would use commas so 25000 becomes 25_000. This has saved me alot of time  because I don't have to count the digits to figure out if I have a million iterations or just a hundred thousand :D,True
@neighborsj,2019-05-31T20:20:54Z,2,I can't wait for this series! Thanks!,True
@user-co7ko3pb2f,2019-05-31T16:39:38Z,3,"Cool to see RL course, but  line 78, in <module>     elif new_state[0] >= env.goal_position: AttributeError: 'TimeLimit' object has no attribute 'goal_position' Exception ignored in: <bound method Viewer.__del__ of <gym.envs.classic_control.rendering.Viewer object at 0x7fd2357647b8>>",True
@nikhiljagtap6587,2019-05-31T16:34:33Z,3,So today I skimmed through the book by Geron and wished for reinforcement learning tutorials.  Harrison is new Aladdin. ðŸ˜€,True
@alexanderpohl1949,2019-05-31T14:35:31Z,1,Thanks for the vids,True
@ThomasPlaysTheGames,2019-05-31T14:05:44Z,7,"Before this video came out I ended up finishing the code for this demo using a shotgun approach for changing the weights. It ended up taking only around 80,000 itterations though, so there's that.",True
@new_owen1496,2019-05-31T13:54:42Z,1,Yo yo this is the CPU song!!,True
@st00ch,2019-05-31T13:51:22Z,2,"Are we going all the way to PPO or AC3 or maybe something with ""TensorFlow Probability"" during this RL series? It's asking lot I know.",True
@shmarvdogg69420,2019-05-31T13:42:39Z,1,Im earlyish,True
@scootscoot2k,2019-05-29T17:32:20Z,0,I feel like there is a show in you just reviewing agents as a sports commentator. If you were to include the distance travelled as a property of the fitness would it try to optimise  travelling as little distance as possible?,True
