author,updated_at,like_count,text,public
@shrirangmahajan9502,2022-12-11T08:23:03Z,1,For those who are watching this 2022 onwards:  We import mnist dataset now as:  import tensorflow as tf mnist = tf.keras.datasets.mnist.load_data(),True
@Faisal-jo5vk,2022-07-17T20:42:09Z,1,"im on windows 10 and couldnt import the input_data, so i wrote this:  (trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()",True
@cccloud3256,2021-03-17T17:58:40Z,1,"Hello, Mr. Sentdex.  I've been following your great tutorials till now. They are totally awesome and helpful. But I can't follow it since TensorFlow has changed very much from this part on. Like tensorflow.examples.tutorials are gone and tf. placeholder is deprecated as well. Is there any way so I can follow your courses? I am suffering from searching replacements from StackOverflow. Please help...",True
@neuron8186,2021-02-23T14:30:01Z,0,This video is good if you wanna learn implementation only thanks me later,True
@mrfrozen97-despicable,2020-12-21T17:10:36Z,0,Since tensorflow has gone through many changes. If something doesn't work you'll definitely find alternative on stack overflow.  Sentdex. Thanks. Even after 4 years. It is as helpful as it could get,True
@mozammilkarim8636,2020-09-17T06:12:59Z,0,thanks for the video . i could not understand how the inputs are given to tf.random() function.,True
@olutolavivian22,2020-08-15T23:07:49Z,0,ðŸ”¥ðŸ™Œ,True
@arppitmadankar3135,2020-08-12T16:17:44Z,0,Much respect for providing quality content.,True
@SuperIronwire,2020-07-20T07:08:04Z,0,"Hello sentdex and sentdex's fans, I am also one of fans. Now we have tf.keras in Tensorflow, if we do exactly this  example again with tf.keras. How do we do? many ways you did haven't been available  anymore.",True
@kevinziroldi3342,2020-07-11T14:14:20Z,0,Does tensor flow have a documentation? I cannot find anything,True
@janitlodha1247,2020-06-21T16:42:14Z,0,"What does [None,784] represents? and Why the first value is none? Thanks",True
@kerolesmonsef4179,2020-05-26T01:36:45Z,0,oh what are you doing ??. where did you come from all of this syntax !!!! i got confusing .  sorry but it is so much to save in your brain .,True
@ibuucoksiregar9024,2020-05-11T14:05:25Z,0,sebastian league's Neural Network tutorial has dynamic code,True
@sagesy9774,2020-05-06T15:39:52Z,0,19:46 I mean come on are we going to drop to a level where we don't even give the computer some good fucking input !XD !! YEAH FIGURE IT OUT BITCH IDK I DON'T HAVE A SINGLE CLUE,True
@vedantgoswami6531,2020-04-27T09:55:32Z,5,from tensorflow.examples.tutorials.mnist import input_data Error while importing the module,True
@oskarpaulsson5885,2020-04-13T22:03:54Z,0,"I'm doing this in 2020 and Tensorflow 2 has come out, there is no longer an attribute called placeholder in tensorflow and im wondering what to use instead, thanks.",True
@bestcasescenario6775,2020-04-10T02:07:33Z,0,"i used the following code to make the code more flexible with layers and nodes. uses a few loops. Works for now... i dont know if this is what you meant.     N_LAYERS = 3 N_NODES = 500  n_nodes = {} for i in range(N_LAYERS):     n_nodes[i] = N_NODES   n_classes = 10 batch_size = 100  x = tf.placeholder('float', [None, 784]) y = tf.placeholder('float')  def neural_network_model(data):     hidden_layer = {}     hidden_layer[0] = {'weights': tf.Variable(tf.random_normal([784, n_nodes[0]])),                        'biases': tf.Variable(tf.random_normal([n_nodes[0]]))}     i=1     while i < N_LAYERS:         hidden_layer[i] = {'weights': tf.Variable(tf.random_normal([n_nodes[i-1], n_nodes[i]])),                       'biases': tf.Variable(tf.random_normal([n_nodes[i]]))}         i = i + 1      output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes[N_LAYERS-1], n_classes])),                     'biases': tf.Variable(tf.random_normal([n_classes])), }      layer = {}     layer[0] = tf.add(tf.matmul(data, hidden_layer[0]['weights']), hidden_layer[0]['biases'])     layer[0] = tf.nn.relu(layer[0])     i = 1     while i < N_LAYERS:         layer[i] = tf.add(tf.matmul(layer[i-1], hidden_layer[i]['weights']), hidden_layer[i]['biases'])         layer[i] = tf.nn.relu(layer[i])         i += 1      output = tf.matmul(layer[N_LAYERS-1],output_layer['weights']) + output_layer['biases']      return output",True
@EverythingTechWithMustafa,2020-03-20T17:01:50Z,0,Wait did I just see a normal cup ????,True
@anonymosranger4759,2020-02-27T10:38:22Z,0,Amazing Vid!,True
@thelaughlab4166,2019-12-22T22:59:53Z,0,How about this for the dynamic hidden layers:  https://github.com/Orion-Flame204/Tensorflow-dynamic-HiddenLayer-creation,True
@jackkelly7701,2019-11-16T21:26:29Z,7,"Hey Sentdex, this code doesn't import input_data with tensorflow 2.0, how can I adapt my code?",True
@kulothungans6193,2019-10-14T05:55:05Z,0,"I have pip installed my tensorflow cpu version.   Name: tensorflow  Version: 2.0.0  Summary: TensorFlow is an open source machine learning framework for everyone.  Home-page: https://www.tensorflow.org/  Author: Google Inc.  Author-email: packages@tensorflow.org  License: Apache 2.0  Location: c:\users\kulothungan\appdata\local\programs\python\python37\lib\site-packages  Requires: opt-einsum, astor, keras-applications, google-pasta, keras-preprocessing, numpy, tensorflow-estimator, wrapt, grpcio, six, gast, protobuf, wheel, termcolor, tensorboard, absl-py  Following is the code which I am trying to run  import tensorflow as tf  from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True)  Following is the error which I get when I try to run my code:  RESTART: C:\Users\Kulothungan\AppData\Local\Programs\Python\Python37\asfd.py Traceback (most recent call last): File ""C:\Users\Kulothungan\AppData\Local\Programs\Python\Python37\asfd.py"", line 2, in from tensorflow.examples.tutorials.mnist import input_data ModuleNotFoundError: No module named 'tensorflow.examples.tutorials'  Following is the location of my code:  C:\Users\Kulothungan\AppData\Local\Programs\Python\Python37  **I have already tried putting it in tuorial folder as other posts have mentioned.  Does anyone have any solution?**",True
@kulothungans6193,2019-10-13T19:04:44Z,3,"I am getting the following. Does anyone know any solution?   from tensorflow.examples.tutorials.mnist import input_data ModuleNotFoundError: No module named 'tensorflow.examples.tutorials'",True
@akshaymarvaniya6227,2019-09-29T06:03:36Z,1,how to download image data for minist in code plz. give replay asap plz. share link,True
@rubenvicente4677,2019-09-06T15:24:12Z,0,"Hello dude, I am from the future. I already have petabytes",True
@subscribes6434,2019-07-22T21:07:39Z,0,"Dumb question but, when you run the code, where does the dataset download to? Also does it keep downloading it everytime you run the code? I imagine you would  run out of storage space after a while.",True
@ihadiuzzama,2019-07-21T18:57:11Z,1,"#Thank you, I am learning a lot from your tutorials, here's some general function for adding n number of hidden layers and the last layer is returned. def neural_network(data, *hidden_layers):         inp_neurons = int(data.shape[1])     layers = [data]     for out_neurons in hidden_layers:         layers.append(tf.nn.relu(tf.matmul(layers[-1], tf.Variable(tf.random.normal([inp_neurons, out_neurons]))) + tf.Variable(tf.random.normal([out_neurons]))))         inp_neurons = out_neurons       return tf.matmul(layers[-1], tf.Variable(tf.random.normal([inp_neurons, n_classes]))) + tf.Variable(tf.random.normal([n_classes]))",True
@sureshothwarang,2019-07-04T03:14:01Z,0,Siraj can deliver fast but he does not know shit what he is talking about... This guy is still the real deal.,True
@sohamshah2127,2019-06-18T19:09:44Z,0,"When I read and watched some other vids about DL and tensorflow I was pretty confused and damn afraid that I would get stuff or not. But now, tf is so simple! You are awesome man and its not just a coding tutorial, but its Coding+Sarcasm Tutorial! Really have fun watching you code  and it makes DL as easy as breathing in air!",True
@larcenciel7428,2019-06-08T19:24:25Z,2,"I love you. Marry me. If I !C on my final, I will donate 5 dollars to your Patreon account!",True
@arun3151997,2019-06-08T06:49:55Z,0,"x=tf.placeholder('float',[None,784])  Why do we use ""None""? Shouldnt it be 1 as it is has a height of one?",True
@melakgezahegne9459,2019-06-03T14:33:49Z,0,My appreciation,True
@debayondharchowdhury2680,2019-04-11T08:23:55Z,0,"why in output layer we don't need ""tf.add()"" in order to add the biases to the output layer?",True
@mankaransingh2599,2019-03-22T07:20:59Z,1,"This tutorial is not actually for complete beginners, you actually need to have some basic knowledge of tf as well as how neural nets work. Once you do that, come here to strengthen your knowledge.",True
@vishalraj691,2019-03-14T03:31:48Z,0,Why in the layer 1 we have taken 784 in tf.random,True
@pankhurigupta8623,2019-03-13T06:47:22Z,1,"hey, can you please make a tutorial video on text summarizer using tensorflow without GPU",True
@bhawnasikriwal6599,2019-03-10T03:55:28Z,0,ERROR /read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version/ how do you upgrade that?,True
@iansong1676,2019-02-27T08:26:08Z,0,"As soon as I write mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True) I get:   WARNING:tensorflow:From <ipython-input-44-0328e0b4fafb>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please write your own downloading logic. WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please use urllib or similar directly. Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes. WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting /tmp/data/train-images-idx3-ubyte.gz Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes. WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting /tmp/data/train-labels-idx1-ubyte.gz WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.one_hot on tensors. Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes. Extracting /tmp/data/t10k-images-idx3-ubyte.gz Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes. Extracting /tmp/data/t10k-labels-idx1-ubyte.gz WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models.   Please help :(",True
@adityashrivastava8733,2019-02-23T03:14:58Z,0,"When I wrote the importing dataset line and ran it, I recieved a warning saying that the current function will be deprecated in the future. Can you suggest any another updated way of doing that. Something in pure tensorflow and not in keras api.",True
@ashishaggarwal6308,2019-02-10T12:47:31Z,1,"ok , your video just summed up a 10 hour deep learning course into 30 min video . RESPECT .....",True
@zenoboi6162,2019-02-05T18:38:02Z,0,> python best_tf_tuto.py    output: sentdex,True
@gustavoexel5569,2019-02-01T02:24:22Z,0,"Hey, I think I figured out a for loop. Check this out.  def neural_network_model(data, n_nodes = [500,500,500], n_input = 784, n_output = n_classes):          #HL Dicts     hidden_layers = []          hidden_layers.append({'weights':tf.Variable(tf.random_normal([n_input, n_nodes[0]])),                       'biases':tf.Variable(tf.random_normal([n_nodes[0]]))})          for hl in range(1,len(n_nodes)):              hidden_layers.append({'weights':tf.Variable(tf.random_normal([n_nodes[hl-1], n_nodes[hl]])),                       'biases':tf.Variable(tf.random_normal([n_nodes[hl]]))})      output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes[len(n_nodes)-1], n_output])),                     'biases':tf.Variable(tf.random_normal([n_output])),}          entry = data     #HL Matrices     for hl in range(len(n_nodes)):         res = tf.add(tf.matmul(entry,hidden_layers[hl]['weights']), hidden_layers[hl]['biases'])         res = tf.nn.relu(res)         entry = res      output = tf.matmul(entry,output_layer['weights']) + output_layer['biases']      return output",True
@SuperNicothebest,2019-01-31T16:31:25Z,1,I don't agree in the part when you say that mini-batch stochastic gradient descend at the end pf the epoch does feedfoward and backpropagation. With backpropagation it's meant the weight update of the nn. This in  mini-batch stochastic gradient descend should happen AFTER the end of the batch_size. I explain myself better: for each data of the batch_size the gradient is computed (but the weights are NOT updated). After all the grandient for all the samples of the batch_size is computed an average between them is done and a wight update is performed (the backpropagation). This procedure is repeted until the data are over (this is the end of the epoch). If someone disagree with me plz let me know! :),True
@benwiles4415,2019-01-29T11:00:00Z,0,"To use a dynamic for loop, couldn't you add variables (like hidden layer 1, 2, 3... Etc) to a list and then use a for in range loop",True
@ubaidmanzoorwani7491,2019-01-20T15:40:38Z,0,N Layer Neural Network in TensorFlow Using The Same Dataset https://github.com/Ubaid-Manzoor/HandWritten-Digit-Recognization/blob/master/Mnist.ipynb,True
@Adarsh-mn7pl,2019-01-19T08:06:44Z,0,it's easy with conda,True
@michaeltrossbach684,2019-01-16T01:44:42Z,0,Made dynamic version of neural_network_model function:  https://github.com/michotross257/Tensorflow-Neural-Network,True
@abdulbakey8305,2019-01-14T21:20:48Z,0,"# u can do the initialisation-portion of the function for any number of layers as follows # just pass the no of nodes in each layer as a list as the second argument # u can get the weights , biases of any layer (whose layer no is l) whenever needed by weights[l-1] , biases[l-1]  # consider first hidden layer as layer 1 # e.g to get the weights of 3rd hidden layer use weights[2]   def neural_network_model(data , layer_list):     weights = [tf.Variable(tf.random_normal([a , b])) for a , b in zip(layer_list[:] , layer_list[1:])]     biases = [tf.Variable(tf.random_normal([a])) for a in layer_list[1:]]       # initialise the network model = neural_network_model( data , [784 , 500 , 500 , 500 , num_classes])",True
@attyuttamsaha8,2019-01-07T18:48:04Z,0,why didnt you use tf.add() at the output ?,True
@ronakkumar1934,2018-12-19T05:18:38Z,0,how did 784 came ?,True
@abhilashsharma1992,2018-12-14T02:30:48Z,0,why we did not add summation and activation on output layer?,True
@MustafaAktasVEVO,2018-12-10T17:23:19Z,0,Could you please clarify me about where to download this mnist and how to implement it?,True
@ankitdesai496,2018-11-25T19:35:23Z,0,"topology = [input_data, 5, 5, 5, n_classes]    for i in range(1,len(topology)):  Layer = {'weights':tf.Variable(tf.random_normal([topology[i-1], topology[i]])),      Â Â 'biases':tf.Variable(tf.random_normal(topology[i]))}   Layers[i-1] = Layer",True
@ssrrapper,2018-11-19T20:40:07Z,0,Starbucks product placement at 10:20.,True
@james3742,2018-11-04T02:18:30Z,0,Your videos are excellent!,True
@nassimmehalli7043,2018-10-09T10:04:11Z,0,what does  it mean the batch size = 100 ? i mean is it the number of data_inputs that we'r gonna to put on eatch one of the 10 nodes of our input layer ?,True
@puthenrohit,2018-10-07T14:29:04Z,0,On line number 45 why are we not using tf.add and instead using '+' operand for addition? I looked up pythonprogramming.net and it too has the same thing..,True
@puthenrohit,2018-10-07T14:21:17Z,0,"In line 36 you have, l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']) + hidden_1_layer['biases'])   It should be, l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])  Since tf.add will add the arguments passed to it?",True
@kranti502,2018-09-26T19:39:20Z,0,"Please correct me if I am wrong I think weight matrix should be [node2,node1],[node3,node2],[classes,node3].",True
@zahidulislam9812,2018-09-21T18:12:07Z,0,hey sentdex will you please make a video on speech recognition and chatbot manually,True
@percyygaming,2018-09-18T19:14:43Z,1,"Why is tf.placeholder,random_normal is used for? What is relu?",True
@user-qn3sg6zm6m,2018-09-12T21:58:14Z,6,... Your videos is good but they will be extremely good if there is an English sub,True
@shashwatsingh8708,2018-09-11T19:55:18Z,11,"If you guys are confused, I HIGHLY suggest reading 2 chapter of Michael Neilson's Neural networkanddeeplearning book. Just google that. This all will make sense in a much MUCH better way. Along with that, watch 3blue1brown's Neural network video on youtube.   Or you'll always be left with questions since he's not going into what's actually happening in the network. Do that and come back here. Speaking from my personal experience. I was always lost since tensorflow is very high level and doesn't explain what is ACTUALLY going on in the bones, which by the way is extremely important.   Thank you.",True
@JoeyFlowers,2018-09-11T03:54:05Z,0,No.     You're not allowed to drink out of conventional cups...    Weird and unique coffee mugs only.,True
@MrBharbhi,2018-09-09T20:31:24Z,0,"Thanks for all the lectures. It is useful for me. Have a question though. Should i continue watching this or should i follow keras one ?. Also for CNN, are your lectures still latest ?",True
@jacobstech1777,2018-08-28T11:51:44Z,0,Love your video,True
@ruis2345,2018-08-28T03:47:14Z,0,"man, the promised land of terabyte of memory",True
@shoebmoin10,2018-08-24T13:45:49Z,0,"I think these two for loops should do it, I am just maintaining a list of dictionaries of layers.  def neural_network_model(data):     hidden_layer = []      for i in range(n_layers):         layer = {'weights' : tf.Variable( tf.random_normal( [n_nodes[i],n_nodes[i+1]] )) ,                       'baises'  : tf.Variable( tf.random_normal( n_nodes[i+1] )) }         hidden_layer.append(layer)      output_layer = { 'weights' : tf.Variable( tf.random_normal( [n_nodes[n_layers], n_classes] )) ,                      'baises'  : tf.Variable( tf.random_normal( n_classes )) }      # (input_data*weights) + biases      layers = []      layers.append(data)     for i in range(n_layers):         l = tf.add(tf.matmul(layers[i],hidden_layer[i]['weights']), hidden_layer[i]['biases'])         l = tf.nn.relu(l)         layers.append(l)      output = tf.matmul(layers[n_layers-1],output_layer['weights']) + output_layer['biases']      return output",True
@fanel1900toamna,2018-08-23T18:06:16Z,0,"Here's a way to get rid of the copy / paste BS:  # 28x28 images NUM_FEATURES = 28 ** 2   def neural_network_model(data):     # Define network architecture     n_classes = 10     n_nodes_hl = [500, 400, 300]  # hidden layer sizes     activation_fn = [tf.nn.relu, tf.nn.relu, tf.nn.relu]  # allows for different activation fn per layer     architecture = [NUM_FEATURES] + n_nodes_hl + [n_classes]  # concatenate lists      # Define layer variables     layers = []     for i, n_nodes in enumerate(architecture):         if i == 0:             continue  # can't reference previous size          prev_n_nodes = architecture[i - 1]         layers.append({             'weights': tf.Variable(tf.random_normal([prev_n_nodes, n_nodes])),             'biases': tf.Variable(tf.random_normal([n_nodes]))         })          # Debug output         print((prev_n_nodes, n_nodes))      # Define model for feed forward     model = None     for i, layer in enumerate(layers[:-1]):  # leave out output layer (only input & hidden layers)         _input = data if i == 0 else model         model = activation_fn[i](tf.add(tf.matmul(_input, layer['weights']), layer['biases']))      # Return output     output_layer = layers[-1]     return tf.matmul(model, output_layer['weights']) + output_layer['biases']",True
@albertvandrejer5003,2018-08-21T07:51:37Z,0,shouldnt you add the biases to the input data instead of the output of the layers?,True
@jcesista4581,2018-08-20T17:07:27Z,1,Why don't we have an activation function on the last layer?,True
@ALhajrasAlgdiry,2018-08-20T12:04:15Z,0,"I want to use this inside a Java aplication in order to use it as a voice recognizer, would it work?",True
@anuragtejwani9233,2018-08-20T09:57:40Z,0,u could have made nested loop for hidden layer,True
@icebrennan,2018-08-19T23:31:05Z,0,"Is it strange that I'm getting 95% accuracy for just training this network for 3-5 epochs, whilst getting 18% accuracy for training this network for 5000 epochs. Shouldn't training the network for longer, yield better accuracies. I don't know what other people are getting, when they try out this network.",True
@pabilbadoespecial,2018-08-18T19:54:30Z,0,Can't you make a list to create each layer and append it? I did that but with bumpy without using tensorflow,True
@yahyan8748,2018-08-15T13:57:31Z,0,"Im getting an indentation error in the line  Hidden_2_layer ={'weights':tf.variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),  The terminal is saying : unindent does not match any outer indentation level                  ^  Help",True
@augustinestephens6097,2018-08-11T18:05:57Z,0,Is it okay if I just skip everything before this and start. Directly at tensor flow,True
@user-np4ub7pu9v,2018-08-10T10:05:00Z,0,"I would like to show my version of creating layers in nn:  def neural_network_model(data, num_hidden_layers=3,  n_classes=n_classes):          # int(data.shape[1]) = int(x.shape[1]) = 784         # n_classes = 10         nodes_in_layers = np.linspace(int(data.shape[1]), n_classes, num_hidden_layers + 2).astype(int)          layers = []              for i in range(1, num_hidden_layers + 2):                 layer = {'weights': tf.Variable(tf.truncated_normal([nodes_in_layers[i - 1], nodes_in_layers[i]])),                                 'biases': tf.Variable(tf.zeros([nodes_in_layers[i]]))}                 layers.append(layer)              layers_activation_functions = []              hidden_layer_0 = tf.nn.relu(tf.add(tf.matmul(data, layers[0]['weights']), layers[0]['biases']))             layers_activation_functions.append(hidden_layer_0)              for i in range(1, num_hidden_layers):                 hidden_layer = tf.nn.relu(                     tf.add(tf.matmul(layers_activation_functions[i - 1], layers[i]['weights']), layers[i]['biases']))                 layers_activation_functions.append(hidden_layer)              output_layer = tf.add(tf.matmul(layers_activation_functions[-1], layers[-1]['weights']), layers[-1]['biases'])              return output_layer",True
@VISWESWARAN1998,2018-08-02T13:11:12Z,0,Thanks for the tutorial! Cannot find the code via basic search..  Somehow managed to get the code :) https://pythonprogramming.net/tensorflow-deep-neural-network-machine-learning-tutorial/?completed=/tensorflow-introduction-machine-learning-tutorial/,True
@Khanradcoder,2018-07-27T15:24:30Z,0,I was cringing when I noticed the syntax error and you kept talking.,True
@smitshah5103,2018-07-23T13:48:07Z,0,Don't we use an activation function even for the output layer? All the codes I have seen and run use an activation function even for the output layer.,True
@nictanghe98,2018-07-17T02:01:23Z,0,noone else getting return outsite function as an error huh ? I thought i had the exzact same code.,True
@techynerdz9566,2018-06-22T20:16:23Z,0,"If a matrix is height by width , then for hidden layer 1 for example, why is it not of shape [nodes of h1,784]?",True
@akashnarang91,2018-06-22T16:12:44Z,34,"I love his quirks and those moments when he drifts off topic for just a bit and then has a funny ""hehe"" reaction.",True
@user-pg6py8wv3m,2018-06-20T00:52:36Z,0,Please can anybody tell me the difference between T flow and matlab please,True
@Su_Has,2018-06-19T19:45:25Z,0,How did you get intellisense enable for tensorflow in Sublime text,True
@XavierXEDO,2018-06-19T00:19:57Z,0,Question: Why on the output layer do we simply matmul instead of doing both matmul and add? Can someone explain the maths of this?,True
@rev_pheonix7254,2018-06-16T19:07:05Z,0,"Hi,  You are already doing a matmul on weights and inputs .. which is like computing w1x1 + w2x2 + w3x3.... And then you add biases using + operation which will give you a value. Why are you then using tf.add() ?",True
@sprajapati566,2018-06-16T09:23:12Z,0,store hidden layers in an array/list then iterate through each hidden layer in the list.....!,True
@kotaprolucharan4032,2018-06-12T06:13:18Z,0,can you explain me about the y = tf.placeholder('float') thing i didnt understand that,True
@niteshrawat576,2018-06-11T17:48:41Z,0,what is that path ('temp/data') means?,True
@Elzelgator,2018-06-09T07:37:23Z,0,"Please put Image of your neural network, for us to visualize the code! please. And your face would be better if it is on top right. ty  BTw not all of us using your game, could you please tell us more about the input ""data""?!",True
@ayushman_sr,2018-06-01T18:25:46Z,0,please explain some mathematics behind it... if it will be fruitful,True
@venkatesh5764,2018-05-30T10:22:21Z,0,Shouldn't the weight in hidden layer be :- batch_size* no_of_nodes_hll1 ?,True
@harshalpingale5744,2018-05-09T10:16:05Z,0,https://gist.github.com/pshrohit/82dbd3a8817192b7c8963a4ecd39d204 try it! i tried to use for loop,True
@mihir2012,2018-05-04T20:49:20Z,0,"So, this is 1 year too late, but if you're still looking for that 'for' loop, here is what I came up with. Replace n_nodes_hl1,2,3 and n_classes with a list n_layers = [784, 500, 500, 500, 10] then in def neural_net... layers = [{'wights': tf.Variable(tf.random_normal([x, y])), 'biases': tf.Variable(tf.random_normal(y))} for x, y zip(n_layers[:-1], n_layers[1:])] output = data for layer in layers:   output = tf.add(tf.matmul(active, layer['weights']), layer['biases'])   output = tf.nn.relu(active) return output  You can modify that for loop to exclude the last layer and do that calculation on its own line.",True
@jackie.p6891,2018-04-29T18:36:20Z,0,"great video :) i'm learning a lot. if you want a for-loop for a variable number of hidden layers, wouldn't it be easier to just create a dictionary holding those layers, and add to them, rather than predefning them like hidden_1_layer, hidden_2_layer do something like hidden_layers[1]...?",True
@rahulvijayakumar5578,2018-04-28T14:31:34Z,0,Your tutorials are great. I am entirely depending on it for learning,True
@SandeepMishra-fr4yf,2018-04-11T09:17:44Z,0,Great video.. can anyone post the link of the code..,True
@ravishankar2180,2018-04-10T19:50:54Z,0,love you brother for making this easy to understand.,True
@sajad3320,2018-04-08T05:28:07Z,0,Perfect!!!!!!!,True
@franciszekszombara8881,2018-03-30T19:15:15Z,0,"I implemented this neural net using the for loop. The network model can be saved as a dict of dicts:  https://github.com/franekantoni/neural_net_functions/blob/master/neural-net-func.py Thanks for the video, it was really helpful",True
@RoshFernandes,2018-03-29T07:03:33Z,0,Hey can u plz give any information regarding this tensorflow in windows 10. How to program ... and install....,True
@souravgames,2018-03-26T10:13:09Z,0,Can you please add a video on how to prune a neural network o decide how many hidden layers to bring in with code.  Thanks in advance,True
@nilsaha8021,2018-03-22T20:41:36Z,0,"Hi, it seems 'return' is an invalid syntax in the current version of TensorFlow. I chekced all the commands in the TesorFlow website as well. I haven't found the 'return' command. What is the alternative syntax?",True
@joesiu4972,2018-03-18T02:40:32Z,0,"Hey guys how do i fix?  Traceback (most recent call last):   File ""C:\Users\Vincent\eclipse-workspace\NN\NN\__init__.py"", line 4, in <module>     from tensorflow.examples.tutorials.mnist import input_data   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\examples\tutorials\mnist\__init__.py"", line 21, in <module>     from tensorflow.examples.tutorials.mnist import input_data   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\examples\tutorials\mnist\input_data.py"", line 29, in <module>     from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\__init__.py"", line 23, in <module>     from tensorflow.contrib import bayesflow   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\bayesflow\__init__.py"", line 28, in <module>     from tensorflow.contrib.bayesflow.python.ops import layers   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\layers.py"", line 26, in <module>     from tensorflow.contrib.bayesflow.python.ops.layers_conv_variational import *   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\layers_conv_variational.py"", line 22, in <module>     from tensorflow.contrib.bayesflow.python.ops import layers_util   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\layers_util.py"", line 24, in <module>     from tensorflow.contrib.distributions.python.ops import deterministic as deterministic_lib   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\distributions\__init__.py"", line 38, in <module>     from tensorflow.contrib.distributions.python.ops.estimator import *   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\distributions\python\ops\estimator.py"", line 21, in <module>     from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\__init__.py"", line 92, in <module>     from tensorflow.contrib.learn.python.learn import *   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\__init__.py"", line 23, in <module>     from tensorflow.contrib.learn.python.learn import *   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\__init__.py"", line 25, in <module>     from tensorflow.contrib.learn.python.learn import estimators   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\__init__.py"", line 303, in <module>     from tensorflow.contrib.learn.python.learn.estimators.dynamic_rnn_estimator import DynamicRnnEstimator   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dynamic_rnn_estimator.py"", line 27, in <module>     from tensorflow.contrib.learn.python.learn.estimators import rnn_common   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\rnn_common.py"", line 22, in <module>     from tensorflow.contrib import rnn as contrib_rnn   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\rnn\__init__.py"", line 83, in <module>     from tensorflow.contrib.rnn.python.ops.gru_ops import *   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\rnn\python\ops\gru_ops.py"", line 33, in <module>     resource_loader.get_path_to_datafile(""_gru_ops.so""))   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\util\loader.py"", line 55, in load_op_library     ret = load_library.load_op_library(path)   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\load_library.py"", line 58, in load_op_library     lib_handle = py_tf.TF_LoadLibrary(library_filename, status)   File ""C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 516, in __exit__     c_api.TF_GetCode(self.status.status)) tensorflow.python.framework.errors_impl.NotFoundError: C:\Users\Vincent\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\rnn\python\ops\_gru_ops.dll not found",True
@niteshkumarsharma8305,2018-03-10T19:11:12Z,0,Your Videos are Awesome dude!!,True
@owaisaos1851,2018-03-09T21:03:38Z,0,whats your age bro,True
@ndiayemodou3474,2018-03-09T00:32:30Z,0,you're awesome,True
@woltron4o,2018-03-04T08:17:08Z,0,Another great video. Thanks :) :) :) !!!!,True
@HariHaran-hq3mk,2018-03-03T07:47:17Z,0,Is there some place where I can find the entire code of p3 and p4? Will be helpful,True
@edwardantonian7296,2018-02-28T15:30:16Z,0,You are my secret man-crush x,True
@brianlink391,2018-02-26T19:24:43Z,1,Sublime key for build 3142 =   â€“ BEGIN LICENSE â€”â€“ TwitterInc 200 User License EA7E-890007 1D77F72E 390CDD93 4DCBA022 FAF60790 61AA12C0 A37081C5 D0316412 4584D136 94D7F7D4 95BC8C1C 527DA828 560BB037 D1EDDD8C AE7B379F 50C9D69D B35179EF 2FE898C4 8E4277A8 555CE714 E1FB0E43 D5D52613 C3D12E98 BC49967F 7652EED2 9D2D2E61 67610860 6D338B72 5CF95C69 E36B85CC 84991F19 7575D828 470A92AB â€”â€” END LICENSE â€”â€”,True
@brianlink391,2018-02-26T18:39:23Z,2,I really like how to talk about the code! A+ thanks man your the best!,True
@johnburnett4035,2018-02-26T06:58:00Z,0,thanks man!  Awesome stuff,True
@akashkalghatgi9352,2018-02-22T16:09:37Z,1,"+sentdex On executing the code mentioned upto 21:40 I got the following error: AttributeError: module 'tensorflow.examples.tutorials.mnist.input_data' has no attribute 'real_data_sets' I searched for a solution but since I'm not so fluent with tensor flows, I couldn't understand the process. Please assist if possible.",True
@arturdusinski4803,2018-02-17T22:24:11Z,0,11:53 -  I'm watching this in 2057 and I can confirm that average PC has petabytes of RAM... also time travel is a thing now.,True
@farzadvaziri9956,2018-02-14T14:31:50Z,0,"Awesome tutorial, I couldn't find a good tutorial for deep learning, all I had seen before were either so theoretical and hard to understand or just coding without explanation. You kinda have both in a very simple and understandable way. Thank you very much.",True
@robertnichol5820,2018-02-13T08:05:05Z,0,"for i in range(n_layers):  exec(""""""hidden_{}_layer = {{'weights' : tf.Variable(tf.random_normal([784, n_nodes_hl{}])),         'biases' : tf.Variable(tf.random_normal([784,n_nodes_hl{}]))}}"""""".format(*((str(i+1),)*3)),   globals())",True
@bruhaspati560,2018-02-12T08:08:40Z,1,U should pit them on udemy u deserve money for this!!,True
@AbeDillon,2018-02-07T00:05:38Z,0,"couldn't you do the loop version like so:  import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""/tmp/data"", one_hot=True)  input_size = 784 n_classes = 10 layer_sizes = [input_size, 500, 500, 500, n_classes] batch_size = 100  x = tf.placeholder('float', [None, input_size]) y = tf.placeholder('float')  model = [init_layer(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]  def init_layer(i, o):  return {'weights': tf.Variable(tf.random_normal(i, o)),          'biases': tf.Variable(tf.random_normal(o))}  def activate(data):  activation = data  for layer in model:   activation = tf.nn.relu(tf.matmul(activation, layer['weights']) + layer['biases'])  return activation",True
@azadehkarimi986,2018-02-05T16:03:19Z,0,"After running this lineÂ x=tf.placeholder(float,[None,784]) I got following error:  ""Cannot convert value %r to a TensorFlow DType."" % type_value) TypeError: Cannot convert value <class 'float'> to a TensorFlow DType. During handling of the above exception, another exception occurred: Traceback (most recent call last):",True
@tw0ey3dm4n,2018-02-03T03:51:16Z,0,"# for loop code  def neural_network_model( data, depth ):  # (input data * weights) + biases <-- model for each layer    hidden_layers = []    for i in range( 0, depth ):   print( 'Adding hidden layer {0}'.format( i ) )   start = msize if i == 0 else n_nodes_hL[ i - 1 ]   hidden_layers.append({          'weights': tf.Variable( tf.random_normal( [ start, n_nodes_hL[ i ] ] ) ),         'biases': tf.Variable( tf.random_normal( n_nodes_hL[ i ] ) )         })     output_layer = ({        'weights': tf.Variable( tf.random_normal( [ n_nodes_hL[ depth - 1 ], n_classes ] ) ),       'biases': tf.Variable( tf.random_normal( [ n_classes ] ) )       })     # sigmeud, signal, activation function  layers = []  for i in range( 0, depth ):   ldata = data if i == 0 else layers[ i - 1 ]   layers[ i ] = tf.add( tf.matmul( ldata, hidden_layer[ i ][ 'weights'] ) + hidden_layer[ i ][ 'biases' ] )   layers[ i ] = tf.nn.relu( layers[ i ] )     output = tf.matmul( hidden_layer[ depth - 1 ], output_layer[ 'weights' ] ) + output_layer[ 'biases' ]    return output",True
@SkielCast,2018-02-02T22:41:34Z,0,"For loop in 18 lines:  n_nodes_il = 784 n_nodes_hl1 = 500 n_nodes_hl2 = 500 n_nodes_hl3 = 500 n_nodes_layer = [n_nodes_il, n_nodes_hl1, n_nodes_hl2, n_nodes_hl3] n_classes = 10 batch_size = 100  def neural_network_model(data, n_nodes_layer, n_classes):     hidden_layers = []     for last, actual in zip(n_nodes_layer[:-1], n_nodes_layer[1:]):         hidden_layers.append({'weights':tf.Variable(tf.random_normal([last, actual])),                               'biases':tf.Variable(tf.random_normal([actual]))                              })     output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_layer[-1], n_classes])),                     'biases':tf.Variable(tf.random_normal([n_classes]))                     }          layers = [data]     for actual in hidden_layers:         layers.append(tf.add(tf.matmul(layers[-1], actual['weights']), actual['biases']))         actual = tf.nn.relu(layers[-1])          output = tf.add(tf.matmul(layers[-1], output_layer['weights']), output_layer['biases'])          return output",True
@ChowChow414,2018-02-02T08:49:35Z,0,You can probably just make a list to fix your hidden layer problem.,True
@sagarrana2859,2018-01-30T17:16:41Z,0,"how is it possible to add the biases as biases is one dimensional tensor and X*  weight is [None,n_nodes_hl1] shape? please give me hint.",True
@vaibhavahuja4001,2018-01-27T10:05:45Z,0,why do we have to perform matrix multiplication instead of element wise multiplication?,True
@alexanderdavidsoncarroll7788,2018-01-25T01:18:56Z,0,this was so helpful. totally worth the time,True
@user-bf9ze3ms5u,2018-01-21T05:41:03Z,0,anyway ),True
@felipeguerra4830,2018-01-14T21:32:05Z,0,"Hi, i think that you switched the size of weights matrices. They are actualy (n_nodes_actual_layer, n_nodes_previous_layer) size.",True
@valluvadhasan944,2018-01-12T15:29:01Z,0,Brother! From where you are learning these ideas,True
@JO-vj9kn,2018-01-10T08:45:56Z,0,Sentdex: IDLE is better than IDE's! Moments later: Makes syntax error.,True
@charimuvilla8693,2018-01-08T18:06:52Z,0,"Make something that will count how many times you have said ""py"" in youtube",True
@Jay0neDE,2018-01-08T17:05:58Z,0,"eech, are you using tabs?  sorry, I just already know most of what's in this video and was bored enough to notice ;)",True
@bluelight9508,2018-01-08T01:26:47Z,0,"Great video as always. One comment is that I think one cycle of feedforward and backprop, which covers 1 batch (ie subset of data) is a step (rather than an epoch), and once you've gone through enough batch as to cover your full dataset, that's called 1 epoch.",True
@amitdharmadhikari1319,2018-01-06T06:48:32Z,0,Please make the entire code available to us.,True
@viaramb,2018-01-02T02:26:21Z,0,"Hi Man,              Great stuff, just my 2 cents to your battle with the sublime vs IDLE, the deal is that sublime will follow the PIP 8 rules so when you posted:      hidden_1_layer = { 'weights': tf.Variable(tf.random_normal([784, n_nodes_hl1])),     'biases': tf.Variable(tf.random_normal([]))     }  This doesnt follow PIP8, what you need to do is:     hidden_1_layer = {         'weights': tf.Variable(tf.random_normal([784, n_nodes_hl1])),         'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))       }  Once you do this the lines start to align without problems, this type of ""Syntax"" correction is also in Spyder where you can run a PIP8 Static Code Analysis and will tell you where the code is not in compliance or out of the convention, will run, but will let you know is not PIP8 standard.  Hope you see this, best regards.",True
@anjopag31,2018-01-01T08:29:27Z,0,First video I watched in 2018.,True
@rj-nj3uk,2017-12-28T17:56:08Z,0,what secret hidden behind your doors?,True
@MauricioMartinez0707,2017-12-24T02:57:31Z,0,"Maybe it's not used so much on this side of the fence, but in the matlab world they refer to it as linearizing the matrix, not squashing or flattening. But all 3 words are pretty descriptive of the same thing",True
@demystifiedcheif7506,2017-12-23T18:57:08Z,0,what does n_classes variable indicate ?  does it mean the number of output in the networks?,True
@shivambharadwaj1939,2017-12-09T16:18:35Z,0,"#%Complete_Working_code%    import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""tmp/data/"", one_hot=True)  n_nodes_hl1 = 500 n_nodes_hl2 = 500 n_nodes_hl3 = 500  n_classes = 10 batch_size = 100 input_size = 784  # input feature size = 28x28 pixels = 784 x = tf.placeholder('float', [None, input_size]) y = tf.placeholder('float')  def neural_network_model(data):     # input_data * weights + biases     hidden_l1 = {'weights': tf.Variable(tf.random_normal([input_size, n_nodes_hl1])),                 'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}      hidden_l2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),             'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}      hidden_l3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),             'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}      output_l = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),             'biases': tf.Variable(tf.random_normal([n_classes]))}      l1 = tf.add(tf.matmul(data, hidden_l1['weights']), hidden_l1['biases'])     l1 = tf.nn.relu(l1)      l2 = tf.add(tf.matmul(l1, hidden_l2['weights']), hidden_l2['biases'])     l2 = tf.nn.relu(l2)      l3 = tf.add(tf.matmul(l2, hidden_l3['weights']), hidden_l3['biases'])     l3 = tf.nn.relu(l3)      output = tf.add(tf.matmul(l3, output_l['weights']), output_l['biases'])     return output  def train_neural_network(x):     prediction = neural_network_model(x)     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))   # v1.0 changes       # optimizer value = 0.001, Adam similar to SGD     optimizer = tf.train.AdamOptimizer().minimize(cost)     epochs_no = 10       with tf.Session() as sess:         sess.run(tf.global_variables_initializer())   # v1.0 changes            # training         for epoch in range(epochs_no):             epoch_loss = 0             for _ in range(int(mnist.train.num_examples/batch_size)):                 epoch_x, epoch_y = mnist.train.next_batch(batch_size)                 _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})                 # code that optimizes the weights & biases                 epoch_loss += c             print('Epoch', epoch, 'completed out of', epochs_no, 'and loss is:', epoch_loss)            # testing         correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))         print('Accuracy:', accuracy.eval({x: mnist.test.images, y: mnist.test.labels})*100,'%')  train_neural_network(x)",True
@nikhil199029,2017-12-09T04:08:47Z,0,how much ram do I require to run this program?,True
@DavidDaverio,2017-12-08T00:13:36Z,0,"For your for loop :-)  Probably not the most clean way (my C++ background....) Quite simple: Uses python lists for your layers = [] , layer_size = [] , .... The output_layer is just a layer... so no need to have a different name:  layer_size = [784,500,500,500,10] layer = [] ll = []  for i in range(1,5):         layer.append( {......[layer_size[i-1],layer_size[i] ])),                                   .......layer_size[i] )) } )   ll.append( data  ....layer[i]['wheights']) ,  layer[i]['biases'])       ) for i in range(1,3):        ll.append( ll[i=1] ....layer[i]['wheights']) ,  layer[i]['biases'])       )        ll[i] = tf.nn.relu(ll[i]) ll.append(  ll[2] ....layer[3]['wheights']) ,  layer[3]['biases'])       )  return ll[3]   then number of layer to be dynamic: here numLayer = 4.... 5=numLayer+1 and 3=numLayer-1 I prefered to keep the first and the last layer outside the loop, but if you want to put it insinde just declare a ll  as a array of numLayer with value NULL, set the first element to be data... and recursively build this array in the loop. But I think it would be damn ugly!  PS: thanks you made me avoid a couple of hours of fight with the mnist tuto of tensorflow :-)",True
@nikhil199029,2017-12-06T10:20:44Z,0,Is it safe to say that Bias is like offset value for number of neuron to be fired?,True
@jaewooko4527,2017-12-04T14:52:27Z,0,Thanks for nice detail. I was not able to find detailed tutorials. It's so great,True
@deepbodra4782,2017-12-01T07:18:44Z,0,Did a little bit of digging and figured out a way to do this with dynamic number of layers https://github.com/deepbodra96/TensorFlow-Mnist-Dynamic-Layers/tree/master,True
@kennyadetiloye204,2017-11-28T01:02:08Z,0,"How did you derive this: { 'weights':tf.Variable(tf_random_normal([784, n_nodes_hl1])),                      'biases':tf.Variable(tf_random_normal(n_nodes_hl1))}",True
@TheRaprulez,2017-11-22T20:16:16Z,0,"Hello, I have a question about input_data module. I tried to use mnist just as you wrote. But then I was told that supposedly I should use input_data because it is basically entire library from which you took the part in which we were interested. So my question is how do I install this library in windows? I am using anaconda's spyder ide and basically everything went smoothly until this moment. I tried to find some clues on Tensorflow site but nothing from there helped me either.",True
@arvindbalajeea1460,2017-11-19T17:01:55Z,0,I can't understand what is the use of biases and weights...what is it used for....,True
@crwhhx,2017-11-19T01:02:07Z,2,"Great video, one question tho, I guess tf.add(a,b) instead of tf.add(a+b) would be the formal way of using that function?",True
@bopeton,2017-11-16T02:37:38Z,0,Is this code posted anywhere?,True
@TheWingjammer,2017-11-11T06:57:27Z,0,thank you !,True
@adamjameson6920,2017-11-10T02:09:02Z,0,"def neural_network_model(data): Â  Â  inputStartWeights = 784 Â  Â  for layer in range(2): Â  Â  Â  Â  hiddenLayer[layer] = {""weights"": tf.variable(tf.random_normal([inputStartWeights, nodesInLayer[layer]])),Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ""biases"": tf.variable(tf.random_normal(nodesInLayer[layer]))} Â  Â  Â  Â  inputStartWeights = nodesInLayer[layer] Â  Â  outputLayer = {""weights"": tf.variable(tf.random_normal([inputStartWeights, nClasses])),Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ""biases"": tf.variable(tf.random_normal([nClasses]))}  Â  Â  inputData = data Â  Â  l = [] Â  Â  for i in range(2): Â  Â  Â  Â  l[i] = tf.add(tf.matmul(inputData, hiddenLayer[i][""weights""]) , hiddenLayer[i][""biases""]) Â  Â  Â  Â  l[i] = tf.nn.relu(l[i]) Â  Â  Â  Â  inputData = l[i] Â  Â  output = tf.add(tf.matmul(inputData, outputLayer[""weights""]) , outputLayer[""biases""]) Â  Â  return output",True
@kuberchaurasiya,2017-10-31T19:40:44Z,0,loved the simplicity and cleanliness!! Kudos!!,True
@CharlesMacKay88,2017-10-27T06:37:18Z,0,Haha sublime sucks for not auto indenting your dictionary,True
@chapasirithunge8704,2017-10-13T08:29:23Z,0,"I get an error in line:  l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases']) The error is, NameError: name `data' is not defined. Can anyone help resolve this?",True
@natikrisi,2017-10-10T12:26:17Z,0,love your tutorials,True
@Arpytanshu,2017-10-08T14:34:03Z,0,"def createParameters(nn_arch):     n = len(nn_arch)     parameters = {}         for i in range(1,n):         parameters[""W{0}"".format(i)] = tf.get_variable(""W""+str(i), (nn_arch[i], nn_arch[i-1]), dtype = tf.float32,                                                        initializer = tf.contrib.layers.xavier_initializer())         parameters[""b{0}"".format(i)] = tf.get_variable(""b""+str(i), (nn_arch[i], 1), dtype = tf.float32,                                                        initializer = tf.zeros_initializer())     return parameters   >>>param = createParameters( [ 3, 4, 4, 4, 1 ] ) {'W1': <tf.Variable 'W1:0' shape=(4, 3) dtype=float32_ref>,  'W2': <tf.Variable 'W2:0' shape=(4, 4) dtype=float32_ref>,  'W3': <tf.Variable 'W3:0' shape=(4, 4) dtype=float32_ref>,  'W4': <tf.Variable 'W4:0' shape=(1, 4) dtype=float32_ref>,  'b1': <tf.Variable 'b1:0' shape=(4, 1) dtype=float32_ref>,  'b2': <tf.Variable 'b2:0' shape=(4, 1) dtype=float32_ref>,  'b3': <tf.Variable 'b3:0' shape=(4, 1) dtype=float32_ref>,  'b4': <tf.Variable 'b4:0' shape=(1, 1) dtype=float32_ref>} >>>",True
@MrSnowmobilefreak,2017-10-04T17:51:44Z,0,12:05 made me laugh,True
@ke_let5072,2017-09-27T15:08:47Z,0,After i saw the deep-learning course from udacity I felt pretty  uncomfortable with programming and understanding the entire mass behind neural networks. I only got an idea of what neural networks are. Now with the help of your videos everything gets clearer and clearer. Thank you very much! Especially the speed of your speach and how you try to explain things are very helpful :),True
@shivamkeshri487,2017-09-22T13:43:14Z,0,your all machine learning videos are really helpful but in this video i have a question. the function tf.nn.relu() is threshold function but i don't know what is it  doing please elaborate this function and reply..,True
@noway2831,2017-09-16T23:16:54Z,0,You so is an optimiser like the solver in excel?,True
@Skybuildhero,2017-09-10T17:53:32Z,0,What about using arrays of Dictionaries to do the weights and biases stuff dynamically?,True
@peterdeng1620,2017-09-02T00:12:38Z,0,Should the height of the data to be 1 instead of none?,True
@jasonk334ever,2017-08-29T15:53:15Z,0,"solving the for loop problem: ``` def create_network(x, nodes=[], num_classes=1):     nodes.insert(0, x.get_shape().as_list()[1])     nodes.append(num_classes)     weights = []     biases = []     layer = x     for layer_num, num_nodes in enumerate(nodes[:-1]):         weights.append(tf.Variable(tf.random_normal([num_nodes, nodes[layer_num+1]], 0, 0.1)))         biases.append(tf.Variable(tf.random_normal([nodes[layer_num+1]], 0, 0.1)))         if layer_num > 0:             layer = tf.nn.relu(tf.add(tf.matmul(layer, weights[layer_num-1]), biases[layer_num-1]))     return tf.matmul(layer, weights[-1]) + biases[-1] ```",True
@nsv.2344,2017-08-27T22:58:58Z,0,Shouldn't the output layer also have a sum and activation function? The output of this sum and act. func. is a value between 0 and 1 right?,True
@MsJavaWolf,2017-08-19T18:53:06Z,0,According to StackOverflow + and tf.add is the same (same for mul etc.),True
@abhisheksubramanian3748,2017-08-15T13:31:00Z,0,Hey wonderful tutorial. Is there any way to actually determine the number of nodes to be used ? Thanks in advance for the feedback,True
@jcf3300,2017-07-30T03:04:03Z,0,"@sentdex @HKinsley Hey Harrison, first I'd like to thank you for your hard work. Your programming tutorials are really helpful for many reasons and in particular for hitting the ground running, so to speak. Question: you brought up step functions versus sigmoid functions in an earlier video regarding the idea behind a simple NN. Sigmoid usage in NNs seem, from my inexperience with NNs, to be continually leveraging logistic regression models at each layer. Is that the case? If not, how are they related, the two? Comment: per biases, to me, it makes the most sense to just define bias like we would in any other statistical model, namely as the error term, in this case sampling from a Gaussian. Thanks again for all that you do! P.S. Make shirts with your actual domain (or similar) on them as opposed to just the python logos. I'll buy such a tee.",True
@DanielYovchev-bg,2017-07-27T15:56:38Z,0,"There you go sentdexÂ I used 3 loops but probably we can reduce it to 2  import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)  size = 784 nodes = [] layers = [] n_classes = 10 batch_size = 100 layers_size = 3  for i in range(0, layers_size):  nodes['n_nodes_hl{}'.format(i)] = 500;  x = rf.placeholder('float', [None, size]) y = rf.placeholder('float')  def nerual_network_model(data):  for i in range(0, layers_size):   curent_node = nodes['hidden_{}_layers'.format(i)];   layers['hidden_{}_layers'.format(i)] = {    'weights': tf.Variable(tf.random_normal([size if i == 0 else curent_node, curent_node])),    'biases': tf.Variable(tf.random_normal(curent_node))   }   output_layer = {   'weights': tf.Variable(tf.random_normal([nodes[-1], n_classes])),    'biases': tf.Variable(tf.random_normal([n_classes]))  }   for i, layer in enumerate(layers):   relu = data if i == 0 else layers[i-1]['relu']   # Layer (input_data * weights) + biases   layer['relu'] = tf.nn.relu(tf.add(tf.matmul( relu, layer['weights'] + layer['biases'] )))   # Output  return tf.matmul(layers[-1]['relu'], output_layer['weights']) + output_layer['biases']",True
@brajbhushanpatidar5649,2017-07-22T12:08:18Z,0,whats the purpose for multiplying input to weight and then add to bias,True
@vaibhavgupta951,2017-07-22T10:55:32Z,0,"You asked for a for loop implementation. I have come up with one for the nn_model:  def nn_model(data, n_input_nodes, list_hidden_layers, n_ouput_nodes):     ''' list_hidden_layers contains [no. of nodes in all hidden layers] '''      layers = [n_input_nodes] + list_hidden_layers + n_ouput_nodes      list_wts_biases = []      for curr_l in layers[1:]:         for prev_l in layers[:-1]:              layer = {}             layer['weights'] = tf.Variable( tf.random_normal([prev_l, curr_l]) )             layer['biases'] = tf.Variable( tf.random_normal(curr_l) )              list_wts_biases.append(layer)      a = data        # a stands for activation      for curr_l in list_wts_biases[:-1]:          a = tf.add( tf.matmul(a, curr_l['weights']), curr_l['biases'] )         a = tf.nn.relu(a)      a = tf.add( tf.matmul(a, list_wts_biases[-1]['weights']), list_wts_biases[-1]['biases'] )      return a",True
@cchen7359,2017-07-13T15:06:29Z,0,It is a fabulous video. I have a question: How can I use this model with a  numerical CSV dataset ?,True
@kevinmcinerney9552,2017-07-11T20:54:11Z,0,"Actually, keeping the data in 28x28 does yield more information than than be exploited by constitutional neural networks.",True
@ruiqianyao9436,2017-07-06T21:59:12Z,0,"Hi. I really enjoyed this video. It is so helpful. There is one thing that I don't quite understand. Let's assume that we have 10000 images. The input data will be in the shape of (1000, 784). We define the 1st layer as the following:           hidden_1_layer = {'weights':tf.variable(tf.random_normal([784,n_nodes_hl1])),                                           'biases':tf.variable(tf.random_normal(n_nodes_hl1))} The result of input_Data * weight will be in shape of (10000,500) but the biases is just a vector with 500 entries. I am curious how are we supposed to add that vector to the result matrix?",True
@nadni1,2017-07-04T22:39:52Z,1,"Here is my solution to the problem you propose regarding a loop that creates any amount of layers the user specifies:  layers_n = 5 nodes_per_layer = [50, 50, 50, 50, 50]  if layers_n != len(nodes_per_layer):     print('\nCheck that the length of ""nodes_per_layer"" variable is equal to ""layers_n""')     sys.exit(1)  x = tf.placeholder('float', [None, len(features[0])]) y = tf.placeholder('float')   def neural_network_model(x):     layers = [{'weights': tf.Variable(tf.random_normal([len(features[0]), nodes_per_layer[0]])),                'biases': tf.Variable(tf.random_normal([nodes_per_layer[0]]))}]      for i in range(1, layers_n):         layers.append({'weights': tf.Variable(tf.random_normal([nodes_per_layer[i - 1], nodes_per_layer[i]])),                        'biases': tf.Variable(tf.random_normal([nodes_per_layer[i]]))})      output_layer = {'weights': tf.Variable(tf.random_normal([nodes_per_layer[2], classes_n])),                     'biases': tf.Variable(tf.random_normal([classes_n]))}      l = []      l.append(tf.add(tf.matmul(x, layers[0]['weights']), layers[0]['biases']))     l[0] = tf.nn.relu(l[0])      for i in range(1, layers_n):         l.append(tf.add(tf.matmul(l[i - 1], layers[i]['weights']), layers[i]['biases']))         l[i] = tf.nn.relu(l[i])      output = tf.add(tf.matmul(l[layers_n - 1], output_layer['weights']), output_layer['biases'])      return output",True
@ShuyangSun10,2017-07-02T23:49:15Z,0,Here's a for loop version on my GitHub repo: https://github.com/shuyangsun/tf_nn_mnist/blob/master/tf_practice_3.ipynb,True
@ThEHaCkeR1529,2017-06-29T04:54:02Z,0,Where exactly is the input data coming from?,True
@hans6973,2017-06-27T14:27:32Z,0,"import input_data mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)   --------------------------------------------------------------------------- ImportError                               Traceback (most recent call last) <ipython-input-6-a5af65173c89> in <module>() ----> 1 import input_data       2 mnist = tf.input_data.read_data_sets(""MNIST_data/"", one_hot=True)  ImportError: No module named input_data ##try!!! import tensorflow.examples.tutorials.mnist.input_data as input_data",True
@yoannmamyrandriamihaja5537,2017-06-23T13:08:34Z,0,"Thanks for the tutorials. Trying to answer your question about doing a for loop to add hidden layers: keras library allows doing that: for k in range(2, depth):         model.add(Dense(output_dim=width))         model.add(Activation('relu'))",True
@abhishekmaiti8332,2017-06-20T19:26:19Z,0,What is the use of this function: tf.nn.relu ?  The tensor documentation didn't'say much.,True
@RexGalilae,2017-06-19T01:26:45Z,0,"A way to do all the tensor operations in one for loop would be to create a list of layers such that layer[0] = l1, layer[1] = l2, and so on but the reason why I imagine noone does it is because lists get really clunky to deal with.",True
@gauravsinghmail,2017-06-18T06:54:01Z,0,"I love your voice... so soothing and calm.. I'm able to understand every word..Thanks. Regarding Siraj (with all due respect), watching his videos makes me feel like in one of those rides where in the end you feel like vomiting...jus out of sheer motion sickness",True
@shyamalvaderia7473,2017-06-15T08:17:26Z,0,Here is a implementation of the code without exact deceleration of each and every layer. (Solve the problem using list comprehension)  https://gist.github.com/svaderia/93de434ad3bb23da704a106b2e8349c5,True
@tomasmendoza775,2017-06-08T08:23:35Z,1,After a lot of struggle i had when starting with neural networks i can gladly say you helped me a lot. You are by far my favorite tutor in youtube. Keep up the hard work!!!!,True
@nintendo_dringus,2017-06-02T11:53:47Z,0,"Here's a lovely for loop for you.  def neural_network_model(data, network_structure):     global total_synapses     neural_network, l_list = [], []     for i in range(len(network_structure)):         input_size = lens_size if i == 0 else network_structure[i - 1]         output_size = n_classes if i == len(network_structure) - 1 else network_structure[i]         total_synapses += input_size * output_size         neural_network.append({'weights': tf.Variable(tf.random_normal([input_size, output_size])),                                'biases': tf.Variable(tf.random_normal([output_size]))})         l_list.append(tf.add(tf.matmul(data if i == 0 else l_list[i - 1], neural_network[i]['weights']),                              neural_network[i]['biases']))         if i < len(network_structure) - 1:             l_list[i] = tf.nn.relu(l_list[i])     return l_list[len(l_list) - 1:][0]  All you have to do is fit train_neural_network with another argument for network_structure to work with the model's network_structure arg.  Then you can do something like train_neural_network(x, [500, 3500, 500, 300]) or what ever. Total synapses is optional statistic, I have mine to print upon running the tensorflow session.",True
@niravgandhi6076,2017-05-26T13:20:15Z,2,"Getting following error while trying to run the code. It seems like matrix multiplication does not work because of different matrix size. ValueError: Dimensions must be equal, but are 784 and 500 for 'MatMul_1' (op: 'MatMul') with input shapes: [?,784], [500,500]. Any suggestions??",True
@fengdonghai7496,2017-05-25T09:29:39Z,0,"It's great! Love it. However,I feel confused at this: l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']) + hidden_2_layer['biases'] ) Is it right? or should be this: l2 = tf.matmul(l1, hidden_2_layer['weights']) + hidden_2_layer['biases']",True
@munemu82,2017-05-24T20:16:43Z,0,"Wow, so good! wonderful explaination, I have attended several machine learning classes, but your tutorials simplied and summarise all the theories I have learned - so thank you so much !!",True
@THEMithrandir09,2017-05-16T09:08:48Z,0,Why not just put the layer variables into a list and iterate over that?,True
@Dillonthewinner,2017-05-15T22:11:04Z,19,Hey man I solved that for loop issue you were talking about around 29:00. I made a gist with the code here if your interested. (I'm using recursion instead of a for loop) https://gist.github.com/grandmasterspoc/2e90566e9497725065388859762a7185  it's still kind of messy but it adds all the layers dynamically :),True
@yakov9ify,2017-05-15T14:50:55Z,0,"What if you are trying to make a neural network to play a game, and you don't know what is the right move.",True
@yulun3211,2017-05-10T02:59:02Z,13,"I am a bit confuse of the function of ""tf.add"" and the ""+"" symbol. Why do you still need to use ""tf.add"" to add the product of input_data & weights to the biases, when you have already use the ""+"" symbol to add ""(input_data * weights)"" and ""biases"" [line 36].  Can you explain again why you remove the tf.add for the output layer [line 43]?  Thanks man!",True
@yulun3211,2017-05-09T10:21:19Z,0,"Hey thanks for the tutorial man!  Juz a quick qn: what is the meaning of the first parameter(""/tmp/data/"") in the line:  mnist = input_data.read_data_sets(""/tmp/data/"",one_hot = True)  mean?  It is giving me the following error when i run it:  IOError: [Errno socket error] [Errno -2] Name or service not known",True
@tayler6000,2017-05-08T07:05:05Z,5,"Here's an example of a for loop: for x in range(3):   exec(""variable""+str(x)+""=500"")  That's the basic structure.  You can also do this for massive if statements by doing if (eval(""variable""+str(x)+""==500"") inside a for loop.  I used similar code in a home automation project.",True
@FinaISpartan,2017-05-07T18:33:05Z,0,"If you need it in a for loop, why dont you just put each layer in an array and itterate through it the use a for each loop which would simplify things alot.",True
@hubertbrykowski787,2017-05-06T18:30:30Z,0,Why you're not using any activation function for output layer?,True
@zhengqunkoo,2017-05-01T13:25:51Z,0,feed forward + backpropagation = EPIC!,True
@alexandredamiao1365,2017-04-28T20:21:49Z,0,Great quality work Harrison! Thank you and congratulations!,True
@AndrosYang,2017-04-27T02:10:44Z,0,"Great series so far! But I had a few questions (correct me if I'm wrong anywhere)   1) the placeholder x you defined it has a 1d array essentially, is there any benefit to doing so? Why not create a 28 x 28 matrix?  2) The activation function is to determine if a summed element in a matrix fires a neuron or not, and replaces it with a one/zero/decimal depending on the threshold, correct?  3) The biases you create for each layer are 1-D, right? How is it that you can add a 1x500 matrix with a 500x500 matrix?",True
@rubencontesti221,2017-04-21T00:07:36Z,1,As usual great videos! I modify the code so that it creates the layers dinamically and can be play around with the number of layers. Nothing but a loop and a dictionary: https://github.com/rcontesti/sentdex_ml/blob/master/deep_net.ipynb. It seems to work well even with two layers and get stuck after four.,True
@shagunhegde1799,2017-04-19T17:44:59Z,0,"That was the best, most useful summary! Thank You, sentdex!!",True
@jmccormac01,2017-04-18T15:32:05Z,0,use vim instead of sublime ;),True
@CariagaXIII,2017-04-17T15:38:14Z,0,i guess copy and pasting is very common in neural networking,True
@coleskinker3733,2017-04-16T20:39:18Z,0,How is the out_channel input argument for the tf.nn.conv2d method computed to be 32/64 etc?,True
@jensdit,2017-04-13T09:30:58Z,0,excellent video!,True
@yujack8177,2017-04-11T11:31:39Z,0,where can i find your code?,True
@Baharsh100,2017-04-07T22:32:56Z,0,"Why didn't you use tf.add to add biases in the output layer? Instead you added by using ""+"". Why didn't we have activation function in the output layer? Thanks!",True
@ZJiang-wu3pm,2017-04-07T08:28:58Z,0,"Hi, In the tensorflow website, he uses mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)  but you use the   mnist = input_data.read_data_sets(""/tmp/data"", one_hot=True) do you download the MNIST data? how do you do that? because  mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True) does not work in my computer Thank you for your reply.",True
@DanielWeikert,2017-04-01T10:28:33Z,0,"Hey sentdex, one question. Shouldn't we also use an activation function on the output? In case no, why not? Could you elaborate on that? Thank you",True
@munnangimadhuri3334,2017-03-28T05:17:23Z,0,Thank you for sharing your valuable knowledge !!!...Good luck!,True
@ekaingarmendia,2017-03-21T12:35:38Z,0,"The model equation is actually 'weights * inputs + biases' (matrix multiplication is not commutative) , but tensorflow inverts the arguments in the matmul function. Love your videos though.",True
@dries1454,2017-03-20T17:33:37Z,0,Why 500 neurons in each hidden layer?,True
@toreadorjj5482,2017-03-19T19:50:01Z,0,It's telling me there's no module named tensorflow.examples.tutorial,True
@anandrahul7691,2017-03-19T09:23:43Z,0,The bias term is already added with the data times the weight vector than why tf.add is used .. I can't understand. Plz help,True
@bloolizard,2017-03-18T23:33:48Z,0,"@sentdex awesome stuff, how do we make a prediction using a trained model?  Didn't see it in the code?  Thanks.",True
@pramodkumar9669,2017-03-17T16:32:39Z,0,"Why its showin error like: NameError: name 'data' is not defined, though I have clearly defined it as: def neural_network_model(data):.......................can u plz help....",True
@garbilan0,2017-03-17T12:21:03Z,0,"i found a solution for the for loop. see code below. from features_set_handle import create_feature_sets_and_labels import tensorflow as tf # from tensorflow.examples.tutorials.mnist import input_data import pickle import numpy as np  train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')  n_nodes = [1500, 1500, 1500] # n_nodes_hl1 = 1500 # n_nodes_hl2 = 1500 # n_nodes_hl3 = 1500  n_classes = 2 batch_size = 100 hm_epochs = 10  x = tf.placeholder('float') y = tf.placeholder('float')  hidden_layers = [] for i in range(len(n_nodes)):     if i == 0:         hidden_1_layer = {'f_fum': n_nodes[i],                           'weight': tf.Variable(tf.truncated_normal([len(train_x[0]), n_nodes[i]])),                           'bias': tf.Variable(tf.constant(0.1, shape=[n_nodes[i]]))}         hidden_layers.append(hidden_1_layer)     else:         hidden_i_layer = {'f_fum': n_nodes[i],                   'weight': tf.Variable(tf.truncated_normal([n_nodes[i-1], n_nodes[i]])),                   'bias': tf.Variable(tf.constant(0.1, shape=[n_nodes[i]]))}         hidden_layers.append(hidden_i_layer)  print (len(hidden_layers)) output_layer = {'f_fum': None,                 'weight': tf.Variable(tf.truncated_normal([n_nodes[-1], n_classes])),                 'bias': tf.Variable(tf.constant(0.1, shape=[n_classes])), }   # Nothing changes def neural_network_model(data):     l = []     for i in range(len(n_nodes)):         if i == 0:             l1 = tf.add(tf.matmul(data, hidden_layers[i]['weight']), hidden_layers[i]['bias'])             l1 = tf.nn.relu(l1)             l.append(l1)         else:             l_i = tf.add(tf.matmul(l[i-1], hidden_layers[i]['weight']), hidden_layers[i]['bias'])             l_i = tf.nn.relu(l_i)             l.append(l_i)      output = tf.matmul(l[-1], output_layer['weight']) + output_layer['bias']      return output   def train_neural_network(x):     prediction = neural_network_model(x)     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))     optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)      with tf.Session() as sess:         sess.run(tf.global_variables_initializer())          for epoch in range(hm_epochs):             epoch_loss = 0             i = 0             while i < len(train_x):                 start = i                 end = i + batch_size                 batch_x = np.array(train_x[start:end])                 batch_y = np.array(train_y[start:end])                  _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,                                                               y: batch_y})                 epoch_loss += c                 i += batch_size              print('Epoch', epoch + 1, 'completed out of', hm_epochs, 'loss:', epoch_loss)         correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))          print('Accuracy:', accuracy.eval({x: test_x, y: test_y}))   train_neural_network(x)",True
@TheStarCycle,2017-03-14T22:49:56Z,0,"Sorry, I don't completely understand - what does the tf.nn.relu(l1) function do?",True
@JonathanAmbriz,2017-03-14T18:41:47Z,0,Thanks for taking us through the steps so methodically! I'm glad I found your channel.,True
@mahnoormalik5114,2017-03-14T10:48:31Z,0,unable to download mnist data set..connection timeout issue.How to resolve this?,True
@pocketman5510,2017-03-12T12:43:15Z,1,hl3 confirmed,True
@johnsuhalitka6704,2017-03-10T01:06:15Z,19,Its very rare to find such high quality content on this complex topic. It is very interesting to listen to someone who really enjoys their field.,True
@emanuelaliaci1858,2017-03-09T15:06:11Z,0,"Hi, are the scripts of your tutorials uploaded somewhere? great tutorials by the way! keep on doing it! :)",True
@cyl5207,2017-03-07T08:34:05Z,0,"thank you so much, sentdex!",True
@anacbfaria,2017-03-04T01:02:09Z,0,"I'm doing a classification using deep-learning for the final project of my graduation course in computer science and I have to say. I was a little lost in all the abstract concepts and your channel is really helping me, so thank you very much!",True
@iulianacraci2658,2017-03-03T09:23:10Z,0,Thank you so much!!! You're a genious ! In 30 min I understood everything I could not understand from an Udacity Deep Learning Nanodegree.,True
@mununulucky7594,2017-03-02T10:38:59Z,0,"Mr. #Sentdex Thanks for your great good and genius tutorials,I have a question on this part of classification.   As you have said while deciding training data and testing data rates, it is said that testing data should be unique or unseen data to the model until you use them for training. How do you remove certain percentage from the whole data set? Considering example below if I give my model this values it means 80% is for training... then 20% is for testing! Does model filter it self 80% and left out 20% or we do by codes?. I am confused about here, any one may help me to understand more. Thanks!    ""training_features = shuffled_tr_features[0:800] training_labels = shuffled_tr_labels[0:800] testing_features = shuffled_tr_features[800:] testing_labels = shuffled_tr_labels[800:] """,True
@Mrpawan786ful,2017-02-25T20:04:29Z,0,"Enhanced code with loops. Happy Computing!!!  import tensorflow as tf  from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  n_nodes_hl = []  range1 =50  for _ in range(range1):  n_nodes_hl.append(500)  n_classes = 10  batch_size =  100  x= tf.placeholder('float',[None,784]) y= tf.placeholder('float')  def neural_network_model(data):   hidden_layer = []   size=784  for i in range(range1):   if i > 0:    size = n_nodes_hl[i-1]   hidden_layer.append({'weights':tf.Variable(tf.random_normal([size,n_nodes_hl[i]])),          'biases':tf.Variable(tf.random_normal([n_nodes_hl[i]]))})    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl[i],n_classes])),      'biases':tf.Variable(tf.random_normal([n_classes]))}   l=[]  data1 = data  i=0   for i in range(range1):    if i > 0:    data1 = l[i-1]   l.append(tf.add(tf.matmul(data1,hidden_layer[i]['weights']),hidden_layer[i]['biases']))   l[i] = tf.nn.relu(l[i])   output = tf.matmul(l[i],output_layer['weights']) + output_layer['biases']   return output  def train_neural_network(x):   prediction = neural_network_model(x)  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))  optimizer = tf.train.AdamOptimizer().minimize(cost)   hm_epochs = 50   with tf.Session() as sess:   sess.run(tf.global_variables_initializer())    for epoch in range(hm_epochs):    epoch_loss = 0    for _ in range(int(mnist.train.num_examples/batch_size)):     epoch_x, epoch_y = mnist.train.next_batch(batch_size)     _, c = sess.run([optimizer,cost],feed_dict= {x:epoch_x,y:epoch_y})     epoch_loss += c     print('Epoch',epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))   accuracy = tf.reduce_mean(tf.cast(correct,'float'))   print('Accuracy: ', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))  train_neural_network(x)",True
@mhhajeer,2017-02-24T07:39:25Z,0,"nice simple explanations, great video. As for your loops, try an array to hold layer objects, or a dict if you prefer strings for layers names. Another solution is to use the eval  or exec, but you have to be careful with exec (not a secure coding).  example: Python 2.7.5 (v2.7.5:ab05e7dd2788, May 13 2013, 13:18:45)  [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type ""copyright"", ""credits"" or ""license()"" for more information. >>> WARNING: The version of Tcl/Tk (8.5.9) in use may be unstable. Visit http://www.python.org/download/mac/tcltk/ for current information. >>> x1 = 2 >>> eval('x'+str(1)) 2",True
@samwise1491,2017-02-22T20:52:38Z,1,"This is much appreciated, thank you so much for making these. Very clear and informative!",True
@subhammahanta2689,2017-02-21T09:23:45Z,0,I am trying to build a tutor chat bot that should be able help the student with the questions related to probability. Is it feasible or advisable to create my own dataset from the prescribed probability text books and use them to train the data?,True
@adeeb12321,2017-02-18T08:44:58Z,1,"thanks , this is so helpful video but how can I make my own "" input_data"" ?",True
@PRSHNTTKMR,2017-02-17T16:56:10Z,0,could you share all the code you wrote in the tutorials on github. It would be useful,True
@rishabhsinha9882,2017-02-13T19:18:50Z,0,"The videos are really helpful, thankyou. I just had a small query, if i were to pass a point cloud dataset(LiDAR data) as input to the network what would be the changes i would need to make?",True
@Dougystyle11,2017-02-13T18:49:03Z,0,"Is it just me or does the mnist = ... line not work? It looks like Yan LeCun's website is down, I'm thinking maybe it has to do with that...",True
@Rocksrock1000,2017-02-09T02:10:01Z,0,You sir have just saved my ass. Great explanations and you have got yourself another subscriber!,True
@wunderlust7252,2017-02-08T23:43:32Z,0,running this on my raspberry pi and getting a memory error,True
@TamilSelvan-xf6ut,2017-02-08T17:22:12Z,0,what does tf.add do?,True
@dineshlamarumba4557,2017-02-05T19:11:37Z,0,new subscriber to your channel.,True
@sonofdad123mc,2017-02-02T22:46:21Z,338,Your videos are at least 5x better than Siraj's,True
@garbilan0,2017-02-02T16:14:25Z,2,you mentioned a for loop all the time so i dont have a perfect solution but a workaround that i am using is just write a script to print those lines then copy paste. it wont make it dynamic but it saves errors.,True
@atheeth2189,2017-02-02T14:29:33Z,0,why are you adding bias unit to output layer?,True
@matthewkantor,2017-02-02T02:19:49Z,0,"I think it's ""flatten"".  Squish is more along the lines of compressing (at least in Ruby)",True
@pratik6447,2017-02-01T10:01:55Z,0,"Hi Sentdex, Any updates on "" Theano"" and ""Torch"" ?",True
@byronshilly3923,2017-01-28T00:51:45Z,0,"Great stuff man. Love your enthusiasm, keep it up.",True
@AshokVardhanIITBombay,2017-01-20T17:56:35Z,3,I have a doubt regarding matmul. Isn't matmul operation meant for product of two matrices A and B? Here we want to multiply a vector x with the weight matrices W. Why are we using matmul here?,True
@im_tanmay_g,2017-01-17T11:44:14Z,0,Awesome work mate. Really helpful!,True
@gimbopgimchi,2017-01-11T16:43:22Z,0,"I am learning a lot from these lectures. Glad to finally understand how TensorFlow works and add it to my toolbox, thank you very much!",True
@SethuIyer95,2017-01-09T08:18:59Z,0,Why biases is initialised as random normal?,True
@carlosrmz2920,2017-01-08T04:00:41Z,0,"I love your channel, Harrison. Thanks for always sharing so much!",True
@luckies6945,2016-12-29T08:04:04Z,3,"Great video at all.  Just one thing i figure out at placeholder[None,784]. None is the hight or the rows of an matrix, you are right but it stands for any number and not rly for nothing. This any number is in case the batch number. But puting in None makes maybe more sense cause we dont wont to care about the end of the split, maybe we have not exactly 100 there. So by an batch size of 100 this placeholder ""x"" will have [100,784]. Thats the input data.  Try and change None to any number and also change batch size... Helps me a lot to understand the input data and matmul stuff.",True
@gauravsahu272,2016-12-26T13:11:30Z,0,Hi ! The tutorial is awesome and I don't know if you know or not but you can use some pep8 auto convention plugin for submlime to at least not worry that much about the syntax  ^_^ !!,True
@chillwinternight,2016-12-23T00:59:34Z,0,"Great tutorial! Thanks. Btw, does anyone ever say to you that you look a bit like Edwards Snowden ;)",True
@daniel100097LPs,2016-12-22T23:44:38Z,0,It's possible to use the exec() function (for loop),True
@lakeguy65616,2016-12-22T14:59:28Z,0,Great video. very clearly explained.,True
@lakeguy65616,2016-12-22T13:55:41Z,0,"in general, how do you determine the number of nodes in a hidden layer? How do you determine the number of hidden layers?  Assume there is a relationship between the number of input nodes and the number of output nodes?",True
@lakeguy65616,2016-12-22T13:51:08Z,0,I enjoy your videos. thanks for all your effort. I have a question about the relationship between the number of input nodes and the number rows in the training set. What is the rule of thumb to avoid curve fitting or memorization?,True
@beno9679,2016-12-21T07:15:00Z,0,Could you OO the Hidden Layer (sorry for my javascript but)? Var Number_of_hidden_layers : int = .... ;  Var MyHiddenLayers : HiddenLayers[ ] ; //further down;  MyHiddenLayers = new HiddenLayers[ Number_of_hidden_layers ] ; for ( i : int = 0 ; i < Number_of-hidden_layers; i++){ Assign Values }.   Something like that. Then something like. MyHiddenLayers[ ii ].GetWeights(); To get the weights and biases.,True
@aleksandarrusinov9974,2016-12-17T17:54:05Z,0,What about placing all the layers into an array?,True
@jivan476,2016-12-15T15:57:26Z,0,"You're losing sooooooo much time (and your videos so much fluidity) by not using an automatic syntax checker and wondering if you forgot a '(' and talking about it for 2 minutes and having to launch a terminal window to be certain - syntax checker plugins exist for every text editor, you should really get yourself one, and your life will change for the better",True
@ScrapeLP,2016-12-14T22:40:35Z,0,Great tutorial so far :D Keep this up!,True
@abdussalambrur8093,2016-12-12T18:10:03Z,0,"What means read_data_sets(""tmp/data/"", one_hot = True) ? It is path where from data read(images), isn't it? I have data in /home/abdus/Downloads/tensorflowData/, where have .png data. When run my code nothing else, wait more than 10 minitues but nothing show anything else. I have also debugging with print() after mnist declaration nothing show. Plz suggestion, How to read data actually or any error my process.",True
@soumen_das,2016-12-09T05:31:03Z,0,why are the biases in output layer an array?,True
@amalikhalil6137,2016-12-06T18:51:10Z,0,"your tutorial really make me more understand on DNN. i'm working on a project to detect people who smoking and using and handphone in petrore station,does it means that i will have two classes which is DANGEROUS and NOT DANGEROUS?? I really hope u canhelp me with this.",True
@NPatel-vk5sk,2016-12-05T03:03:02Z,0,"You seem to keep thinking about labeling the layer index so that you can use the for loop.  In python, Can you not just make a list/vector of dictionary? That way, the index corresponds to the ith layer, which you can easily loop over.  Thank you for the videos. :)",True
@Undeworld667,2016-12-04T15:43:32Z,56,Glad to see some education that is laid out in a digestible manner. Feels like all the TensorFlow stuf I've seen to date is too abstract/fast paced.,True
@AdarshSodagudi,2016-11-29T10:01:38Z,0,I am a total beginner and i don't know what neural network is can you suggest me from where i should start. please,True
@andrewdudley3408,2016-11-24T05:10:13Z,0,"Isn't the whole point of a bias to change the threshold parameter to 0 instead of some other number? IE, simplifies the equation because now the threshold is just a part of the linear function?",True
@rokeyshane4318,2016-11-21T17:45:56Z,0,AWESOME AWESOME AWESOME.........................AWESOME Thanks a lot,True
@TheMinimumPC,2016-11-14T22:57:21Z,0,"Aren't Hidden layers supposed to be actually hidden? I read in a tutorial that Hidden layers are basically what you apply within existing layers in order to inject non linearity into the layers.   A hidden layer would therefore be the application of a non linear function like ReLU or something to the output of the function x.w+b. So as to give us another variable h which denotes the number of hidden layers or the non linearity of the overall network.   w.x would only represent the data that falls on a straight line that passes through the origin.  w.x+b allows us to represent all the data that passes through any straight line. NonLinearFn(w.x+b) would be able to represent data that represents any simple curve (degree 2 or less) NonLinearFn(NonLinearFn(w.x+b)) would be able to represent any curve of degree 3 or less  Finally with a variable number of hidden layers, we'd be able to represent any curve of any degree.  I'm just learning so feel free to correct me if you find this wrong.",True
@leonmcdowell197,2016-11-14T03:27:16Z,2,I have a solution for how to automatically generate the hidden_level statements for this tutorial.  Should I just post the solution as a comment like this or is there some other path for submitting code solutions?,True
@vabble6432,2016-11-11T15:24:02Z,0,"Dude, your cursor at 9.10 gave me a heart attack.. Thought it was dead pixels on my screen..",True
@matteotesti2337,2016-11-09T09:09:41Z,0,"Hello there I'm running the code but I got this error: File ""deep-net_2.py"", line 28     hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),                                                                                          ^ IndentationError: unindent does not match any outer indentation level  Any suggest ?  Many thanks",True
@freakydude4u2001,2016-11-08T20:05:40Z,0,Love your series. great work.. keep it up... Quick question. In the line where you are calculating output why did you chose to remove tf.add to add the result of the tf.matmul and output['biases'].,True
@yuanhaolee3478,2016-11-08T13:03:13Z,0,"I have a question regarding the adding of biases. If the objective is to prevent zero input into L1, why are biases added for L2 and L3 as well. My concern is that using Relu as an activation function, there will naturally be zeroes which mean no fire of the ""neuron"". Wouldn't adding a bias to these L2 inputs remove all the zeroes?",True
@rajupowers,2016-11-06T18:01:29Z,0,you rock buddy! I am following your tutorials,True
@akshatagrawal819,2016-11-05T03:56:55Z,1,"Even I'm dying to have such a for-loop, in pretty much most of my codes!! >_<",True
@nicholasmowattlarssen4701,2016-10-30T21:07:57Z,0,Shouldn't the activation function be applied to the output as well? As in return tf.nn.relu(output),True
@MrFromEurope,2016-10-25T15:26:45Z,0,Isn't it true that the bias is just the negative threshold? bias = -threshold??,True
@dctmbu,2016-10-11T20:16:46Z,0,"the best tutorial, everything that ive been reading before makes sense now",True
@Starscreen60,2016-10-10T10:03:23Z,0,"Hey! Today I tried running the model above, but it says that tensorflow.examples.tutorials.mnist cannot be imported since the module cannot be found! I think tensorflow was updated over the last 2 months, so what can I change it to?",True
@mattw9489,2016-10-06T03:35:51Z,0,any update on if anyone figured out that for-loop yet? i'd rather not result to meta-programming.  also amazing tutorials. your work has had a tremendous impact,True
@FLuXGriZzly,2016-10-04T15:50:41Z,24,"Why would the biases be a random normal? Isn't it good practice to have a simple bias of 1 for each NN layer (excluding the output) ? Even if each neuron were to receive a bias, what benefit do you get from making them a random variable ? I've coded fairly complex ANN in Java, and cannot understand why you have added this step. It won't affect the classification of this particular network, but if you run the same identical network again, since different random biases would be used, couldn't the overall classifications differ slightly ? Loving the series so far, since I am completely new to TF and Python. Maybe it's just me not grasping a the concept or tf.random_normal correctly. Would really appreciate an explanation.",True
@qzorn4440,2016-09-29T04:02:51Z,1,this helps to explain what the heck MIT was stating. thanks.,True
@darchcruise,2016-09-27T21:12:09Z,4,"Is there a way to find out ""how many nodes"" and ""how many hidden layers"" are needed for any given neural network? In other words, how did you come up with 500 nodes and 3 layers?",True
@darchcruise,2016-09-27T15:27:15Z,1,"What does  n_classes  represent in the model? I understand input, output, nodes, weights, and bias, but I don't see where classes are explained. Overall, great video! Thank you!",True
@willhatch7721,2016-09-26T16:10:34Z,134,"Hey dude, @27:30 when you're using the + symbol inside the call to add, you're actually doing the addition before making the call to add when not separating variables with a comma...",True
@cashphattichaddi,2016-09-10T19:03:33Z,0,Amazing man!!,True
@nicnl255,2016-09-09T04:06:00Z,0,Awesome tutorial.,True
@jkljklqweqwe5221,2016-09-02T23:45:04Z,0,Thank you !,True
@santiagopenate8644,2016-08-25T13:35:24Z,0,"Hi,   This would be the recursive function:  def neural_network_model2(data, input_nodes, layer_nodes, output_nodes):          nodes = [input_nodes] + layer_nodes + [output_nodes]          n_layers = len(layer_nodes) + 1     for i in range(n_layers):                 print('Layer', i, ':' )         print('\tweights:', nodes[i], 'x', nodes[i+1])         print('\tbiases:',  nodes[i+1])         w = tf.Variable(tf.random_normal([nodes[i], nodes[i + 1]]))         b = tf.Variable(tf.random_normal([nodes[i + 1]]))          # formula = data * weight + bias         if i == 0:             output = tf.add(tf.matmul(data, w), b)             output = tf.nn.relu(output)         else:             output = tf.add(tf.matmul(output, w), b)             if (i + 1) < n_layers:                 output = tf.nn.relu(output)      return output  BTW awesome tutorials!",True
@hunarahmad,2016-08-21T19:28:23Z,0,thanks,True
@brians7100,2016-08-05T17:15:23Z,0,"13:40 I think the proper term would be ""unroll""",True
@paulpunguta7104,2016-08-04T21:15:00Z,0,I'm trying to use TF for text clasification. Do you know a good way to transform text into vectors? Almost all tutorials are about images...,True
@Diogotresmil,2016-08-03T14:52:44Z,0,"this question may be stupid, but is there a good reason not to do this in matlab? please don't kill me python lovers .p",True
@Freeball99,2016-08-02T00:49:00Z,3,"Thank you for an excellent tutorial!!  I have refactored your example to allow for arbitrary number of hidden layers (and nodes) and learning rate.  I see that others have already posted, but this might be simpler.  https://github.com/apf99/TensorFlow/blob/master/tensorflow_example.py",True
@NikitaYVolkov,2016-07-30T11:38:15Z,0,> feed forward + backprop = epoch  Wat?!,True
@petecanfield8432,2016-07-29T05:54:39Z,1,I realize that there are already a ton of solutions to the dynamic layer amount problem but I came up with a solution anyway. It seems like it yields similar performance and is super easy  to use.  https://github.com/pieisbetterthancake/examples/blob/master/dynamic-deep-net.py  sorry that I couldn't put the code directly in the description (I was having formatting problems),True
@alexspmiranda,2016-07-25T20:45:00Z,0,Is possible use a personal dataset? I have a folder with 50.000 pre-processed images and I would like to use it.,True
@muhammedeltabakh852,2016-07-25T01:13:56Z,0,"If I'm working on a mac what should this piece of line be (""/tmp/data/"") because it will throw me an error !",True
@RoleModelUSC,2016-07-24T17:09:55Z,0,"Awesome tuts +sentdex ! Thanks a bunch for demystifying tensor flow with NN. Nonetheless, I was wondering what was your intuition behind using '/tmp/data/' and not mnist = input_data.read_data_sets('MNIST_data', one_hot=True) or mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True) in the documentation.  Responses from any other person that knows why will be highly appreciated as well. Thanks guys!",True
@edoardopona553,2016-07-21T15:08:06Z,4,"Thank you a lot for all the tutorials! Love them! This is what I could come up with, for the for loop, am I going in the right direction? How can it improve?   import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data  mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True) n_nodes = [784, 500, 400, 300] #number of nodes, so that n_nodes[1] is n_nodes_hl1 etc.. the first value is the 784 that is needed at the beginning  layers = [] hiddenLayers = [] n_layers = 100 #number of layers  n_classes = 10  def neural_network_model(data):  layers.append(data) #the first is going to be the input data  for i in xrange(len(n_nodes)-1):    if i < len(n_nodes-1):    hiddenLayers.append({'weights':tf.Variable(tf.random_normal([n_nodes[i], n_nodes[i+1]])),     'biases':tf.Variable(tf.random_normal(n_nodes[i]))})     layer=tf.add(tf.matmul(layers[i], hiddenLayers[i]['weights']) + hiddenLayers[i]['biases'])    layers.append[tf.nn.relu(layer)]    elif i == len(n_nodes-1):    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes[i], n_classes])),      'biases':tf.Variable(tf.random_normal([n_clases]))}    output = tf.matmul(layers[i], output_layer['weights']) + output_layer['biases']    return output",True
@michaelcho,2016-07-21T15:08:00Z,0,Yes Keras is awesome!,True
@wth123msg,2016-07-21T01:41:10Z,14,"dynamic  def networks(data, nodes_and_layers):     length = len(nodes_and_layers) - 1     # create hidden layers     hiddenLayers = [0 for i in range(length)]     for i in range(length):           hiddenLayers[i] = {'weights': tf.Variable(tf.random_normal([nodes_and_layers[i],nodes_and_layers[i+1]])),                                           'biases' :tf.Variable(tf.random_normal(nodes_and_layers[i+1])) }          hiddenLayers[-1]['biases'] = tf.Variable(tf.random_normal([nodes_and_layers[-1]])) # change the biases of output       # relu(input_date * weights + biases)       layers = data       for i in range(length - 1):           layers = tf.nn.relu(tf.add(tf.matmul(layers, hiddenLayers[i]['weights']) + hiddenLayers[i]['biases']))       return tf.matmul(layers[-1], hiddenLayers[-1]['weights']) + hiddenLayers[-1]['biases'] size_of_output = 10 size_of_input = 784 nodes_and_layers = [ size_of_input, 500, 500, 500, size_of_output ] networks(data, nodes_and_layers)",True
@robweatherston2156,2016-07-20T23:10:44Z,52,"I'm loving your tutorials, thank you very much.",True
@johannesgh90,2016-07-20T20:59:51Z,14,suggestion: Meta-programming to print the out a python script containing the nn model instead of a for loop.,True
@panagiotispetridis7961,2016-07-20T19:08:42Z,1,"Really nice tutorial! Definitely looking forward to the next one!  By the way did you know that ""epoch"" at Greek (""ÎµÏ€Î¿Ï‚"") is read as ""epos"" and was mostly used at odyssey and Iliad and comes from the verb ""á¼”Ï€Ï‰"" that means ""say"" at greek. Epoch was something like a story/poem with either heroic, mythological or education content.",True
@onepunchsaitma1413,2016-07-20T17:38:56Z,3,I'm curious. How do you learn all these modules\libraries so well and in such a short time (guessing here). What's your secret? So many more questions....,True
@mikberegov,2016-07-20T16:35:33Z,4,You can just use Keras which can compile a model into Tensorflow code instead of inventing any loops :),True
@levyroth,2016-07-20T15:56:23Z,0,"I don't know what you really mean by samples. Do you mean cases, observations, units of analysis?!",True
@SunilKumar18,2016-07-20T14:29:12Z,14,This is awesome. Thanks man. Been wanting to take a dive into deep networks for a while now but it just scared me. But your work is awesome.  #Respect,True
