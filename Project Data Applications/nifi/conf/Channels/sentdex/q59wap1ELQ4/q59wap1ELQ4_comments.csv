author,updated_at,like_count,text,public
@martinizvorov,2024-05-21T13:19:23Z,0,How long did it take to create the model,True
@Matthew8473,2024-02-06T19:04:57Z,0,"This is a marvel. I read a book with similar content, and it was a marvel to behold. ""The Art of Saying No: Mastering Boundaries for a Fulfilling Life"" by Samuel Dawn",True
@svenbardos6637,2023-12-26T14:08:10Z,0,If you say your random bot didn't win a single game. Who did it play against? Another random bot? So it was a tie because it never got to an end?,True
@kailalueni3251,2023-12-03T21:14:41Z,1,I love you idea of drawing your own minimap! Thats a smart way to make more information available easily.,True
@Kaszanas,2023-12-02T20:43:50Z,0,If you'd like to collaborate on this as a research topic I would very much appreciate that. I am working on StarCraft 2 research for about 6 years at this point.,True
@HubertRozmarynowski,2023-12-01T18:29:06Z,0,i'm not usually a fan of your videos but this one did not spare crucial information and presented the topic nicely,True
@SocalNewsOne,2023-06-26T04:45:31Z,0,Thanks! Your tutorials were the first that worked for me.  Biggest problem that I had was the directory path for the Starcraft maps.,True
@ichevtchenko,2023-04-23T06:50:18Z,0,Is this possible to do for Age of Empires?,True
@Neceros,2023-04-16T10:23:15Z,1,This is great! I'd love to see something like this could compete in the arena,True
@AngryGnome87,2023-03-12T07:34:12Z,0,"Trash, AI is not an AI, it's just this idiot programming a bot to play",True
@GameReality,2022-11-18T00:28:10Z,0,Can AI learn how to play  3D Tetris called BlockOut or BlockOut2  ?,True
@BalimaarTheBassFish,2022-11-15T00:58:54Z,0,Well its been a while. A little sad we have stopped here -or at least seem to have.  I was always curious how multiple agents working together could be easily implemented but I guess I am doomed to not know.,True
@mybachhertzbaud3074,2022-11-03T01:35:57Z,0,You sure this isn't a DARPA project!???üòí,True
@augiblutz2852,2022-10-29T13:15:00Z,0,That's what we need; more violence & stupidity,True
@floydbarber7528,2022-10-14T03:49:30Z,0,"oh man, i needed that book 3 months ago, made with 3 others our own NN and genetic algorithm to play mario. also with reinforment learning. i was thinking about how hard it would be for sc to do so. but it doesnt seemed too hard, but you used didnt wrote your own neural network right?",True
@achillestroy3122,2022-10-12T14:08:17Z,0,I remember when you live coded AI plays GTA v and that too on python's default IDLE. Bring those days back. Great video though.,True
@redaslife1392,2022-09-28T22:13:21Z,0,If else,True
@cmilkau,2022-09-21T10:19:45Z,0,"Interesting actions. Not only do they encode a lot of knowledge about the game, they include deep causal chains that otherwise would take long to learn.",True
@cmilkau,2022-09-21T10:07:32Z,0,"If you already know the things and their positions, why give the model extra work by encoding them into an image? You could just input them directly as vectors.",True
@JokeryEU,2022-09-19T18:29:47Z,0,"would prefer where you tell the AI to learn to beat the other player/AI by experimenting whats best without giving specific instructions other than end goal and how to achieve it, beat enemy and maybe some basic logic how to mine to speed things up",True
@Hoshikani,2022-09-02T22:36:12Z,0,16:10 why all these computation hungry algorithms should be performed in a such slow and incredibly inefficient language python?,True
@lucasbussinger3955,2022-08-31T11:05:58Z,0,Does anyone knows what app he uses to keep track of the data ? ( 13:59 ).,True
@abjgupta,2022-08-26T09:04:06Z,0,"""Instead it would want to build lots and stay alive for as long as possible"", looks like the best strategy for beating a human.",True
@teardowndynamic6171,2022-08-10T13:50:27Z,0,i know nothing about programming or AI but this is just so fun this watch,True
@teardowndynamic6171,2022-08-10T12:04:07Z,0,"i am trying to make a AI that will farm for me in rust, but ia mso lost xD, if I understand you are not using computer vision because the camera movement is to complicated ? so you are building data from minimap only ? if i wanted to train my AI to farm sulfur nodes in rust what would be your approach ?",True
@Veptis,2022-07-25T10:57:13Z,0,"Coming up and implementing a good evaluation function is the hidden challenge if any deep learning project.  What I feel like your model is lacking - is finer options. It's just classifying one out of 5 actions and that's it. So it's strongly limited by your hard crafted actions. As usual, those interfaces between models lose fine details. Which I believe is why large langauge models with all the residual connections do so great as they do all the tasks implicitly.",True
@Schizomaniac,2022-07-09T09:54:41Z,0,,True
@anthonyjobey8821,2022-06-05T15:19:56Z,0,I didn't know Nicloas Cage played star craft!,True
@cursedmountainstudios,2022-06-05T13:09:58Z,0,"You were so preoccupied with if you could, you didn't stop to think if you should... and so it begins.",True
@KennTollens,2022-06-05T11:49:04Z,0,"Hey Lieutenant Commander Data, I'm going to go buy your book so I can join Starfleet too!",True
@cedrickram3180,2022-06-05T06:50:39Z,0,"Some time series analysis (windowed access to what has been searched, where stuff was, ...) would probably help the AI make better decisions. The data of just the map does not do a good job of storing time-information.  Your rewards seem like a good fit. Great video!",True
@damanrando7608,2022-06-04T19:26:59Z,0,I...... I think I understand what I just watched... but I'm not sure,True
@atol71,2022-06-02T08:04:53Z,0,Against the AI of Starcraft? Pseudo?,True
@bronsoncarder2491,2022-06-01T18:58:28Z,11,"Here's my issue with your approach: Your actions are basically just a hard coded list of commands. You could essentially just create a hierarchy of those commands and apply a little probability and get similar results.  The way you've set this up, the AI will never develop novel strategies. It can, at best, play with the topmost level of human strategy available (and that's only if you spend the time to hardcode that into each action). And, that's cool, but... I feel like the point of an exercise like this should be to see how the AI ""thinks"" about the task and what novel strategies might arise from that.  Idk, I do understand that the computing power to decide between the thousands of different options available at any given moment in an RTS is beyond most personal computers, but... I feel like hard-coding the actions kind of defeats the whole purpose.",True
@cryzz0n,2022-06-01T18:51:28Z,0,"Kind of neat, I'm wondering if you looked at the AlphaStar research at all to do this, or looked into the StarCraft 2 AI community? There's about 70 coders of various bots and AI that compete against each other and it'd give you a ton of ideas on build choices and especially unit control and decision making.",True
@rustythecrown9317,2022-06-01T17:56:35Z,0,"I don't personally feel the machine uprising is even a possibility , but if it happens , it's fukwits like these that taught the AI how to fuck us over.",True
@ButtersDClown,2022-06-01T17:43:33Z,0,"Very cool idea. I think programing a few meta builds into your algorithm and seeing how it learns with time (if achieved ""this"" by ""this time"" do ""this"" otherwise do ""this"") like doing a rush build ect.",True
@Magicks,2022-06-01T16:35:03Z,0,well done sir,True
@MarinePoolee91,2022-06-01T14:00:49Z,0,This whole thing was a fucking tease.  I just wanted to watch the ai play.,True
@troyh3628,2022-06-01T13:30:03Z,0,"I read the title and thought ""Isn't that redundant? Star Craft has its own A.I."", then I thought ""Oh, crap you're mixing A.I.s. Who could have known the singularity was going to come from the greatest RTS ever?""",True
@bobo-lc4yi,2022-06-01T07:12:17Z,0,"""with this many actions to sift through before getting an actual reward its far too challenging for a model to discern which one was the good action and which ones were bad actions""  dude you just re-invented the concept of religion :)",True
@Ocodo,2022-06-01T04:57:04Z,0,I don't understand any of this. Looks great.,True
@andrewstidham7950,2022-06-01T01:43:21Z,0,Yeah keep teaching machines things and they may say eh we don't need these humans... üòÅ,True
@IlIWarGIlI,2022-05-31T03:16:33Z,0,years of learning starcraft leads me to simply queue a few things but otherwise not give too many orders as that delays facilitation of actions.,True
@BDi321,2022-05-30T18:34:15Z,0,"What would have happened if you made the incentive a positive to gather resources and a stronger negative to the opponent gathering resources? Something like ""do whatever you can to prevent the opponent from gathering resources, but if you don't see how to do that yourself, then gather all the resources you can."" That would make the AI aggressive to attack and disrupt economies.",True
@HalIOfFamer,2022-05-30T13:38:56Z,0,"Maybe code a reward for seeing unique enemy units/buildings. That way ai would have to scout the map for enemies, then double the reward for attacking if the attack unit was recently seen by a scout unit.",True
@TheRealSether,2022-05-30T10:00:26Z,0,I would love to be able to play a rts with a real learning AI.,True
@jaydaksrules5316,2022-05-30T08:23:27Z,0,thats extreme bro,True
@Ulmaramlu,2022-05-29T20:33:15Z,0,"Having the rewards be dependent on how long the game has been running would also be a good way to go (only watched up to 10 minutes in so far).  Higher rewards early in the game, especially for winning.",True
@matheusmterra,2022-05-29T15:47:45Z,0,"Well, you could add rewards for scouting new locations, rewards for keeping units alive, and check out the math of pro starcraft players of what units you should use and when.  Also tier rewards for which units and buildings it will destroy to reinforce priority targeting for better performance.",True
@francenkovcan5211,2022-05-29T12:25:18Z,0,"another idea is to give a negative reward for every second the game lasts, meaning that the AI would want to end the game as soon as possible",True
@tronowolf,2022-05-27T19:30:19Z,0,What if the reward incentive was striking a perfect flow and balance of resource gathering/construction?,True
@romualdaskuzborskis,2022-05-27T16:25:04Z,0,"Imho, one reward factor you have missed is efficiency - how quickly the game is won as a miltiplier to winning screen. This is basis of every player motivation - how quickly can you win against other player",True
@ericzahn274,2022-05-27T11:20:22Z,0,Great vid. Buying the book.,True
@alrey72,2022-05-27T04:38:00Z,0,Can some of the values be included in the iteration or training ... like for example the reward values?,True
@RidiculousThisIs,2022-05-26T17:46:04Z,0,"Sooo long story short ""machine learning"" is what you tell it to learn, gotcha",True
@witherslayer8673,2022-05-22T18:43:21Z,0,"how about building more than just void rays(having stats of each unit, cost, and space. may save for big units, or LOTS of small units) and where air units can go, and were ground units can travel",True
@romanlee7082,2022-05-22T10:05:58Z,0,"Hi, thank you very much for sharing this video.It opened a new window for me to know AI. May I know where I can download the code corresponding to your video please?Thanks again.",True
@ivanmakara7320,2022-05-20T21:48:07Z,0,"What issues did you run into using a reward mechanism of: Reward = Resources_Harvested + Resources_Remaining * X where X = 1 in a win, and X = 0 in a loss This shouldn't prolong a game, because any resource harvested from the map will equally drain resources remaining. I would imagine it would actually speed up the game, because the only activity that reduces the reward is the enemy harvesting resources, which is a thing you would want to disrupt anyway. (I guess also a probe dying while carrying a unit of resources, but I think that change would be negligible)",True
@RickBeacham,2022-05-20T17:29:46Z,0,I really want to buy that book.,True
@RickBeacham,2022-05-20T17:20:30Z,0,Great stuff!  Super interesting.,True
@mattxp3389,2022-05-20T07:19:43Z,0,i literally understood none of this... but i actually watched the whole video...,True
@SmallSpoonBrigade,2022-05-20T03:36:52Z,0,"SC2 is one of my biggest letdowns from Blizzard, that and Diablo 3 being so terrible. I wish gaming companies would remember that not everybody wants to play at the highest levels, we need a game that's actually fun to play without having to spend hundreds of hours learning the nuances. After such a long wait, getting a game as boring as SC2 is just inexcusable.",True
@OnlyKoolaid,2022-05-20T03:23:55Z,6,SENTDEX: I'm going to teach AI to rush Voidrays.  Protoss mains: STOP! I can only get so aroused.  Zerg: This is a war crime.,True
@HamguyBacon,2022-05-19T03:00:42Z,0,Isn't it cheating since there is no fog of war and knows where all minerals and resources are?,True
@FuneralProcession,2022-05-18T14:59:53Z,0,Reward for attacking and killing is so psycho though üò≤,True
@FireTouched,2022-05-17T19:46:33Z,0,"I wonder the reward structure. It doesn't realy feel like looking for optimised play as the only negative reward you mentioned was the loss itself and after that only determining efficiancy by the total score. But what about tracking negative rewards (loss, loss of units/structures/resource access, etc.) and comparing the positive and negative score? That way the AI could pick a winning strategy that accrues few losses over one that accrues many losses - despite both having the same end score. And in turn the AI would be able to know the errors due to the dip in the comparison.  Also maybe implementing a way that reduces positive/increases negative score over time? That way stalling would also be discouraged.",True
@tibielias,2022-05-17T12:35:56Z,1,What an awesome video! I wonder how making an API like this for other RTS games would be possible and then training AI models for those separately. ü§î,True
@stairsofteegarden9787,2022-05-17T09:09:00Z,0,We played Vs computer instead of Vs player 2 way before ML was a thing but yeah incorporating learning based on reinforcement,True
@dracomurdock6349,2022-05-16T22:14:56Z,0,"The criteria I would try to ensure it has highest on its priority is- if you win, only- unit efficiency. IE: how many resources did this unit earn, or destroy for an opponent, relative to its own cost? Averaging them out, and defining those units by a percentage based on the actions they were made to perform- and segmenting the game into the first 5 minutes and the rest of the game- you could provide a huge assist to the AI learning more complicated macro and micro strategies.",True
@FF7Cloud,2022-05-16T21:37:22Z,0,it might help to allow a phoenix now and then for scouting purposes since void rays are super slow,True
@gacserosaurus,2022-05-16T04:25:19Z,0,does that work with Age of Empires as well?,True
@Gameboy499,2022-05-14T16:49:53Z,0,"Hello, may I ask how to automate process of training there? Or I need to manually restart game everytime?",True
@VanWinger,2022-05-14T16:08:09Z,0,bonjwAI,True
@boon1580,2022-05-14T07:48:36Z,0,"i think if can make the ai go apeshet on micro-ing the fights, u can pretty much win at the first minute using just drone on drone action.",True
@electricimpulsetoprogramming,2022-05-14T02:23:44Z,0,9:00,True
@Bubu567,2022-05-13T15:44:38Z,0,"Rewarding unit destruction over victory could lead to the bot learning NOT to win, and instead stalemate, in order to maximize unit destruction.",True
@AcMcRevo,2022-05-13T15:35:47Z,0,I am studying medicine. I am big Python fanboy since I was 14 years old. Is there actually any way that medicine and Python could be combined in a research?,True
@TerminalGear,2022-05-13T11:50:28Z,0,Reward it whenever it builds additional pylons.,True
@whateverppl1229,2022-05-12T11:57:04Z,0,"9:20 that's what I figured you'd do but my question is would it be a bad idea to take away points if an enemy unit/building dies? because then, it would be rewarded for attacking. (more points from a kill than a loss, or individually price every enemy unit/building as its own value and same with ally losses) to help teach it to not lose units but to do damage.",True
@linus3nvy,2022-05-11T15:53:31Z,0,"The amount of suggestions from ppl telling you what you ""should"" have done is highly amusing",True
@d43w_,2022-05-11T04:09:45Z,0,this dude codin with pickels,True
@StrzelnicaFX,2022-05-10T13:57:21Z,0,didn't the game already have an AI to play against by default?,True
@J3553xAnotherFan,2022-05-09T21:49:11Z,23,This is now the 3rd programming/ artificial intelligence channel that I've found myself watching even though my ability to code (or even Math) is so awful that if there was a gun to my head I would beg to just be shot. But I find it satisfying to watch. Like a time-lapse of an ant colony diligently working away.,True
@stonecoldscubasteveo4827,2022-05-09T19:36:48Z,0,"Reward for resources spent. this will incentivize expansion and rapid army growth until max out. At that point change the reward to enemy units/structures killed. Something like (big reward) for spending money on nexus/probe/stargate (bigger reward) for void ray, (penalty) for having too much money banked up unless supply is >190. Then (big reward) for killing enemy unit/structure, while dialing back on rewards for building structures. zero out the rewards for probes over 70-80 and for pylons over 200 supply. When supply drops due to combat, flip the rewards back to making void rays to max out again.",True
@ReallyWhy123,2022-05-08T03:08:42Z,0,this book is impressive,True
@kristopherleslie8343,2022-05-07T23:43:08Z,0,I would love to see you apply same thinking to Diablo 2 Resurrected,True
@djsyntic,2022-05-07T10:52:46Z,0,"When you got talking about how to handle the gas extractor on your minimap was that you handled it strangely. So keep in mind that the RGB values for the colors you put on your map are arbitrary and serve to help you visually more than the computer. But you could have encoded some meaningful data into the RGB itself. For example, instead of saying ""This building is green, this building is dark green"" and so on, you could have put all building/unit type info into the R-value of RGB. IE: This building is R-value 12, this building is R-value 13, and so on. Then the G-Value could represent something else, like building health. IE: R-12, G-255 means it's a Refinery at full health while R-12, G-1 means the Refinery is about to explode if it takes any more damage. Finally, the B-Value could then be used as some sort of indicator of something specific to that building. R-13 might be a Barracks, and B-2 might mean that it's in the middle of training something and has 2 units of time before it finishes and can do something else. On the other hand, R-14 might be a Gas node, and B-# could indicate how much gas that node has, while R-15 indicates that this is an extractor with the B-# still indicating how much is still in the node.  Sure to YOU R-14 and R-15 are basically the same amounts of red and your eyes wouldn't be able to tell the difference, but to a computer, those are two distinct values.",True
@whitedaydevi6917,2022-05-07T06:40:01Z,0,This is gonna result is in the protos,True
@mstrkllr,2022-05-07T04:17:02Z,0,"You're like Code Bullet, except without the Adderall and Self Depricating Humor ü§£",True
@achtsekundenfurz7876,2022-05-06T22:07:57Z,0,"Just a quick note: the ""can afford"" check at  04:47  is NOT totally redundant.  You're inside a ""for each idle stargate"" sort of loop, and if two are idle, you could end up in a situation where you can afford one but not the other -- and depending on the capabilities of the ex-handler, tripping an exception doe to insufficient resources could crash the AI.",True
@Zorlac,2022-05-06T13:09:01Z,0,An AI has been playing starcraft since launch. It's called the AI !,True
@randomguy-dy3uy,2022-05-06T02:34:00Z,0,"AI will never have a will of it‚Äôs own that‚Äôs what a humans soul provides, the soul is conscious not blood guts and brain cells",True
@SuperMonkei,2022-05-06T02:30:32Z,0,Is this video about two years late?,True
@raymoreclef,2022-05-06T02:28:49Z,0,"What if you sent a few units to the starting locations in a clockwise method using waypoints of some sort.   (At x time go to y waypoont repeat) These waypoints could be also used to pinpoint locations the computer hid and resulted in a loss.   Also, what if a losing so many units quickly it was set to build the upgrade function. I think both of these ideas could be implemented with your ""engines"".   This was extremely entertaining!",True
@tophmcgoph9229,2022-05-06T00:36:23Z,0,Ahh the copy paste methos,True
@Telos8,2022-05-06T00:18:54Z,0,Any plans on a part 2 with the microgame plan implemented and see how it runs in tandem?,True
@Shadow-yl2tf,2022-05-05T22:53:29Z,0,"9:00 another reward could be time. If you win a match, then the shorter the time, the extra rewards you get. Like wise, the opposite if you lose.",True
@EranM,2022-05-05T19:09:30Z,0,bruh sentdex.. it's impossible to understand anything.. Like where is the RL model? Where is the learning? the map is the input? the actions are the output? is it a NN? what is the structure.. you put a state and action into the NN and get a new_state.. how do u calculate if its good or not ? how do u chose/calculate the y's ???,True
@a25885200,2022-05-05T16:56:49Z,0,It remind me my FYP in university.,True
@DrumFFx,2022-05-05T11:09:05Z,0,this guys voice sounds like a young Rand Paul..  like wow,True
@daldorian,2022-05-05T04:46:45Z,0,at the 9:00 min mark... anyone else concerned we are rewarding an ai for killing?,True
@daindrumhiller3329,2022-05-05T00:09:09Z,0,this is just wargames,True
@danielglidewell,2022-05-05T00:02:52Z,0,"I wasn‚Äôt in the mood to watch the video when I read the title, but when I realized what the thumbnail was I stopped by to drop a like lol.",True
@nwsteg2610,2022-05-04T22:12:24Z,0,this guy legitimately sounds like Nicholas Cage,True
@nastrimarcello,2022-05-04T16:54:37Z,14,"This amazing. Amazing code, amazing explanation, amazing editing.  Only one suggestion: when possible, don't use try:...except:pass As this can lead to hellish problems.  If you know what exception you are having in that try-except statement, using that exception explicitly is better (even if you are just going to 'pass' it)",True
@ccgamerlol,2022-05-04T15:51:50Z,0,"like Deepmind Alphastar, cool, would love to see full gameplay of this, please?",True
@thepacific2933,2022-05-04T14:43:32Z,0,I think the best limitation would be a time limit to win the game. It would optimize all the aspects to achieve the best result,True
@jakubzachnik1070,2022-05-04T13:01:39Z,0,I understood around 1.2% of what you said but it was very entertaining nonetheless,True
@davet80000,2022-05-04T09:05:20Z,0,"well youtubes algo / ai(?) is on point with me again today, wd",True
@MrSlyFoxJr_,2022-05-04T06:17:26Z,0,Definitely a very Protoss approach to SC2,True
@raze667,2022-05-04T04:58:45Z,0,vespeen. peen. peeeeeeeen.,True
@JathTech,2022-05-04T04:51:16Z,0,"a FAR better reward scheme would be kills to losses. Each unit would have a value assigned that you would lose points if you lost, and gain points if you killed. This is ultimately going to reinforce itself since by killing enemy units without losing your own, you always increase your relative combat power.  Just ""being in combat"" incentivizes entering engagements that are losing propositions when you could choose not to fight, and instead wait until you have the advantage.   Read the Art of War by Sun Tzu, and you may gain some insights on how to reward your AI.",True
@Crazy-Chicken-Media,2022-05-04T04:44:56Z,0,Me that knows nothing of coding.            Fascinating!,True
@investor.z,2022-05-04T02:50:02Z,0,Machine learning is not AI.,True
@kuronekogamer2065,2022-05-04T01:04:06Z,0,Splitting tasks reminds me Zero-K campaign where you can actually play with your friend and all the game does is add another cursor.,True
@erics3596,2022-05-03T21:22:21Z,1,Do you want Skynet?  Because this is how you get Skynet :) (also great strats and explanation on how this works),True
@cowjuicethepallytank,2022-05-03T20:27:33Z,0,"Some potential rewards (or punishments) could be losing a voidray is a negative percentage of the positive reinforcement for attacking. Locating the enemy could be a small reward every x seconds to incentivise optimal searching patterns. Another question I have is what information does the API have access to? Does it have the capability of identifying enemy units? Are you able to get unit counts of the AI's specific units? Do we have the capability of training upgrades?  In general, I think that with given the correct training it may be possible to find certain timings of when best to scount and taking optimal scouting paths as well as best attack timings in terms of time in game as well as potentially within build order. The difficulty, depending on how far you take it, could come down to army composition and as you were saying, micro.  Lastly, showing my lack of knowledge in AI learning. Would it be possible to train the AI using professional gameplay wins, then use that as a baseline ""build order"" for then using the reinforcement learning?",True
@BasicAndSimple,2022-05-03T14:22:52Z,0,Book Purchased. Thanks,True
@gavinmorton7682,2022-05-03T13:23:45Z,0,this is such a cool project! would love to see this keep going,True
@jacobbenson7420,2022-05-03T08:54:49Z,0,Why not add a punishment for idael time with a semi exponential path ?,True
@JohnJackKeane,2022-05-03T08:26:42Z,2,"I do not code or have the desire to code, but this video is beautiful. I enjoy StarCraft videos seeing people micromanage, but the thought and process that goes into creating a ‚Äúprogram‚Äù to do the same thing is fascinating. The amount of work and work to obtain the knowledge that goes into the work is far underrated. I hope for you the best!",True
@Kopie0830,2022-05-03T07:38:23Z,0,The best way for technology to grow is to make an AI that makes robots that build and learn on it's own building better versions of itself whose primary functions is making techs and machines that automates and new advances in space colonization.,True
@0ADVISOR0,2022-05-03T06:38:41Z,0,"How about splitting the AI in separate AI's (Generals), like one for base building, fighting, scouting, expansion and then having them share the ""experience"" etc. Is this even feasible?",True
@matheusGMN,2022-05-03T06:12:25Z,0,your strategy of multiple Ais to coordinate everything at the end that you mention is the same one Paradox Entertainment uses in games like Stellaris and EU4,True
@SuperShiki666,2022-05-03T06:06:07Z,0,"You should make one for total war it's way simpler because there's no resources or capturing, just manuvering and using abilities.",True
@HannibalGraham,2022-05-03T04:59:02Z,0,Just A moving to a corner of the map would likely get you better results.,True
@awsamalmughrabi860,2022-05-03T04:20:10Z,1,"I like how in depth this video is, really enjoyed it!",True
@calebb4782,2022-05-03T04:02:05Z,0,Couldn't you use the mini map and cursor to move camera by clicking said mini map? or am I missing something?,True
@dogtato,2022-05-03T03:47:37Z,0,very interesting to see how you structured it to use ML decisions for higher level decision making. would definitely be interested in seeing how you approach a micro script and specifically wonder about the ability to add new behaviors without having to retrain from scratch,True
@kv3435,2022-05-02T11:57:03Z,0,Im a nurse so I literally dont understand anything but im angry that no one told me teaching robots how to think was a possible career path ü•≤,True
@nickmagrick7702,2022-05-02T06:09:44Z,0,"somehow if cheating gets aggressive enough, people start respecting the cheating",True
@oxyuran5998,2022-05-02T05:43:53Z,0,"Since we basically have a ""Game AI"" this looks a bit like a GAN situation. Is there a way to judge the game AI performance and compare it to your NNs performance? The idea would be to make the punishment weighted so that bad decisions get punished harder than good decisions that just weren't good enough so that good strategies get reinforced and refined more than bad ones.",True
@union1112,2022-05-02T05:37:29Z,0,would it just be better to reward for damage done to enemy buildings + winning.,True
@Lithane97,2022-05-02T02:13:29Z,0,"It put a freaking gateway in the mineral line in the last clip, sadge",True
@carlotheemo,2022-05-02T02:05:16Z,0,"not a programmer, but what if you give multipliers based on how fast the ai can get a victory?  lets say the first idk 20 min or so would be a very high reward then counts down until a certain point like an hour being default rewards and anywhere beyond that is negligible. so it would incentivize the ai not only to finish but finish fast.  might also reduce even further those times where it would stall for more points in attacking.",True
@NVMDSTEvil,2022-05-02T01:29:26Z,0,Might be good idea to set reward based on time,True
@DeathxStrike18,2022-05-02T01:19:11Z,0,"Instead of telling it what to build give it a list of items to build with and resource costs info, it should have a network for Collect Resources then have to decide what resource to collect. Have Punishments for a unit or buildings destruction say a scout will have a -1, defensive/offensive troops -3, builders -5, minor buildings - 10, major buildings -25, alternate bases -50, loss of starting base -100. Give the opposite point score for destruction of the enemy items. In this way your getting it to learn the main purpose of the game preventing the loss of your own units while at the same time destroying your enemies, in this way it would need to gather resources to build units it would also learn that placing nothing will result in loss of points, sending smaller troops to attack a stronger troop may result in more point loss than risking more points valued troops and killing more of their troops.",True
@jonbecherer5103,2022-05-01T22:24:35Z,0,"For the fitness function, I think trying a reward for killing units and a punishment for losing units would work well.",True
@jsomiller44,2022-05-01T22:22:47Z,0,Don't scout with your army. Keep using probes to scout. Keep the army together as a single unit. Splitting the army is what caused your win % to drop. Also consider adding Zealots and a few cannons.,True
@cianwyn4605,2022-05-01T20:40:45Z,0,Remember when YouTube was random shit,True
@McdcLP,2022-05-01T20:39:19Z,0,just wow,True
@Derrekito,2022-05-01T17:51:00Z,9,Never before has a marketing ploy worked so well on me. I'm looking forward to receiving the hardcover version of the book!,True
@eight7934,2022-05-01T16:10:38Z,0,looking forward to see how this turns out after its polished.,True
@adye88,2022-05-01T15:07:39Z,12,"This is freaking intense! also for the hunters problem: Why not make a ""return to safe space"" function for them when they detect enemies. That way they only perform scouting duties.",True
@elyaskreil2628,2022-05-01T14:42:40Z,0,But what's with the crawler from the zerg,True
@anndrey93ify,2022-05-01T12:29:02Z,0,Can we actually see the AI gameplay instead of a video full of codes and charts?  Interesting things to say but very uninteresting video for people who want's to see the gameplay. Other youtubers making AI for games shows a lot of actual gameplay instead of BS...,True
@sebbes333,2022-05-01T11:17:29Z,84,"*_<@sentdex>_* One thing I feel is missing from the map, is a kind of ""ghost"" of where enemies have been seen previously, which could become ""points of interest"" for scouting in the future. The ""ghosts"" could ""fade"" over time, but never fade to zero again (caped at minimum 1, starting at like 255 or something), to make the algorithm prioritize the most recent ghost locations. Also, instead of scouting with void rays, wouldn't it be cheaper to scout with drones (to generate ghost areas) (scouting probably targets mineral areas without ghost, to see if enemies have expanded, while voidrays can scout areas WITH ghosts, to see if the enemies are still there & try to defeat them there, can also send a probe first to ghost area, to determine enemy strength before attacking).",True
@norik1616,2022-05-01T11:10:23Z,0,"Please, start using `pathlib` and the `Path` object üôèüèº",True
@majorduff3220,2022-05-01T10:49:04Z,0,"what would happen if you gave a reward if your void killed or destroyed something (enemy army and buildings), and when a void dies you got a negative reward? and give really small rewards for beeing alive. when the steps bring you to a positive result, like killing an enemy, you can give also the last 10 steps, or whatever you like more rewards. if will this end in a lose of your army or a building, negative reward. maybe this will help, learning faster and better?",True
@arashiiku417,2022-05-01T08:38:54Z,0,Would it be possible for you to script the last few enemy corordinates where the ai encounters them and then project a trajectory to where the enemy may be?,True
@MFTomp09,2022-05-01T01:02:40Z,6,I wonder if modifying the reward structure to include a small reward for scouting. Like finding new enemy structures or something would be useful to get more wins in those games where you said they regrouped and came back with a larger force to beat you later,True
@zenlis1291,2022-05-01T00:01:32Z,0,has anybody ever made an ai that gets smarter as a reward for winning a game? that would be interesting.,True
@pognar,2022-04-30T17:49:00Z,0,"I have played starcraft for years and years, and I love this channel.  This is going to be great.",True
@alexanderher7692,2022-04-30T17:29:12Z,0,for some reason you can just magically get a container with all the units you need? where do they come from,True
@alansmithee419,2022-04-30T16:03:57Z,0,"2:20 But does the ai then think there's very little there, or are they dim just for us to more intuitively understand the video?  If the former, why would the AI go to those areas if it believes there's nothing there? Or does it have to learn itself that dim means there's very little, thereby also learning for itself that very dim means unknown?",True
@BretBowlby,2022-04-30T15:31:29Z,1,"I like the ideas here, but be sure that you've got task that can understand the adv. of having a high ground vision giving better attacking vs not having high ground vision.   Also, I'd consider having the model constantly scouting as all information gained on the players actions can lead for better counter attacks and so forth.   But yeah I'm loving this.  keep'em coming!",True
@XmKevinChen,2022-04-30T14:44:05Z,3,"It‚Äôs a very interesting video about the ML + gaming. As a newbie to this AI world, it also gives lots incentives to continue learning.",True
@faithful451,2022-04-30T14:15:44Z,2,I'd love to see the next video in this series with dual macro and micro algorithms and improving the win percentage,True
@Shazumbi,2022-04-30T14:09:50Z,0,"I know nothing about coding or anything else, really, that went on in this video. But I do have a question, as this is incredibly interesting; how do you make the program know if an action is ""good"" or ""bad""? Trying to rack my pea brain on how one would write this out. Even if you say the final outcome is greater than x (as a ""good"" outcome) how do you 'convince' your program that it should continue to try for that outcome?",True
@silvanodesimone6582,2022-04-30T13:58:50Z,0,"I am just starting with machine learning, but cant you implement a function that penalizes long games, something like that decreases the reward the longer the game goes on?",True
@Swearinbag,2022-04-30T12:44:40Z,0,interesting project and book too! Thesis currently is about deep learning and I've learned so much during it! Shame I didn't have your book to help me in the early stages hahaha,True
@nevokrien95,2022-04-30T12:37:45Z,0,Stelth is going to be aj issue,True
@itchykami,2022-04-30T11:59:20Z,1,"This makes me really wonder how someone managed to make a Starcraft AI that could do things like effectively use terrain to get fog of war advantage, and use chokepoints to control flow of battle.  Idea though: You don't need to hunt random coordinates for enemies. For the most part you just need to keep an eye on all the resources. Maybe put a small reward on just being able to 'see' the bases, which can be defined simply by the position of minerals and vespine.",True
@matthias916,2022-04-30T11:43:04Z,0,"Read the first comment at 5:14. Usually what I do is this: ``` if not hasattr(self, ""var_name""):     self.var_name = initial_value ```  but I believe it is good practice to initialize the variable somewhere else like in the constructor",True
@rhettbaldwin8320,2022-04-29T23:56:08Z,0,"Make the reward for winning start at a huge reward 10000 and count down to zero after a specific timeframe,  faster win = bigger reward.",True
@BalimaarTheBassFish,2022-04-29T23:21:31Z,0,"I'll be interested to see how you link the different AIs with there different specialties together. My only concern would be there is bound to be some overlap, how would the AI resolve 'competition' against itself when one or more AI specialties want to control the same thing?  ugh I can English I swear!",True
@wric01,2022-04-29T22:32:15Z,0,"Random 4v4 Pvp is depressingly filled with idiots,noobs, team killing etc. If only popular games give team the ability to vote replace those bad players with ai.",True
@kimes2329,2022-04-29T11:10:52Z,0,Ïù¥Ï†ú Î∞îÎëëÏóê Ïù¥Ïñ¥ Ïä§ÌÉÄÎèÑ ÎπºÏïóÍ∏∞ÎäîÍ±¥Í∞Ä..?!,True
@RichardPeterShon,2022-04-29T08:53:56Z,0,Our current AI is beyond war games now. We can literally simulate world war three and see who wins.,True
@matejnovosad9152,2022-04-29T06:31:29Z,0,"Starcraft is tough, you should probably start with something simpler",True
@Peter-rn5bu,2022-04-29T00:49:04Z,0,"maybe understanding how ai learns how to get to a known goal, for example losing as fast as possible, could help develop or program ai which is more efficient or accurate",True
@EnderSword,2022-04-28T18:29:47Z,84,"Kind of neat, I'm wondering if you looked at the AlphaStar research at all to do this, or looked into the StarCraft 2 AI community? There's about 70 coders of various bots and AI that compete against each other and it'd give you a ton of ideas on build choices and especially unit control and decision making.",True
@Singularitarian,2022-04-28T15:53:44Z,1,Very illuminating!,True
@rpraver1,2022-04-28T15:47:00Z,0,Long time follower and purchased your book. Why not touch on genetic algo from scratch?,True
@festro1000,2022-04-28T08:31:17Z,0,"Maybe you could try using playtime as a reward to incentivize it to win the game rather than endlessly farm kills, this would work even better if rewards could be prioritized that way attack could put it on the right path, but first and foremost it would want to win the game as soon as possible and maybe even put negative rewards if a certain period of time elapses.",True
@vladimirtchuiev2218,2022-04-28T05:44:46Z,0,"This video is so God damn cool, I have a current project that I try to make chess self play work on very limited resources, I think SC2 will be my next project if the actual python API is open. What is your GPU, and how long did it take for you to train the agent?",True
@protoplmz,2022-04-27T23:30:21Z,0,"Hey! I love the update here. I followed the original series you put out. As a SC2 veteran I noticed deficiencies and deviated in a strong way halfway through. I setup separate models to handle the decision making for each aspect of the game. This makes it so it can make the decision to use its army separately from the decision of progressing tech (or not). I stopped around the time I couldn't figure out how to have it build its own strategies as I ended giving it a long set of possible actions and letting it pick and it felt too 'guided'. It was able to beat ""Very Hard"" 50% of the time vs random's 0%.  Was my first exercise with ML. I got the chance to apply the concept it at work for something outside of my scope. Used both that and the SC2 project as demonstration in an interview and got a promotion out of it. This inspires me to try my hand at it again!  EDIT: To handle army movement which you mentioned in the video, I chopped the maps up into a grid and gave it decisions to make where it could attack-move its army to any of these at will. 9 worked the best but you could make it much more granular. It used this to both attack and defend.",True
@HeavenlyWarrior,2022-04-27T22:48:33Z,0,"Starcraft 2 is one of the most basic RTS ever, very easy to learn and to play. The only thing you need to do is playing as fast as possible, regardless of the strategy and you win. It's a terrible RTS game so very easy for AI.",True
@UsernameAwesomeSauce,2022-04-27T22:29:00Z,0,"So trying to reproduce your project, I am getting the error "" AttributeError: 'IncrediBot' object has no attribute 'mineral_field' "" and I was wondering if you had any idea how to diagnose this?",True
@KamuiPan,2022-04-27T19:13:20Z,0,"Is good to do that with mostly dead games, just don't do it like Dota2 that they have being losing every round to the Learning Neural Algorithm. Seriously, think about it. The whole thing around Dota2 is their tournament, if some random AI developed by people that have 0 interest in the game beat the so call world champions, how you think that make the consumer base that make all these tournament possible feel? They can't even beat the best players that are always in touch with update patch's, something that not everyone has the time or even patience to read, specially the casual players. They literally ruin Dota2 with those Algorithmic Players. It took the fan of playing away, it wasn't fun anymore.  Is obvious not hard to make a machine or a algorithm do the work on perfecting to the max a game, it was proven even before neural networks programs. It started with Chess, then they had try other table games, even Go they end up beating the top players, ridiculous. Is more a power move than anything else, destroying culture and niches to prove something that everyone already knew for decades. Bad sportsmanship.",True
@themaster8432,2022-04-27T11:12:29Z,0,is there a c# version of the code snippets? maybe another resource that can teach machine learning algorithms in c# also? :),True
@etienne8110,2022-04-27T09:08:43Z,0,"Yeah let's make an AI perform flawlessly at 4X, what could go wrong?",True
@toratoma6197,2022-04-27T07:17:24Z,0,this is how IT play the game they codes a AI and let them battle each other,True
@andrzejgorski3932,2022-04-27T03:58:46Z,0,Expend into Hierarchical Reinforcement Learning for better reward to action and assignment and to provide a method of long term rewards.,True
@Alex4n3r,2022-04-26T21:47:12Z,0,Maybe take replays of good players and jump in at some point and try over and over again to achieve a better intermediary result but not worse than the following recorded one?,True
@BryceRoche,2022-04-26T19:36:14Z,0,If you could reveal the army supply for each bot and reward the RL if your army supply is higher.  Also another reward if your worker count is higher,True
@Easy2own,2022-04-26T16:48:21Z,0,Arent you some years to late for that ? Wasnt there already an AI that plays Sc2 ?,True
@apoorvgupta9680,2022-04-26T16:40:37Z,0,"hi, can you do a video on latest game Age of empires 4",True
@popviz3316,2022-04-26T11:29:41Z,0,"You can have the glorious experience of the SC2 visuals under Linux with a bit of wine magic.  I use a Lutris install but base wine should also work, the instructions are in BurnySc2 readme.  I use this, modify to suit.   export SC2PF=WineLinux  export SC2PATH=/home/popviz/Games/starcraft-ii/drive_c/Program\ Files\ \(x86\)/StarCraft\ II/  export WINE=~/.local/share/lutris/runners/wine/lutris-fshack-7.2-x86_64/bin/wine64  export __GL_SHADER_DISK_CACHE='1'  export __GL_SHADER_DISK_CACHE_PATH=/home/popviz/Games/starcraft-ii  export WINEDEBUG=-all",True
@kimcarrier9834,2022-04-26T10:51:55Z,0,is AI equal to if(if(if(if(if(...))),True
@aaronrovinsky11,2022-04-26T10:18:33Z,0,"for the comment at the top around 5:00, you can use: if not hasattr(self, ""last_sent""):     self.last_sent = 0",True
@davidcristobal7152,2022-04-26T10:00:34Z,0,"Does Stable-baselines allow to store states - reward pairs in harddisk? I developed a modification of the MemorySequential class in keras-rl to use little memory in ram. My algorithm uses a thread to store states (images or whatever) as numpy arrays in my ssd disk, and keeps a randomized subset of the states in every loop of the algorithm in order to train the agent without using tons of RAM (which i don't have). It's a sloppy implementation so I was wondering if stable-baselines has something like that",True
@ZhiYin,2022-04-26T05:07:33Z,0,I love how I know every word but have no idea what you're talking about. (youtube recommended this video because I follow startcraft2),True
@kja2ja,2022-04-26T03:03:04Z,0,"Interest! But isn't the build in AI already do all these? just need to adjust the difficulty level, no? What is the difference between this Python code vs the SC AI? Awesome info!",True
@tmerkury2813,2022-04-26T01:26:49Z,0,Can you make merch???,True
@VaSoapman,2022-04-25T20:38:01Z,44,"Why not give rewards based on how many enemy units/buildings are destroyed? Then give a penalty based on how many units/buildings are destroyed? Also to help the AI prioritize winning over stalling, you could increase the value of a win based on how fast it won.",True
@wootcrisp,2022-04-25T19:12:49Z,0,Nicely done.,True
@protectedmethod9724,2022-04-25T18:14:39Z,0,"This is a little coincidental for me, because I was just playing with stable-baselines3 a couple days ago making a custom environment to play the Slither.io game in a browser using a Selenium and Firefox. I never got the AI to learn anything useful, but the environment worked quite well. I would assume you could do something similar with this project to avoid the file communication hack.",True
@canadiancoding,2022-04-25T17:16:05Z,0,Might also want to look into upgrades if you haven't. Units in mid-late without any upgrades are much worse in SC and this might have quite an impact.,True
@Ammothief41,2022-04-25T16:41:14Z,0,Thanks for putting all of that together.  Looks neat.,True
@kylee.7654,2022-04-25T16:23:49Z,1,"At 4:52 regarding your comment, I added  async def on_start(self):         self.last_sent = 0 after the on_step function. It makes it a little clearer",True
@philip7468,2022-04-25T14:47:34Z,0,I so want to see SentdexStar to go up against AlphaStar XD,True
@TheThunderSpirit,2022-04-25T13:18:09Z,0,u have to use pipes for interprocess comm. or at least udp,True
@jin416,2022-04-25T11:55:22Z,0,supper cool !!!!!,True
@Markus-zb5zd,2022-04-25T09:27:45Z,0,tbh SC2 is a lot easier to code than AoE2 or such as the maps are set in SC2,True
@keanamrazek3745,2022-04-25T09:09:59Z,0,What if you were to only use the reward function you did in the initial training and then use the win-loose reward for model refinement.,True
@yeokc8757,2022-04-25T04:37:42Z,0,Have you thought about pretraining the network on the millions of hours of gameplay replay files online from online tournaments?,True
@binxuwang4960,2022-04-25T04:03:05Z,1,Already super impressive that you could do rl for macro level strategy!  Totally agree that to solve a csrtain problem how to formulate the state action and reward is key,True
@kritikusi-666,2022-04-25T03:27:34Z,0,Is there a way to put this model to test vs ladder?,True
@jeremyheng8573,2022-04-25T01:52:01Z,0,very inspiring video! Looking forward for more reinforcement learning tutorial!,True
@whitey9933,2022-04-24T23:24:01Z,0,"Looks great, always been interested in the Alpha Star gameplay and how it manages all the different tasks. For the enemy search, can focus on undiscovered minerals (enemies would normally congregate around minerals fields) and probably better than random search.",True
@benoitkinziki3916,2022-04-24T21:04:19Z,1,For the reward mechanism you could probably build a LSTM that gives you the probability of winning for each action you take and you should probably include a time penalty to avoid the bot dragging the game out,True
@robwolters7401,2022-04-24T16:53:42Z,0,In my experience grouping attacks and synchronising targets is very important.,True
@PathToPrestige,2022-04-24T14:15:27Z,0,"I'm replying very  rarely to those kind of videos.. but hats off. Even though the project structure is messy, your genuine ""realistic"" practical approach was very  enjoy some to watch.",True
@Ranshin077,2022-04-24T13:39:35Z,0,You could also give the ai a larger gradient win reward or larger gradient lose reward for shorter games.,True
@BDM775,2022-04-24T12:59:50Z,0,"I heard that google had blizzard develop custom client for starcraft, ran it on their massive infrastructure at stupidly accelerated speeds and had ladder data and replays to learn from. Yes, in the end they got grandmaster-level ai for each race, but I am curious what you can achieve and what your goals are, not being a massive corporation with unlimited resources.",True
@ducnguyen4973,2022-04-24T08:24:41Z,0,Cool video,True
@denisf.7409,2022-04-24T08:10:17Z,0,It's amazing how are you doing it. Your videos are really inspiring,True
@cheddar500,2022-04-24T08:04:45Z,0,Very satisfying to watch,True
@arianasghari3907,2022-04-24T08:02:52Z,0,WHEN IS THE P10 COMING OUT?,True
@serta5727,2022-04-24T07:18:01Z,1,So coooool ü§ó,True
@serta5727,2022-04-24T07:17:42Z,1,One day I also will be able to code this ü•π,True
@serta5727,2022-04-24T07:17:26Z,1,Can‚Äôt get enough of learning this awesome stuff,True
@serta5727,2022-04-24T07:17:11Z,0,So good blows my mind,True
@serta5727,2022-04-24T07:16:35Z,1,Such good learning materials! ü§óü§ì Most professional educators can‚Äôt do this,True
@serta5727,2022-04-24T07:16:03Z,17,"I have to say you make the most understandable learning materials  Your website together to the videos. All the Code is there, the book, the playlists from scratch. Most professional educators can‚Äôt do this ü§ó",True
@serta5727,2022-04-24T07:13:56Z,0,Thanks for the amazing content,True
@serta5727,2022-04-24T07:13:47Z,0,I recommend your Channel every now and then to people learning python ü§ó,True
@serta5727,2022-04-24T07:13:06Z,0,That is very cool and powerful,True
@serta5727,2022-04-24T07:12:56Z,0,Wish to also learn those skills for software testing,True
@serta5727,2022-04-24T07:12:37Z,1,Wow congratulations I think what you did is amazing ü§© I would like to do something like this for software testing for a while but it is so complicated,True
@serta5727,2022-04-24T06:53:41Z,1,Huggingface ü§ó is organizing a free Reinforcement learning course starting in May ü§ìüòç https://forms.gle/oXAeRgLW4qZvUZeu9,True
@serta5727,2022-04-24T06:51:59Z,1,I find it very interesting ü§ì,True
@JanHDD,2022-04-24T06:48:45Z,0,I think Google actually made an ai that plays starcraft really well like top level play. Alphastar is what its called I wonder what they used or how they even managed to do that. Their ai can actually beat professionals and I wonder how they set up their algorithm and reward functions to train their ai,True
@robanson32,2022-04-24T03:48:32Z,0,Great book! Love my copy,True
@2blakarrow,2022-04-24T03:45:45Z,0,I thought this was Code Bullet when I read the title.,True
@le_med,2022-04-24T00:39:35Z,0,Any chances of putting the books on amazon as well?,True
@jahcane3711,2022-04-23T23:38:04Z,0,mu zero + secondary network for movement?,True
@harshal.rathore,2022-04-23T22:05:04Z,0,Dang it and in the evening i was learning R.L üåùüëç,True
@MyAeroMove,2022-04-23T19:44:57Z,0,Ideas for next videos: - distributed training (via 1 mil subscribers) - cheating example in distributed training to influence final model behavior üòÇ,True
@adityachawla7523,2022-04-23T19:19:24Z,51,"Here is an idea: You can use more then 3 channels to give spatial information to your network. No need to limit yourself by conventional idea of 3 channels! If you are worried about how to visualize this, just think of it as an extra map.",True
@philtoa334,2022-04-23T18:37:15Z,0,Very good ... hight Level from scratch  :),True
@th1nhng0,2022-04-23T17:29:35Z,0,This is what Im looking for <3,True
@GodsAndMe,2022-04-23T17:03:02Z,0,"i like you,   you are kind.",True
@terrabys,2022-04-23T16:55:29Z,0,You add reward logic if unit takes damage and reward it for high unit count.,True
@LecherousCthulhu,2022-04-23T16:42:51Z,36,"You can actually improve your algorithm in very specific steps. There is a maximum number of workers you want to achieve and there is a maximum number of units total that you want to achieve. Getting the model to understand that it needs to reach those maximums as quickly as possible is a nice reward curve as if your model is able to get the fastest possible time to maximum unit capacity with X workers and Y void rays then it will likely do better.  You can also change the model to go from attacking to destruction of units and unit loss. You'll have to weight each unit based on how many units you currently have of that type, but this will allow you to eventually build the model to work with any build in SC2 as most of these builds will be able to be switched out for any other list of weighted units from any other faction.  You can actually find out the exact build orders and steps from someone like WinterStarcraft https://www.youtube.com/channel/UCk3w4CQ_SlLH4V0-V6WjFZg He will give you very specific steps for the model to focus on based on how skilled the models opponent should be with his Low APM Bronze to Masters Series",True
@mawkuri5496,2022-04-23T16:36:28Z,0,"not another starcraft RL,, make a thetan arena RL, that would be AWESOME!",True
@kevintyrrell7409,2022-04-23T16:00:33Z,3,14:49 That's some next-level Gateway placement.,True
@N0tTh3Pr0,2022-04-23T15:55:03Z,1,"How?... Why?.... Jokes apart, every time I think this is the best you can do, you just amp it up by a mile. Great job.",True
@ellnino,2022-04-23T15:53:50Z,0,This is some next level AI Sci-Fi stuff if you ask me. I wish this kind of search and destroy machine learning algorithms could be used one day to cure viral illnesses or cancer. What do you think about that?,True
@harishuthravalli456,2022-04-23T15:50:25Z,1,Please i need NNFS part 10!!!!,True
@Stthow,2022-04-23T15:37:51Z,0,Amazing video dude. Gj.,True
@user-gs6lg4gd3b,2022-04-23T15:29:39Z,2,You actually need more then just voidrays. And for other units some scripts for fighting patterns. You can probably star with archon-immortal-charge zealots composition. It has almost none fighting patterns,True
@Mutual_Information,2022-04-23T15:22:44Z,3,"Wow I'm literally working on a series on RL theory and I was just wondering how the hell you'd code things up to actually play Warcraft 3. Starcraft 2, close enough! Such a useful channel",True
@usamabinmuzaffar692,2022-04-23T15:16:54Z,0,Mate... Still waiting for part 10 of the neural networks from scratch series... Is it cancelled or is it just taking too long?,True
@fuba44,2022-04-21T22:02:19Z,2,"In regards to rewards, did you try ""resource worth of kills"" devided by ""resource worth of loses"" + the win or lose bonus. ? (Maybe with a modifier on workers to make them more juicy targets) + maybe something to do with map exploration.. to find hiding bases..  just spitballing here, i know you already did a lot on this project.",True
@fuba44,2022-04-21T21:48:35Z,277,"Very interesting idea with a macro ai and a strategic ai, sort of working in tandem forming a symbiotic relationship of sorts..  could maybe even break that down even further, like on a per unit type basis... Tho i imagine the complexity explodes at that point.",True
@fuba44,2022-04-21T21:40:57Z,28,"This was an interesting video. I will have a look at your example code for sure, wanna try to tinker a bit. Thanx for all your hard work.",True
@Stinosko,2022-04-21T20:14:10Z,0,Whooop whoooop ,True
