author,updated_at,like_count,text,public
@RabeeQasem,2024-05-24T17:30:32Z,0,if you could do a tutorial on fine tunning the new version of it  thank you for your videos <3,True
@TylerMatthewHarris,2024-02-23T12:52:06Z,0,This was wonderful news. Gpt4 is declining,True
@BryanChance,2023-12-04T13:57:26Z,0,"The pace of development in LLM is at light speed. My brain hurts trying to keep abreast of the myriad of applications and use cases. The opportunities are endless and all of this (general public use) happened, as you have stated, within the last 12 months or so. I have a feeling it's going to continue for the next 2-5 years. Hopefully, I'll still be employed as a DevOPs engineer. LOL",True
@popothebright,2023-11-22T07:51:35Z,0,"Try asking time differences. If it's 1:00am in Tokyo, what time is it in London. Amazing how few LLM's get this right.",True
@LordYodel,2023-10-21T10:40:36Z,0,What you think is the best free open source language model today? thx a bunch!,True
@the_traveller6994,2023-10-05T14:00:05Z,0,"Would you be so kind as to refer me to a video explaining, at a non - computer person level, how to set this up?",True
@alrashidd,2023-09-19T14:39:47Z,0,gpt often makes mistakes. Sometimes it becomes stupidity and not artificial intelligence. I haven't tried Falcon yet. Is it better?,True
@user-kf8ci5vw8p,2023-09-19T12:01:37Z,0,"Does conversation history works with Falcon40B Instruct, anyone tried?",True
@ander300,2023-09-16T17:47:25Z,0,"Part 10 of Neural Net from Scratch, about analytical derivatives??? Please bring the series back!",True
@trieule2012,2023-09-02T15:59:23Z,0,Do you have any videos about fine-tune falcon model?,True
@lmtr0,2023-08-03T13:10:27Z,0,what is this lambda platform you talk about?,True
@horus4862,2023-08-03T05:23:52Z,0,You ROCK!!! Love your Work!,True
@5eZa,2023-07-31T21:57:21Z,0,you want to give GPT4 a terminal huh lol,True
@DLmadison100,2023-07-28T00:40:05Z,0,"I have a question. I currently have GPT4All on my PC. I'm using a simple install package that uses  the CPU and system RAM. My PC is a workstation (custom build) running with an Intel Xeon E5-2680 v4 CPU, (14 cores / 28 threads), and 32GB of system RAM. I also have an RTX 2060 graphics card, with 12GB of GDDR5 memory but, that would not seem needed for this framework I have installed. The largest chatbots available are 13B and have a minimum system requirement of 16GB RAM.  I have dedicated 12 threads, which seems to be, functionally, just the same as when I had 8 dedicated to the Chatbot. It typically responds faster than the time it takes me, (usually) to ask it questions but, I notice it hallucinates. My question is, would I be able to run this 40B local LLM (Falcon) with my current system and, if so, where would I download such a Instruct LLM like this; (that would run on my CPU /system RAM)?",True
@techie_guy,2023-07-27T13:41:31Z,0,@sentdex - could you pls try a similar vide on MPT-30 by MosaicML #llm #ai #mpt30b #mosaicml,True
@mickelodiansurname9578,2023-07-24T19:43:31Z,0,okay so yeah... better get my act together putting in a proposal there. My guess is they will need to see a MVP.,True
@brianmarks3972,2023-07-22T18:05:15Z,0,Anyone know what RBRMs are at 9:40? Havenâ€™t been able to Google it successfully.  Edit: Rule-Based Reward Models!,True
@jonasls,2023-07-20T22:07:49Z,0,Love it! Can you take a look at the new Llama 2 models? Love to see a comparison to Falcon 40B,True
@kevinbacon8716,2023-07-20T14:33:31Z,0,Now I just need to put a couple A100s on layaway.,True
@sonilakshya,2023-07-20T05:16:18Z,2,"i was trying to use falcon 40b instruct on a 96vCPU , 360 GB RAM and 4 NVIDIA T4 GPUs, but it takes almost an hour to give a single output. can someone please tell me if there is something that I might be doing wrong, for the inference time to be this high or does it usually take this much time to run?",True
@fahmidaakterdina875,2023-07-16T17:13:06Z,0,"As it's trained on web data, can it answer questions with accurate information? Like, can it be used to create social media posts about specific topics?",True
@just_A_doctor,2023-07-16T14:36:23Z,0,It is not 100% open source  60% of the model only.,True
@tizianonakamader8177,2023-07-15T14:09:36Z,0,I still remember how I started following you my dude â€¦ with tutorials on trading with python ðŸ˜‚ a long time ago,True
@Krath1988,2023-07-15T13:30:51Z,0,"Can you do a video about save the models and training data locally and running them from your own GPUs? Lots of people have GPUs, few people want to pay hourly for cloud services..",True
@6Azamorn9,2023-07-13T22:08:34Z,0,"This one of the HF models i'm looking into pairing with Falcon-40b, the idea being keep Falcon-40b context window small, maybe 8k tokens max and use something that's trained to handle much larger context windows (ie kz919/mpt_30b_32k_v2 on HF) as a sort of in-context way of utilizing another language model to compile the information the smarter model needs to correctly solve the task it's given. Still working on the implementation which does involve quite a bit of prompting and programming but I think it could really make the models do way more than they are right now capable.",True
@PujanPanoramicPizzazz,2023-07-13T14:26:29Z,1,Again as the comments suggest here.. can't wait for your Fine tuning video ðŸ˜…,True
@erictheawesomest,2023-07-13T12:41:28Z,1,"Do you think you could do a video explaining hardware requirements and cpu vs gpu?   Also how does the bit size affect ram and performance?  For example, I'm considering buying more ram for my pc 2 x 32bg for a total of 96gb (already have 32gb). But I have no idea if that would be enough for a 13-15b model (I would be running on cpu)   Unfortunately reddit is closed so I can't really ask these questions. But maybe this is an easy video for you if you were looking for content ideas.",True
@markoh9974,2023-07-13T03:18:09Z,0,Pop filter bro...,True
@hirefiedinc6313,2023-07-12T11:46:14Z,0,Your sir are awesome!,True
@juansuescun7956,2023-07-11T23:35:40Z,0,ðŸ˜Œhttps://youtube.com/clip/UgkxR-HWkPQGdDlo_i2CoWWHkp88M3ijR5GM,True
@th3ist,2023-07-11T22:53:05Z,0,Funded by the UAE? LOL,True
@Gringohuevon,2023-07-10T17:17:07Z,0,I tried it and wasn't impressed at all,True
@ir8293,2023-07-10T13:53:25Z,0,Blaaaa. And next week itâ€™s bluppfibupp2. Who gives a f.,True
@Ryan-yj4sd,2023-07-10T02:44:47Z,1,"How do you fine tune? Also, if you wanted an API endpoint, how would you host it without breaking the bank? It seems like it would be more expensive than Open AI",True
@ghaithkhelifi5064,2023-07-09T14:34:21Z,0,can i run it 49b falcon on my ryzen 9 5900x and rtx 3090,True
@EvenTheDogAgrees,2023-07-09T12:55:05Z,0,"I just tried asking ChatGPT that practicing law question, and it got it right. I'm on the free plan, so that would still be 3.5, right?",True
@vlogsbynazreen,2023-07-09T09:09:15Z,0,Hey. Please increase the volume of future videos if possible :),True
@danielmz99,2023-07-09T08:12:40Z,1,Hi thanks for sharing this content. Could you create a video in fine tuning this model or create chain of thoughts/ fine tuning for complex tasks on Falcon-40b??,True
@USBEN.,2023-07-09T03:57:54Z,0,"Can these models access the internet like bing gpt? If not, how will that be possible?",True
@jayhu6075,2023-07-08T09:58:13Z,0,"What a amazing topic about the open-source model Falcon 40B. A very important sentence what you say it is ""YOURS"" ( model).",True
@erbterb,2023-07-08T09:58:06Z,0,"Its all fun and games, but can it predict the future? Otherwise what is the point. Not inventing the future by controlling data, but purely predicting it by the inputs of the world.",True
@jaredzhao665,2023-07-07T19:40:19Z,0,how about wizardcoder? It seems like wizardcoder might turn out to be a better coding LLM than falcon 40B?,True
@ummnine6938,2023-07-07T19:15:45Z,0,"will falcon 7B be able to run on a rtx 3070 8gb gpu, any ideas? thanks.",True
@marcol869,2023-07-07T18:28:18Z,0,Will this run on a nivida p40?,True
@OtherTNSEE,2023-07-07T18:05:25Z,0,"Just need a larger context window, 8k now really (6000) is about most of us with 24Gb cards can push now.  Nvidia and AMD need to give us more vram.",True
@rafaelmartinsdecastro7641,2023-07-07T15:19:46Z,0,Good stuff,True
@hakoren4444,2023-07-07T12:17:28Z,0,"Hi, model 40b-instruct works on my 3090 with torch.bfloat16. It takes about 23GB VRAM and 62GB RAM.  Am I doing anything wrong? ðŸ˜",True
@14zrobot,2023-07-07T06:41:21Z,0,"I don't know about GPT4 not making mistakes as often as Falcon. I do not remember a single time when anything code came out with no syntax errors, or that ran on the first go",True
@icandreamstream,2023-07-07T02:01:07Z,4,"Stoked for the fine-tuning video, canâ€™t wait",True
@timmygilbert4102,2023-07-07T00:49:13Z,0,"If you constraints the logits selection to the user input context, after subsequents regressive updates, the model performance shouts up",True
@antindie,2023-07-06T20:48:26Z,0,how do i concretely do finetuning?,True
@deltaanalytics3407,2023-07-06T18:29:14Z,0,Neat outro,True
@graham8316,2023-07-06T16:08:44Z,0,Making a mixture model with falcon might get it closer to gpt4,True
@rog0079,2023-07-06T15:02:36Z,2,Great video! Do create a tut where we can finetune this model on our own custom dataset!,True
@sczoot6285,2023-07-06T14:45:50Z,0,AGI is impossible to realize and the sooner we realize that the better,True
@Ryan-yj4sd,2023-07-06T14:19:08Z,0,"Possible to use with functions? I want to extract json data from blocks of text. I have 8mm records, so open ai will be too expensive",True
@redthunder6183,2023-07-06T13:54:17Z,2,"Please finish your neural network from scratch series, thereâ€™s only one more episode needed to finish it and it would help so many people its the only good series I found on YouTube that explains it clearly.  I followed it all the way through and it took me absolutely ages to figure out back propagation, there were so many tiny questions I had that could have saved hours if it was just explained through an example. And once I got it working, I thought it was wrong because the network cost was decreasing but the accuracy stayed the same, and it took me forever to realize that it was a combination of the size, learning rate, and number of epochs that caused that, not my code.  Any please finish it, it would help so many people who trying to are learning machine learning fundamentals. Anyone who has made it to the final episode is looking to learn, and will find it extremely useful",True
@davzmore3623,2023-07-06T13:50:15Z,0,Can you do a comparison to mpt30b,True
@noahkeys-asgill5664,2023-07-06T12:51:19Z,0,its actually because i am a magic fairy which makes it work,True
@streamshorts7833,2023-07-06T12:35:25Z,1,all worthless without book size context limits.,True
@nuclear_AI,2023-07-06T11:50:20Z,5,"Just realised for the first time that Im watching your videosfor work...  I used to watch them for fun, and now I get paid to watch them!!! Feeling quite humble â˜º",True
@jokmenen_,2023-07-06T10:57:13Z,15,This is awesome! Not having to send all your data to openai is crucial for privacy reasons. Wonder how it performs in languages different than english...,True
@fcolecumberri,2023-07-06T10:13:44Z,0,"Scifi claiming for decades that machines can't understand emotions, they only understand math. Now LLMs understand emotions very good, but have struggle with math.",True
@nathank5140,2023-07-06T10:07:42Z,1,Any chance you could cover possible techniques to running a model over multiple GPUs so that we could for example run 80 billion parameter models,True
@radus8832,2023-07-06T09:25:40Z,0,how is Falcon performing in other languages?,True
@MyTubeAIFlaskApp,2023-07-06T08:27:28Z,1,"If you had a bigger microphone, it could cover your whole face.",True
@sajjaddehghani8735,2023-07-06T08:21:20Z,0,What is your opinion about mpt-30B,True
@serta5727,2023-07-06T07:35:51Z,0,Really cool thing â¤,True
@evgeniermakov1522,2023-07-06T06:15:49Z,0,"Ð­Ñ‚Ð¾Ñ‚ Ñ‡ÐµÐ» Ð·Ð°Ð½Ð¸Ð¼Ð°Ð»ÑÑ Ð½ÐµÐ¹Ñ€Ð¾Ð½ÐºÐ°Ð¼Ð¸ ÐµÑ‰Ñ‘ Ð´Ð¾ Ñ‚Ð¾Ð³Ð¾, ÐºÐ°Ðº ÑÑ‚Ð¾ Ð±Ð¾Ð¼Ð±Ð°Ð½ÑƒÐ»Ð¾)",True
@1PercentPure,2023-07-06T04:40:58Z,0,hahahahahaha yes man i feel you so much,True
@shephusted2714,2023-07-06T04:27:43Z,9,my comments would include - having model run on much more data and much more recent data and also training the model on all your docs plus having more aggregated data and aggregated plugins - i think the main bottleneck for most open source ai LLM is the amount of nvram (gpu ram) available - it would be nice to find ways around this via ram disks or lower cost gpu cluster nodes - eventually we will see more gpu with lots more ram but it could take a while - lots of growth and interest in ai will help push things forward quickly - the hardware is catching up it is just not quite there yet for the common man - in 10 years we will likely have quantum functions helping out and face a similar situation all over again but it is more than enough now to just enjoy what has been wrought and look at the constant daily improvement and be happy with that and not project too much,True
@rudy9546,2023-07-06T04:20:51Z,0,"@21:30  is a spicy take , i'm in",True
@micycle8778,2023-07-06T04:20:16Z,4,"Holy shit, last time I watched you you were teaching me game development in your bedroom, now you're living in a data center",True
@footballtocricket6989,2023-07-06T04:18:27Z,0,"But can't implement using code, getting error. Can you share any working codes or method",True
@Zzznmop,2023-07-06T03:51:48Z,0,noob question: why is hugging face benchmarking important?,True
@arpitsrivastava2996,2023-07-06T03:34:22Z,0,Any recommendations on SQL and ability to answer questions from multiple tables and plot graphs let's say from a CRM dataset?,True
@shobhitbishop,2023-07-06T03:00:48Z,2,Are these model capable enough to parse tabular data? Just like gpt turbo is after creating a csv agent?,True
@spiderjerusalem,2023-07-06T02:27:14Z,1,"This LLM is the best. He wears this necklace that resembles an egg with disoriented eyes and nose and mouth, looks a bit creepy, but he is a very good person.",True
@snarkyboojum,2023-07-06T02:02:29Z,23,"Very cool. I got the Falcon-7b-instruct model working on my home PC while I watched your video. Only took about 5-10 mins to get it all going. Inference works well on my RTX 4080 (16GB) GPU too. As soon as I load the model using torch.bfloat16, the transformers library allocated ~14GB of GPU memory, but it works really well!  I'm going to have to replace my LLM app development with this local endpoint to save cost on OpenAI API calls ;) I wonder if that's a thing, a local dev loop pointing at a smaller, locally hosted LLM, and then when pushed to production, a large model or hosted endpoint, a la GPT-4. Depending on how you use the LLM in your application, I can imagine this could possibly lead to a whole new class of heisen-like bugs. Interesting to think about.  Great vid btw. I like how you keep things simple, and high-level. This is the perfect level of depth/complexity for video.",True
@qu765,2023-07-06T01:53:29Z,0,11:00 HA the scifi trope of making computers autistic is dead wrong.,True
@Personnenenparle,2023-07-06T01:27:12Z,4,I really hope an llm as powerful as gpt4 becomes available open source soonish.. having an llm running in an engineering business's server would allow for safer use.. without sharing sensitive information to a server,True
@aleksay2142,2023-07-06T01:25:35Z,0,How much did they pay for this promo!?,True
@jolieriskin4446,2023-07-06T01:10:51Z,66,I would guess the reason why some of the more modern models at much lower parameter counts are performing better than GPT3/3.5 is because the latter were trained pre the chinchilla paper on datasets that were too small in relationship to their parameter counts. Prior to chinchilla it was common to use a 2:1 ratio compared to post-chinchilla were 20:1 or 30:1 is now the norm.,True
@MeanGeneHacks,2023-07-06T00:53:07Z,2,Can't wait for qlora fine tuning video!,True
@kostik,2023-07-06T00:38:38Z,0,Too bad the sequence length is just 2048,True
@PASTRAMIKick,2023-07-06T00:15:37Z,0,"I was just playing around with Falcon chat a day ago, it's pretty good and awesome for being open source",True
@khalidal-reemi3361,2023-07-05T23:53:57Z,1,can we fine tune this model to create custom embeddings ?,True
@aungkhant502,2023-07-05T23:43:12Z,0,Have you tried anything censorship related with  the model? There were some posts about it acting weird about Saudi politics and LGBTQ topics.,True
@fire17102,2023-07-05T23:42:24Z,0,Thanks sentdex! Is there an open source implementation for function-calling like openai's that works with falcon or any locally runnable model?,True
@yiwensin5913,2023-07-05T23:38:08Z,8,"Do you think the 7B model can be fine tuned to auto-completing code and be used as a local and good substitute for co-pilot? (for those who have the required compute power, which I don't :D)",True
@jsalsman,2023-07-05T23:34:50Z,0,"Is it true that the Falcon models won't usually say anything negative about the UAE or is that just a rumor? The only official word on censorship is that they removed adult content and machine generated text from its training data. (Also, how did they identify machine generated text? That's known to be an extremely hard problem.)",True
@MrAquaktus,2023-07-05T23:26:02Z,1,"As a researcher, Iâ€™ve found Star coder and star chat (beta) to be very effective instruction tuned models, even for NL.  In general, how have you found the comparison of LlaMa vs star coder/char vs falcon?  Also on inference times given the flash attention in the huggingface models.",True
@dabunnisher29,2023-07-05T23:21:44Z,5,I wonder how slow this would be on a Raspberry Pi 4.,True
@jimdelsol1941,2023-07-05T22:55:27Z,0,Audio is fine for me.,True
@szymon308,2023-07-05T22:46:12Z,1,Great videos lately!,True
@WadRex,2023-07-05T22:25:52Z,6,"Amazing vid. One question, tho, is base ChatGPT actually a 175B? Was it confirmed by anyone? I mean, the ""original default"" version probably was somewhere around those amount of params. However, since they introduced the ""turbo"" version, I feel like they just scaled it down. It feels to me that it actually got dumber in some instances, and additionally, how would they actually speed it up if the underlying architecture is still GPT-3.5. I definitely do agree tho that the Falcon 40B and LLaMA-65B ""feel"" more knowledgeable than 3.5 from my experience, with LLaMA slightly outperforming Falcon. This is all subjective ofcourse and it depends on what your use case is. This ties neatly with final observation. The coding part of these models is still FAR from what I could get even with 3.5. This might change, however, if we finetune the base models to act as a sort of agents for specific tasks since the models are ours to modify. I tried playing with LoRA / QLoRA, but I couldn't achieve any good results for some reason (LLaMA models). I tried replicating early Alpaca training, and it all flopped. There are probably some errors in the code I can not seem to recognize... As for Falcon, it just takes a huge amount of time, and unfortunately, I can not afford not to use my PC for more than a day or two, so I didn't have a chance to play with it.",True
@jonathan-._.-,2023-07-05T22:16:30Z,78,ðŸ¤”i think it would be neat to always have like 6 examples per prompt to get a good overview over a models capabilities,True
@NeuroScientician,2023-07-05T22:15:09Z,0,aaaa you are still alive :D,True
@merrell_io,2023-07-05T22:11:39Z,2,RunPod is another solid alternative to Lambda to run these models :),True
@antopolskiy,2023-07-05T22:10:35Z,13,would be awesome to see you finetune the model. do you know if something like LORA could work to reduce the cost of fine-tuning?,True
@MenkoDany,2023-07-05T22:07:01Z,0,"While I have everyone's attention, now that reddit is dead to me, what's a good resource like /r/LocaLLaMa? I'm too lazy to monitor all the discord servers, and hacker news is just hacker news",True
@fuba44,2023-07-05T21:49:55Z,2,Audio is fine for me.,True
@fuba44,2023-07-05T21:49:22Z,4,Ups is delivering my 100usd Nvidia P40 card tomorrow.. hoping I can make it run these models. Won't fit the 40b model tho.. maybe if I find one more card in my price range..,True
@bigphab7205,2023-07-05T21:45:18Z,0,What would Jordi do?,True
@kyber.octopus,2023-07-05T21:44:48Z,2,Isn't the model fully deterministic if you use the exact same seed and weights are exactly the same for each prompt?,True
@JimBobsBass,2023-07-05T21:21:53Z,1,5th,True
@PrimordialLegend,2023-07-05T21:18:55Z,2,Low volume Please fix it from the next one! Thanks!,True
@CheapDeath96,2023-07-05T21:16:23Z,1,1st,True
@ramen4953,2023-07-05T21:15:43Z,3,not gonna continue neural networks from scratch series?,True
