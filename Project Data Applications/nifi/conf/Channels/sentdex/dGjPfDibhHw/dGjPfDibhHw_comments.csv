author,updated_at,like_count,text,public
@007rustyryan,2019-03-04T07:56:42Z,0,How to test a trained svm with unlabelled data,True
@AB-vw8wz,2019-01-24T04:49:46Z,0,I am learning SkLearn now SKR SKR SKR,True
@bilh1,2018-12-17T17:22:19Z,0,can u make tutorial on how to use dialogflow with django?,True
@Farming-Technology,2018-12-16T15:08:59Z,0,"So... i wanted to incorporate spawning and dropoff  creation from day one and tried this:         command_choices = [ship.move(Direction.North),                                                   ship.move(Direction.South),                                                   ship.move(Direction.East),                                                   ship.move(Direction.West),                                                   ship.move(Direction.Still),                                                    me.shipyard.spawn(),                                                   ship.make_dropoff()] ... .... .... move_choice = np.random.choice(range(len(command_choices)),p=[0.199, 0.199, 0.199, 0.199, 0.199, 0.004, 0.001]) command_queue.append(command_choices[move_choice])  (weighted to encourage moving over spawning and construction) which works, but balancing the data results in no files to train (as the make dropoff option len is 0 (not enough payoff in 50 moves..)) So is balancing strictly necessary in this case give that the data is generated randomly? or is it a better idea to introduce spawning and construction at a later date once the bot has movement figured out? Also anyone have an idea to speed up training an the initialization phase? i see there is a gym to download but that's beyond my comprehension..",True
@henningsperr8063,2018-12-14T16:19:25Z,1,"Hey Sentdex,  happy to see the new video! Keep them coming ;) couple of questions:  * What do you think about using the output of the model (probabilities fo each move) as the weights for the random choice? In this way you do less exploration when the model becomes more sure about a move * I wonder if you can compare means like this and conclude a model is better than the other without actually letting them play against each other and check the win-rate, one model may have 50% of its samples above 4200 and one may have 20% above since you collect the same amount of samples for both models it could in theory look like the 20% model is better just by chance since you ""handpicked"" the good ones unless you have a massive sample size. If you check the rate of games needed to produce lets say 500 samples above 4200 though you may be able to conclude which model collects more? Or just let them duke it out.  I am really curious to see how you will go about continue training the model, I made several experiments on my side and am a bit stuck. (e.g. phase 3 is not really getting better)",True
@marc-alexandrepaquet7696,2018-12-14T15:54:20Z,3,"Shouldn't we be running lets say 10 games of the same seed and always pick the bests 2-3 of those 10 games ? The way you are trainning right now favorites maps with lots of halite near the spawn Or maybe always keep only the winning bot when playing vs himself ? This way, we get trainning data for any map halite distribution! (in this case, we shouldn't reject games based on a threshold)",True
@sidewinder3422,2018-12-14T04:51:54Z,1,"I have been wanting to ask this, are you standing while doing tutorials? üòÅ",True
@SM-ht7qf,2018-12-14T00:16:30Z,0,"Ok not related to the video but i want to make a multiplayer game with pygame on two different computers, just give me a program i should use",True
@Cleanblue1990,2018-12-13T23:03:01Z,6,"Isn't there a typo in line 16? ""sys.stder = stderr"" . There is a 'r' missing",True
@toastrecon,2018-12-13T22:47:18Z,2,Ring the church bells! Glad to see this posted.,True
