author,updated_at,like_count,text,public
@matthewgjevre9483,2023-12-23T01:46:24Z,0,"New to Python, following this series after going through your basic of Python series. GPT is a cheat code for this lol",True
@adarshsharma681,2023-01-18T13:42:58Z,0,"running compile_data() shows ""None of ['Date'] are in the columns"" may be because many companies had delisted there data from yfinance they didn't share there data many companies data is empty.Can anyone solve this???",True
@amacodes7347,2022-12-19T13:49:54Z,0,"For anyone watching this video as of Dec 2022.  this version works better and uses only the yahoo finance package to get data the pandas-data-reader package doesn't work  import bs4 as bs import datetime as dt import pandas as pd  import requests import yfinance as yf  def get_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'html.parser')     table = soup.find('table', {'class':'wikitable sortable'})     tickers = [row.find('td').text.strip().replace('.', '-') for row in table.find_all('tr')[1:]]     return tickers  def get_stock_data(ticker):     start = dt.datetime(2000, 1, 1)     end = dt.datetime(2020, 11, 26)     df = yf.download(ticker, start=start, end=end)     df['Ticker'] = ticker     return df  def create_stock_dataframe():     tickers = get_sp500_tickers()     df_list = [get_stock_data(ticker) for ticker in tickers]     df = pd.concat(df_list)     return df  df = create_stock_dataframe()   or this version from the tutorial  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'html.parser')     table = soup.find('table', {'class':'wikitable sortable'})     tickers = [row.find('td').text.strip().replace('.', '-') for row in table.find_all('tr')[1:]]          with open('sp500tickers.pickle', 'wb') as f:         pickle.dump(tickers, f)     return tickers  def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open('sp500tickers.pickle', 'rb') as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')          start = dt.datetime(2000, 1, 1)     end = dt.datetime(2020, 11, 26)      for ticker in tickers:         if not os.path.exists(f'stock_dfs/{ticker}.csv'):             df = yf.download(ticker, start, end)             df.reset_index(inplace=True)             df.set_index('Date', inplace=True)             df.to_csv(f'stock_dfs/{ticker}.csv')         else:             print(f'Already have {ticker}')  get_data_from_yahoo()",True
@adampayne5619,2022-05-24T17:52:50Z,0,You should consider registering your Sublime,True
@tinatipton3291,2021-12-05T20:03:09Z,0,"What might you do if you wanted to keep other columns, and not just adjusted close, it throws an exception that the data overlaps. I attempted to rename each of the columns, however, it doesn't quite work the same with multiple, and it gives me positional keyword argument errors. I could just be doing it wrong though, I'm not super familiar with doing data plots, so any advice from anyone would be appreciated.",True
@vijaynyaya6603,2021-09-12T07:10:38Z,1,"One of the reasons I love computer science is because the ""programming"" community is so much supportive and shares knowledge.",True
@abhishekdoke6102,2021-06-03T05:28:11Z,1,"Anyone getting ""ValueError: columns overlap but no suffix specified: Index(['MMM'], dtype='object')""?",True
@FireTeamSix.,2021-05-12T16:57:17Z,0,"I have been trying to do this tutorial, but keep running out of memory after running compile_data(). It keeps saying ""Unable to get 121 MB of Memory"", so I think my RAM might be too small (16GB DDR4). Does anyone have a workaround to this? Would I need to only pull 200 companies as opposed to all 500? Thank you so much.",True
@landonrobin8910,2021-04-08T22:31:24Z,0,"Where do I find the folder with all of the .csv files? I ran the code and everything looks correct in the console with no errors, but I am struggling to find the files. I am on a mac as well, if that makes a difference",True
@varadjams,2021-04-03T06:28:28Z,0,"Compiling this in April 2021 - I end up getting a name error: name 'main_df' is not defined. Dont understand why, anybody got any solutions?",True
@hosseinzakariaee5465,2021-03-01T13:24:33Z,1,"vey good but when i run it it give me thie eror: pandas_datareader._utils.RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader whati should do ?",True
@Anthony-db7ou,2020-12-08T19:36:14Z,0,"Hey, any idea whats going on here?  i checked the stock_dfs folder and it didn't store anything. Trying to figure our what went wrong.  Best, Carmine  ------------------------------------------------------------------------- OUTPUT:    KeyError                                  Traceback (most recent call last) ~\anaconda3\lib\site-packages\pandas_datareader\yahoo\daily.py in _read_one_data(self, url, params)     156             j = json.loads(re.search(ptrn, resp.text, re.DOTALL).group(1)) --> 157             data = j[""context""][""dispatcher""][""stores""][""HistoricalPriceStore""]     158         except KeyError:  KeyError: 'HistoricalPriceStore'  During handling of the above exception, another exception occurred:  RemoteDataError                           Traceback (most recent call last) <ipython-input-24-179ef744beb2> in <module>      25             print('Already have {}'.format(ticker))      26  ---> 27 get_data_from_yahoo()      28       29   <ipython-input-24-179ef744beb2> in get_data_from_yahoo(reload_sp500)      17         # just in case your connection breaks, we'd like to save our progress!      18         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)): ---> 19             df = web.DataReader(ticker, 'yahoo', start, end)      20             df.reset_index(inplace=True)      21             df.set_index(""Date"", inplace=True)  ~\anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)     212                 else:     213                     kwargs[new_arg_name] = new_arg_value --> 214             return func(*args, **kwargs)     215      216         return cast(F, wrapper)  ~\anaconda3\lib\site-packages\pandas_datareader\data.py in DataReader(name, data_source, start, end, retry_count, pause, session, api_key)     374      375     if data_source == ""yahoo"": --> 376         return YahooDailyReader(     377             symbols=name,     378             start=start,  ~\anaconda3\lib\site-packages\pandas_datareader\base.py in read(self)     251         # If a single symbol, (e.g., 'GOOG')     252         if isinstance(self.symbols, (string_types, int)): --> 253             df = self._read_one_data(self.url, params=self._get_params(self.symbols))     254         # Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])     255         elif isinstance(self.symbols, DataFrame):  ~\anaconda3\lib\site-packages\pandas_datareader\yahoo\daily.py in _read_one_data(self, url, params)     158         except KeyError:     159             msg = ""No data fetched for symbol {} using {}"" --> 160             raise RemoteDataError(msg.format(symbol, self.__class__.__name__))     161      162         # price data  RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader",True
@jamesburns9933,2020-10-17T19:29:06Z,0,"I didn't want to drop columns like 'Volume', 'Open', 'High', 'Low', and 'Close'. So instead of using the code in this video, I used the below code to get a large df.  import glob, os files = glob.glob('stock_dfs/*.csv') df = pd.concat([pd.read_csv(fp).assign(Ticker=os.path.basename(fp)) for fp in files]) df['Ticker'] = df['Ticker'].str.replace('.csv', '')  Thanks for the videos, sentdex!",True
@hill2750,2020-10-03T01:55:06Z,0,Over 3 years later and these videos still blow my new python mind. THANK YOU FOR BEING YOU :),True
@TheZ10Z,2020-07-28T11:15:34Z,0,"I did small adjusts,  df = pd.read_csv('stock_dfs/{}.csv'.format(ticker.rstrip().replace('.', '-'))) instead of df = pd.read_csv('stock_dfs/{}.csv'.format(ticker) I did it because yahoo api change and now the wikipedia list doesn't work if you don't change the . to -. But in order to combine them you need to change it back to "".""",True
@prempatil3263,2020-07-13T10:35:51Z,1,"I am facing a memory error, please help me out",True
@oriol-borismonjofarre6114,2020-07-09T00:41:23Z,0,"You magestic beast! you are a brilliant mind!.  I loved all the videos from your list ""Python Programming for Finance""",True
@brunorafael6497,2020-05-27T00:53:15Z,0,"""sp500_joined_closes.csv: tokenization, wrapping and folding have been turned off for this large file in order to reduce memory usage and avoid freezing or crashing."" Should i forcefully enable features? (I am using vs code)",True
@joneasterbrook1379,2020-05-18T08:06:34Z,4,"THIS CODE WORKS 2020-05-18   import bs4 as bs import datetime as dt import os import pandas as pd from pandas_datareader import data as pdr import pickle import requests import yfinance as yf  yf.pdr_override   def save_sp500_tickers():     resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, 'lxml')     table = soup.find('table', {'class': 'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text.replace('.', '-')         ticker = ticker[:-1]         tickers.append(ticker)     with open(""sp500tickers.pickle"", ""wb"") as f:         pickle.dump(tickers, f)     return tickers   # save_sp500_tickers() def get_data_from_yahoo(reload_sp500=False):     if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)     if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')     start = dt.datetime(2019, 6, 8)     end = dt.datetime.now()     for ticker in tickers:         print(ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = pdr.get_data_yahoo(ticker, start, end)             df.reset_index(inplace=True)             df.set_index(""Date"", inplace=True)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}'.format(ticker))  def compile_data():     with open (""sp500tickers.pickle"", ""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count, ticker in enumerate(tickers):         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker.replace('.', '-')))         df.set_index('Date', inplace=True)          df.rename(columns = {'Adj Close':ticker}, inplace=True)         df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)          if main_df.empty:             main_df = df         else:             main_df = main_df.join(df, how='outer')          if count % 10 == 0:             print(count)     print(main_df.head())     main_df.to_csv('sp500_joined_closes.csv')  compile_data()",True
@hansel7203,2020-05-13T00:11:24Z,0,"In the previous video I selected on the first 10 companies, [:10]. Then when I ran this new script I got the error that I could not find file for #11. Where within this code would I tell it to just use the 10 files I downloaded?",True
@christosroniotis1903,2020-05-02T21:53:08Z,0,"Below is my code:   def compile_data():     with open('sp100tickers.pickle','rb') as f:         tickers = pickle.load(f)             main_df=pd.DataFrame()     for count,ticker in enumerate(tickers): # enumerate is equal to range(len(sth)) .         df=pd.read_csv('stock_dfs/{}.csv'.format(ticker)         df.set_index('Date',inplace=True)         df.rename(columns={'Adj Close':ticker},inplace=True)         df.drop(['Open','High','Low','Close','Volume'],1,inplace=True)         if main_df.empty:             main_df=df         else:             main_df=main_df.join(df,how='outer')                  if count % 10==0: # Το πηλίκο της διαίρεσης είναι 0             print(count)      print(main_df.head())     main_df.to_csv('sp100_joined_adj_close.csv')  compile_data()   I get a syntax error for df.set_index('Date',inplace=True).Could someone help me with this?I do not get where is the error.",True
@yuxuanliu4661,2020-04-28T08:32:49Z,0,"After combining these data by 'Date', dataframe only contains 'MMM'(only one company)? Anyone had the same problem???? thanks!!!!!",True
@danyalpanjwani14,2020-04-13T01:18:49Z,0,"I'm getting the following error:     File ""/Users/danpanjwani/opt/anaconda3/envs/ECON341/lib/python3.7/site-packages/pandas_datareader/yahoo/daily.py"", line 160, in _read_one_data     raise RemoteDataError(msg.format(symbol, self.__class__.__name__))  RemoteDataError: No data fetched for symbol MMM  using YahooDailyReader    what should I do?",True
@yudikubota293,2020-04-08T18:16:33Z,0,"in my case I was getting date is not a column error for some reason, the csv file was missing the first column (date)   solved this by resetting index and renaming   df = web.DataReader(ticker, 'whatever', start, end) df.reset_index(level=0, inplace=True) df.rename(columns = {'index': 'date'}, inplace=True) df.to_csv(csvPath)",True
@gabrielberbig9454,2020-04-08T06:20:47Z,0,Anyone else notice that df.inplace causes the date header to appear one row below all other headers,True
@tanmayebhatia,2020-04-01T14:44:46Z,0,"Anyone else not seeing the exported ""Sp500 joined closed"" CSV anywhere? Keep getting deprecation warnings but the script runs. Pls help!",True
@diegomedina1898,2020-03-25T22:22:51Z,2,When I run the code I get this issue: TypeError: 'set' object is not callable. Can anybody help?,True
@vk1094,2020-03-23T12:31:41Z,0,Generated frames are not getting saved. Folder is created but file are not saved. Pleas help anyone?,True
@MrRaznos,2020-03-03T12:54:18Z,6,"IN order to solve 'ValueError: columns overlap but no suffix specified'   Change:    main_df = main_df.join(df,how = 'outer') to:   main_df = main_df.merge(df,how = 'outer',on='Date')",True
@juliusuotila5930,2020-03-01T16:05:54Z,2,"Those with ""ValueError: columns overlap but no suffix specified: ...."" and cant get around it with other advises here.   add this to your loop:           if count % 10 == 0:              print(count)             #added             if count == 500:                 print(main_df.head())                 main_df.to_csv('sp500_joined_closes.csv')                 return False   .... you can remove                  print(main_df.head())                   main_df.to_csv('sp500_joined_closes.csv')  ...this part from the end of the code.  This gets the file saved and you can continue with the tutorial. Something wrong with the .join command, I couldnt get around otherwise.",True
@guillecobo_,2020-01-08T18:12:11Z,2,"def compile_data():     with open(""sp10tickers.pickle"", ""rb"") as f:         tickers = pickle.load(f)     main_df = pd.DataFrame()      for count, ticker in enumerate(tickers):         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))         df.set_index('Date', inplace=True)         df.rename(columns={'Adj Close': ticker}, inplace=True)         df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], 1, inplace=True)          if main_df.empty:             main_df = df         else:             main_df = main_df.join(df, how='outer')      print(main_df.head())     main_df.to_csv('sp10_combined_data.csv')   compile_data()",True
@solutioncomedy9681,2019-12-30T10:23:47Z,0,"Does anyone also experience problems when the tickers reaches 'BRK.B'?  I get the "" Key error: 'Date'  ""  warning. Unfortunately i am not able to find the solution by myself...",True
@privateeye242,2019-12-25T14:40:39Z,1,"Everything works except for the module compile_data. I got the sp500tickers.pickle file, I got the stock_dfs directory and I got the csv  files in that directory and these files have the data.  This is the code: def compile_data():      with open(""d:sp500tickers.pickle"",""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count, ticker in enumerate(tickers):         df = pd.read_csv('d:stock_dfs/{}.csv'.format(ticker))         df.set_index('Date', inplace=True)          df.rename(columns={'Adj Close': ticker}, inplace=True)         df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)          if main_df.empty:             main_df = df         else:             main_df = main_df.join(df, how='outer')          if count % 10 == 0:             print(count)       print(main_df.head())     main_df.to_csv('d:sp500_joined_closes.csv')  compile_data() It does not print main_df.head and does not make a sp500_joined_closes.csv file. If it run's it prints the counts till 500 and shows the next error message: ValueError: columns overlap but no suffix specified: Index(['MMM'], dtype='object')  If I put the commands in the if count % 10 = = 0 loop by putting them at the samen indentation as print(count) the data is printed and the sp500_joined_closes.csv file is produced. But it takes a long time. Even then I get error messages.  Any one has an idea?   It has something to do with:   main_df = main_df.join(df, how='outer' )",True
@FranVarVar,2019-12-14T20:04:56Z,23,"Running this in December 2019. I was having problems getting data form these tickers: BKR, BRK.B BF.B, CTVA, DOW, FOXA, FOX NLOK. Apparently DataReader complains that there is no 'Date' value. I just wrap the function in try except clause and ignore those tickers:               try:                 df = web.DataReader(ticker, 'yahoo', start, end)                 df.to_csv(f'stock_dfs/{ticker}')             except:                 print(f'Problems found when retrieving data for {ticker}. Skipping!')",True
@xilin1063,2019-12-02T12:02:36Z,9,"Thank you for sharing the knowledge, it's really priceless to me",True
@user-um8sw6ii5z,2019-11-10T01:32:40Z,0,"Not sure if anyone facing a strange issue , there have 2 companies, BRK-B and BF-B, which i cannot read the history data from Yahoo,  but i manually search it, they are really exits and have all time data. also ,    i guess the index of SP500 from WIKI ,the format is BRK.B, but in Yahoo, the company data is stored as BRK-B. same thing happened to BF-B or BF.B,    if I run the code , there have an error :   FileNotFoundError: [Errno 2] File b'stock_dfs/BF.B.csv' does not exist: b'stock_dfs/BF.B.csv   one by one come out , does anyone else facing that problem? i am using VScode to program code.   thanks",True
@arturthesimplehuman,2019-11-09T21:03:52Z,1,"no adds, wow. Thank you)",True
@assiaes-saihi3299,2019-10-20T19:18:37Z,0,"hello there, I have problems executing the code and I will like to now why is not working. I'm using pycharm  the latest version of python, numpy and matplotlib.    message error :    Traceback (most recent call last):   File ""C:/Users/xxxxxxxxxxxxxxxxxx/6.0.py"", line 46, in <module>     get_data_from_yahoo()   File ""C:/Users/xxxxxxxxxxxxxxxxxx/6.0.py"", line 27, in get_data_from_yahoo     with open(""sp500tickers.pickle"", ""rb"") as f: FileNotFoundError: [Errno 2] No such file or directory: 'sp500tickers.pickle'       can someone help me please",True
@marcelocanetta1892,2019-09-29T14:35:08Z,1,"Hi sentdex, thanks for the videos. I think there is an error in putting the series together since the values ​​returned Adj Close values ​​in ""sp500_joined_closes"" are different from the real ones in the individual df. In my code they went well, but in the video they are different , Regards",True
@KevinJohnson01,2019-09-03T11:09:33Z,1,"I had two issues by the end of this video: both caused by me... 1) I only wanted to analyze 5 stocks instead of the entire S&P500, because my internet is super slow. 2) I didn't see a point to the count function, so I decided to remove it, whoops.   How to fix: problem 1) I stuck with Sentdex's style and wrote a definition to reduce the S&P500 list we acquired from an earlier lesson then pointed to the new .pickle appropriately.   step 1) Add the following definition into your code and run the call to get your new .pickle:   def sp500_pickle_reducer():     tickers = pickle.load(open(""sp500tickers.pickle"", ""rb""))     print(tickers[0:5])  # prints list for verification.     with open(""sp5tickers.pickle"", ""wb"") as f:  # name the file accordingly, this example has 5 tickers.         pickle.dump(tickers[0:5], f)  # replace 5 as needed, this is how you control the number of tickers put into your new pickle.   # sp500_pickle_reducer()  # you'll need to un-comment this i order to call the function.   step 2) Update the 2 references in def compile_data():   with open(""sp500tickers.pickle"", ""rb"") as f: main_df.to_csv('sp500_joined_closes.csv') changed to: with open(""sp5tickers.pickle"", ""rb"") as f: main_df.to_csv('sp5_joined_closes.csv') note: it isn't necessary to change to 'sp5_joined_closes.csv' and it'll probably make things more interesting in future lessons, but it looks nicer to me.   problem 2) You must have ""count, ticker"" in the line ""for count, ticker in enumerate(tickers):""   step 1) if you get the error ""FileNotFoundError: [Errno 2] File b""stock_dfs/(0, 'MMM').csv"" does not exist: b""stock_dfs/(0, 'MMM').csv"""" then you probably removed ""count, "" from ""for count, ticker in enumerate(tickers):"" and just need to put it back to get everything right again.",True
@pritamsarkar3371,2019-08-19T12:06:39Z,0,"try:             df=pd.read_csv('stock_dfs/{}.csv'.format(ticker))             df.set_index('Date',inplace=True)             df.rename(columns={'Adj Close': ticker},inplace=True)             df.drop(['Open','High','Low','Close','Volume'],1,inplace=True)              if main_df.empty:                 main_df=df             else:                 main_df=main_df.join(df,how='outer')             if count % 10 ==0:                 print(count)         except:             print(""FileNotFoundError"")             pass",True
@BrandonJacobson,2019-08-04T16:30:07Z,7,"I just ran this code as is and it worked perfectly.  If you haven't made it through video 5 or video 6 in this series, then you may be having problems with the stocks having \n after them or running into issues with stock symbols having a ""."" instead of a ""-"" like BRK-A and Yahoo! not being able to pull the data.  Look through the comments on those videos for tips on getting rid of those errors.",True
@darwinchan5573,2019-07-06T03:34:43Z,0,"i just copied the whole code and ran in vscode, but got error in the 2nd iteration in the count ticker enumerate for loop. ValueError: columns overlap but no suffix specified: Index(['Unnamed: 0'], dtype='object')   Seems it about the dataframes have problem. I just wonder why code in Sentdex doesnt get such error.",True
@thevaibhavgaur4953,2019-07-04T12:06:39Z,0,how can we change the currency?,True
@mschuer100,2019-06-11T23:50:36Z,2,"Great videos, i am really enjoying these. Thank you for putting all the time in to all of your videos. They are a great education, great resource. One thing i noticed is that when you scrape Wiki for the tickers, it returns BRK.B for Berkshire. Yahoo Finance does not recognize this symbol anymore, it goes by ticker BRK-B (at least that is what i noticed) so you will get an error that the BRK.B ticker is not found and it breaks...so, for anyone currently watching this video I would just use a Try and Except for the compile_data function with the exception as follows:: except FileNotFoundError: continue .  then have it print the ticker it cannot find. You can then swap this ticker for Yahoo Finances ticker so it can pick it up with a bit of amended code....Please continue to make these awesome videos..they are truly enjoyable and a great learning experience.",True
@Xtremefiresnake,2019-06-01T02:53:35Z,0,"if you are having problems with ""Date"" , try ""date""",True
@amirvahid7143,2019-01-17T02:55:39Z,0,Thanks it appears to me that pip install yahoo_finance_fix solves all of the problems since quandl is not quite a good fit for this tutorial b/c it only downloads ~55 S&P500,True
@simonromano,2018-10-31T16:49:26Z,0,Anyone else getting a KeyError: 'Date' when running the function??,True
@aligh8803,2018-10-15T22:28:06Z,0,"Just for the info:  right now the Yahoo, Google, and Morningstar do not work. you could use ""Quandl"" instead.",True
@kosnowman,2018-09-15T16:56:06Z,3,"hey guys , so I try to run the code, from apatel32 as well as sentdex official one, whenever I have reached BRK.B it has an error, can i do anything about it? could i skip it , or just compile whatever I have with me up to this point? thanks !",True
@fbmemar,2018-09-13T03:14:14Z,0,"Anybody knows why I'm getting this error after running the get_data_from_yahoo()? It happens in the middle of syncing tickers. ... Already have BAX BBT Already have BBT BDX Already have BDX BRK.B Traceback (most recent call last):   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\indexes\base.py"", line 3078, in get_loc     return self._engine.get_loc(key)   File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""C:\Files\Projects\Python\SP500 Webscraping\SP500 Scraping yahoo.py"", line 55, in <module>     get_data_from_yahoo()   File ""C:\Files\Projects\Python\SP500 Webscraping\SP500 Scraping yahoo.py"", line 50, in get_data_from_yahoo     df = web.DataReader(ticker,'yahoo',start,end) #query google for historical data   File ""C:\Python\Python37-32\lib\site-packages\pandas_datareader\data.py"", line 311, in DataReader     session=session).read()   File ""C:\Python\Python37-32\lib\site-packages\pandas_datareader\base.py"", line 210, in read     params=self._get_params(self.symbols))   File ""C:\Python\Python37-32\lib\site-packages\pandas_datareader\yahoo\daily.py"", line 142, in _read_one_data     to_datetime(prices['Date'], unit='s').dt.date)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\frame.py"", line 2688, in __getitem__     return self._getitem_column(key)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\frame.py"", line 2695, in _getitem_column     return self._get_item_cache(key)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\generic.py"", line 2489, in _get_item_cache     values = self._data.get(item)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\internals.py"", line 4115, in get     loc = self.items.get_loc(item)   File ""C:\Python\Python37-32\lib\site-packages\pandas\core\indexes\base.py"", line 3080, in get_loc     return self._engine.get_loc(self._maybe_cast_indexer(key))   File ""pandas\_libs\index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item   File ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item KeyError: 'Date'",True
@petealwayslovesu,2018-09-04T10:23:39Z,1,"import bs4 as bs4 import pickle import requests import datetime as datetime import os import pandas as pandas pandas.core.common.is_list_like = pandas.api.types.is_list_like import pandas_datareader.data as data   # save standard & poor's 500 ticker list  def save_sp500_tickers():     response = requests.get(""https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"")     soup = bs4.BeautifulSoup(response.text, ""lxml"")     table = soup.find(""table"", {""class"": ""wikitable sortable""})     tickers = []     for row in table.findAll(""tr"")[1:]:         ticker = row.findAll(""td"")[0].text         tickers.append(ticker)      with open(""sp500tickers.pickle"", ""wb"") as file:         pickle.dump(tickers, file)      print(tickers)      return tickers   # save_sp500_tickers()   # fetching sp500  def morning_star():      if not os.path.exists(""stock_data_frames""):         os.makedirs(""stock_data_frames"")      with open(""sp500tickers.pickle"", ""rb"") as file:         tickers = pickle.load(file)      start = datetime.datetime(2000, 1, 1)     end = datetime.datetime.now()      count = 0     count_e = 0     no_such_tickers = []     for ticker in tickers:         count += 1         print(str(count) + "". "" + ticker)          if not os.path.exists(""stock_data_frames/{}.csv"".format(ticker)):             while True:                 try:                     data_frame = data.DataReader(ticker, ""morningstar"", start, end)                      if str(data_frame.head()):                         data_frame.to_csv(""stock_data_frames/{}.csv"".format(ticker))                         count_e = 0                         break                 except Exception as e:                     count_e += 1                     print(str(count_e) + "". "" + ticker + "" "" + str(e))                      # no such tickers in morningstar                     # ['ANDV', 'BKNG', 'BHF', 'CBRE', 'DWDP', 'DXC', 'EVRG',                     # 'JEF', 'TPR', 'UAA', 'WELL']                     if count_e >= 10:                         no_such_tickers.append(ticker)                         print(no_such_tickers)                         count_e = 0                         break         else:             print(""already have {}"".format(ticker))   # morning_star()   # merge sp500  def merge_data():     no_such_tickers = ['ANDV', 'BKNG', 'BHF', 'CBRE', 'DWDP', 'DXC', 'EVRG',                        'JEF', 'TPR', 'UAA', 'WELL']      merged_data_frame = pandas.DataFrame()      with open(""sp500tickers.pickle"", ""rb"") as file:         tickers = pickle.load(file)      for i, ticker in enumerate(tickers):         if ticker in no_such_tickers:             continue         data_frame = pandas.read_csv(""stock_data_frames/{}.csv"".format(ticker))         data_frame.set_index(""Date"", inplace=True)         data_frame.rename(columns={""Close"": ticker}, inplace=True)         data_frame.drop([""Symbol"", ""Open"", ""High"", ""Low"", ""Volume""], 1, inplace=True)          if merged_data_frame.empty:             merged_data_frame = data_frame         else:             merged_data_frame = merged_data_frame.join(data_frame, how=""outer"")          if i % 10 == 0:             print(i)      print(merged_data_frame.head())     merged_data_frame.to_csv(""merged_sp500.csv"")   merge_data()",True
@Rygorius,2018-08-23T19:16:17Z,1,"When I run the code, the counter starts to slow down and then grinds to a halt at about 410, I left it running and my computer restarted. Since I don't have an error for this its hard to troubleshoot. Has anyone else run into this issue? Thanks!",True
@liangyumin9405,2018-08-23T16:36:51Z,0,"pip install fix_yahoo_finance  ---> import fix_yahoo_finance as yf ; yf.pdr_override()--> df = web.get_data_yahoo(ticker,start=start, end=end)   may work   2018-08-24",True
@bobbiewang6663,2018-08-18T22:35:29Z,0,"for those who are using robinhood  def compile_data():     with open(""sp500tickers.pickle"",""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count,ticker in enumerate(tickers):         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))         df.set_index('begins_at', inplace=True)          df.rename(columns = {'close_price':ticker}, inplace=True)         df.drop(['high_price','interpolated','low_price','open_price','session','volume','symbol'], 1, inplace=True)          if main_df.empty:             main_df = df         else:             main_df = main_df.join(df, how='outer')          if count % 10 == 0:             print(count)      print(main_df.head())     main_df.to_csv('sp500_joined_closes.csv')  compile_data()",True
@markd964,2018-08-16T04:44:31Z,0,"Minor correction to this code, using a : instead of a , as shown here (from https://pythonprogramming.net/combining-stock-prices-into-one-dataframe-python-programming-for-finance/):          df.rename(columns={'Adj Close':ticker}, inplace=True)",True
@jayc578,2018-08-11T21:06:06Z,0,"Regarding the axis, here is a source for you to refer to.  https://stackoverflow.com/questions/25773245/ambiguity-in-pandas-dataframe-numpy-array-axis-definition",True
@erichanek253,2018-07-24T18:21:16Z,0,"def compile_data():     with open(""sp500tickers.pickle"", ""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count, ticker in enumerate(tickers):         try:             df = pd.read_csv(f'stock_dfs/{ticker}.csv')             if not df.empty:                 df.set_index('Date', inplace=True)                  df.rename(columns={'Close': ticker}, inplace=True)                 df.drop(['Symbol', 'High', 'Low', 'Open', 'Volume'], 1, inplace=True)              if main_df.empty:                     main_df = df             else:                 main_df = main_df.join(df)              if count % 10 == 0:                 print(count)                          except:             print(f'Cannot obtain data for {ticker}')      print(main_df.tail())     main_df.to_csv('sp500_joined_closes.csv')   compile_data()",True
@Martin-ms7nb,2018-07-10T20:52:25Z,0,"For some reason, some of the Morningstar pulls have a column labeled 'Symbol' and some don't, further, Morningstar just can't pull certain tickers. For the first issue, where sentdex drops Open, High, Low, Volume, Close, *YOU*  should drop Open, High, Low, and Volume,  then a try/except where you drop ""Symbol""  For the second issue, throw the logic in his ""for"" loop under a try condition, then except and pass/print your non-working tickers.    def compile_data():     with open('sp500tickers.pickle', 'rb') as f:         tickers = pickle.load(f)         print(len(tickers))     main_df = pd.DataFrame()          for count, ticker in enumerate(tickers):         try:             df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))             if not df.empty:                  df.set_index('Date', inplace = True)                                  df.rename(columns = {'Close': ticker}, inplace = True)                 df.drop(['Open','High','Low','Volume'], 1, inplace = True)                 try:                     df.drop(['Symbol'], 1, inplace = True)                 except:                     pass                          if main_df.empty:                 main_df = df             else:                 main_df = main_df.join(df)             if count % 10 == 0:                 print(count)         except:             print('Cannot obtain {}'.format(ticker))                               print(main_df.head())     main_df.to_csv('sp500_joined_close.csv')",True
@vish647,2018-07-05T19:02:54Z,0,"@sentdex,  Morningstar doesn't procure Adj Close column from the website. It only stores the date, open,high, close, low, and volume in the csv file.",True
@Theminecrafter2598,2018-07-02T10:58:32Z,0,"when i run this code i get the error ""File b'stock_dfs/ALL.csv' does not exist"" can you help me figure out how to fix this?",True
@samg7247,2018-07-02T00:32:36Z,0,"Can someone maybe explain to me why I am having issues with panda and datareader? I was able to recreate the code from part 5 with no issues but from this video, it is giving me issues with lines (in order of traceback) 5, 2, 14, and 1. At the bottom of the error it says Importerror: cannot import name 'is_list_like_' I am trying to use morning star data as I have noticed comments saying there are issues with google and yahoo APIs",True
@rohitupadhyay4665,2018-06-29T18:12:07Z,0,Trying the code for Indian Nifty 50 stocks. Facing memory error in concatenate_join_units     concat_values = concat_values.copy() MemoryError. Any solution?,True
@rajivkumar8160,2018-06-27T09:38:18Z,0,Hi sentdex. Great work. There's a slight problem here as morningstar does not provide us with the adjustment close data and the google and yahoo finance api have been depracated. I would be grateful if you or anyone can help me out with this as to how to get the adjustment close price of the tickers,True
@mhj2724,2018-06-21T03:13:06Z,3,"If you get 'ValueError 'Open' label axis doesn't exist ~~ (I can't remember exact error message), try this code. ( * I'm using morningstar api, I'm python newbie, I'm not good at english) this code will ignore 'error data', and just emerge data in one.  morningstar api doesn't give you adj close. so, I just emerge data with 'Close'. and sometimes morningstar can't download some of  sp500tickers.pickle. try with fix_yahoo_finance lib.  [ CODE ]  def compile_data():     with open(""sp500tickers.pickle"", ""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count, ticker in enumerate(tickers):         try:             df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))             if not df.empty:                 df.set_index('Date', inplace=True)                  df.rename(columns={'Close': ticker}, inplace=True)                 df.drop(['High', 'Low', 'Open', 'Volume'], 1, inplace=True)              if main_df.empty:                     main_df = df             else:                 main_df = main_df.join(df)             print(main_df.head())         except:             print('Cannot obtain data for')      main_df.to_csv('sp500_joined_closes.csv')",True
@jasonrbodie,2018-06-19T20:52:26Z,1,"Good tutorial so far. If you're getting a ValueError: columns overlap ...... try the following code.   ##### COMPILE DATA INTO ONE FILE  ##### # 1. Utilizes python try catch due to not having all 500 tickers  # 2. Opens each ticker file, strips unwanted information and leaves ticker value and 'Close' value # 3. Joins stripped data into one file ##### END COMPILE DATA INTO ONE FILE #####  import requests import pickle import requests import datetime import os import pandas import pandas_datareader.data as web from time import sleep  # Utilizes python try catch due to not having all 500 tickers def compile_data():     try:         # 'sp500tickers.pickle' populated with 'morningstar' data         # Not yahoo or google         # Wasnt able to download all 500 but did download 44 tickers          with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)          mainDataSet = pandas.DataFrame()          for count, ticker in enumerate(tickers):             sleep(1) # 1 sec sleep allowing me to watch progress of print             fileDataSet = pandas.read_csv('stock_dfs/{}.csv'.format(ticker))             fileDataSet.set_index('Date', inplace=True)              # Used column 'Close' due to not having 'Adj Close' from morningstar             fileDataSet.rename(columns={'Close':ticker}, inplace=True)             fileDataSet.drop(['Symbol', 'Open', 'High', 'Low', 'Volume'], 1, inplace=True)              if mainDataSet.empty:                 mainDataSet = fileDataSet             else:                 # Just joined(fileDataSet) without how='outer'                 # Required for it to work. I kept getting                  # ValueError: columns overlap                 mainDataSet = mainDataSet.join(fileDataSet)              # Prints progress in terminal instead of counting             # Good if you dont have 500 tickers             print(mainDataSet.head())           mainDataSet.to_csv('sp500_joined_closes.csv') # Saves after for loop (if you have all 500 tickers)      # Except triggers due to not having all 500 ticker files.      # And saves final version of csv file     except FileNotFoundError:         mainDataSet.to_csv('sp500_joined_closes.csv') # Saves after you reach your last ticker",True
@sarelg21,2018-06-07T23:41:49Z,0,"In case anybody sees this, I encountered an error where the date isn't recognized as a key, I think this can happen if your csv is empty for some reason, i.e. the data for that company wasn't downloaded.  I think this can lead to other function-breaking errors as well. you can try bypassing these companies by adding a condition that the data frame you're loading isn't empty:  for count, ticker in enumerate(tickers):         df = pd.read_csv('stocks_dfs/{}.csv'.format(ticker))         if not df.empty:             df.set_index('Date', inplace=True)              df.rename(columns={'Adj Close': ticker}, inplace=True)             df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], 1, inplace=True)              if main_df.empty:                 main_df = df              else:                 main_df = main_df.join(df, how='outer')          else:             print('{} data is missing'.format(ticker))          if count % 10 == 0:             print(count)      main_df.to_csv('sp500_joined_closes.csv')",True
@rajancutting6925,2018-06-05T03:12:59Z,3,"Anybody else getting ""ValueError: columns overlap but no suffix specified: Index(['Symbol'], dtype='object')""? Anybody know what to do?",True
@natevannortwick4554,2018-05-08T23:37:27Z,0,"In the part 6 video I capped the S&P 500 query from yahoo (I used IEX thought because yahoo is broken now) at 10 (for ticker in tickers[:10]:). Now when I try to execute the compile_data() function it doesn't work because I don't have all of the .csv files that I need, it appears to work for the first ten, but then when it finds a ticker in the S&P 500 pickle file that doesn't have an associated .csv file I get an error. Any ideas on how to fix this? Sorry if this is a stupid question, I'm a noob, thanks in advance.",True
@iNotSoTall,2018-04-19T08:47:54Z,0,"It's still giving me the ""https://www.youtube.com/watch?v=j0zW_KXyQJ4&index=7&list=PLQVvvaa0QuDcOdF96TBtRtuQksErCEBYZ"" error when trying to enumerate. What sort of code can move past that error so it can just skip enumerating it if the file isn't there?",True
@yuehu5315,2018-04-13T15:19:24Z,0,"What if companies were included in the index after 2000, or were excluded some point in time after 2000, what will csv look like? And how to deal with that?",True
@ritujha7900,2018-03-22T18:28:40Z,2,"Hi,  I am getting an error in the line :  main_df = main_df.join(df, how='outer')  ValueError: columns overlap but no suffix specified: Index([u'Ex-Dividend', u'Split Ratio', u'Adj. Open', u'Adj. High', u'Adj. Low',        u'Adj. Volume'],       dtype='object')  Does anyone have any idea about how to correct this? I have downloaded data from google as yahoo didn't work.",True
@nikifoxy69,2018-01-30T17:04:10Z,0,Can someone help on below error.   valueerror stat path too long for windows python,True
@skythianz,2018-01-28T10:28:47Z,1,"I am getting the remote data error. Weird that it is actually pulling data from  yahoo but in chunks before throwing the error. Ran it some 20 times and got 71 tickers files and seems unable to fetch anymore. Any workaround, guys ?",True
@seanbatir4115,2018-01-25T02:35:35Z,0,"Soo.... what did you guys do to fix the issue wtih Yahoo's URL changing? Did everyone just switch from                 df = web.DataReader(ticker, 'yahoo', start, end)  to df = web.DataReader(ticker, 'google', start, end)? I tried this and my script runs, but for every possible stock I get the feedback, "" Cannot obtain data for <Insert Stock name>",True
@Madmartigan6,2018-01-16T02:38:25Z,0,"I keep getting the error ""RemoteDataError: Unable to read URL"", every time I run it the code can get 1 or 2 stocks then I get the error and have to run it again. Any idea why this is happening?",True
@beansgoya,2017-11-30T03:54:42Z,0,"im getting a ton of ""can not obatin data"". and for a bunch of files, they are not even stock data. just text. Anyone else running into this?",True
@nenadnikolic2728,2017-10-03T16:18:30Z,1,"When I display main_df.head I only get the column of the first company, the join doesn't work. Anybody had issues with that?",True
@Nickiziboy,2017-09-12T18:19:58Z,7,"Complete setup below, using google that works for missing LMT etc.  Previous video has been commented out.  import bs4 as bs import pickle import requests import datetime as dt import os import pandas as pd import pandas_datareader.data as web  def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text, ""lxml"")     table = soup.find('table', {'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:             ticker = row.findAll('td')[0].text             tickers.append(ticker)      with open(""sp500tickers.pickle"", ""wb"") as f:             pickle.dump(tickers, f)      print(tickers)      return tickers  # save_sp500_tickers()  def get_data_from_google(reload_sp500=True):      if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"", ""rb"") as f:             tickers = pickle.load(f)      if not os.path.exists('stock_dfs'):          os.makedirs('stock_dfs')      start = dt.datetime(2000,1,1)     end = dt.datetime(2016,12,31)      for ticker in tickers:         try:             print(ticker)             if not os.path.exists('stocks_dfs/{}.csv'.format(ticker)):                 df = web.DataReader(ticker, 'google', start, end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             else:                 print('Already have {}'.format(ticker))         except:             print('Cannot obtain data for ' +ticker)  # get_data_from_google()  def compile_data():     with open(""sp500tickers.pickle"", ""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count, ticker in enumerate(tickers):         try:             df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))             df.set_index('Date', inplace=True)              df.rename(columns = {'Close':ticker}, inplace=True)             df.drop(['Open', 'High', 'Low', 'Volume'], 1, inplace=True)              if main_df.empty:                 main_df = df             else:                 main_df = main_df.join(df, how='outer')          except:             print('stock_dfs/{}.csv'.format(ticker) + ' not found')           if count % 10 == 0:                 print(count)      print(main_df.head())     main_df.to_csv('sp500_joined_closes.csv') compile_data()",True
@azazeljaxshark69,2017-08-27T22:05:44Z,1,"I'm losing data on the .join() function. compile_data() iterates through the pickle file fine, grabbing 505 ticker values. For some reason, when merging the dataframes (pushing ""df"" into ""main_df"" through ""main_df = main_df.join(df, how='outer')"") I end up with a final dataframe that has ~47 columns.   These aren't even sequential columns. In my case the ticker columns are accurate until ""AKAM"", skips to ""LMT"", and then skips a few more times. Total column count of ~40ish columns instead of the expected 505 columns.   I verified that every other functions seems to work fine. I matched the pickle ticker values to their relevant .csv files in the stock_dfs folder, etc. Can't find anything on stack overflow about column loss in dataframes. Any ideas?",True
@Locke19901,2017-08-16T16:27:18Z,0,"Great video, as always.   What would be considered best practice here?  We have all the individual CSVs - and it's quick to create the combined dataframe.  We then output and save it to a new csv.  Would it be more efficient to just pickle the df (using pandas of course) and save that and not mess with csv?  Or is there a reason we may want the combined CSV instead?  Or is this completely trivial and don't worry about it?",True
@hans6973,2017-07-06T18:02:15Z,1,KeyError: 'Date' how to solve this?,True
@ps9012lcog,2017-06-30T13:38:43Z,1,"HI  have encountered the following error, pls help if you know what happen.  Traceback (most recent call last):   File ""C:\Users\aileen\AppData\Local\Programs\Python\Python36-32\SourceCode\Fmodule4.py"", line 59, in <module>     df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)   File ""C:\Users\aileen\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\generic.py"", line 2161, in drop     new_axis = axis.drop(labels, errors=errors)   File ""C:\Users\aileen\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\indexes\base.py"", line 3624, in drop     labels[mask]) ValueError: labels ['Open' 'High' 'Low' 'Close' 'Volume'] not contained in axis  thanks a lot",True
@Aerozine50,2017-06-25T09:42:01Z,2,I constantly get the date error no matter what I do to try and correct it...,True
@Anthonypython,2017-06-16T07:29:13Z,15,"Note: if you have trouble and get KeyError 'Date' this means there is A Nothing in the file with the word Date or B no data at all was downloaded, look for files that are 1kb. these most likely have no data at all and didn't download.(I'm using yahoo, yahoo has a set limit so this is why) Since yahoo will limit you as this means you grab the data to fast(ironic, eh?) so just delete those 1kb files and keep running till they all download. you may also see a ValueError: cannot reindex from a duplicate axis not sure why this happens, but just keep running it. till it gets everything.  I'm sure a slight delay will fix the fast data grabbing problem.",True
@adamdavis9718,2017-06-16T02:54:02Z,4,"If you downloaded the list recently and were getting a KeyError:'Date', there are two B-class tickers with no data (BRK.B and BF.B) from yahoo finances that will kill your program. Without knowing why there wasn't any data in them, I simply put the labels in the .csv file and it was able to run and didn't skew any of my data. Hope that helps.   put this at the top of your files .csv files in a text editor --> Date,Open,High,Low,Close,Adj Close,Volume",True
@kmillanr,2017-06-14T04:25:11Z,2,"hi, I'm not getting all the tickers in my compiled file. as anyone run into this?",True
@asivolobov,2017-06-05T15:27:55Z,5,"Yahoo changed API so there are some runtime errors in current version of code. To repair:  1. All information is here: https://pypi.python.org/pypi/fix-yahoo-finance  2. Install fix_yahoo_finance using pip: $ pip install fix_yahoo_finance --upgrade --no-cache-dir  3. Import fix_yahoo_finance into your code (add this at top of your file after ""import pandas_datareader.data as web""): import fix_yahoo_finance   4. Change a line with 'yahoo' string to:             df = web.get_data_yahoo(ticker, start, end)",True
@dahwood2522,2017-06-05T00:11:44Z,3,For some reason when I run this code my CSV file only has a column of dates? Is there any fix for this?. Thanks in advance,True
@patelal,2017-06-01T04:41:57Z,10,"Here is the code if you are using the Google stock API since yahoo no longer seems to work.  import bs4 as bs import datetime as dt import os  import pickle import requests  ##import matplotlib.pyplot as plt ##from matplotlib import style ##from matplotlib.finance import candlestick_ohlc ##import matplotlib.dates as mdates import pandas as pd  import pandas_datareader.data as web  ##style.use('ggplot')  ##start = dt.datetime(2000,1,1) ##end = dt.datetime(2016,12,31)  ##df = pd.read_csv('tsla.csv', parse_dates = True, index_col=0)   def save_sp500_tickers():     resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')     soup = bs.BeautifulSoup(resp.text)     table = soup.find('table', {'class':'wikitable sortable'})     tickers = []     for row in table.findAll('tr')[1:]:         ticker = row.findAll('td')[0].text         tickers.append(ticker)     with open(""sp500tickers.pickle"",""wb"") as f:         pickle.dump(tickers, f)      print(tickers)      return tickers  #save_sp500_tickers()   def get_data_goog(reload_sp500=False):          if reload_sp500:         tickers = save_sp500_tickers()     else:         with open(""sp500tickers.pickle"",""rb"") as f:             tickers = pickle.load(f)          if not os.path.exists('stock_dfs'):         os.makedirs('stock_dfs')      start=dt.datetime(2000,1,1)     end=dt.datetime(2016,12,31)      for ticker in tickers:         print(ticker)         if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):             df = web.DataReader(ticker, 'google', start, end)             df.to_csv('stock_dfs/{}.csv'.format(ticker))         else:             print('Already have {}',format(ticker))              #get_data_goog()  def compile_data():     with open(""sp500tickers.pickle"",""rb"") as f:         tickers = pickle.load(f)      main_df = pd.DataFrame()      for count,ticker in enumerate(tickers):         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))         df.set_index('Date', inplace=True)          df.rename(columns = {'Close':ticker}, inplace=True)         df.drop(['Open','High','Low','Volume'], 1, inplace=True)          if main_df.empty:             main_df = df         else:             main_df = main_df.join(df, how='outer')          if count % 10 == 0:             print(count)      print(main_df.head())     main_df.to_csv('sp500_joined_closes.csv')  compile_data()",True
@leeli2318,2017-05-11T23:44:09Z,32,"Little change in the original code: then works perfectly in the for loop:         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker.replace('.', '-')))         df.set_index('Date', inplace=True)",True
@DAcasado,2017-05-07T13:05:57Z,0,"Hi sentdex, thanks for your videos.  I have a question: how would we modify the program so that we get for each stock and date: the adj close and volumes, all together.  I used: ""df.rename(columns={'Adj Close': ticker,'Volume': ticker}, inplace=True)"" which works, but I see output in this format:                          MMM        MMM            ABT                ABT   Date                                                                        2000-01-03  2173400  31.131128  10635000  9.459574      2000-01-04  2713800  29.894130  10734600  9.189300   2000-01-05  3699400  30.760029  11722500  9.172408     2000-01-06  5975800  33.234026  17479500  9.493358      2000-01-07  4101200  33.893758  15755900  9.594710     With this format, as MMM is both volume and price, I wouldnt know how to interact between both.  Any ideas, anyone?  Thanks in advance.",True
@dougp4503,2017-03-15T23:10:34Z,24,"I did a similar thing to what Jan Blake did in the compile_data function. I kept getting an exception because there is no Z.csv. Here is the code I used to get around that. I hope this helps someone because it worked for me:   for count, ticker in enumerate(tickers):   try:    df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))    df.set_index('Date', inplace=True)       df.rename(columns = {'Adj Close': ticker}, inplace=True)    df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)       if main_df.empty:     main_df = df    else:     main_df.join(df, how='outer')        except:    print('stock_dfs/{}.csv'.format(ticker) + ' not found')    if count % 10 == 0:    print(count)",True
@abluntdaily,2017-02-17T03:05:00Z,0,if I rerun the function again will it create duplicate files or continue where it left off? its taking a really long time so I think I might be getting throttled. On my last attempt I was able to get 60 files but then I deleted the folder and it started right up again when I ran the function. I have 256 files now so I don't want to delete the folder but I also don't want to rerun the function if it will create duplicate files. Its been about an hour that I am stuck at 256 files,True
@janbalke5900,2017-02-12T16:33:23Z,3,"I had an error a few videos earlier, where it throws an exception when i tried to get yahoo data for a ticker that contains a point, like BRK.B for example. for getting the data from yahoo i just did the following...             try:                 df = web.DataReader(ticker, 'yahoo', start, end)                 df.to_csv('stock_dfs/{}.csv'.format(ticker))             except:                 print(""DataReader Error!"")  now in this case i just put the code that gets executed in the for loop additionally in an if statement, so ist only executet if the ticker does not contain a point...             if not '.' in ticker:  but still... really great videos, keep up the good work!!",True
@alanwangedcc,2017-02-11T23:40:02Z,0,"In the code main_df = main_df.join(df, how = 'outer' )  ## Is how = 'inner' also ok?",True
@DiptiranjanHarichandan,2017-02-09T21:58:33Z,0,"It's showing this error and I am unable to figure out what exactly it means. Can you please help me out :( -----------------------------------------------------------------------------------------  File ""combining-sp500-p7.py"", line 72, in <module>     compile_data()   File ""combinin-sp500-p7.py"", line 55, in compile_data     df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))     File ""/home/diptiranjan/.local/lib/python3.5/site-packages/pandas/io/parsers.py"", line 646, in parser_f     return _read(filepath_or_buffer, kwds)   File ""/home/diptiranjan/.local/lib/python3.5/site-packages/pandas/io/parsers.py"", line 389, in _read     parser = TextFileReader(filepath_or_buffer, **kwds)   File ""/home/diptiranjan/.local/lib/python3.5/site-packages/pandas/io/parsers.py"", line 730, in __init__     self._make_engine(self.engine)   File ""/home/diptiranjan/.local/lib/python3.5/site-packages/pandas/io/parsers.py"", line 923, in _make_engine     self._engine = CParserWrapper(self.f, **self.options)   File ""/home/diptiranjan/.local/lib/python3.5/site-packages/pandas/io/parsers.py"", line 1390, in __init__     self._reader = _parser.TextReader(src, **kwds)   File ""pandas/parser.pyx"", line 373, in pandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)   File ""pandas/parser.pyx"", line 667, in pandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)    FileNotFoundError: File b'stock_dfs/MMM.csv' does not exist ------------------------------------------------------------------------------------------ But I have stock_dfs/MMM.csv file stored.",True
@danielbuhler7024,2017-02-08T13:38:21Z,0,"Just as a sidenote, whenever you are iterating over something timeconsuming, instead of the counter method you could use tqdm. Gives you a nice progress bar, with estimations for duration. Just use it like this: ""for ticker in tqdm.tqdm(tickers): [...]"" and it works.",True
@pmunin,2017-01-27T22:59:23Z,8,"Is there any way to download other timeframes besides 1day (1 min, 5 min, 15min, 1hr) from Yahoo or other sources? Sorry if i missed in some other videos.",True
@ImGooblie,2017-01-27T06:59:48Z,1,"Hey Sent,   Do you cover any methods for finding outliers in any of your episodes?",True
@andreipoehlmann913,2017-01-26T00:57:02Z,0,Yahoo API requests are limited to 2k/hour per IP via public access. However via OAuth API Key it's 20k/hour. I've been trying to figure how to set up the OAuth in combination with python/pandas but couldn't find any solution. Would really appreciate any help on this like maybe as a side note on your website. (or as a comment here ;) ),True
@ReactsRebirth,2017-01-25T20:17:00Z,0,"Hey Sentdex,  I tried to do this with a specific sector and I'm getting an error. Do you think you can point me to the reason? the AAL file is located inside a folder named airline_sector, so I'm not sure.  def compile_data():     main_df = pd.DataFrame()     for ticker in enumerate(stockToPull):         df = pd.read_csv('airline_sector/{}.txt'.format(ticker))  IOError: File airline_sector/(0, 'AAL').txt does not exist",True
@PCAN411,2017-01-25T15:03:25Z,0,do you have a video that imports custom built functions from other files to a main file?,True
@walkops,2017-01-24T16:00:18Z,2,I keep getting that file stock_dfs/MMM does not exist and it is clearly there.  Any ideas?,True
@julianurrea,2017-01-24T10:01:45Z,1,"The axis refers whether you're using rows or columns to apply the pandas' function. axis 0 means rows, axis 1 is columns. the error it throws up is because you don't have any row with the index ""Close"" for example, but once you have it, the entire row will be dropped if axis = 0",True
@htstffcmnthru,2017-01-24T01:33:09Z,39,Dude. You have the best python videos. Thank you for your hard work.,True
@IonicCascade,2017-01-23T18:34:55Z,0,"since not all the companies started back in 2000, how do you replace all the empty cells cleanly?",True
@skmn07,2017-01-23T17:31:13Z,0,awesome,True
@andreasj3018,2017-01-23T17:19:48Z,11,thanks for the tutorial. i think it would be great if you'd explain how to do the daily refresh for the new data as well... :) at least im very curious for this,True
,2017-01-23T16:37:27Z,0,Nice :P,True
@naveenv3097,2017-01-23T15:53:22Z,0,"Hi, is there a way to stack data on top of a data in one dataframe...cause i have the same stock data in 3 or 4 files split into 2 year period...Thanks :)",True
@ryanshrott9622,2017-01-23T15:28:37Z,0,great tut!,True
