author,updated_at,like_count,text,public
@ThankYouESM,2024-05-08T13:01:05Z,0,"I have proof that this was my approach JUL 31ST, 2023 shared publicly... which I was just tinkering around.",True
@cepi24,2024-03-03T17:40:48Z,0,Which tool do you use to add notes to PDF?,True
@rachelhwung5822,2023-12-01T22:03:41Z,0,"what if we use the same method to classify fraud, or some other binary ground truth? would it work too, has anyone used this on anything not sentiment?",True
@ander300,2023-09-16T17:46:50Z,0,"Part 10 of Neural Net from Scratch, about analytical derivatives??? Please bring the series back!",True
@trombonemunroe,2023-09-12T10:55:55Z,0,"Sorry if I sound a bit pissy, but where's the link to the older K nearest neighbours implementation video you promised to put in the description at 1:50?",True
@onhazrat,2023-08-24T06:58:20Z,0,"üéØ Key Takeaways for quick navigation:  00:00 üìö The speaker came across a paper on low resource text classification using compressors, claiming to beat Bert in sentiment analysis using K nearest neighbors and gzip for feature extraction. 00:27 ‚öôÔ∏è The simplicity and effectiveness of the approach compelled the speaker to test it. They have experience in contracting and consulting for machine learning tasks. 03:10 üéØ The proposal involves using normalized compression distances (NCDs) as feature vectors for sentiment classification. NCDs are calculated by compressing text and comparing compression lengths. 04:17 üìä NCDs are computed for all training samples, creating a feature vector of compressed distances. Each sample is compared against every other sample. 06:07 üìâ Training a K nearest neighbors classifier with NCDs for sentiment classification. Tuning the number of neighbors (K) affects accuracy. 08:43 üìä Achieving around 70% accuracy with this simple approach, far better than random guessing. Acknowledging the surprising effectiveness of compressed distances as features. 09:12 ‚ùì Expressing skepticism about the method due to its simplicity and reliance on compression lengths for sentiment classification. 13:34 ‚ö°Ô∏è Optimization using multiprocessing to speed up NCD calculations for larger datasets, maintaining order of NCD vectors. 15:56 üß™ Testing the method on a larger dataset (10,000 samples) achieves 75.7% accuracy, comparable to the original paper's results. 18:55 üí° Reflecting on the unexpected success of the method, considering its limitations and exploring other aspects like data set choice and dimensionality reduction.  Made with HARPA AI",True
@minerharry,2023-08-18T22:48:20Z,0,"Cool video, would have been nice to see a higher level explanation of what you were going to do before you went into the nitty gritty (an explanation of what NCDs are before coding them, for example, might help explain the motivations behind how you normalize).",True
@tenchudjmusic,2023-08-16T10:01:57Z,0,"Apart from all the other detailed why does gzip work explanations, here's another one. the result of gzip length sums up the frequencies of patterns in a single number, and that's enough to guess.",True
@BerenddeBoer,2023-08-15T08:40:45Z,0,"This must be crazy, and crazy we can't tell why!",True
@julian1000,2023-08-15T05:38:06Z,0,"Since you've gotten plenty of ""haha told ya so""s I'll mention that the current algorithm with the highest compression ratio is nncp v3.1. It uses a custom C library for running a hand-build transformer neural network optimized for the task of compressing Wikipedia data.  I played around tonight trying to get it to work, but the data science working environment is just abysmal.",True
@richardfarmbrough2335,2023-08-14T22:19:07Z,1,"Angry people repeat themselves more, perhaps.",True
@dahahaka,2023-08-14T13:24:04Z,0,I feel like this just shows how much we can still improve on this task,True
@somerandompersonintheinternet,2023-08-13T01:32:47Z,0,"A way to ""dumb down"" what is happening is to think, if two pieces of text have the same word, say, ""disappointed"", that means that the compressed form of those two combined texts will be relatively smaller. So the more words the two pieces of text share, the closer they will be together.  This is essentially a faster way to classify sentiment by just comparing words. (""faster"", as opposed to directly comparing those words)  It will be more accurate than random chance simply because some words are inherently negative. But where Machine Learning models win is when a piece of text has words that could be considered negative, but the context is positive. (""I'm disappointed that this video isn't longer!"")  BTW I don't think it relies on gzip itself at all. Any compression algorithm that is relatively good for text should work fine. Also, the lengths are not as important as you might think, except on weird extremes.",True
@AssasinZorro,2023-08-12T12:45:22Z,0,1:45 you didn't put any link to a video on KNN in the description,True
@leeterthanyou,2023-08-12T08:03:51Z,0,[ incoherent yelling in bzip2 ],True
@greenhowie,2023-08-11T23:06:50Z,0,"Hi, sorry to jump in with a question but have you seen the ""AI Tomb Raider"" videos by Foxmaster? Google says you're the best to explain that kind of stuff and I'd like to know if it really is legit or if it's just an entertaining fabrication.  Sorry if I'm barking up the wrong tree here but the comments there are full of people saying it's fake and clearly isn't a robot - but nobody's debunked it yet, which is unusual considering its popularity.",True
@nekomata_mottsii,2023-08-11T10:34:57Z,0,has anyone counted how often he said sample,True
@Gunth0r,2023-08-11T03:05:11Z,0,Good to see the air quotes are still a thing after all these years. A remnant of your humble beginnings. A piece of nostalgia if you will.,True
@JohanWierenga,2023-08-10T23:20:31Z,0,Reminds me of this paper: https://homepages.cwi.nl/~paulv/papers/similarity.pdf,True
@thedragonduelchampionofycs9020,2023-08-10T22:21:11Z,0,"Hate to bother, are you still responding to comments on the nnfs google doc? And if not, may I approach you with a question here?",True
@ishimaru123,2023-08-10T14:46:00Z,0,"Every youtuber ever: ""link in description"" Also every youtuber ever: *forgets to link*",True
@friedrichdergroe9664,2023-08-10T14:05:38Z,0,"If string x is ""my dear aunt sally"", and string y is ""sally aunt dear my"", would the compression of the two strings reveal the similarity as is obvious from the uncompressed strings? It's been a long time  since I looked at the zip algorithm, but I'd be surprised if you could gain further compression from the concatenated compress strings. I could be wrong, here.",True
@wernersmidt3298,2023-08-10T11:16:45Z,0,"You can use the line ""even if it predicts it right, it does so for the wrong reason"" on the vast majority of deep learning models",True
@fluffycritter,2023-08-09T23:50:55Z,41,"Back when Bayesian classification was first getting really popular for spam filtering (in the late 90s or thereabout), I remember people talking about using a technique similar to this for spam classification: keep a corpus of spam messages and a corpus of ham messages, and then append the test message to both corpuses and see which one compresses better. It was just a fun little curiosity then, and wasn't really feasible for running an email server at scale, but it's fun to see that the idea's still got merit.",True
@dubfather521,2023-08-09T21:12:59Z,0,What? How are you representing words as numbers with length? What if two worda have the same length? Then how would network know the difference?,True
@gameprogramme,2023-08-09T20:11:19Z,0,"I‚Äôm curious about the bias of the original dataset?  Or if there is a correlation between the normalized length of the UNCOMPRESSED sample data and the sentiment?  This may be totally off base, but sounds like a possible explanation to your findings. (My primary reason for suspecting biased sample data is the tendency for unhappy customers to complain compared to happy customers usually staying silent.) Perhaps if you broke out incorrect negative predictions from incorrect positive predictions this may be more obvious?  Thanks for the video! Stay curious!",True
@sebastianp4023,2023-08-09T18:59:12Z,0,i did something similarish for a course/project in university where i compressed different time series with an autoencoder and then clustered it with k-means. worked quiet well (obviously!?...).,True
@Halopend,2023-08-09T15:41:03Z,0,"Wait‚Ä¶.. is the compression applied to a contencated form of each test/train sample (which literally just picks up on similar words and results in smaller overall compressed files)?  Or is the compression of each test/training sample handled completely separately?  In one case, the compression acts a cheap tfidf esque metric with string length as the value‚Ä¶.. in the other case you kind of have to assume that positive or negative statements compress differently which feels like a fluke.  Another thing worth asking is if negative sentiment tends to be longer (before compression) and if that‚Äôs all that‚Äôs really being picked up. I also suspect when people are angry, they tend to use more complex language which doesn‚Äôt compress as easy since they are attempting to ‚Äúwin‚Äù and argument or be more convincing to appear justified.  It would be interesting to see what your results would be to literally just work on string length and to get rid of the compression all together and see if just that gives a weighting.",True
@Tapuzi,2023-08-09T03:00:53Z,0,"Innovative and intriguing.  I would maybe postulate that it works because of some interesting (and surprising)  ""emergent properties"" that occur in large bodies of text. ""Benfort's law"" or ""Zipf's law"" are examples that can maybe give basis for this intuition. In short - some sequences appear more than others, but the relative ratio (distance) across terms is predictable. So it may be a cause for this to work that there are predictable patterns implying likelyhood of one sentiment over another.  Of course its just an assumption.",True
@twistedbydsign99,2023-08-09T00:07:37Z,0,Did the paper cherry pick the dataset? Thats pretty common. Maybe the answer to your question of is it happenstance or accidental is in the sentences is classifies worst. Take a look at its biggest mistakes.,True
@piyushsingh178,2023-08-08T21:07:21Z,0,Your KNN code is very strange. Typically the fit and predict functions take as argument the data matrix not the distance matrix. This means you are essentially using distances from each training point as features instead of directly voting for the K nearest neighbors where the distances are measured using ncd,True
@dc443,2023-08-08T20:16:34Z,0,"Oh yeah this totally makes sense why it works well once I read the comments that explained that one of the values being used is the compressed length of the concatenation of a pair of samples... This gives a strong signal of how similar the two samples are to each other. Also I had an idea which is, what if instead of making this comparison with each other sample, how accurate would it be if we had two strings, one with all the positive sentiment training input, and one with all the negative sentiment training input, and then to ""inference"" a new sample, simply: (1) run gzip on the concatenation of the big positive string with the sample, and (2) run gzip on the concatenation of the negative string with the sample, and just choose whichever sentiment version compresses out to have a smaller delta. The one that compressed in with a higher ratio wins. Tempted to try it I bet someone else here could knock that out a lot quicker than me.",True
@kossboss,2023-08-08T19:50:55Z,0,Someone please explain what sentiment means here?,True
@Llirik13,2023-08-08T16:08:50Z,0,"I not sure if gzip compress data in chunks, if so, you need for this to tell it to analyze whole text as one piece.",True
@jasongoldberger1,2023-08-08T02:56:01Z,1,joe armstrong predicted this.,True
@yugiohsc,2023-08-08T01:18:30Z,0,"Fascinating! Functionally, ML and compression are kinda two sides of the same coin, so this is a fun demo",True
@FransHenskens,2023-08-08T00:52:06Z,0,"I'd guess this works because people tend to use similar complexities in the word sets to describe a certain sentiment, leading to islands of highly similar compression distance for reviews of similar length for either sentiment?",True
@berndeckenfels,2023-08-08T00:21:08Z,0,Did you try just count duplicate words or substrings in the sample-pairs? That‚Äôs what basically means when compression of the concatenated strings is better.,True
@turun_ambartanen,2023-08-07T21:26:18Z,1,"What a funny technique! Now we need to compare gzip, zstd, lzma, etc in terms of their sentiment analysis potential, haha! I know that zstd also supports precomputed dictionaries, so instead of KNN you could create a positive sentiment dictionary and a negative sentiment dictionary from the sorted and concatenated training data and then for inference you compare the compression performance using the two dictionaries separately. Whichever dictionary compresses better is the sentiment that is predicted for that sample!  EDIT: just tried it, the zstd dict didn't work :(",True
@ifatreefalse,2023-08-07T19:52:52Z,0,"unhappy people type long ass paragraphs back and forth in fights. and they would be quoting each other and using similar language to refute each other. possibly using distinct words repeatedly that probably aren't used in other happier samples from the same source. did they use reddit as a data source? edit: don't normalize for starting length or try to force ""realism"". the takeaway is that the length of response being further away from 'norm', for that source, indicates sentiment as a function of how humans produce text. if you want to pre-filter the false negatives from short positive responses, look at the age of the account of the reviewer and the number of reviews left of a certain quality metric (which might include length).",True
@screwsnat5041,2023-08-07T13:50:33Z,0,Tbf looking at ML and LLM and how people are still mystified about how they work I think there‚Äôs a method to the madness if you have enough data you can plot correlations of different kind even where one does not exist which can be used to some extent until you realise it cannot be generalised past some extent just my opinion tho . I‚Äôm currently working on a new way that does alot with small amount of data and doesn‚Äôt strive for correlations rather strives for contextual correlations. Meaning is dependent on context and a word meaning can be vastly different from its true meaning in a giving context if we could reliable predict the meaning of a word reliably and plot a local correlation we could make a better LLM with  less data,True
@CZghost,2023-08-07T12:48:45Z,0,"Gzip has been around for ages. It's been quite literally used to cramp up as much data on tapes as possible to minimize the needed space for archiving purposes, especially when dealing with very large number of files that are in a text format, or when dealing with such files like logs and configurations archival. Tar is a powerful tool to combine multiple files together, as it literally stands for ""tape archive"", it's been largely used to store somewhat large amount of files on tapes in the mainframes in the Unix days, however if you are dealing with an absurdly large number of files, mostly in text format, and you want to have as much documents as possible on the same tape (which wasn't frankly infinitely long), then some compression was needed. Hence why Gzip became the norm. Since Tar takes care of the archival purpose on tapes, Gzip was only assigned the role of compression algorithm once the tape archive is actually created. If you then feed the compressed tape archive onto the actual tape, you can store much more files in it. Pretty neat, huh? The separation of compression from archival also gave possibility to compress singular files directly, which is great for long system logs for example. This can be automated. At the end of the day, at midnight, current latest system log is fed into Gzip compression and archived. The log could very well be also archived weekly or monthly, or one archive could contain a whole year. It really depends much on the amount of data that's being put through it. Because Gzip is more effective for very large files. That means like thousands of lines in log files for example.",True
@Horesmi,2023-08-07T11:34:40Z,0,Now classify cats and dogs with normalized jpeg size,True
@thatstupiddoll,2023-08-07T00:32:22Z,0,I want to see this with something like zstd compression dictionaries,True
@Acceleratedpayloads,2023-08-06T23:46:06Z,0,"This is just clustering by lexical distance with extra steps.  If you cluster by lexical distance on embeddings, you can likely improve performance. I suspect in the dictionary used by the compression algorithm is some latent clustering of semantics",True
@VitiatedMovies,2023-08-06T23:02:58Z,0,"Compression algorithms have been used in the past to identify authors of text, it's not surprising that it works somewhat for sentiment analysis. There were some papers 10 years ago or so iirc",True
@cannonball7,2023-08-06T22:58:57Z,0,"Signals don't work right in machine learning systems. This is like a fat fingered fix on turning sentences into their reduced signal complexities, and is really cool, but that's why this doesn't make sense. This significantly reduces encoder concept complexity in the nn, and gets it much closer to the language-less, signal-based, and reduced representation of the working, trainable, signal.",True
@JYK-F1211,2023-08-06T22:28:46Z,0,I don understand anything could some one explain to me as if I‚Äôm 5 cuz this just goes over my head butt I‚Äôd love to understand it,True
@coced,2023-08-06T20:00:01Z,0,"Does this ""work"" with data in other spoken languages ? I might just be a quirk of the English language",True
@Feel_theagi,2023-08-06T19:44:49Z,0,"The features are interesting but a transformer based LLM far outperforms this approach in sentiment analysis as it learns the semantic structure from the attention heads, the difference in sentiment can come down to a single word.",True
@DustinRodriguez1_0,2023-08-06T17:55:59Z,0,What confuses me about this is the underlying implications... apparently there is a substantial difference in information content (entropy) of positive and negative sentiment text? If that's the case then there are likely more efficient ways to calculate the entropy than doing the compression and looking at the length of it. I would think it very odd if that were a real feature of language... and it would likely be a significant finding with extremely wide-ranging implications. Most unusual would be how such a thing was missed for so long? I might need to do some analysis just to convince myself that this isn't true.,True
@KilgoreTroutAsf,2023-08-06T16:54:33Z,0,All ML is a form of lossy compression,True
@donno2048,2023-08-06T11:14:38Z,0,"Wouldn't double negation break it easily, as the zipping of multiple ""not"" and ""no"", etc will be vary close to the zipping of one of them..?",True
@PaulOvery001,2023-08-06T10:06:40Z,0,"Now I'm wondering, swap for encryption - should be 50/50 ?1?  Scary if not",True
@anwiseru9064,2023-08-06T07:31:10Z,0,youre a legend for exposing the nsa,True
@bitshuffler,2023-08-06T03:43:47Z,0,Is it definitely the case that there's no bias linking sample length with a positive or negative sentiment? ie people who disliked the movie ranted and left longer comments?,True
@RoboticusMusic,2023-08-06T03:23:04Z,1,Not sure why we're watching you type code.,True
@roelbrook7559,2023-08-06T01:16:57Z,0,"Just an idea, but might gzip compression just be ""too good""? Maybe a simpler compression algorithm like LZ77 would help?",True
@EionRobb,2023-08-05T19:02:39Z,2,What level of gzip compression (1-9) are you vs the original paper using? Does increasing the compression level give better results than just the default?,True
@reed6514,2023-08-05T15:39:41Z,0,One of these days i need to learn some ML. This is all so over my head.,True
@meepk633,2023-08-05T12:25:05Z,0,What happens if you compare this to an algorithm that scores a simple with a dictionary of typically negative words and a dictionary of typically positive words?,True
@peterroschke3394,2023-08-05T11:52:59Z,0,"Thanks for showcasing this and the discussion. Great stuff.  Two thoughts on how to improve it.  Use Z=standard compression instead (much more modern, tighter compression that runs about 10-100x faster than gzip).  Consider converting all text to upper case before comparing as this reduces a (small bit) of extraneous variation.  Would be interesting if these two small changes improve the result a bit.",True
@llamasaylol,2023-08-05T11:45:43Z,0,"To work better with strings of different lengths, would it not be better to create a different model for different length ranges? E.g. If the string is 200 to 300 chars, use a model trained on strings of those lengths only. To improve this you may want some overlap in buckets, e.g also have a bucket for 250 to 350 and see which midpoint the string is closest to.",True
@stevel6422,2023-08-05T10:39:48Z,1,isn‚Äôt frequency of byte repetition a hallmark of gzip compression? it seems mind boggling that sentiment is somehow transmitted by the frequency of alphabetic character occurrences.   Wouldn‚Äôt that be the same as analyzing the occurrences of repeated sequences in DNA and determining that a person with a certain set of protein sequences is likely a positive or negative person?,True
@gingeral253,2023-08-05T09:01:31Z,0,Interesting,True
@bart2019,2023-08-05T03:58:03Z,0,The length of the compressed string correlates to the length of the string.,True
@XetXetable,2023-08-05T01:13:10Z,0,"I think the way this is working is essentially using gzip to approximate the conditional shannon information. Essentially, if we have two strings we want to compress, x and y, then they may share some structure (such as shared words) that would allow one to compress both more together than apart. That is I(x ++ y) < I(x) + I(y), typically. We can also ask what advantage rules learned while compressing one string gives when compressing another. This would be I(x|y), the shannon information of y given what we've learned from x. If we already learned a word in x, we can reuse the map for that rule in y and so don't need to store it in the compressed representation of y. Based on this, if we observe that, typically, I(M|A) < I(H|A), for a bunch of mad messages, M, and happy messages, H, then we can infer that A is statistically more like the mad messages than the happy ones, and I think that's essentially what this is doing.  Intuitively, my expectation is that one could do better by compressing a message relative to an entire dataset, that is calculate I(MD|A) where MD is the concatenation of all the mad messages, etc. In that case, MD is acting more like a theory for compressing similar messages, and we'd expect MD to do better than HD for new mad messages, etc. But that's less structured than the disintigrated comparisons, so maybe not. Perhapse there's an advantage to keeping things separate, but I wonder what that's teasing out, exactly.",True
@lampforthepoor,2023-08-04T23:07:35Z,0,"Is the claim simpy that gzip is computing something like https://en.wikipedia.org/wiki/Levenshtein_distance ? Because that seems highly plausible. But anything more than that is suspect at best. Put more concretely say we have examples ""good | bad"" and ""good | great"". The second example maybe compress better because there's a repeated 'g' but not because gzip knows anything at all about those concepts or that 'good' and 'great' are more similar in meaning than 'good' 'bad'. Right? In this view it's really nothing special. Am I'm missing something?",True
@Saturn2888,2023-08-04T21:14:20Z,0,I didn't understand any of this. How does this have 80K+ views? What am I missing?,True
@chicolofi,2023-08-04T16:55:51Z,0,It isn't. All you need is love.,True
@frecio231,2023-08-04T16:15:35Z,0,"All you need is Gzip (tu turururu) All you need is Gzip (tu turururu) All you need is Gzip, Gzip  Gzip is all you need",True
@SquidOnWeed,2023-08-04T12:46:14Z,0,I actually wouldn't be surprised if it turned out you can distinguish negative from positive sentiment text taken from the internet with >50% accuracy based on how compressible an example you're trying to classify is on its own. Most angry comments on the internet have near zero informational content.,True
@timseguine2,2023-08-04T11:51:06Z,3,"I have been doing research for quite a long time on similar methods, so here is some additional insight that is hopefully helpful in some manner:  The normalized compression distance is closely related to the kolmogorov complexity. Given a perfect (therefore non computable) compression algorithm, it would describe precisely how much additional information it would take to produce one string given the other one as input. So it is a natural measure of similarity by weakening it to a real-world compressor. And how good of a measure of similarity it is depends on how good the compression algorithm is up to the non-obtainable asymptotic result.  The rest of the method is just basically that you can turn any distance metric into a vector embedding by constructing a matrix of pairwise distances. And that these embeddings generally perform well on downstream tasks if the distance measure is a good one. If I remember correctly you can extend the whole thing beyond KNN in a fairly natural way by using the SVD of the training matrix to whiten the calculated feature vectors during inference.",True
@Gunbudder,2023-08-04T08:57:28Z,0,YES! K nearest neighbor is SO GOOD. it makes me so insane to see some of these insane solutions like radial basis function networks or massive neural networks when k nearest neighbor smokes them all,True
@Jkauppa,2023-08-04T08:09:19Z,0,"why not just sort the source file, kinda compress (radix sort)",True
@cortexauth4094,2023-08-04T07:43:22Z,0,"> This should not work Cybernetics was a thing, and literally NNs also do compression in a way tbfh",True
@stevenhe3462,2023-08-04T07:16:26Z,1,Joe Armstrong tried to use Gzip to reduce the number of code files he had before he's gone‚Ä¶,True
@theawesomeharris,2023-08-04T07:15:24Z,0,seems like besides the transformer solution there exists other first principle solutions to sentiment analysis!,True
@luh034,2023-08-04T06:45:26Z,0,wtf this is wild,True
@iOverThoughtThis,2023-08-04T06:20:36Z,0,"yes, negative comments do tend to be longer than positive comments",True
@israelfigueroa7858,2023-08-04T04:42:51Z,5,"I'm counting on that you already learned that compression length gives you a measure of how much information there is in the text.  Basically is a measure of entropy, because when you type words you use ascii encoding to codify the letters. Now you can do a smaller space if you will take so many tokens. But basically compression will say how many is the least to encode the text.  I was thinking how you might improve the distance between two strings. As you join it and make it proportion. I would recommend using xoring the strings between themselves (if they match it will cancer the numbers to zero) making similarish text. You can say is normalized to zero if the both text are the same. And the top take the divide it by the bigger length (as it likely that it had unmatched characters on the other).  Now you might began to to go to the philosophical side of the prediction is regarding how much information does a text have and match it with a sentiment.",True
@supercompooper,2023-08-04T04:42:22Z,0,Maybe try Burrows‚ÄìWheeler transforms and do this on Lyndon words?,True
@kuroexmachina,2023-08-04T03:45:05Z,0,uwaterloo represent,True
@pedromoya9127,2023-08-04T03:22:32Z,0,"thank you @sentdex! and all the commentators! I learned so much. IMHO in sentiment analysis and almost all this kind of problems, we need to choose a smart way to make representation of the dataset, in this paper gzip at first is a apparently strange selection, but looking closer yes! make entirely sense!, the reasons are excellently argued in previous comments (and also in very correct POVs, helping to understand). After that, one can continue with a correctly choosed cost function, which is minimized with a selected optimization method.... But the paper also shows and warn us don't forget that simply using directly a k-neighbors algorithm (that not train, but compare distances applying a majority voting-based decision) can also work.",True
@cilibrar,2023-08-04T00:57:20Z,0,"I was one of the cited scientists from decades ago who helped coin the term NCD. You can find many more examples of text clustering and classification in the paper ""Clustering by Compression"" and the thesis ""Statistical Inference Through Data Compression.""  gzip has a 32K window so it only really works well with texts that are less than 16K in size. Otherwise you are mostly ignoring some of the input data and just comparing the head of one file with the tail of another. Another small difference in the way this presenter explains the feature vectors they are high dimensional. In the examples I did I would instead choose 4-10 ""anchors"" and use those only not all texts/strings for a 4D-10D feature extraction. And I more often used SVM instead of kNN.",True
@jingwangphysics,2023-08-03T22:26:10Z,0,Training data becomes the model itself. I see scalability will be an issue to bring back training data every inference.,True
@TheD3adlysin,2023-08-03T21:40:34Z,0,Im pretty sure the problem is it doesnt havw a LK99 superconductor.,True
@tshock22,2023-08-03T19:54:50Z,0,Your PCA/TSNE graphs at the end had too many points to do much visual deduction.   I ran 2D PCA on the 500 item train set and the scatterplot did show some observable patterns to explain around 55-60 percent accuracy.   I actually ran K nearest on the PCA reduced 2D dataset and got that range of accuracy depending on n_neighbor settings.,True
@itsme7570,2023-08-03T15:23:43Z,0,This is Ed Snowden's cousin,True
@popularmisconception1,2023-08-03T14:43:53Z,22,"The thing is the vector of features is not comparing compressed length of dataset entries to each other. It is comparing how their separate compressed lengths differ from compressed length of their concatenation. Given what gzip does with the strings in terms of information theory, appending a string containing similar information to the original string will result in more effective compression of the second string making use of dictionary already created from the first string. So similar strings will have smaller NCDs. It is a vector of how un-similar that string is from all other strings, i.e. how much new all the other strings have to say if you say them after the original string. Similar texts will naturally and understandably have small distance from each other and form clusters, that's why it works. Semantics is secondary, gzip is agnostic to data interpretation domain. A better, domain attuned compression algo will produce better clusters and discover new ones. But it is a nice dissection and factorization of what these algorithms really do without having to rely on any kind of NN/AI stuff, plain statistics and theory of information. Nice.",True
@popularmisconception1,2023-08-03T14:12:22Z,0,I wonder what Kanye's neighbors think about this.,True
@user-vq8mn8iy4u,2023-08-03T13:08:09Z,0,Heeeeeeey. Neural Networks part 10. Need this ASAP,True
@anykeyh,2023-08-03T10:05:56Z,0,"It works but a trigram histogram differencial distance would work much faster and better, with not much more code involved.",True
@dorbie,2023-08-03T09:33:19Z,0,I wonder if it could be improved by prefixing every compression with a standard set of dictionary strings indicative of sentiment.,True
@robvdm,2023-08-03T07:36:53Z,87,"It‚Äôs long been theorized that text compression is very closely related to AI, so it isn‚Äôt totally crazy that this works. The Hutter Prize is a text compression competition with a half million Euro prize pool whose purpose is to advance AI.",True
@overloader7900,2023-08-03T06:43:03Z,0,"isnt it basically just a dictionary comparison? Concatenting two strings and compressing, if they use similar words and expressions, the dictionaries of both strings will be similar, decreasing compressed size significantly (as the dictionaries merge). And with different expressions and words, the dictionaries of the two strings would be (more) different, resulting in higher length.",True
@brandoncheng4127,2023-08-03T05:58:52Z,0,"i feel like the sentiment classification doesn't come from the actual distance measuring, but from how the compression technique uses statistics, which might include some form of sentiment analysis, to create the compressions, and the k nearest only extracts this analysis from the compression algorithm",True
@spudd86,2023-08-03T02:07:19Z,0,I wonder if better compression algorithms would perform better? Like LZMA. Though with short strings I'm not sure if there's going to be much difference. Or would worse compression do better? Like the ones optimized more for speed than compression like Snappy.,True
@gregorymorse8423,2023-08-03T01:44:03Z,0,Creating dictionaries of the individual gzip lengths which is linear to minimize the quadratic step seems better than only hammering with multiprocessing which will still be necessary for joined string.  So still pretty inefficient to not reduce the reducible part of the quadratic operation.,True
@Lantalia,2023-08-02T23:09:24Z,0,There is an argument that gzip _does_ tokenize,True
@whoisabishag3433,2023-08-02T22:59:34Z,0,[ 00:00:30 ] ... Telling Clients: They don't need deep learning,True
@kitastro,2023-08-02T22:08:06Z,0,use a rolling window to determine an aggregate deviation,True
@Michael190054,2023-08-02T21:34:57Z,0,Oh s*** it's Snowden,True
@alkeryn1700,2023-08-02T21:28:21Z,0,i feel like any problem a DNN can solve could be solved more efficiently without one but it is more work on the engineering side and it's easily easier to put the work on the machine.,True
@commandcracker42,2023-08-02T21:21:45Z,0,zpaq better,True
@ck-dl4to,2023-08-02T18:45:36Z,0,"Well, NN is just another compression method. You know LSTM",True
@sandraviknander7898,2023-08-02T18:44:02Z,0,Would also be cool to see if this work on other sequence data like proteins :),True
@hxllside,2023-08-02T18:08:01Z,0,"Once thing to keep in mind is that sentiment analysis is easy. If you take a few common words and feed bits into a shallow NN that are 1 if the sentence contains the given word, you can get ~85% accuracy.",True
@planktonfun1,2023-08-02T17:41:25Z,0,huh sentdex is still alive!,True
@shadowpenguin3482,2023-08-02T14:44:19Z,0,"The way I understood this to work is that negative sentiments use more fruitful language and might have spelling mistakes, wich both lead to a higher compressed length",True
@SHEEP0972,2023-08-02T10:58:03Z,0,"This video reminds me of another application. After training several models with random initializations (or not knowing when to stop at a certain iteration), compress them using gzip -9, and then observe the file sizes. The smaller the file size, the better the performance on the test set or any unseen data.",True
@aakashsingh8120,2023-08-02T09:46:10Z,1,Yeah! Computer science bitch!,True
@AThagoras,2023-08-02T00:29:20Z,11,"These features do make sense because if there is some similarity/correlation between two strings, gzip should produce a shorter result from the concatenation of the two strings due to redundancy in the information.",True
@Achrononmaster,2023-08-01T16:39:46Z,0,"Yeah man, this is a case of _with a screwdriver I can knock in some nails into some wood._",True
@justindressler5992,2023-08-01T16:10:40Z,0,"The thing that has me stumped is the use of length, any dataset with the same number of tokens or keys will have the same length. It feels like this method identifies length as a factor of sentiment. Is there a correlation between positive review being longer than negative reviews or vise versa. The dataset could have a big impact on these results I think.",True
,2023-08-01T15:21:09Z,2,"Well there is also a bias in the dataset which could artificially inflate the accuracy. Indeed if I use text length instead of len of gzip, I get 60% accuracy (so 10% more than what I would expect)",True
@benjaminthomasson,2023-08-01T15:19:30Z,1,Understanding is compression of information.,True
@justinwhite2725,2023-08-01T14:27:06Z,0,Ms coffee bean channel talks about another paper 'mixing is all you need'. It doesn't matter how you mix (attention is one way) but it seems it all works. She read something that talked about using Fourier transforms instead of attention.,True
@revimfadli4666,2023-08-01T14:11:51Z,0,"Why shouldn't it work? Compression basically preserves the most important information while mapping to a smaller space, so it's unsurprising that KNN in that space is gonna be powerful lol",True
@DaTruAndi,2023-08-01T13:46:33Z,0,Is the dataset mixed or ordered? Is the gzip running for each string separately or for the stream? So that there are no artifacts of gzip itself starting with one sentiment,True
@germimonte,2023-08-01T13:11:36Z,0,"Maybe I'm dumb, but to me it makes sense, if a sample says ""this is awesome"" and the test says it too, when you concat and compress every additional ""this is awesome"" will add very little length to the compressed output",True
@nevokrien95,2023-08-01T12:00:59Z,0,Was this peer reviewed?   Can u give me ur critisisem about th3 implementation in writing so I can quote u?,True
@felipedidio4698,2023-08-01T11:59:10Z,0,"Hold on, I might've got it wrong. Does this method assume that one sentiment requires more information to represent than another?",True
@voiceoftreason9212,2023-08-01T11:29:58Z,246,"The way the compression distance is constructed makes it an approximate measure of the mutual information between strings. That's why it works. Similar strings will yield a smaller NCD. So for example, strings containing similar words will compress together into shorter gzip files (relative to the lengths of the compressed files for the separate strings). But gzip is very general; unlike BERT it's not tuned to extract language features per se. In many ways, LLMs (and all generative data models) act like data compressors, but tuned to a very specific set of data (in BERT's case English text). The hidden layers of a deep NN are essentially compressing data and yielding similar outputs for similar inputs (i.e. mapping the compressed data). They are doing something really similar to a general compressor hooked up to a KNN model, but more tuned to a particular data set. If, instead of using gzip, you used a more specialised English language text compressor, you'd get better results and possibly not too far off BERT's performance (although LLMs are capable of much more than sentiment analysis).",True
@jaakjpn,2023-08-01T10:41:05Z,1,"Feels like gzipping is doing what people did 10 years ago: bag of words as features and using Jaccard distance of the bag of words. So it is not surprising that you are getting performance over 50% because bag of words is known to work reasonably well. The reason why gzipping is related to bag of words is that gzip can compress two sentences better (i.e., more) if they share same words.",True
@staviq,2023-08-01T07:18:43Z,28,"You know, I've been using gzip since while I was at uni I had this crazy idea of using compression to calculate entropy and detect encryption, and I thought that was clever  But this, is just plain insane :)  I love it :)",True
@CharlesVanNoland,2023-07-31T21:43:39Z,0,This is seriously just using the size of Gzip's output to identify text?,True
@CharlesVanNoland,2023-07-31T21:36:26Z,0,#IDontWantToTalkAboutMyLogicRightNow!!!!!,True
@henpark,2023-07-31T18:53:24Z,0,"Can you perhaps do a tutorial of non-huggingface NLP tools? They still are developed and I would like to know what pros and cons these libraries have over huggingface. NLTK, Spacy, Gensim are the libraries I am specifically referring to.",True
@toddnedd2138,2023-07-31T17:22:25Z,0,"Try another language, especially one with not roman characters. ;)",True
@mikechung7316,2023-07-31T16:50:35Z,0,You should try this with other than binary classification it might suffer?,True
@wojpaw5362,2023-07-31T16:10:32Z,0,This is the sort of content I come here for.,True
@bertobertoberto242,2023-07-31T13:40:14Z,0,"I'm pretty sure that it would fail in cases with small difference in text but opposite meaning, like ""i think this film is great"" and ""I think this film is not great"", because the ncd compression length difference of those two, given any mere statistical approach with no notion of what is a word, would consider them very close.. same for any other phrase, adding a negation in the middle, where BERT i'm pretty sure can capture this easily.... IMHO this just shows that we are just repeating machines and we all use the same phrases to descrive the same things, with similar phrasing and structure",True
@MrBoubource,2023-07-31T12:15:45Z,0,"Isn't the ncd matrix symmetrical ? Like ncd(str1, str2) = ncd(str2, str1), almost maybe just like ncd(str1, str1) = 0.2, but you could half your compute time using this.",True
@wootcrisp,2023-07-31T02:38:06Z,0,Would it make a difference to train a classifier to predict three words at a time instead of one? This would be to remove overlapping words from sentiment.,True
@brandonbahret5632,2023-07-30T21:25:34Z,2,"This is only tangentially related: When GPT-3.5 was released to the public, I came across an article critiquing OpenAI, which argued that computational intelligence boils down to compression. According to the article, achieving the perfect compression algorithm would essentially mean achieving strong AI. This is because to compress all of our knowledge effectively, the algorithm would need to be capable of decompressing it from the very fundamental principles and beyond.",True
@vicaya,2023-07-30T21:09:21Z,0,"The intuition is straightforward: most compression algos are based on some kind of dictionary of tokens (e.g., LZ77 family), similar text will share more of the dictionary, which is a kind of sparse form of dot product for comparing vectors... I bet some much faster algos (like lz4 at memcpy speed, without the entropy (e.g., huffman) coding) might work equally well.",True
@hanskraut2018,2023-07-30T18:04:54Z,0,"compression traditional and by model that lerns to do it (just like humans make momonics, concepts and rules of the real world) is strong and should help limited working memory/memory systems by the way on a meta level. GPT4 does not even seem to have any of those just a certain lenght of text",True
@pushkarparanjpe,2023-07-30T16:06:02Z,1,How does gzip compare to a baseline using L2 distance on TfIdf vectors ? Just curious in case this was tried.,True
@nathenmiranda2822,2023-07-30T15:16:46Z,0,Run this sentiment analysis on the transcript for this video. You absolutely hate and love it at the same time. Should throw it for a loop.,True
@luke2642,2023-07-30T11:51:22Z,0,"Great video, but at 7:10 you should have said compressions of every concatted pair of texts, then your explanation would have been much clearer! Then at 7:12 ""similar sentimented pairs"" etc.",True
@user-wr1bs3oo2z,2023-07-30T07:50:51Z,51,"üéØ Key Takeaways for quick navigation:  00:00 üß© A low resource text classification method using K nearest neighbors and gzip compression for sentiment analysis. 01:48 üìä The proposal involves compressing text and using normalized compression distances (NCD) as features for the K nearest neighbors classifier. 04:17 ‚è±Ô∏è The slowest part of the algorithm is computing the NCDs for all training samples, while the K nearest neighbors classification is fast. 06:07 üèÖ Achieved around 70% accuracy in sentiment analysis with just 500 samples using K nearest neighbors and NCDs, outperforming random classification. 09:12 ‚ùì Questions remain about the method's validity, potential problems, and why NCDs alone would be sufficient for sentiment classification. 10:36 ‚ùì The creator expresses skepticism about the method, questioning the validity of comparing lengths of compressions for sentiment analysis. 11:00 üìà Accuracy varies significantly depending on the sample size, reaching around 75.7% for 10,000 samples. 13:20 ‚è±Ô∏è Linear NCD calculation for 10,000 samples takes hours, prompting the need for parallelization using multiprocessing. 15:00 üíª Practical usage involves compressing input strings, calculating NCD vectors against training samples, and using K nearest neighbors for sentiment classification. 17:46 üß† The method's success challenges the dominance of deep learning, reminding us of the value of revisiting first principles and exploring alternative algorithms like K nearest neighbors for NLP tasks.  Made with HARPA AI",True
@Nandarion,2023-07-30T02:20:26Z,0,"word2vec will work better, than compression here. Compression here just to calc a distance between samples, and gzip is not best choice.",True
@ThinklikeTesla,2023-07-29T21:26:11Z,0,I wonder how this would work for general graph distance...,True
@Kram1032,2023-07-29T21:12:45Z,1,"I wonder whether it'd be doable to, like, take GZIP compressed text (or even other data structures) and train a regular transformer on that, using it to *predict GZIPed data that will yield something reasonable when uncompressed,* so you can use way more effective context length as text compresses rather well...",True
@PhilipSportel,2023-07-29T18:52:37Z,0,"Fun fact: if you say gzip out loud, it sounds like Polish for mushroom.",True
@meguellatiyounes8659,2023-07-29T17:20:21Z,0,"Why the two stings are joined with a space character "" "" rather  than  empty string  """" ?",True
@meguellatiyounes8659,2023-07-29T16:48:10Z,0,Thanks for gzipping the paper's concept.,True
@ataadevs,2023-07-29T16:36:34Z,0,"This video got me back 2 years ago, when I start leaning from your videos about those beatiful algorithms, you're the best. Please more of this!",True
@Meskalin_,2023-07-29T15:25:28Z,0,"love your vids man. have been for so long, but rn they are even relevant on a broader basis bc of all the AI hype.  would love to see something like the GTA 5 bot again, maybe a narrowed down version for a 2d game...? say, DDNet (single maps)? but do not mention the games name, as its quite small and it might ruin the game for normal players if you publish the source/methodology.",True
@sapienspace8814,2023-07-29T15:11:55Z,0,"The letter to ""pause AI"" had a chart of different methods, and one of the more efficient methods was using ""Fuzzy"".  Fuzzy is a rough form of statistical compression.  The focus of ""attention"" may also be achieved by K-means clustering.  What I am most interested in is how, or if, Reinforcement Learning (RL) is being used in these systems.",True
@potatoonastick2239,2023-07-29T15:01:46Z,0,"Lmao great video brother, never seen any of your other videos (yet) but I absolutely love the pace of explaining everything in this video. You seem to be a combination of very knowledgeable and very well-spoken, making for a great teacher (or edu youtuber)",True
@coAdjointTom,2023-07-29T14:18:43Z,0,Don't have time to watch the vid but friends of mine could not replicate,True
@NepYope,2023-07-29T13:58:02Z,0,you could normalize each dimension of the feature by sample length,True
@Veptis,2023-07-29T13:52:40Z,0,I recently heard about Poincar√© embeddings which use a hyperbolic distance measurement to forced hierarchical structures into embeddings spaces. And it made me wonder if you could optimize this numerically by actually using the very bits of your fp32 numbers for this hierarchical structures and make the distance estimate a point wise xor on the bit,True
@solsticeprojekt1937,2023-07-29T13:30:10Z,0,"I enjoy your enthusiasm about this. :D It's really obvious once you figure out how zipping works. Brains are pattern machines. Most people have purely pattern based language with low amounts of awareness of their output. While all language-use is pattern based, there's differences in the *active* thinking-process between people who use language *actively*. Passive usage is just blind first-pattern-matching for responses. Such people can actually easily be spotted in the wild once you know how to do so. ""How would I know what I have to say, before I'm saying it?"" is an actual quote I can't ever forget about. Anyhow, babbling. Sorry. ^_^",True
@gl33q,2023-07-29T13:02:13Z,0,I ran a naive bayes for comparison... KNN: Accuracy: 0.7029702970297029 NB: Accuracy: 0.5544554455445545 Very interesting!,True
@birkett83,2023-07-29T11:55:01Z,0,"You say ""it would be different if we were comparing patterns in a tokenized way or something like that"".   Uh. You kinda are.   Compression methods like gzip work (in part) by looking for sequences in the input that appear frequently and assigning them a short sequence of bits in the compressed output. I don't know the full details of the gzip algorithm but in simplified terms, if you compress a string that contains the word ""excellent"" twice, the compression algorithm will assign a short code to the sequence ""excellent"" and use that short code to represent each instance of the word ""excellent"". These aren't exactly tokens because they're generated on the fly for each compression but it's kinda filling a similar role.  So if you have a sample you want to classify and it contains the word ""excellent"", and one of your training data points also contains the word ""excellent"", when you concatenate them and compress the result, gzip can use that repeated word to compress the text better, meaning the length of the compressed output is shorter than the compressed output of concatenating the sample with a training data point that doesn't contain the word ""excellent"". The negative sentiments probably don't contain the word ""excellent"". In this way, samples that contain positive words will get shorter compressed outputs when concatenated with training data that contain positive words, and samples that contain negative words will get shorter compressed outputs when concatenated with training data that contain negative words. It doesn't seem at all surprising that you can get 75% accuracy this way.",True
@jonmichaelgalindo,2023-07-29T11:52:39Z,13,Compression is amazing. You can compare creativity in LLMs' outputs for writing prompts just by comparing compressing ratios.,True
@Alexander_Sannikov,2023-07-29T11:50:49Z,1,"what if instead of appending two arbitrary samples together you append them to a larger set of known positive sentiments, compress, then append to a known negative set, compress, and then compare the result?   the idea is that positive statements should compress better when appended to other positive statements and vice versa. and compression size should be much more indicative with more text in it.",True
@JirkaBalhar,2023-07-29T11:36:38Z,1,"Ok, so I have forked your code and added a simple TF-IDF + logistic regression baseline. The result is better accuracy and much lower time complexity: 70% vs 86% accuracy and 37 seconds vs 0.1 seconds. I think the paper shows an interesting idea of using compression as a similarity metric but no revolution is happening here.",True
@sidehat1655,2023-07-29T11:07:20Z,0,You forgot to put the link to K Nearest Neighbours in the description.,True
@terjeoseberg990,2023-07-29T11:01:23Z,101,"I get it. You‚Äôre compressing two texts individually, and then combining the two texts and compressing that. If the combined texts compress well, it means they‚Äôre more similar. If they compress poorly, then they‚Äôre more different.  More similar texts are more likely to have a similar sentiment while less similar texts have less similar sentiment.",True
@JonACopas,2023-07-29T10:57:44Z,0,"Couldn't it just be that movie reviewers tend to write more about bad movies than good, or vise versa?",True
@crackwitz,2023-07-29T09:23:14Z,2,"It's using compression to assess how SIMILAR the query is to every example. Similarly sentimented strings, with the same idioms, concatenated, compress better. Dissimilar strings don't. So it's looking for a training string that is most similar in speech pattern. The compression distances go into a ""how dissimilar is it"" score. The feature vector of a query isn't really a position in a feature space. It's saying how close the query is to each training point. I don't see a point in using KNN here. Just take the sentiments for the lowest few scores in the score vector. That's not KNN. Doing the Cartesian on the training data would only make sense to, uh, compress *it* to find the most unique samples, so we don't have to run on the entire training set. That is where I can imagine KNN making sense to use: to cluster that, then just use representatives/centroids. KNN on the scores vector seems to merely train the classifier on knowing which index has which sentiment. That is identical to train_y",True
@penguin6600,2023-07-29T09:10:49Z,1,This feels really like a text similarity algorithm which they happen to use for sentiment: compressions of similar texts concatenated are smaller than different ones concatenated. If you apply this to sentiment on a very different text domain we should see  performance drop significantly...,True
@210Artemka,2023-07-29T08:54:51Z,0,"What about the length of the text? It seems to me that the longer the text samples the less accurate this method would be. Same goes for the domain. The more broad and abstract the topic discussed in the samples, the more similar words are introduced in each category. Though, this is just my assumption, would be great to check, but I am too lazy...",True
@JirkaBalhar,2023-07-29T08:14:34Z,0,Does anyone know what is the nontrivial baseline accuracy on the dataset? Something simple like tf-idf.,True
@elgireth,2023-07-29T07:55:02Z,0,What is the accuracy for uncompressed lengths? Is gzip the key feature of this or just the length itself?,True
@kevshow,2023-07-29T06:45:54Z,0,The dataset you would need to make this work is too large.,True
@KidariHengnim,2023-07-29T06:08:47Z,34,"In line with what many have mentioned already, I feel like you could just replace the compression algorithm with splitting the text into words and making a set out of that, so that the additional length of the set isn't much longer when concattenating two strings that have many words in common.",True
@kaishang6406,2023-07-29T02:21:34Z,8,"I've a hypothesis.  Compression algorithum produce a longer result when there is more variation. So, when compressing the concated string, there is a vague relationship between the similarity of words and length of the compression. eg(very simplified), ""this is amazing."" and ""amazing product."" contains the same word, so the resulting reduction is shorter than ""this is amazing."" and ""horrible product."". Running KNN would get the indices that correspond to strings in the training set that use similar wording with the string that is being predicted. Then, if most sentences that are similar to the string being predicted have positive sentiment, then this new string is likely to have positive sentiment.  In other words, starting with string A. When concatenated with a similar string B, the entropy doesn't increase much, so the length of the compression doesn't increase much, so the distance between these strings is small. When concatenated with a different string C, the entropy does increase, so the length of the compression increase, so the distance between these strings is big. If this hypothesis is true, then the units that the compression algorithum acts upon probably contains some vague information about the sentiment.",True
@B_dev,2023-07-29T01:45:24Z,0,bro what,True
@harrytsang1501,2023-07-29T01:28:15Z,0,3:51 This is to simple yet so crazy and it sounds too simple to be true,True
@goldenfox27,2023-07-29T01:07:00Z,0,this one of those things that you think about it and laugh because is bad and irrational but in the end it works like magic. Is not perfect (far from it) but is a miracle that you can do it,True
@wg2,2023-07-29T00:55:37Z,0,"how these all works is bugging me, by all means, it shouldn't, time to roll up my sleeves and have a crack at it.",True
@babali1014,2023-07-29T00:44:32Z,162,"Gzip is used as a tape for finding the ""distance"" between two strings.  Think of it like cosine similarity except the ""dot product"" is taken directly from the two strings; no vector embedding in the picture.  Your xx2 = len line is the key which measures the length of two strings concatenated.   Suppose A and B are similar strings, different only in few characters or words.  Then  z(A)+z(B) >> z(A//B) ~= z(A) ~= z(B). Here z() is compressed length, // is concatenation, ~= almost same value.  If A and B are dissimilar, then  z(A)+z(B) ~= z(A//B).  If you understand why gzip gives these two equations then you will understand why the method works.",True
@AnitaSV,2023-07-29T00:14:58Z,65,"It is not comparing length of compression, the fact is that if two strings x1 and x2 are very dissimilar then concatenating x1 + '<space>' + x2 will entropy worth sum of their entropies. If they are similar then the entropy of them put together will be quite less. For example if x1 = x2, then gzip can simply encode x1 first and say repeat it twice. So if x1=x2 the distance would be very close to zero as the excess after subtracting min is just ""repeat once more"".   nd = (ENTROPY(x1 + '<space>' + x2) - min(...)) / (ENTROPY(x1) + ENTROPY(x2))  ENTROPY (x) = LEN(GZIP(x)) in this case. The better entropy function you use you are likely to get better results.",True
@4mb127,2023-07-28T23:39:40Z,1,Zip is basically a custom tokenizer for a chosen data stream.,True
@Nerdimo,2023-07-28T23:23:34Z,0,"Goes to show how we‚Äôve come so far in ML, yet we still can‚Äôt fully comprehend why certain methods ‚Äújust work‚Äù",True
@alireza202,2023-07-28T23:10:34Z,0,"Nitpick: To optimize the code, you can do gzip once on all samples, and store the lengths, and then do the distance operation. That reduces the gzip operations to N, rather than N**2.",True
@theDrewDag,2023-07-28T23:05:04Z,0,If all of this is indeed true then it should be language independent? Creating a dataset with text of mixed languages should yield the same performance as English alone?,True
@mytechnotalent,2023-07-28T22:52:45Z,0,KNN !=2,True
@mytechnotalent,2023-07-28T22:51:49Z,0,"Harrison, once again, WOW. 25 lines and GZIP did it.  I need to re-evaluate life.",True
@wurstelei1356,2023-07-28T22:47:50Z,2,Train AI on zipped text snippets might increase your context length without the need of a larger model. I like the idea.,True
@bigphab7205,2023-07-28T22:32:19Z,6,Do you think it could work for hamming distance on more abstract features? Like maybe vectorized values based on pixels and their locations relative to each other.,True
@twiddle7125,2023-07-28T22:11:04Z,0,Are you going to resume the NNFS videos? I bought the book because I thought I was going to be able to follow along. :(,True
@ruroruro,2023-07-28T21:38:13Z,0,"Please, correct me if I'm wrong, but it seems to me that you are training the kNN using the NCDs as features. So you are effectively computing the euclidean distances between vectors of normalized compression distances. Why? That seems like a really weird thing to do. Why not use the normalized compression distances directly as the metric in kNN?  Edit: I've read the original paper and it seems to me that the authors of that paper are using the approach that I am describing, not what you are doing in this video.",True
@PatrickHoodDaniel,2023-07-28T20:25:51Z,0,That is so funny. This is weird and makes as much sense as if angry sentiment is long winded and pleasant sentiment is short winded. Awesome video!! Thanks for sharing.,True
@aoeuable,2023-07-28T20:23:59Z,16,"Without reading the paper (and not going back to the code): Suppose you concatenate a movie review written in English to one in Swahili, compress them together, and compare those lengths against the compressed length of either. Those two lengths are going to be quite  different from the first  because Swahili and English share basically no features that gzip could deduplicate (which is basically how Huffmann(esque) codecs work). If OTOH you do the same with a review in English and another in English, and they're going to be way closer.   Now, assuming that similar sentiment is expressed in similar language, the classifier works. What it won't be able to do is distinguish ""This movie is better than this other movie"" and ""This other movie is better than this movie"" -- the combined string will compress to only a couple more bytes resulting in low distance, but the sentiment is opposite.   But then there's the question of how accurate you need your results to be, and how many people are going to come up with attack vectors to confuse your classifier, and whether ""good enough"" isn't a much better result than ""let's throw a language model at it"". If anything this kind of thing can work as a baseline: Anything capable of doing actual semantic analysis has to be at least as good  or something is fundamentally wrong with it.",True
@timmygilbert4102,2023-07-28T19:57:10Z,0,"I literally called it very early on the LLM, ngram are huffman analog, therefore there is a way to requalify semantics as compression analog, exploring that instead of jumping to neural network wad key to untangle llm. Glad someone did something before i could repair my computer. I'm gonna tell you, with copy is all you need paper ,YAGNNNI you aren't going to need neural network for intelligence. That's what I'm exploring.",True
@GameSmilexD,2023-07-28T19:48:38Z,1,"we ve known for decades that intelligence is just data compression xD, keep the videos coming i rly appreciate the cutting edge updates :)",True
@bobsavage3317,2023-07-28T19:47:26Z,0,Great vid!,True
@sithbrixstudios5688,2023-07-28T19:30:55Z,0,"Instead of using t-SNE to visualize the NCD arrays in 2 dimensions, you could try using UMAP (Uniform Manifold Approximation and Projection) for visualization. UMAP's cost function is formulated so that the local and global structure of the data in high dimensions is preserved when it is mapped to a lower dimension. t-SNE does not have this special property, so its embeddings are generally not as good.",True
@japrogramer,2023-07-28T19:15:00Z,0,Interesting üòÆ,True
@rauhan_sheikh,2023-07-28T19:12:38Z,0,btw do you plan to make an updated playlist ML/DL because that one is 7 years old?,True
@KuMan1323,2023-07-28T18:49:44Z,0,four letter words in negative sentiment compresses well? :D,True
@SimGunther,2023-07-28T18:49:03Z,1,"This is basically Piped Piper, but for LLMs",True
@vincentvoillot6365,2023-07-28T18:43:40Z,8,"Looking to the pseudo code of LZ77, the algorithm handle matches and distances between strings. In a way, gzip index strings by frequencies within a sliding window. This representation can be use to segregate common words (the, an, etc..) high frequencies, isolate less frequent ones . They use a lossless compression to do a statistical analysis. This is smart.   Lempel - Ziv 1977 : while input is not empty do     match := longest repeated occurrence of input that begins in window          if match exists then         d := distance to start of match         l := length of match         c := char following match in input     else         d := 0         l := 0         c := first char of input     end if          output (d, l, c)          discard l + 1 chars from front of window     s := pop l + 1 chars from front of input     append s to back of window repeat",True
@marcfruchtman9473,2023-07-28T18:31:07Z,0,"The Reason this works is that Nearest neighbor won't change if the only thing that changes is the symbology for the word used. Essentially all this is doing is compressing every individual word into a new symbol that will be the same for all other words of the exact same spelling. Technically, I don't see how this will affect ""anything""... you should get the exact same results.. with the one exception that you might not have as much of a problem with memory constraints because of the compression, and you might have to spend time performing compress / decompress to get valid human readable outputs to queries.",True
@MariuszWoloszyn,2023-07-28T18:30:08Z,0,How about using normalized hamming distance between samples instead? Should work on similar principle.,True
@shocklab,2023-07-28T18:25:25Z,14,"In your NCD method you are calculating the gzip output each time. Why can't you do this just once for each sample and then use that in the NCD method. It should reduce the time greatly, if I'm not mistaken. Surely that takes the most time from within that single function.",True
@dogabarisozdemir,2023-07-28T18:19:15Z,0,Hashed string check is being used to compare big strings if they are same. So at infinite end for example angry sentimental sayings will be same. If accuracy is enough to do the business we want it is interesting and efficent way to do sentimental classification with compression algorithms :) thanx for sharing,True
@hantuchblau,2023-07-28T17:59:56Z,5,"My intuition is that we compare *additional* length of compression, i.e. how well one string acts as a compression dictionary for another. Higher score for common words and sequences/sets of nearby  words.   Then each training sample acts as a bag of subsequences. Maybe doing dimensionality reduction on the score vectors would be interesting, to see if common subsequences control similarity and how they correlate with sentiment? Edit: Sorry, should have finished the video first.  Edit2: if the theory is correct something based on stemming+onehot encoding the words and then using percentage of common bigrams as distance should work similarly?",True
@julioomarsantistebanpablo2123,2023-07-28T17:59:27Z,9,"Gzip implements LZ1 and Huffman, which are encoders. They reduce 8-bit ASCII codes to fewer bits codes. It is like using a language of small size and with 100% the same meaning.  Gzip is not a statistical model.",True
@Ferdii256,2023-07-28T17:03:20Z,2,"Tried with brotli compression instead of gzip and ""only"" got 63 %. Idk how to interpret that, though ü§î.",True
@14zrobot,2023-07-28T17:01:06Z,0,"Wow, what is next? Training on encrypted data?",True
@angrybob8126,2023-07-28T16:52:03Z,1,Works just as well as you doing the end to dt2.,True
@ConnorMcCormick,2023-07-28T16:39:53Z,27,did you mention that you tried running k-nearest neighbors on just the length of the original string?  Two plausible causal models to explain this are: 1. people are more / less effusive when positive / negative 2. people are more / less repetitive when positive / negative  Either of these would be sufficient to explain the fact that this can learn,True
@pknytl,2023-07-28T16:35:34Z,12,"To me this suggests that on average, there is more information in either positively or negatively valenced sentiment. IIRC, from my PhD research, negative feedback in human communication contains on average slightly more information. Would be good to calc that here and see if it potentially explains what is happening.",True
@thenoblerot,2023-07-28T16:15:52Z,1,ummm... mind blown. C'MON! 10:58,True
@_XoR_,2023-07-28T16:15:02Z,10,"Hear me out: Imagine training a transformer on compressed text representation, this would theoretically improve the attention span window in token size by the compression ratio ü§î",True
@caseymorean83,2023-07-28T16:00:43Z,3,Pretty sad the paper got approved given such an egregious mistake.,True
@andrewm4894,2023-07-28T15:56:19Z,10,Would be nice to train on one dataset and see how well it does then on a different dataset. So how well It generalizes across two different but maybe similar enough actually sentiment datasets.,True
@lennarth.6214,2023-07-28T15:44:34Z,308,Someone should take a dataset for positive words and negative words and compare the average entropy of both. Maybe positive words have more entropy because they are used less in the English language or vice versa.,True
@Maxjoker98,2023-07-28T15:43:31Z,1,K nearest naval. Can't figure out if it's dating for belly buttons or marines.,True
@wktodd,2023-07-28T15:42:16Z,1,It can only work at all if words expressing positive are longer or shorter (post compression) than negative (or vice versa)   Language dependent?,True
@sv_gravity,2023-07-28T15:41:19Z,4,I need that 64 core 1 TB of RAM PC,True
@kelkka7,2023-07-28T15:38:49Z,4,"I mean using Principal Components Analysis is basically a type of compression and it can be tied with KNN to create a classifier. So not that surprising, surely?",True
@akshaybhardwaj10,2023-07-28T15:32:41Z,19,"Although I understood everything you said in the video, it‚Äôs still mind boggling for me how this can work",True
@wktodd,2023-07-28T15:31:59Z,1,I suppose there are ways to compress text without losing too much information. But gzip  on a long string seems unlikely ???,True
@HemangJoshi,2023-07-28T15:30:35Z,2,Please don't shoot at 24 fps.  It looks like sh##t... Shoot in 60 fps it is new standard,True
@MichaelBedford,2023-07-28T15:26:47Z,3,God I hate click bait titles.,True
@st33lbird,2023-07-28T15:24:48Z,1,Did you read the paper? The combined the train/test cycle into a O(N^2) algorithm and claim it is comparable to traditional ML techniques. It shouldn't be surprising that it worked.,True
@HemangJoshi,2023-07-28T15:23:07Z,3,Initial comment,True
@fuba44,2023-07-27T07:50:58Z,7,"K-nearest nibbles ftw :-) and back to the roots, love it.",True
@Stinosko,2023-07-27T05:27:09Z,3,Hello üéâüòÆ,True
