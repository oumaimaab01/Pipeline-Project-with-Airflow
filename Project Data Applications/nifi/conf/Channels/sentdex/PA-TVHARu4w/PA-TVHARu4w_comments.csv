author,updated_at,like_count,text,public
@amitbiswas7943,2018-04-05T06:31:32Z,0,how to check whether a message is received or not in mpi4py.,True
@rh654,2018-01-13T22:04:42Z,2,"Update for anyone still using these tutorials...   When trying to use the send command across more than 1 worker node (i.e. -np 3) I received an error: Connection Refused. This happened even though the devices could communicate through SSH. I could see this if I removed the file from a node, I would get an error saying it didn't exist. I also tested using each node individually as rank 1 and they worked on their own.  To fix this problem I had to pass the argument -disable-hostname-propagation to mpirun. This bug is from hydra passing the hostnames to MPICH2 by default if you put the hostnames in your machinefile.  Another fix is to use IP addresses instead of hostnames in your machinefile. But this is undesirable when working with multiple nodes.",True
@dav264,2016-01-18T21:26:18Z,1,"Thanks for your videos.  I was able to setup my four node Raspberry Pi cluster with no problems.  I don't know what I'm going to do with it, but just the exercise of setting it up was enjoyable.  Thanks again.",True
@alexandervaldez1646,2014-08-04T04:35:26Z,0,"Hello Mr. Sentdex, i have a little problem with this example, all tutorial before this video works very well, but when i try to compile this example there is no output. For this reason. I've wrote some print lines to identify when the program locks so i thing i have problems with comm.send(data,dest=1). Could you give me some help please  thank you!",True
@JeffLarkin,2014-07-31T23:57:12Z,0,"Oh, and I meant to comment earlier that I'm really impressed by how much the python MPI wrapper you're using simplifies the send/recv calls. Message type, size, and tag are all optional or unnecessary, that's really nice. ",True
@JeffLarkin,2014-07-31T20:00:04Z,1,"The reason the order doesn't matter is because you're using blocking send and recv, so the receiving rank will block in the receive until it gets the data that it's expecting.  What you've essentially done by adding the sleep is introduce an artificial load imbalance between the two processes, but since both are using blocking send/recv, neither will progress until its part has completed. (For most MPI implementations, the send may actually start progressing again before the data has actually been sent over the wire. The data doesn't have to arrived, but it must be safe to overwrite the send buffer once the send routine returns) Now, if you replaced your receive with a non-blocking receive, your print could (and likely would) happen before the data has been received. I think the reason you were able to run multiple ranks  on that node without things getting hung up due to python's global lock is because you're likely running 2 python interpreters in two separate processes (as opposed to threading within a single interpreter). Keep up the good work, it looks like you're learning a lot as you go, which is awesome.",True
