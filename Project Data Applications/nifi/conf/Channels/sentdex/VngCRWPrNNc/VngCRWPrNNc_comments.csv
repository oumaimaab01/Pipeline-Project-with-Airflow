author,updated_at,like_count,text,public
@mertfromhell,2023-10-12T14:12:58Z,0,why this shit so hard to understand lmao,True
@Faisal-jo5vk,2022-07-07T11:05:15Z,0,uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuhhh,True
@huojinchowdhury3933,2022-02-03T08:48:23Z,1,Why we need to multiply class y with equation x.w +b?,True
@RishikavsAnnie,2021-08-16T01:03:24Z,0,Could the magnitude on the RHS of these equations have been anything other than 1?,True
@dankquan743,2020-11-07T07:32:47Z,1,why do you leave out information at important parts and then go into detail on how to do 6th grade algebra like subtracting 1 from each side of the equation.,True
@cj2074,2020-08-30T10:46:35Z,0,i think...w is not perpendicular on the line from origin as said in video ...cause then we wont need to find w! ...it is just a vector on the line where 'u' will be projected,True
@cj2074,2020-08-30T10:29:52Z,0,"i guess im not the only one who didnt understand at 2:55 ...you see, dot product shows the directions these two vectors are inclined to each other i.e a.b.cos(theta) => if theta is less than 90 cos would be positive and vice versa ....now, we know how much one is inclined to other",True
@neillunavat,2020-08-27T16:48:24Z,5,It is sad to see how the views go down with each video... Hope all reading this don't give up and be successful machine learning engineers.,True
@bornofdata,2020-08-10T00:01:28Z,1,"y(xw + b) -  1 = 0  is not the same for both, remember y is positive and negative for the other.",True
@alexmattheis,2020-06-28T12:29:33Z,0,Hard to understand since support vectors were not explained (had to look it up on a different page).,True
@aaronge5195,2020-04-08T00:10:47Z,0,"Wouldn't U dot W always be positive, since mathematically, both vectors are located in the first quadrant, meaning their x and y components are both positive? How is it then that U dot V will ever be less than 0?",True
@irishuser2k11,2020-03-03T19:30:12Z,0,"Great video! Yourr really making machine learning accessible!  I have a question on the value of b, could you explain its value a little more? When you say it is the bias, is that the spread between the two support vectors?",True
@johnshepard2310,2020-02-17T18:13:01Z,1,But why these values are 1 and -1? It is just an example and these can be 3 and 5 or anything else?,True
@Gentleman217,2020-01-18T17:50:58Z,0,aaaaaaaaaahhhhhh every video has an opening I can't believe. I feel like my 20 seconds has stolen with each videos.,True
@evgen422,2019-12-27T13:00:16Z,4,"5:06 looks like he skipped the explanation of supported vectors so it's not clear what X-sv and X+sv are. From his website:""What if x.w+b is 0? This means we're right on the decision boundary. Otherwise, if it is above 0, then we have a positive class. If below zero, it's a negative class. We're going to take advantage of this, and then go a step further and suggest that if x.w+b is 1, then this is a positive support vector. If x.w+b is -1, then this is a negative support vector."" also: https://medium.com/deep-math-machine-learning-ai/chapter-3-support-vector-machine-with-math-47d6193c82be",True
@chamberhorsefeeder,2019-09-12T14:48:19Z,9,"I am not sure if it is mentioned during the video, but the projection of u to w is the dot product of u and the w's unit vector, not w. u.w/||w|| = u*cos(alpha)= projection of u on w.",True
@becauseiwanttoanime9541,2019-08-22T19:22:53Z,2,"so first of all what is like, what's the, we know that eventually xD nice one thanks for an amazing video i couldn't understand the math so i had to go search on my own for a couple of days but still thanks for the tutorials",True
@antonburenko2852,2019-07-21T09:16:43Z,0,Hate him saying y of i's (p.s. it happens to be y sub i),True
@madhavadiflorio2550,2019-05-18T13:46:22Z,1,"If anyone is not familiar with vector projections, here is a pretty good video explaining the concept.  https://www.youtube.com/watch?v=fqPiDICPkj8",True
@Joe-cs5zk,2019-02-15T23:35:23Z,1,I'm pretty confused as to why you're not using the projection formula to determine length to the decision boundary.,True
@rafaelsofi-zada1850,2019-02-04T10:07:17Z,1,"The math behind SVM is obviously pretty complicated, and couldn't be covered in any sufficient detail in this video.  To everyone who is interested in understanding all the math behind it, I recommend you alternating between reading this e-book http://ebooks.syncfusion.com/downloads/support_vector_machines_succinctly/support_vector_machines_succinctly.pdf?AWSAccessKeyId=AKIAI3HOYFDJLLWDMZSA&Expires=1549273383&Signature=nn7Cf3jFfAl3bccIvGq05e0wUUY%3D and watching this course on Udacity https://classroom.udacity.com/courses/ud262 Both are completely free and helped me immencely :D",True
@mohammednagdy6661,2018-09-13T12:36:51Z,6,I don't get the part with X_sv and X+sv what are those?,True
@thecactus7950,2018-08-13T07:45:28Z,7,I'm probably late but I get confused at 2:55. When you project u onto w you say u is in the + side of the hyperplane if u * w + b >= 0.  Couldnt you just check if u projected onto w > ||w|| or something?,True
@SanketPatole,2018-07-14T14:25:06Z,1,Isn't the projection of vector u on vector w = u.w/||w|| ? Why did you consider only the dot product?,True
@rajivkumar8160,2018-05-11T14:05:32Z,4,How to get the we get the output of u.w+b as -1 or 1? why not other numbers?Please explain.,True
@chandeepadissanayake6975,2018-03-31T13:00:56Z,1,"Did you ever explain what is a support vector? All the previous videos in this series helped me out in understanding from scratch(though I learnt the math behind linear regression from Khan Academy), this makes no sense to me.",True
@JohanSebastianCorn,2017-12-22T02:18:48Z,19,could someone explain why it has to be -1 or 1? like what determined this value? from 5:10 to 5:40,True
@HilaryKansiime,2017-09-25T12:17:26Z,0,Love the fact that you explain the principles involved,True
@zafarahmad1999,2017-09-23T13:12:52Z,0,A big thank you here. Whole series is not just very comprehensive but pellucid as well.,True
@dude2260,2017-08-13T16:43:21Z,0,"u.v = 0 (ignoring bias) only if u and v are perpendicular , but here u is perpendicular to decision boundary (db), now if we have any point on the db from (0,0) we are making a vector to that point how come u.v = 0 ,  can someone explain a little, shouldn't the equation be u.w <= ||w||",True
@surajkumarjena5808,2017-07-27T11:14:05Z,0,I didn't get the concept of y sub i....What is  that...and why are we multiplying it with the equation?,True
@shivankpathak4367,2017-06-10T02:43:23Z,1,What is the significance of u.w+b ?  Please explain.,True
@CurtisBRO,2017-05-13T21:58:02Z,2,"Really neat stuff but you hurt my ears when I hear you say, ""We're going to minus one here..."" The verb is ""subtract"". Minus is not a verb. Thanks. Sorry to be so picky (former mathematics teacher).",True
@user-bt7ms4oj4b,2017-04-18T01:53:52Z,0,"I really love your video.You explain things very well.Could you make some videos about other algorithms like random forest,decision tree:)",True
@rpcruz,2017-04-09T10:30:28Z,0,Very well explained! Terrific!,True
@floschar77,2017-03-14T14:57:46Z,2,-1-1 =  0 ? in school I learned -1-1=-2,True
@cypherk,2017-02-21T07:55:48Z,0,"Though I find myself quite liking your videos, your vector notation is frustrating as w bar usually denotes the complex conjugate of w. I know, you probably just introduced this notation as a simplification, but personally, I wish you hadn't. I may be biased on this, but I also think using transpositions for the dot products would have helped.",True
@hahablahblaah,2017-02-06T19:29:11Z,0,"Errata at 3:05. For u.w + b = 0, your example suggests that the point would be classified as both positive and negative, which is impossible.",True
@danhunt5016,2016-06-15T10:44:47Z,0,"I see this question asked below too, so I'm glad I'm not the only one, but your response still doesn't make it clear (sorry). I'll try to ask it a different way. What would the Support vector look like on the example graph?",True
@michaelwalczyk8109,2016-05-25T15:08:13Z,11,"Wonderful video series! I'm learning a ton from it. I had a few questions related to this video.   At around 5:05, you introduce the formal notion of a ""support vector"" as X sub -sv and X sub +sv. What exactly do these variables denote? Are these the data points that are ""closest"" to the decision boundary (on either side)? Also, what happened to the inequality, i.e. <= 0 or >= 0? How do we know that the equations at 5:05 equate to -1 and +1?   Finally, I'm confused by Y sub i. Is this just a constant that we introduce for clarity? At around 8:30, why do we multiply the two equations by this value?  I know that's a lot...but any help would be appreciated. Thanks so much!",True
@masumeh_7904,2016-05-18T13:22:10Z,0,"what is the best sound file for python game?why? and what are the advantages and disadvantages of  using a loop, repeat and arrays?",True
@badnam3189,2016-05-17T18:08:33Z,0,love your videoes!,True
@EatShitCh,2016-05-17T16:09:08Z,43,"man you're amazing, thank you!  ps please never stop spreading knowledge",True
