author,updated_at,like_count,text,public
@stonkks,2024-05-11T07:32:08Z,0,"Your content was very good till machine learning. When you started making generative AI videos, I feel value add is lesser. For example, fine tuning data with business use-cases should be discussed, intuition in attention should be discussed.",True
@starkgaming1425,2024-03-12T06:28:24Z,0,Please release a step by step guide on how to fine tune Gemini API in Python.....I tried by refering to documents but encountered a lot of errors with OAuth Setup please...........!!!,True
@shriharinair1999,2024-03-07T11:45:43Z,1,how will backprop work? or is this applied after fine tuning?,True
@pumpnation,2024-03-05T05:53:38Z,0,"what does -1,1,0 arrive at there addition and how does it relate to next word",True
@cristianpercivati5097,2024-03-04T18:30:09Z,1,"I'm not quite sure how this prevents precision errors. Quantization is not new at all, it is commonly used to reduce the model's size, at the expense of some precision errors. There's a sort of trade-off in quantization. Float point values in weights are there for some reason, they provide flexibility in the network and neuron's activations. Furthermore, and no mean of criticizing this paper, I don't' see any example or implementation. There's only a comparation with LLAMA, with great results, but no model is explained. The only thing that is truly explained is the quantization function. Maybe some of you had a better insight of this model and can explain if I'm missing something important, if this really works as is expected, it would be nice to have a good understanding of it because it could change neural networks as we know them right now.",True
@Bigjuergo,2024-03-04T13:36:05Z,0,can i test this llm locally?,True
@pranavireddy8221,2024-03-04T10:59:07Z,0,"i want to do generative ai professional certifiaction where can i refer, can you suggest me more in detail",True
@basilalharbi3293,2024-03-03T18:54:02Z,1,get new mic,True
@RameshBaburbabu,2024-03-03T11:38:29Z,0,"Hold on , when we compare the performance , that means the model exits, it is not in theory right ..?. It might not be available for public, but model has been build m and did all the testing  with open source LLM ...",True
@amanmehrotra44,2024-03-03T08:45:57Z,0,"Hey @Krish, kudos to you for all the content in videos related to finetuning, just one request, can you make a couple of video on LLM model evaluation, types of evaluation, because that part is something which is frequently asked in interviews.",True
@zerorusher,2024-03-02T15:31:06Z,10,"The clever technique of this paper is that they are training the model using FP16 for the Backpropagation.   From my understanding, while the model weights are only [-1,0,1] all the error calculation is done with FP *at training*.   The result is something like ""hey model, you can only use -1,0,1 but im analyzing your responses with FP16. So your job is to find a set of weights that is as good as if you where using FP16 for your calculations.""  This paper seems to be training and quantizing at the same time, which is better than training first and only then quantizing the resulting model.",True
@shailavijay1,2024-03-02T07:35:55Z,0,Thanks for sharing this informative content Krish. Your video helped me a lot to understand the core concepts in Data Science. Sure I will read this research paper.,True
@masabattulatejanikhil3561,2024-03-02T07:01:01Z,0,"Thanks for making us updated. But my doubt is that wont there be any information loss because of narrowing down the values to 0,1,-1",True
@ernesttan8090,2024-03-02T06:37:47Z,0,the golden question is...can i have the code please haha,True
@RPG_Guy-fx8ns,2024-03-01T15:44:39Z,0,"3 values would make it 2 bit. not 1 bit. you might as well include a fourth value, because you will use the same amount of bits for 3 or 4 values.",True
@amlendrasharma190,2024-03-01T09:28:12Z,0,Sir plz suggest how to start and from where to start machine learning i want to cover from 0-100,True
@user-lq7sx8qw5t,2024-03-01T08:15:57Z,0,You nailed it as always ...I need link for this research paper...it would be best if u share links of related research papers in description of every video...,True
@bloggerblackyworld1749,2024-03-01T06:47:56Z,0,"Hi Krish Sir!  I am working with pdf + langchain + prompt engineering + BAAI  But I am having some issues 1. The pdf which I am working with is a bit complex, it's contains lots of data in tabular format with multiple columns and rows. So want to know which library should I use to load that pdf.  2. I was using BAAI model but I can't use prompt engineering with that because it will return only similar text, so I want to know if there is any model which is free to use.",True
@Bhanu-uo3wo,2024-03-01T06:01:41Z,1,"Sir i'm a btech final year student and frankly speaking i know just basic python not more than that.. sir can i follow your data analyst road map and the main question is ""ARE THERE GOOD OPPURTUNITIES FOR DATA ANALYST JOB ROLES FOR ESPECIALLY FRESHERS LIKE ME WHO ARE GOING TO COMPLETE BTECH?"".. please reply sir(or any people having good knowledge)",True
@sanjaynt7434,2024-03-01T03:47:05Z,0,"@krishnaik06 Sir, In this constantly changing technology, how do you know what new papers letters to read and follow? Can you start news letters, series, or blogs for posting new papers and research-based content? Example : List of the best papers to read as its not possible for one person to explain all in sessions.",True
@abdelkaioumbouaicha,2024-02-29T20:18:31Z,1,"üìù Summary of Key Points:  üìå The research paper discusses the concept of 1-bit LLM (Large Language Models) where every parameter or weight of the LLM is represented as -1, 0, or 1, significantly reducing resource requirements.  üßê The transformation to 1-bit LLM involves using ternary values for weights, leading to reduced GPU requirements during operations like forward propagation due to simplified addition operations.  üöÄ Bitnet, the 1-bit LLM model, offers advantages in terms of reduced inferencing costs, latency, throughput, and energy consumption while maintaining model performance.  üí° Additional Insights and Observations:  üí¨ Quotable Moments: ""The transformation to 1-bit LLM simplifies operations by using ternary values, leading to significant improvements in performance and reduced resource requirements.""  üìä Data and Statistics: The 1-bit LLM model showed reductions in memory requirements and latency compared to traditional LLM models, showcasing the efficiency of the new approach.  üì£ Concluding Remarks:  The introduction of 1-bit LLM models like Bitnet presents a promising direction in the field of data science, offering a cost-effective and efficient solution for large language models. The research paper highlights the potential benefits of this approach in terms of resource optimization and performance improvements, paving the way for future advancements in generative AI. Generated using TalkBud",True
@dibu28,2024-02-29T19:51:55Z,1,Where to download model for inference?,True
@affanraza3168,2024-02-29T19:49:13Z,2,I see some disadvantages of this will make some information loss for example  0.28 will be converted to 0. maybe your weight 0.28 has some kind of effect in the model result. Yes model has been reduced.  Even though quantization helps reduce high value but still keep the information in lower value but if you are making it 0 there will be loss of information. Basically anything that multiplies with zero will be zero so that mean it that weight that has been assigned to any feature means dont use that feature. The accuracy will not be effected but that means that when there is a time that we need to use that specific feature we have to manually update it.  This is my understanding hope you can clear it for me.,True
@manavagarwal3405,2024-02-29T18:19:16Z,7,"insane efforts, as a student this helps in increasing interest in reading research papers, also take curiosity to next level in field of LLM",True
@Hairregrowthjourney,2024-02-29T17:38:16Z,0,Thank you Krish for awesome explanation! Request you to make a video on building hallucination evaluation model,True
@arsiveparkour6251,2024-02-29T17:33:24Z,0,"You forgot to explain about why they mentioned 1.58bits. I have read it somewhere, maybe entropy or Information systems?",True
@AbhishekSharma-ml2dj,2024-02-29T16:39:30Z,4,How to get this type of research papers??,True
@michaelmccoubrey4211,2024-02-29T16:02:42Z,1,"If this proves to be true then obviously this would have many benefits for LLMs but couldn't this be applied to any neural net to make it more efficient, more explainable a better in many other ways?",True
@Hellow_._,2024-02-29T15:23:24Z,2,Any number multiply with 1 is 1 only üòÇ,True
@GeraDeluxer,2024-02-29T15:21:06Z,0,"Good explanation, thank you!",True
@buzzprime93,2024-02-29T15:10:54Z,0,"Can't thank enough, Love ‚ù§ from Mumbai",True
@ramkrish-kv7we,2024-02-29T13:07:35Z,0,awesome explanation @krish,True
@RishiRajxtrim,2024-02-29T12:39:42Z,0,üëçüôè,True
@Geen-jv6ck,2024-02-29T12:12:59Z,0,Thank you.,True
@Rk-mv8sz,2024-02-29T11:49:50Z,0,"Hi sir, any new course is available on AI?",True
@curiousskeptic,2024-02-29T11:11:33Z,3,Wow my friend sent me this paper just now! And you published a video about it üòÇ Love your Content,True
@squadgang1678,2024-02-29T10:37:51Z,3,"I am still not convinced that 1 bit LLM can match the accuracy of current LLM models, ofcourse the energy consumption required hardware config might get reduced but accuracy precision will also get reduced is what i feelüòÖ",True
@divyaramesh0101,2024-02-29T10:30:15Z,3,"I like to read research paper but sometimes i may get struggle . I followed all your previous  videos,so that while you're reading this paper,I was able to get the concepts clear. Thank you krish sir.",True
@beingrishi6450,2024-02-29T10:17:25Z,1,Sir what's the prerequisites to go for generative AI?,True
@vijaymunavalli335,2024-02-29T10:04:22Z,4,Awesome explanation Krish,True
@baskarkevin1170,2024-02-29T10:04:03Z,2,1st like,True
