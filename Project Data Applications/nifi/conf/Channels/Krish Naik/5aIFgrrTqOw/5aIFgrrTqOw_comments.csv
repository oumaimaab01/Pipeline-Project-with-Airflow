author,updated_at,like_count,text,public
@parmidanik5691,2024-04-29T10:11:56Z,0,"Dear Krish, why the subtitle in this video is disctivated? i'm not perfect in english and it's diffict for me to undrestand your explanation.",True
@zeropt4891,2024-04-18T15:29:09Z,0,how we will calculate information gain after gini impurity??,True
@emadfarjami8436,2024-04-11T01:19:28Z,0,"Thank you for explaination. üëç For this, I plotted  0.5 (Entropy) - Gini The actual equation would be: y = -0.5(x(log(x))+(1-x)(log(1-x)))-(1-(x^(2)+(1-x)^2)) Intutively, for splits that class probabilities are between 0 and 0.5 Entropy penelizes splits more than Gini. Therefore, using Entropy instead of Gini, it is more likely to choose a feature that create a leaf node and an evenly distributed node. Overall, I think trees with Entropy have more early leaf nodes and are deeper. On the other hand, trees with Gini are wider.",True
@nahidzeinali1991,2024-03-10T15:39:15Z,0,"you are the best on I could learn about every complicated question so easily! Thank you so much , Love U",True
@pratikmandlecha6672,2024-02-22T16:50:08Z,0,Damn 126K views for this. I agree its a decent video but it makes me feel like skip working for tech giants and start making youtube videos. So the take away is that it's computationally efficient (that's it?),True
@sumant1937,2023-11-22T01:18:58Z,0,Does the gini purity change multi class classification?,True
@sumitpal6797,2023-10-26T17:58:46Z,0,i am confused for multi-class. i got gini value 0.58. as you mention gini values lies between 0 to 0.5 example :- Gini = 1 - [ (6/10)^2  +  (2/10)^2  +  (1/10)^2  +  (1/10)^2 ] . please help anyone,True
@tacticalforesightconsultin453,2023-10-12T18:45:09Z,0,There are also notable differences that arise as the cardinality of the categories increases.,True
@abdulahiopejin8949,2023-10-11T19:32:02Z,4,"I have know this Prof. since 2019. If he is talking about something and I don't understand, I always know I'm the problem. Thank you, Professor",True
@ahmo9128,2023-05-25T16:37:23Z,0,really really great man,True
@anupanthony5416,2023-05-17T12:49:34Z,0,how to calculate the Information Gain after calculating the Gini impurity,True
@neptunelearning9249,2023-05-01T05:56:58Z,0,how will we apply GI in inforamation gain formula,True
@marishakrishna,2023-04-12T18:25:25Z,0,"But Still, when to use Ginni Index and when we should use ENtropy",True
@praveenpandey.77,2023-03-25T13:06:52Z,0,"i respect and love you sir , reason is your teaching technic is really admire us .  thank to explain gini index and entropy",True
@swL1941,2023-02-15T14:30:55Z,0,"Sir, does the decision tree work same way for multiclass classification ?? since you have taken binary (Yes or No), what about multi class ?",True
@codingworld6151,2023-01-29T07:25:53Z,0,Good hugya sir üíï‚ù§Ô∏è,True
@amarnaiknenawat7506,2023-01-26T21:15:18Z,0,Great explanation,True
@tejaltatiwar4682,2023-01-16T05:29:51Z,1,I subscribed when you had 5k subscribers,True
@tanzeelmohammed9157,2023-01-01T16:51:23Z,0,"Sir, range of Gini Index is from 0 to 1 or 0 to 0.5? i am confused",True
@user-py9ps8je5b,2022-12-29T05:24:32Z,0,"Though we use GINI (instead of entropy), anyway we need to calculate entropy for IG right?",True
@karunamayiholisticinc,2022-11-30T21:43:42Z,0,"At around 5 min 30 seconds you say that entropy increases when percentage of positive increases, but shouldn't it be more appropriate to say that entropy increases when the set is more impure and highest when it is most impure at 50/50. Overall wonderfully explained with diagram. Despite the fact you spoke a little fast in the middle, you were able to convey it pretty well through diagram and math explanation. Thanks.",True
@mayurpatil7910,2022-11-09T19:13:17Z,0,"how is (0/3) * log(0/3) = 0, shouldn't it be equal to 0 * (-infinity)",True
@fghgffgvbgh,2022-10-23T13:20:35Z,0,"if it all comes down to log and computation,  why did you explain those ranges with the graph. I cant get the advantage of gini maximizing to 0.5 vs entropy to 1. Could someone explain.",True
@kabirbaghel8835,2022-10-09T21:48:45Z,0,lovely content sir üòÄ,True
@rileywong5225,2022-10-03T15:48:02Z,1,"This video is computational efficient, coz u don't need others after watching this",True
@niloufarfouladi6997,2022-09-30T13:21:00Z,1,"for the entropy curve that you have described, I think this explanation is better: when your probability is 0.5: It is the worst case and entropy is maximized, after that either if the positive probability is increased, or it is decreased(means that the negative probability is increased) the system is purer and thus, the entropy is reduced.",True
@praos143,2022-06-28T19:59:45Z,2,"very well said, just one correction at 5:24, starting from 0 when the probability of + increases and reaches 0.5 (50%), entropy maxes out at 1. After that the probability of + /continues to increase/ ,while the entropy will start to decrease.  Probability will reach 1(100% probability) and entropy returns back to 0.",True
@vivekgiri822,2022-05-20T17:11:20Z,0,At that same time we use both of it ?,True
@fengjeremy7878,2022-05-20T09:55:31Z,0,Thank you! You are really a great teacher. Such a good lecture can save me a huge amount of time.,True
@utkarsh1368,2022-02-21T09:46:34Z,0,Great explanation sir!  I gave the 2kth like. Now you give my comment heart <3,True
@strawberryshortcake5779,2022-01-27T15:34:45Z,0,Does Gini and entropy work for regression data set .if so how do we find which one is better,True
@21stcenturyessentials11,2021-10-29T06:46:09Z,0,sir can you suggest the best book for learning machine learning,True
@manishayadav4083,2021-10-21T06:33:52Z,0,Which is the best for decision tree classification?,True
@prithicksamui6056,2021-09-01T00:14:23Z,0,"Well, I don't know if it's a problem for other people or not but I am used to this teaching technique(Board and marker) so I find it more comfortable than ppt. Randomly saw your video and this is exactly what I needed.",True
@rahulbatish8704,2021-08-19T10:18:26Z,0,we have to compare information gain and gini impurity not entropy and gini,True
@rishabkumar9578,2021-07-20T09:55:35Z,19,This is how everyone should teach üíØ,True
@diyusan7181,2021-07-16T12:21:52Z,0,Aaaaaa,True
@louerleseigneur4532,2021-07-16T02:50:35Z,0,Thanks Krish,True
@harisjoseph117,2021-07-11T13:16:38Z,0,Small suggestion: When the probability is increasing from 0 to 0.5 entropy is increasing. When the probability increases from 0.5 to 1 then the entropy is decreasing. Am I correct Krish?.. Thank you in advance.,True
@charugera7654,2021-07-07T10:20:02Z,0,Simple clear explanation. Godsent.,True
@premranjan4440,2021-06-21T12:52:02Z,0,Thank you sir,True
@varunshrivastav3578,2021-06-21T08:17:26Z,2,I was watching Stanford University lectures Even they can't teach like you Thankyou sir Video was amazing,True
@Varaprasad-pe3ed,2021-06-16T08:09:22Z,0,Greetings to you. What is your education qualification? Thank you.,True
@Varaprasad-pe3ed,2021-06-16T08:07:22Z,0,Greetings to you. What is your education Qualification?. Thank you.,True
@engineeringaspirants3863,2021-06-12T22:39:41Z,0,Because of gini impurity(0.5) use as default parameter that's why Decision tree or ensemble models gets overfited.,True
@awanishkumar6308,2021-05-18T16:24:24Z,0,I mean to say after 0.5 probability value both are inversely proportional to each other not directly proportional after 0.5,True
@awanishkumar6308,2021-05-18T16:23:11Z,0,Sir Krish after Probability value 0.5 Entropy is decreasing but Probability value is increasing,True
@saumyamishra5203,2021-05-15T18:21:53Z,0,"sir, i don't get the difference bcz for both graph is decreasing after 0.5 so the only diffence is calculation & range that entropy results between (0,1) & Gini results between (0,0.5)",True
@waichingleung412,2021-05-05T16:16:35Z,0,"This is great, Krish!",True
@arindamghosh3787,2021-04-22T09:54:33Z,0,"What if the target variable has 3 different categories . e.g yes, no,  and something else . What will be the formula for entropy then ?",True
@arindamghosh3787,2021-04-22T08:30:16Z,4,"There was a small error at the video when u said the entropy increases as p+ increases ,after that entropy  decreases with the decrease in p+ .I think the p+ is also increasing .  Anyways  sir great explanation and got to learn all the things clearly .",True
@shashankvashishtha9149,2021-04-09T04:02:49Z,0,so you are saying that that gini impurity is preferred just bez it is computational easy ?? can you tell nothing else to give solid answer while at the time of interview,True
@brightmindsconversation,2021-04-08T10:28:08Z,5,Gini impurity starts at 06:06. Thank me later.,True
@kalasanisatya9886,2021-02-19T02:53:14Z,0,"Could you please help me, what should we do, if we have 50:50 case. Like 3yes and 3No. I know it is worst split. Think that, I have a binary tree, at one level, my bot the nodes are giving 50:50. Like left node has 3yes and 3 no, right node has 4yes and 4 no . What to do in this case?",True
@deepakdhaka.,2021-02-16T18:21:41Z,3,"thm toh bade heavy teacher ho bhai, maja aa gaya",True
@surajpagad7759,2021-02-12T10:17:49Z,0,log (0) is zero!!!! i hope mathematicians should be able to digest it!,True
@gunjansethi2896,2021-01-07T07:36:07Z,0,Good one!,True
@shailens6056,2021-01-01T11:24:16Z,0,AT 6:56 THE SIGN SHOULD BE -VE,True
@rsivaranganayakulu6879,2020-12-08T03:16:25Z,1,"Thank you so much, good explanation Explain remaining Algorithm mathematical calculations.",True
@victorreyesalvarado8329,2020-11-05T20:50:08Z,4,"Than you so much, you are amazing! Greetings from Peru.",True
@Magmatic91,2020-10-31T02:04:53Z,0,Does using the Gini over Entropy improve the accuracy of the a decision tree model? Thanks.,True
@marcus47069,2020-10-27T04:40:00Z,4,"Hi Krish, I appreciate the video. Can I confirm whether the graph that you drew for Gini Impurity changes as the number of classes increases? Ie if you have 4 classes and a node in which each class was 25% then your GI formula is 1 - ((.25^2)+(.25^2)+(.25^2)+(.25^2)) = 1- .25 =.75. Conversely, if you had 50%, 25% and 25% in a node with 4 classes you get a GI of .625. In this situation the greater data messiness is when each classification is p*=.25 rather than .5 as in your binary example. Accordingly I'd expect the graph to peak earlier. Are there any rules to become aware of as number of classes increase?",True
@ellingtonjp,2020-10-21T22:05:19Z,2,"Wow, awesome video! Very helpful. One question: it's mentioned that we typically use Gini Impurity because it's more computationally efficient, which is definitely a plus. But are there any downsides to using it over entropy?",True
@romeojatt3492,2020-10-21T05:17:03Z,0,sir you have consider binary classification problem here but what if we have n - feature output classification problem how to calculate entropy and information gain for it and what will be the formula for it? pls reply as soon as possible thanks:),True
@soumitramehrotra5547,2020-10-18T21:50:58Z,1,Thanks for the video but I believe the intuition is still missing.,True
@lokesh542,2020-10-18T03:42:52Z,0,Great explaination,True
@adiflorense1477,2020-10-08T01:46:56Z,0,"Sir, may I ask for a video explaining the calculation of the information gain ratio at C4.5",True
@harishlakshminarayana2487,2020-10-06T14:46:15Z,2,"Hello sir, How to Information Gain calculated, when Gini Impurity is used",True
@nishidutta3484,2020-09-28T06:21:46Z,6,Sir can you please take a  small but proper dataset and show how tree is created with calculation of gini or entropy..its really hard to visualize the split this way,True
@vishalaaa1,2020-09-24T08:56:17Z,0,Excellent krish,True
@sandipansarkar9211,2020-09-17T18:47:04Z,0,Thanks Krish for the explanation.,True
@sunnyluvu1,2020-09-06T04:24:27Z,4,"Hey!! I am new to here and want to ask f1 have value c1,c2 why you consider yes No? For training we give feature value why here target value? Pls correct me.",True
@andreypavlov2410,2020-08-27T17:39:50Z,0,Thanks!,True
@dikshabhati1441,2020-08-27T02:28:57Z,0,"hello sir,I have a doubt you said that log(0/3) is 0 but this is not 0 this is undefined or we can say that it is -infinity.So now what is the entorpy here",True
@neelark,2020-08-07T04:34:56Z,1,Wow so easily explained.  I was hating maths but now with your videos i am gaining confidence and feeling it is simple..  Thanks Krish.,True
@ramarajudatla229,2020-07-08T08:03:37Z,0,thanks for nice explanation,True
@MechiShaky,2020-06-27T03:12:17Z,3,"Hi Krish, I have 2 questions Then in which case we need to use gini impurity or entropy ? What's the difference between gini impurity and gini index ?",True
@kurbanchoudhary2489,2020-06-21T03:56:23Z,0,Very nice content sir..,True
@3satech201,2020-06-10T04:59:26Z,0,@krish why should use entropy  if G.I is faster?,True
@AbhishekGupta-gb9rh,2020-06-03T19:45:33Z,0,How is gini impurity used to calculate information gain?,True
@sonamkori8169,2020-05-24T04:51:05Z,0,Thank you soooo much Sir.,True
@hamzakazmi5150,2020-05-13T11:40:48Z,0,best explanation ever,True
@mayukhdifferent,2020-04-19T08:27:50Z,0,"Hi Krish, can you please do a video on explaining the concept behind twoing in decision tree?",True
@abdellatifthabet568,2020-04-11T19:24:31Z,0,"great work krish, keep it up",True
@vasaviedukulla5141,2020-04-05T22:53:54Z,7,"thank you so much. Wonderful explanation. Your videos have been a savior at many circumstances, especially for the beginners .",True
@mizgaanmasani8456,2020-03-30T14:52:08Z,0,lovely explanation...,True
@mujeebrahman5282,2020-03-26T02:17:56Z,10,Small correction* at 5:30 as the p+ increases entropy increases till its value becomes 1 post that it starts decreasing.,True
@siddharudtevaramani1055,2020-03-09T12:13:16Z,11,If gini is simple to calculate and gets things done then why do we have the option of entropy. In what cases we use entropy over gini,True
@kedargoud6408,2020-03-07T07:01:31Z,5,"hai krish,  i got a ques in interview , the ques is ,  what is the  relationship btw gini index and gini impurity?  thanks.",True
@shubhangiatkari4023,2020-03-06T12:56:39Z,2,"Thankyou Krish ,Could you please take an Example with real features when we say node has value some yes and some No values.",True
@cutyoopsmoments2800,2020-03-06T08:59:01Z,2,"Bro, I love your lecture.",True
@cutyoopsmoments2800,2020-03-06T08:58:46Z,1,"Bro, I love you lot.",True
@saitharun3334,2020-03-06T07:30:47Z,1,"Hey krish, why gini index and entropy choose different features to split?",True
@abhishekpaul3486,2020-03-06T04:48:31Z,2,"Superb video, cleared many doubts!",True
@yogoai136,2020-03-05T15:30:30Z,1,Much awaited video Krish. Thanks a lot.,True
