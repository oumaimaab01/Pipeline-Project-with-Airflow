author,updated_at,like_count,text,public
@bikashhota3628,2024-05-17T11:43:46Z,0,"When you say information gain for one node, it has nothing to do with information gain of other nodes. How will you know 2 other nodes if you need IG for first node, sorry but its misleading and people can get confused",True
@Kai26448,2024-03-27T09:38:41Z,0,LOVE YOU I WATCHED YOUR ENTROPY VIDEO AND NOW THIS. ITS SOOO HELPFUL FOR ME SINCE I WANT TO BE A DATA SCINETIST,True
@dianafarhat9479,2024-03-14T11:47:09Z,0,Thank you!,True
@ozancck,2024-01-21T15:56:09Z,0,t√ºrk sandƒ±m... √úzd√º,True
@ooohdraft,2024-01-11T12:47:51Z,0,legend,True
@anandiyer_iitm,2023-07-27T11:10:33Z,0,9Y/5N can't get you an entropy of 0.94 as shown in the video.  It is around 0.831.,True
@theamici,2023-05-20T11:53:04Z,0,Thank you! Very good explanation!,True
@romeoanto1476,2023-05-10T16:01:47Z,0,Total mess,True
@user-uo2ry2gc8e,2023-04-20T11:09:22Z,0,ENTROPY VALUE SHOULD BE CHANGED IN INFORMATION GAIN AT LASTR STEP WHILE CALCULATION,True
@Mahmoud-ys1kt,2023-04-17T14:32:25Z,0,"Great efforts , Thanks a lot",True
@MrDoubara,2023-04-11T11:16:02Z,0,"Pleae I can't see the link to Enthropy i, where is it?",True
@humbleguy9891,2023-03-22T01:47:25Z,0,Shoaib Akhtar proud student.,True
@mbhazimangoveni3068,2023-03-06T21:02:54Z,0,"Man, you rock!",True
@hood9352,2023-02-03T02:37:50Z,0,where did the 0.91 came from ?,True
@codingworld6151,2023-01-29T04:45:08Z,0,And on robotics framework,True
@codingworld6151,2023-01-29T04:44:38Z,0,Kindly create playlist on computer vision,True
@Lucia-el6ex,2023-01-08T21:13:51Z,1,"Gracias por tus explicaciones, eres incre√≠ble! Nos ayudas much√≠simo!",True
@phoebemagdy1554,2022-12-20T00:04:21Z,0,"Thanks for the highly informative tutorials, My question to you : Is there is any option in Decisiontreeclassifier in sklearn to make a node split into three child nodes when the feature used in splitting is categorically coded as (0,1,2) for each of the three categories?",True
@mohittahilramani9956,2022-12-01T15:38:26Z,0,Sir u are a life saver ‚ù§,True
@joannewardell8396,2022-11-10T17:44:16Z,0,"How do we determine the leaf nodes? Better, how do we determine where to put the labels?",True
@spider279,2022-11-07T15:09:12Z,0,"standing ovation for his nice explanation , thanks you very much guy you're so kind",True
@bharathwajan6079,2022-10-30T18:42:09Z,0,how decision scores are choosen ?,True
@brave_v,2022-10-13T15:01:10Z,0,"Thank you for the video Krish! When RF uses Gini Index, is it just supplementing H(S) in the information gain formula with GI? In other words, does the information gain concept still applies when using GIni Index?",True
@anilmurmu6675,2022-09-17T05:42:39Z,1,Key takeaway is : if Entropy is closer to 1 or equal to 1 then it is more impure. if Entropy is 0 then it is considered to be leaf node or it is a pure split. Decision tree classification model will be chosen based on Information Gain with highest value.,True
@faizanmajeed6039,2022-09-07T06:05:19Z,0,this man is saving me in college. Thank you,True
@diegobarrientos6271,2022-08-16T09:31:15Z,0,"Thanks a lot!, very clear explanation",True
@Devpatel-oi1er,2022-08-16T05:43:48Z,1,krish why are you not applied decision tree practically on python,True
@bishwajeetsingh8834,2022-08-13T22:02:18Z,0,So clearly you made it. Thankyou,True
@rashmidhawan4783,2022-08-01T12:23:50Z,0,Hats off.... No match of you exist  ...excellent,True
@upscandssctips6922,2022-07-10T16:07:08Z,0,How come the answer is 0.81... please explain,True
@TheLtricky,2022-06-12T10:39:54Z,0,Amazing! You explain everything so well! Thank you!,True
@stevemungai3542,2022-03-15T11:59:47Z,0,These are the only tutorials I watch and understand the first time,True
@yamika.,2022-03-03T11:31:15Z,0,thank you so much,True
@adityasoni1639,2022-02-24T13:07:36Z,0,How the root node is selected..???,True
@runnsingha,2022-02-04T12:13:18Z,0,krish.... you are  a true gem,True
@originalgamer4962,2022-01-31T00:42:36Z,0,but why would we start our tree from f2 when we know the entropy of f1 is smaller than the entropy of f2 ?,True
@rahulkeerthiparameswara8511,2022-01-14T22:29:05Z,0,"hey krish, actually im doing a problem on decision tree and my gain for two attributes is same. Which one should I consider now??",True
@chandrasekhargogula7991,2021-12-16T10:55:12Z,0,Here H(S) about target variable and then take the difference for each average entropy value of   Feature from Entropy of Target to see where to split..,True
@rccgcol9404,2021-12-10T21:32:21Z,1,Thank You so much for this. A lot of confusion in the textbook but the two videos on both entropy and information gain is a lifesaver! You are the best. Just Subscribed,True
@robertelizondo7841,2021-12-09T05:40:49Z,0,The goat fr,True
@polishettysairam6466,2021-10-31T09:57:12Z,0,Is this decision tree is only for binary classification or it an be used for other classifiers and regression problems  if anyone knows reply back please,True
@naeemsutar4283,2021-10-26T06:57:56Z,0,Hii.. If Entropy comes out to be same for two or more features then what should be done or is it the case that Entropy is never same for two or more features?,True
@vishaljhaveri7565,2021-10-13T04:48:48Z,0,"Thank you, Krish sir. Nice video.",True
@sarahschlund4750,2021-10-05T03:49:27Z,0,You are amazing!,True
@TheAwesomejay,2021-09-30T02:33:24Z,1,I love the way you explain things. Very clear and easy to digest,True
@alish7872,2021-09-29T21:35:53Z,3,"man i truly appreciate you more than any of my college doctors , hope you achieve all ur dreams",True
@avinashpandey6518,2021-09-19T18:03:31Z,0,Thank you sir very helpful video for me. I will definately share with my friends as well,True
@ashiqhussainkumar1391,2021-09-14T03:01:54Z,0,Who is finding patterns of Victor laverenko here...   Anyways thanks Sir,True
@deltechdiaries5907,2021-09-12T06:34:02Z,0,very nice explanation sir!,True
@ankitchoudhary5585,2021-09-04T18:54:51Z,2,I am seeing his videos from my last 2+ years (from college days)...proud see this community and growth he made..good luck mate.,True
@nitinchan,2021-09-03T08:14:58Z,0,Very good and simple explanation Krish. Much easier to revise the concept on your channel. Keep up the good work.,True
@debjyotibanerjee7750,2021-08-23T10:07:08Z,0,"is log 2 because of the 2 classes, or is it fixed?",True
@mellowftw,2021-08-23T08:44:25Z,0,You're no less than andrew ng for me.. respect++,True
@auroshisray9140,2021-07-22T18:41:10Z,0,Beautifully explained....thank you sir!!,True
@sushiltry,2021-07-20T15:05:43Z,0,How it works using Python?,True
@lahiru954,2021-07-16T03:28:22Z,1,Great explanation. This is the best channel to becoming a perfect data scientist.,True
@louerleseigneur4532,2021-07-16T01:44:48Z,0,Thanks Krish,True
@keshabdoley1727,2021-07-14T06:53:15Z,0,Great sir,True
@shanbhag003,2021-07-12T08:38:37Z,0,"Does Decision Tree use ""one vs rest"" mechanism for calculating entropy in multi class classification ?",True
@wealth_developer_researcher,2021-06-24T05:54:32Z,0,Got the right channel to learn machine learning :) . Thanks Bro.,True
@lynch8888,2021-06-10T14:26:26Z,0,Slow Clap!,True
@chocky_1874,2021-05-29T10:23:34Z,0,"Bro, ThankYOu :)",True
@resurrectingsynergyproduct756,2021-05-22T06:40:59Z,0,Great explanation <3 Loved it <3,True
@kishore3785,2021-05-20T02:51:01Z,0,excellent Explanation Sir,True
@saitarun6562,2021-05-01T16:06:16Z,0,h(S) is 0.94 you wrote it as 0.91?why,True
@jaianushanu,2021-04-30T07:00:37Z,1,"First of all a big thanks to you as you have made learning very easy and interesting :),,,,if the information gain on one leaf node calculated as 1 and in the other leaf node calculated as 0.4(any value less then 1) then which leaf node to be considered?",True
@akshithareddy5491,2021-04-27T05:22:04Z,0,Plzz show implementation of decision tree for any dataset,True
@kushalhu7189,2021-04-25T17:44:41Z,0,As Always you are the best,True
@help2office427,2021-04-20T04:23:31Z,0,"P+ is not percentage of yes numbers,  p+ is fraction of yes numbers",True
@RishabhBisht1997,2021-03-13T16:17:23Z,0,Better explanation because of the numerical used. Absolutely Beautiful. Superlike,True
@ashwinideshmukh4920,2021-03-10T13:49:48Z,2,Hats off to you man!! Your teaching skills are amazing.,True
@scaratlas3347,2021-03-08T05:01:13Z,0,You made a mistake in the video where the you 0.91 instead of 0.94 but otherwise its all correct.,True
@adiflorense1477,2021-03-04T08:58:54Z,0,"6:54 Sir, what is the difference between the entropy of the class computed at the initial separation and the entropy of each attribute?",True
@adiflorense1477,2021-03-04T08:51:37Z,0,"2:58 Sir Krish, does the symbol P mean probability?",True
@mdiftekhar6876,2021-03-03T20:33:13Z,0,Thank you so much,True
@Koyaanisqatsi2000,2021-02-17T21:53:02Z,0,thank you!!!,True
@jovinol,2021-02-04T09:19:53Z,1,I can see his passion for machine learning,True
@tassoskat8623,2021-01-17T16:10:35Z,0,Is information gain the same as foil algorithm?,True
@michaelkareev2046,2021-01-07T00:47:18Z,5,H(s) = H(f_1) = 0.94. While in the gain formula (black marker) it's 0.91,True
@soumyaiter1,2020-12-27T14:17:15Z,3,Information gain is entropy of parent - weighted entropy of child,True
@sreelalb1729,2020-12-11T10:46:25Z,4,"Hi, Thanks for the video. While explaining entropy in the beginning section, you said P+ and P- are percentage of positive and negative values respectively. Is that correct, should it be defined as probability of positive and negative values rather than saying percentage?",True
@BytemeMaybe,2020-11-25T21:55:15Z,0,best explanation in entire youtube videos!,True
@mahroozekiani6392,2020-11-22T03:38:05Z,0,this video was very useful. But please solve a question with attributes.,True
@davidbatista3457,2020-11-04T20:59:55Z,109,Bro you are a legend and a half...Prof spent 3 weeks on this and your 12 minute video just explained this beautifully.,True
@gigyjohnson1607,2020-11-03T04:16:21Z,1,Is H(f1) = H(s)  ?,True
@dhristovaddx,2020-10-23T11:14:36Z,0,Very clear explanation. Great job on this video!,True
@jyap9675,2020-10-22T04:41:54Z,2,H(S) should be 0.94 right? instead of 0.91,True
@lokesh542,2020-10-18T03:24:47Z,5,Simply amazing loved the way you explained the concept it was really easy to understand,True
@navaneetham5303,2020-10-12T12:16:30Z,0,"hey, why do we use log2 instead of using log10",True
@marioluoni3899,2020-10-06T18:23:40Z,0,How do you calculate the IG if there are more than 2 decision levels?,True
@MercyGraceThomas,2020-09-30T03:57:42Z,0,"Good explanation,want more videos on machine Learning, thank you so much krish",True
@sahebganguly4867,2020-09-26T10:33:13Z,2,"sir, h(s) value was 0.94 but why there is 0.91 in the formula",True
@vishalaaa1,2020-09-24T07:44:42Z,0,"Naik. Your trainings are very clear and smart. 50% of the professional sare from r back ground. Can you keep r codes also in all videos. Ordering and appropriate long titling is an excellent way. One tip. Please include data types in every discussion as many functions are sensitive to data types. This gives an option to give information of alternatives. Ex: K-Mean clustering, In 99% of articles no one explains how to do this on categorical data.  R code will double the no of hits.  I wish that you will eventually start your own consulting company on data science",True
@pranithareddy6133,2020-09-20T16:23:24Z,0,"What if the class attributes are having 3 outcomes instead of 2 , yes or no!? How do we go about then!?",True
@sandipansarkar9211,2020-09-17T18:30:21Z,1,Thanks Krish. There is a slight confusion between entropy and information gain.I am sure it will be clarified in the process.,True
@sunnyluvu1,2020-09-16T15:16:43Z,0,"One issue ....tree node  has all features then we split , but what is the criteria?",True
@arnabdutta404,2020-09-11T17:32:26Z,4,"in place of f2 and f3, there should actually be the values of the feature f1 (say, yes or no / high or low). after the split we will get entropy at individual points and can calculate IG for f1. similarly IG for f2, f3 etc can be found. highest IG feature to be chosen. then go ahead with further split. @krish pl correct me if i am wrong",True
@ShiVa-jy5ly,2020-08-25T04:38:37Z,1,"Thanks sir,all sessions are very informative.",True
@samriddhlakhmani284,2020-08-16T14:40:49Z,0,https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8,True
@hellostar3063,2020-08-13T09:21:44Z,0,"Sir, the video on over fitting that you mentioned you would do isn't there in the playlist.",True
@abhinav9561,2020-08-11T10:02:09Z,0,How to calculate H(s) is not shown.,True
@patrickbateman7665,2020-08-06T20:20:22Z,1,Thanks Alot Krish :),True
@tanmayvaidya8337,2020-08-03T16:28:53Z,0,Actual working mechanism behind decision tree algorithm is clearly explained. Thanks for uploading !,True
@1988soumya,2020-07-28T06:53:02Z,17,6:36 information gain starts,True
@neprobos3246,2020-07-24T06:53:24Z,0,"Thankyou Krish, you explain everything in detail ! No words to thaankyou",True
@shindepratibha31,2020-07-20T16:37:59Z,0,"I think f1 is divided into 8yes/6no. Another thing, initially we took f1 as root node and divided into f2 and f3. If this split gives highest information gain, then we will proceed for the next split of f2 and f3. Similarly, information gain will be calculated for next split for f2 and f3 by treating f2 and f3 as root nodes and the process goes on till we reach leaf nodes. Is this understanding correct? Please reply and correct me if I am wrong.",True
@subbu621210,2020-07-19T10:01:33Z,0,Sir can you help me to find research problem to find in ml,True
@shubhamgosavi6703,2020-07-19T06:15:35Z,0,"great understanding,Than you",True
@maseedilyas203,2020-07-08T12:38:17Z,0,best tutorial on ml i could find .Thank you very much krish sir. God bless you,True
@ramarajudatla229,2020-07-08T07:37:29Z,1,thanks for nice explanation,True
@ShivamKumar-em9nr,2020-07-04T08:33:58Z,1,deserves million views,True
@adiflorense1477,2020-07-03T02:50:42Z,0,what if there are 3 labels,True
@DanishAli-lk7si,2020-06-30T13:27:20Z,0,"Hi Sir, what is A in Gain( S, A)?",True
@SteerMods,2020-06-15T17:23:24Z,0,"Arjun is a very bad actor , which kind of irritates in the middle of video.",True
@uttamchoudhary5229,2020-06-09T04:29:53Z,5,"your teaching skill is amazing ,",True
@naveenchauhanindian,2020-06-06T05:14:49Z,0,"Hi krish, it is very helpful to understand the famous paper ""Induction of Decision Trees , J.R. Quinlan"". one question in my mind, do we need to covert the features into qualitative values. if yes, than we need to generate the clusters too. If Im right, then how to decide the no. of clusters. because my data is purely quantitative in nature.",True
@PrianshuBhat,2020-06-05T06:45:09Z,0,Thanks,True
@sarabjeetsingh5033,2020-06-04T14:33:55Z,2,"Hi Krish, it would be more precise to use probability of + than percentage of +.",True
@andrewwilliam2209,2020-06-03T12:33:40Z,0,"Hey Krish, for the information gain, will they count of all the subsets until the leaf node? let's say over here we want to find information gain for the f1, but the f2 splits further into f4 f5, then will the information gain be calculated based on f2 f3 only, or will it go for f4 f5 f3?",True
@suhelalatekar3410,2020-05-31T06:04:15Z,0,@krish could you upload the whole video of you working on live projects using decision tree and random forest..that would really help,True
@sugata83,2020-05-27T12:54:41Z,0,superb video and very nicely described..thanks Krish,True
@abebeteklehaymanot2370,2020-05-27T03:58:28Z,0,"pleae answeer this type of question1.	Decision tree using Information gain method (ID3) 2.	Decision tree using Information gain ration method (C4.5) 3.	Na√Øve Baye‚Äôs Method 4.	Nearest neighbor method with 3 neighbors (use Euclidean distance measures)  Training set No	Feature1	Feature2	Feature3	Class 1	Yes	Large	High	No 2	No	Medium	High	No 3	No	Small	Low	No 4	Yes	Medium	High	No 5	No	Large	Medium	Yes 6	No	Medium	Low	No 7	Yes	Large	High	No 8	No	Small	Medium	Yes 9	No	Medium	Low	No 10	No	Small	Medium	Yes  Instances with unknown class labels No	Feature1	Feature2	Feature3	Class 11	No	Small	Low	? 12	Yes	Medium	Medium	? 13	Yes	Large	High	?",True
@sonamkori8169,2020-05-24T04:33:56Z,0,Thank you üòä,True
@jaydipnigul3159,2020-05-23T08:38:12Z,0,"Very well explained ,really helpful .ü§ó",True
@mukulsharma9673,2020-05-21T23:08:23Z,2,"You said that 0 entropy is the worst and Information Gain is actually finding the avg of the Entropy of the whole structure, then according to ur definition the lesser the information gain the better is should be but at the end u said the more the IG the better. You hv contradicted you statements.... Yes explain that correctly",True
@ravibhat2849,2020-05-21T13:38:56Z,0,Is model considered good if the Information Gain value is less?,True
@datascientist2958,2020-05-20T19:34:03Z,0,Sir can we select features using information gain prior to the decision tree. How do we implement information gain in preprocessing phase without using decision tree in sklearn?,True
@shivateja7574,2020-05-16T12:41:41Z,0,is it percentage or probability?,True
@shaiksuleman3191,2020-05-15T03:15:42Z,0,SImply Super Sir,True
@arpit8273,2020-05-14T06:37:44Z,0,great video. Keep going.,True
@saurabh.kulkarni336,2020-05-10T13:27:35Z,0,"If  instead of yes and no (i.e. 2), o/p has more options say good,bad,ugly then same formula to be used or log to the base 3 is used?",True
@bhargavasavi,2020-05-07T16:54:24Z,3,"Hi Krish, I want to mention a small correction here .... f1 has 9Y|5N, so f2 's Y and N should sum up to 9 and also f3's Y and N should sum up to 5...But they are different in both your examples :) ......but other than this your lucid explaination of concepts is quite amazing. You rock !",True
@chaitanyasonavane3871,2020-05-06T07:01:30Z,0,"Thankyou so much sir, helped alot",True
@Agrima_Art_World,2020-05-01T15:18:49Z,1,"It means to calculate information gain various structures of trees will be created.And structure with highest information gain will be taken for Decision Tree training.How it will calculate,how many structures of trees to consider to create the information gain.Basically how many combinations of trees it will create.What is decision criteria for same ?",True
@rajkumarbatchu19,2020-05-01T12:55:57Z,0,"hi krish, your videos are good, can you please make a videos on different feature selection methods i.e, filter, wrapper and embedded methods together in detail. thanks in advance",True
@hemantdas9546,2020-04-15T00:40:07Z,0,Don't get it Sir. You are only splitting F1 and getting two different sets. Where are you splitting F2 and f3,True
@adityavipradas3252,2020-04-11T00:17:25Z,1,Really appreciate your effort and the videos. Thank you very much Krish.,True
@vijayvithal9263,2020-04-10T18:42:57Z,0,Sir can u please upload video on how to explain the Algorithm in Interview,True
@sriramp8478,2020-04-02T09:40:18Z,0,Can we tell entropy as a measure of randomness??,True
@priyasri4398,2020-03-13T20:29:07Z,0,Your just awesome in teaching online,True
@hanman5195,2020-03-06T18:01:17Z,1,Ultimate Video once again Sir,True
@Brownchickonyoutube,2020-03-05T03:44:51Z,3,"Hello Sir, could you please do a project using logistic regression with strings?",True
@abhisheksurya5790,2020-03-04T06:26:17Z,2,As per my understanding the entire data is present in root node.And the split happens based on features/column values with respect to target variable by computing entropy and information gain.If one column/one feature is used at one split that feature/column  wont be used for further splitting.Correct me if i'm wrong.,True
@143balug,2020-03-04T05:49:10Z,1,"Krish, Can you make some videos on PyTorch",True
@143balug,2020-03-04T05:48:34Z,1,"Simply say Superb, thank you",True
@yashodhansatellite1,2020-03-04T04:37:21Z,1,You are awesome Krish,True
@apoorvshrivastava3544,2020-03-04T02:46:38Z,2,sir logistic regression part 3,True
@apoorvshrivastava3544,2020-03-04T02:46:10Z,1,woww sir,True
@raviirla459,2020-03-04T01:37:37Z,17,"Krish, can you please do vedios on time series analysis please...",True
@kaisersayed9974,2020-03-03T16:17:54Z,7,Sir please make more videos on web scrapping from scratch for beginners I am not from cs background..but for data science I think we should have knowledge of web scrapping...sir please make videi on this topic. You are a great teacher and you are a roll model  for me Thank you sir.,True
@AnkJyotishAaman,2020-03-03T16:17:28Z,3,Sir can you upload linear & logistics regression in details with python coding and implementations with respect to missing values interpretation & also the ML   That would be really helpful,True
@arpitcruz,2020-03-03T16:11:48Z,4,Sir complete the RNN playlist please,True
,2020-03-03T16:10:20Z,0,Goob job! :) Keep it up! Would you like to be YouTube friends? :),True
@kushshri05,2020-03-03T16:08:39Z,2,Where is tutorial 37???,True
@tanwar_rahul19,2020-03-03T16:06:16Z,2,"Hey Krish , I have done pandas and matplotlib and seaborn  what will be next please help I am confused.",True
