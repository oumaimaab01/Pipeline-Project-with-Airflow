author,updated_at,like_count,text,public
@YoussefBerro,2024-04-13T21:48:23Z,0,üî•üî•üî•üî•,True
@deepknowledge2505,2024-01-10T19:09:25Z,0,"Backprobagation with example, very simple  https://youtu.be/EQPjZwyBa1g",True
@gopikishanmahto001,2024-01-01T12:49:11Z,0,Watching this on 1st jan 2023 .. happy new year üòÇüòÇ,True
@ITSimplifiedinHINDI,2023-12-31T09:54:31Z,0,"I am learning on ""Happy New Year 2024"". üòÄ",True
@22shubhankar22,2023-12-08T14:59:03Z,0,bro i have an exam tomorrow and i guess i am doomed because all this is too much for my small brain. thanks for explaining.. idk what u did but it sounds correct.. keep it up man,True
@user-lz5vh6nk4w,2023-09-22T19:39:38Z,0,"Since the weight is shared across all folds, we only backpropagate once for each weight? From end to beginning, without updating the weight in the inbetween folds?",True
@kanishksaxena7735,2023-07-22T08:43:53Z,0,bhai aap irritate bahut krte ho,True
@nithinm.kannal467,2023-03-10T13:47:31Z,0,when it knows to end i mean it has reached global minima,True
@ashwathjadhav2859,2023-02-04T13:04:00Z,0,"Hi everyone,  can anyone tell me how does the chain rule applies on the w' weights, and how we update them? ",True
@Vijay-cz7pe,2022-11-22T13:26:42Z,0,"We are computing the gradients at every timestep right. What is the use of all these timestep gradients, as we update the weights only after 1 iteration right. Moreover the weights are the same throughout the iteration at every time step.",True
@dmg8529,2022-11-01T20:55:06Z,0,"5:30 while doing back prop after finding Loss, do we update same w 4 times, cuz it's a same w in all time steps ?.",True
@ganeshsubramanian6217,2022-08-10T05:30:50Z,0,"sir, did you missed to mention about the learning rate to be multiplied while updating the weights?",True
@RodrigoLopezF,2022-08-10T00:40:51Z,0,Thanks Krish!,True
@kingsmengames9950,2022-07-03T06:45:29Z,0,Now it 2022,True
@subhajitmondal5230,2022-06-01T11:38:56Z,0,You have said you will reach the global minima‚Ä¶. How can you prove that RNN will reach global minima‚Ä¶.any reference?,True
@namansharma9490,2022-02-09T19:26:51Z,0,"hi I have a doubt regarding the topic pls see though it  1st . when we find the weight w then we have to find it only once ?? (since weight is shared ) 2nd can u pls tell how we can calculate/update the weight w' pls reply thanks",True
@abhisai594,2022-02-03T23:27:54Z,0,"Any one answer please. So an input of 10 words are passed to RNN. After the first word is processed, we get 'y1' and also 'o1' . Now the output we get is not final. But will 'y1' this be passed through the activation layer and intermediate result or the activation happens only when the 10th word is trained?",True
@nuzhat_tasfia3431,2022-01-15T20:03:03Z,5,Only thanking you is not enough. Lots of dua for you Sir.,True
,2021-11-09T01:44:51Z,0,watching the first 10 lectures will make your process faster than ever!,True
@rajathslr,2021-10-06T18:40:59Z,0,"In this video, where is the output 'Y' defined ? is it part of the dataset where there is a collection of sentences and thier sentiments are represented as ""Y"" output column?",True
@apicasharma2499,2021-07-08T21:53:15Z,0,How do we use RNN for multivariate time series models?,True
@sachinpriya88,2021-06-26T04:54:53Z,0,Is Elman Recurrent NN or Simple Recurrent NN anonyms to each other or they r different in theory?,True
@divyanshushrivastava8451,2021-06-09T14:21:04Z,0,"So in this situation, we are not going to add the learning rate in the formula to compute the new weights? Like we were doing in the ANN back Propogation?",True
@louerleseigneur4532,2021-05-20T22:07:53Z,0,Thanks Krish,True
@ashishjain871,2021-05-17T04:40:17Z,1,Nicely done. The notation could have been slightly different to make it slightly less confusing.,True
@rijulsingh9803,2021-05-07T14:28:58Z,2,"Great explanation sir! Just one doubt, since the weights are same for inputs, do we need to propagate back through all the time sequences?",True
@apunbhagwan4473,2021-04-26T10:44:29Z,1,So much for Happy new year 2020üòÄüòÅ,True
@shahrinnakkhatra2857,2021-04-24T00:16:33Z,1,"But in Andrew NG's video from the deep learning course of coursera, there were outputs (y^) counted for each feature seperately. But you calculated one final output only. Will these two methods generate different results?",True
@suvarnadeore8810,2021-04-21T10:38:46Z,0,Thank you sir,True
@ronylpatil,2021-04-19T15:54:26Z,0,Please make a seperate video on how actually the chain rule working in back propagation and please someone help me that what will be the chain rule equation for w of x11 and w' of O1.,True
@ayushsingh-qn8sb,2021-04-19T13:14:36Z,0,So no learning rate in RNN?,True
@manishbolbanda9872,2021-04-15T19:58:22Z,1,"in the previous video, you said that weight assigned with input features that is x1*w or x2*w is the same which is represented by w (as in this video as well)  and while explaining weight update at 4:48 you said that various equations will be considerd while updating weights, if weights are same then why is it needed to update weights independently ??an quite confused with this video.",True
@sriranjaniganesan,2021-04-13T10:21:07Z,0,What is the difference between w' and w1?,True
@dheerajkumar9857,2021-03-15T17:35:53Z,2,"Krish, ""agr aap na hote to kya hota"" , i am thinking :) , amazing explanation. could you please make a video on what is loss function.",True
@saranzeb2183,2021-02-19T10:54:25Z,0,but in RCNN the backprpgation is done in time steps as opposed to other neural networks,True
@DineshBabu-gn8cm,2021-01-13T06:34:47Z,3,Is each hidden layer giving an output y^ or in final we are getting a single output y^ ?    because in the beginning of this video he told that this is single hidden layer based on different time intervals. what is w ' ' .... is w' and w' ' are same ? please anyone explain.,True
@rahuldey6369,2020-12-29T16:52:53Z,0,the gradient dL/dw=(dL/dO4) (dO4/dw),True
@harishlakshmanapathi1078,2020-12-02T10:26:34Z,0,"I have a doubt so while forward propagation the same weights are used for each time step right, but while back propagation we are talking about different weights for taking derivative and updating them, I am confused now...",True
@vatsal_gamit,2020-11-26T08:45:38Z,0,I think learning rate also should be there in that formula written.,True
@romananalytics2182,2020-11-13T13:06:18Z,2,"Great content Krish! just a suggestion, try using different color markers to explain such contents where you have existing forward propagation and explaining back propagation on top of it!!",True
@maheshbiradar374,2020-11-05T13:55:19Z,0,"can anyone clear my doubt? My doubt is "" Number of words is equal to no of hidden layers?""",True
@aravindnaidu1286,2020-10-30T05:53:38Z,0,"if we change the weights of neural network ""w"" will that weight ""w"" will change in all neural networks Plz help!!!",True
@raahulkalyaan8391,2020-10-06T03:09:29Z,7,How does weights get updated during reverse time? As he mentioned that the diagram is the same and he has drawn it only to show how it works in time frames. So is it something like for every time frame we use different set of weight? or didn't I understand the concept properly?,True
@RajibDas-kq2uz,2020-10-04T16:01:07Z,0,how will i find you w` any suggestion?,True
@pawanbisht3488,2020-09-28T08:45:43Z,0,Happy new year 2020 (seriously) [Just kidding ] love your work and appreciate your hard work.,True
@sounakmojumder808,2020-09-27T09:19:23Z,0,hi just one thing its not global minima it's a local one.......in any case,True
@alphonseinbaraj2959,2020-09-24T02:02:58Z,0,input weights and output weights are same ? I know input weights are same and output weights are same .,True
@user-zc5pd3nc6u,2020-09-19T20:34:51Z,3,Is input weight at layer t-1 and output weight at layer t-1 are the same or different? if different what is the relation?,True
@vlogwithdevesh9914,2020-09-06T14:57:18Z,0,Thus we can conclude that the only difference in weight updation b/w RNN and ANN is the learning rate absence coz it seems like rest all process is same. Please tell!,True
@commonboy1116,2020-08-04T07:08:18Z,0,Krish you are best as always,True
@donaldzhou895,2020-07-01T05:31:00Z,2,"So for each process of weight updating, the RNN updates T times the length of the sequence?",True
@Official-tk3nc,2020-06-26T07:51:07Z,1,yeah happy 2020         :(:(:(,True
@Official-tk3nc,2020-06-26T07:50:27Z,45,"if you are watching this in lockdown you are one of the rare species on the earth . many students are wasting their time on facebook, youtube, twitter, netflix, watching movies playing pubg, but you are working hard to achieve something . ALL the best ...nitj student here",True
@rohanprabhu9899,2020-06-01T16:34:05Z,1,What about the learning rate that needs to be multiplied to the partial derivative when calculating the new weights while backpropogating?,True
@FamFitFun,2020-05-26T12:02:28Z,0,short and precise :),True
@TheFirstObserver,2020-05-25T13:25:07Z,43,"I have to admit, I  was stuck on Recurrent networks for the longest time until I found your forward and backward propagation videos. Everything seems to have finally clicked. Thank you!!",True
@arjundev4908,2020-05-09T08:31:30Z,0,small confusion sir.. please clarify.. dl/dw = (dl/dyp x dyp/d04 x d04/dw) <-------- am  referring to this.. isn't d04 not dependent on dw11? if yes then why do we have to skip this part? sorry if it sounds lame..,True
@delllaptop5971,2020-05-06T17:49:39Z,2,you mentioned that w and w' is the same for all inputs and outputs in the beginning but during back propagation you state that each w and w' will change? Arent they all the same?,True
@ashishmehta2198,2020-05-01T06:45:26Z,5,Hello sir. The videos and the entire deep learning playlist is very informative. Learnt alot from you. Thank you so much.   I had a doubt.  Dont we apply learning rate while updating the weights in RNN?,True
@jakaseptiadi9845,2020-04-28T22:09:37Z,0,LSTM please,True
@sandipansarkar9211,2020-04-21T15:23:46Z,0,Great video Krish. Keep it up.Thanks,True
@yashumahajan7,2020-04-17T09:23:05Z,0,is the hidden state and hidden layer is the same also this hidden layer is same as we have the in ANN so how the dimension of this calculated,True
@islamicinterestofficial,2020-04-07T07:20:26Z,2,Sir please make a video that how the backpropagation equations are derived in RNN. Thanks Sir.......,True
@mizgaanmasani8456,2020-03-26T19:57:56Z,4,Is y^ is only dependent on w'' not on O4? why do we neglect O4 and w'' while updating the weights in  back propagation?  waiting eagerly for the answer...,True
@RanjanKumar-ue5id,2020-03-24T11:07:48Z,0,only last updated value of W will be in record ?,True
@moeshams4504,2020-03-05T23:38:10Z,0,you are the best,True
@overdrivegain,2020-02-12T01:31:38Z,5,Danke from Germany!,True
@EkNidhi,2020-01-29T14:37:54Z,0,thanku sirrr,True
@adityapatnaik6079,2020-01-05T18:15:08Z,1,YOU LOOK TIRED ! TAKE REST AND BOUNCE BACK,True
@sambitnath9853,2020-01-03T16:08:31Z,4,Great content üôè,True
@sahubiswajit1996,2020-01-03T05:08:58Z,4,"Sir, @ 3:41 I think that, It will  W`` = W`` - [ (learning rate) * (dL/dw``)  ]    But in the video, it is written that  W``= W`` - (dL/dw``)  =================> learning rate is missing     Am I right sir?",True
@adityajacob2246,2020-01-03T03:37:11Z,1,I am planning to buy a laptop for machine learning ....but confused between Mac book air and Lenovo legion y540...which will  be better?,True
@amitjajoo9510,2020-01-02T19:37:09Z,1,Thanks üòä sir,True
@akashpawar9058,2020-01-02T19:11:02Z,2,You work hard bro,True
@sanjaybalasubramanian9468,2020-01-02T18:42:25Z,1,Hlw bro... Which laptop is best for machine learning (for beginners) under 40000? Can I use it for long time (After becoming an intermediate)? Or should I upgrade?,True
@harshstrum,2020-01-02T18:39:22Z,1,thank you for the effort you are making,True
@manikantasai4766,2020-01-02T17:42:56Z,13,I defintely recommand your tutorials to data science learners who ever asks me ....before entering into this feild.  When i think to give up all.... just i go through the video then i think how easy this made ..you  motivates a me  lot  Thanks helping Sir.,True
@pankajjoshi8292,2020-01-02T17:36:54Z,2,"Happy new year , u r doin so much efforts yaar. God bless",True
@nitayg1326,2020-01-02T17:26:23Z,2,Isnt there a bias in RNN?,True
@nitayg1326,2020-01-02T17:25:07Z,6,Bought your book. Liked it so far!,True
