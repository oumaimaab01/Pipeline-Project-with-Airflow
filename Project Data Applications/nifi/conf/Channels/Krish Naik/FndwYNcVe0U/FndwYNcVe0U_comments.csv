author,updated_at,like_count,text,public
@talkingbirb2808,2024-05-26T14:53:17Z,0,"you dropped all correlated columns.... So, instead of dropping one of two correlated columns you drop both of them...",True
@miteshrajpurohit9033,2024-05-22T09:37:53Z,0,"could you please make a video on "" how auto encoder can be used to extract importance feature "" ?",True
@youcefyahiaoui1465,2024-05-16T02:57:18Z,0,You did not get into what's the role of x_test?  Can you please expand on what you do with it?,True
@youcefyahiaoui1465,2024-05-16T02:10:30Z,0,"Great tutorial, but I think you're mistaken about the abs().  You're actually considering both with abs().  If you remove abs() and you keep the > inequality then a 0.95 would be > Thresh=0.9, but -0.99 would not satisfy this condition!  If you want to remove abs(), then you need to test 2 conditions, like if corr_matrix.iloc[i,j] > +1*thesh (assuming thres is always +ve) and corr_matrix.iloc[i,j]<-1*thresh then....",True
@meshmeso,2024-04-29T07:56:44Z,0,"These are on numeric features, what of correlation between categorical features ?",True
@sivadevil4845,2024-04-23T17:32:36Z,0,"Hi @krish naik, i want to know how much data cleaning and models selection and models performance and how we can do that. I hope u will explain if u find this comment.",True
@nahidzeinali1991,2024-03-10T19:00:23Z,0,Thanks so much! very useful. you are so good,True
@suhailsnmsnm5397,2024-03-05T16:51:46Z,0,amazing teaching skills you have bhaai ...  THNX,True
@nihar.rnihar.r2053,2024-02-26T12:28:27Z,0,"sir can you please say how do we use correlation on ""RNA"" dataset",True
@dinushachathuranga7657,2024-02-25T14:52:30Z,0,Thanks a lot for very clear explanation.‚ù§,True
@channel-lk6xz,2023-12-14T06:58:05Z,0,Why not use columns but you are using feature name. Where is each one used based on.,True
@salihsartepe2614,2023-11-13T12:27:08Z,0,Thanks Krish üòä,True
@abebebelew2056,2023-09-20T15:54:49Z,0,Best!,True
@jannatunferdous103,2023-09-18T07:34:13Z,0,"Sir, what you've shown in the last of this video, in that big data project, after deleting those 193 features, how I can deploy the model? Please share a video (or link if you have in your playlist) the deployment phase after deleting features. Thanks.  ‚ù§",True
@rmrz2225,2023-07-27T22:52:01Z,0,"Hi, sorry for my question, but why is he dropping the features most correlated, it shouldnt keep those features and drop loss correlated features?",True
@user-fb3dp4bz8v,2023-07-21T06:35:31Z,0,again I wish if you explain how to handle the test set...but the explination is excellent am really gratful,True
@user-zo5ky6dk4r,2023-07-19T20:53:35Z,0,Should small values of correlation such as -0.95 be deleted or they are good to train our model and they should stay in data frame?,True
@JenryLuis,2023-07-01T04:28:36Z,1,"Hi friend, I think the correlation function is removing more than expected because when the fors loops are iterating not validate if for a value > threshold the column and index already was removed before. I corrected the function and in this case the features removed are these: {'DIS', 'NOX', 'TAX'}. Also I tested creating the correlation matrix again and verify that there is not values > threshold. Please can you check it. def correlation(dataset, threshold):     col_corr = set()     corr_matrix = dataset.corr()     for i in range(len(corr_matrix.columns)):         for j in range(i):             if abs(corr_matrix.iloc[i, j]) > threshold:                 if (corr_matrix.columns[i] not in col_corr) and (corr_matrix.index.tolist()[j] not in col_corr):                     colname = corr_matrix.columns[i]                     col_corr.add(colname)     return col_corr",True
@thecitizen9747,2023-06-27T18:05:04Z,0,You are doing a great job but can u please do similar series on categorical features in a regression problem?,True
@abdelkaderkaouane1944,2023-06-14T22:43:47Z,0,"Why you do not test if the features have Normal distribution?  Pearson correlation assume the Normal distribution, otherwise we can't confirm the correlation",True
@cocgamingstar6990,2023-05-28T09:27:00Z,0,I think  while building the model this. Feature  selected were important but you drop them üò¢,True
@elvykamunyokomanunebo1441,2023-03-25T21:50:57Z,0,"Thanks krish, You've earned a rocket point from me :)  Would have been nice, if the function also printed which feature it was strongly correlated with: because from the code you dropped all the features that met the threshold, not one was kept.",True
@lwasinamdilli,2023-03-16T09:54:26Z,0,How do you handle correlation for Categorical variables?,True
@sanketargade3685,2023-03-04T10:32:54Z,1,Why we are droping highly correlated feature after spliting train and test either it is easy to drop features from original data set and then we can simply split the dataset?‚ùìüòïü§î,True
@MominSaadAltafnab,2023-01-10T18:06:37Z,0,I didnt understood why we are just considering X_train for finding corr  you said to avoid overfitting we are doing that  but i am still not getting it like how it will be overfitted it we use all data  can someone pls tell me why we are doing that,True
@andyn6053,2022-12-19T14:33:38Z,8,"In which order should u do the feature selection steps?  0. Clean the dataset, get rid of NaN and junk values. Check format for datatypes in testset etc  1. Use z-method to eliminate outliers   2. Normalize the train_X data  3. Check correlation between x_train variables and y_train. Drop variables that have a low correlation with the target variable.  4. Use pearsons correlation test to drop highly correlated variables from x_test   5. Use variance threshold method to drop x_train variables with low variance.   All variables that have been removed from the x_train data should be removed from the x_test aswell.  6. Fit x_train and y_ train to a classification model  7. Predict y(x_test)  8. Compare the predicted y(x_test) output with y_test to calculate accuracy  9. Try different classification models and see which one performs the best (have the highest accuracy)  Is this the right order?  Have I missed something?",True
@saivardhanreddy6698,2022-11-20T15:13:58Z,0,"Why can't you perfrom correlation on entire dataset , I mean df rather than performing it on X_train",True
@World_Exploror,2022-11-20T15:05:29Z,0,Can we drop features while comparing correlation of dependent variable with independent variables by taking some threshold....!,True
@World_Exploror,2022-11-20T15:03:15Z,0,Multi collinearity has checked but what about the Correlation  of dependent vs independent variables,True
@TejusVignesh,2022-11-09T16:15:29Z,0,You are a legend!!ü§òü§ò,True
@doggydoggy578,2022-10-10T17:19:22Z,1,Hello can I ask a question ? Is Pearson Correlation the same as Correlation-based  Feature Selection ?,True
@suhailabessa9901,2022-09-29T11:44:39Z,0,"thank you sOOo much , perfect explaining :) good luck with your channel that is recomended",True
@nkechiesomonu8764,2022-09-08T07:27:18Z,0,Thanks sir for the good job you have been doing . God bless you. Please sir my question is can we use correlation on image data. Thanks,True
@pankajkumarbarman765,2022-09-01T08:19:51Z,0,Very helpful . Thank you sir.,True
@Eric-bq1jo,2022-08-07T08:17:23Z,0,Is there any way to apply this approach to a classification problem where the target variable is 1 or 0?,True
@SachinModi9,2022-08-04T05:22:03Z,0,If absolute is not used then threshold can not be 0.85. If any features are highly co-related negatively like -0.85   Still it wont qualify for the drop. Hence Absolute is necessary.,True
@kjrimer,2022-08-03T18:33:05Z,1,"Hello nice video, how to do feature selection if we have more than one target variable? i.e. In case of MultiOutput Regression problem how we can do feature selection. do we have to perform the pearson correlation individually on each of target variable or is there another convenient way that can solve the problem?",True
@drshahidqamar,2022-07-13T17:12:27Z,0,"LOL,  you are jsut amazing Boss",True
@marcastro8052,2022-07-08T06:38:38Z,0,"Thanks, Sir.",True
@neelammishra5622,2022-07-07T22:14:12Z,0,Your knowledge is really invaluable. Thanks,True
@arunkrishna1036,2022-07-06T05:26:41Z,0,Hi Krish.. how about using an VIF to find the correlated features?,True
@clintpaul6653,2022-06-22T15:30:21Z,0,"Bro i ‡¥ó‡µÜ‡¥±‡µç‡¥±‡µç error after the line.. Corr_features=correlation (X_train, 0.8) ### the error is 'dataframe' object has no attribute",True
@nmuralikrishna4599,2022-06-21T05:57:40Z,0,General Question - What if we drop few of the import features from and data and train again ? Will the accuracy drop ? or precision ?,True
@killerdrama5521,2022-06-15T09:54:24Z,0,What if we have some features numerical and some features are categorical against categorical output .. which feature section method will be helpful,True
@asha4545,2022-06-15T09:45:02Z,0,"Hello Sir  my dataset contains 17000 features, when I execute corr() its taking more than 5 minutes to execute and also for generating heatmap memory related error generating. Can you help to solve the issue?",True
@athulsuresh736,2022-05-09T16:12:54Z,1,"You shouldn't remove abs, u should consider the negative correlation as well. Also u have to check its correlation with the target to decide which feature to be removed. Please pull down this video or correct it or else people will study wrong things.",True
@niveditawagh8171,2022-05-09T11:16:39Z,0,Nice explanation.,True
@ajaykushwaha-je6mw,2022-04-29T06:08:36Z,0,Hi everyone i need one help. this technique to select numerical features only. Suppose we have done one hot encoding on categorigal data and converted into numerical then can we apply this technique on that features as well(entire data set with numerical column and categorical column converted into numerical with some encoding technique.)  Kindly help me to understand.,True
@gabrielegbenya7479,2022-04-26T18:53:04Z,0,great video.  very informative and educative. Thank you,True
@erneelgupta,2022-04-15T16:12:51Z,0,"what is the importance of random_state in train_test split ? How the values of random_state (0,42,100 etc.) affect the estiamation???",True
@moizk8223,2022-04-12T09:57:10Z,1,I have a doubt. Suppose if A and B have correlation greater than threshhold and the loop includes column A from the pair. Further B and C are highly correlated(although C is not highly correlated with A)and the loop includes B in the list. Now if we drop A and B wouldn't that affect the model as both A and B will be dropped?,True
@vorems3910,2022-04-04T10:19:05Z,0,hey! if possible can you share the link to the source code!!,True
@siddharthjain4361,2022-03-13T04:34:18Z,0,negative correlated varibales kyon hata diye bhai ne??,True
@nurnasuhamohddaud728,2022-02-28T07:40:22Z,1,Very comprehensive explanation for someone from non AI background. Thanks Sir keep up the good work!,True
@hirakaimkhani3338,2022-02-24T21:05:59Z,0,wonderful tutorial sir!!,True
@rafibasha4145,2022-02-20T01:13:38Z,0,"Hi Krish,how to check in case of categorical variables",True
@md.younusahamed4969,2022-01-22T23:17:47Z,0,Where did the MEDV come from?,True
@perumalelancgoan9839,2022-01-16T09:45:15Z,0,please clear it the below if any independent variables are highly corelated we shouldn't remove them right because its give very positive outcome,True
@niklausmikealson3115,2022-01-09T17:46:27Z,0,Can't we find correlation for categorical columns?,True
@MrCaglar1993,2021-12-15T20:09:09Z,0,"How many ""particular"" or ""particularly"" says these indian guys, just a world record..",True
@josephmart7528,2021-11-27T03:53:39Z,0,"The abs takes care of both positive and negative numbers. If not specified, the function will only take care o positively correlated features",True
@StanleySI,2021-11-24T15:40:03Z,0,"Hi sir, there's an obvious flaw in this approach. You can't drop all correlated features, but only some of them. e.g. perimeter_mean & area_se are highly correlated (0.986507), and they both appear in your corr_features. However,  you can't drop all of them because from pairplot, you could see perimeter_mean has a clear impact on the test result.",True
@waytolegacy,2021-11-23T12:16:46Z,81,"I think instead of dropping ""either of"" 2 highly correlated features, we should check from both of them how each of them correlates with the target as well and then drop the less correlated with the target variable. Which might increase some accuracy instead of considering dropping whichever comes first. Again, I think it is.",True
@abhishekd1012,2021-11-16T06:43:37Z,2,"In this video it's said negatively correlated features are both imp. lets take an example, when we have both percentage and ranks in a dataset, for 100% we have 1 in rank and 60% lets say 45(last) in rank. both resemble the same importance in the dataset. So what I think is we can remove one feature among those 2 features, otherwise we will be giving double weightage for that particular feature. Hope someone can correct this if I was wrong.",True
@antoniodefalco6179,2021-11-14T20:19:52Z,0,"thank you, so usefull, good teacher",True
@nishadseeraj7034,2021-11-12T12:03:22Z,0,"Can someone explain how the 2nd for loop is working? I am not getting it. For instance ""for j in range(i)"", wouldn't that give an error when i=0 for the first iteration of the first for loop when i=0, unless I am missing something?",True
@SuperNayaab,2021-11-05T18:58:36Z,0,watching this video from Boston (BU Student <3),True
@laxmanbisht2638,2021-11-01T05:46:34Z,3,"Hi, thanks for the lecture. What if we have a dataset in which categorical and numeric features are present. Will pearson's correlation be applicable?",True
@rahuldevnath14792,2021-10-01T14:58:54Z,0,"Krish, can we not use VIF for collinearity?",True
@shivarajnavalba5042,2021-09-17T18:28:40Z,0,"Thank you Krish,",True
@JithendraKumarumadisingu,2021-09-12T17:57:41Z,0,Great tutorial it helps a lot thanks @Krish Sir,True
@Learn-Islam-in-Telugu,2021-09-06T22:51:19Z,0,The function used in the example will not deliver high correlation with the dependent variable. Because at the end you dropped the columns without being checking the correlation with dependent variable.,True
@abinsharaf8305,2021-08-31T15:39:14Z,2,"since we are giving only one positive value for threshold, the code abs allows check for both negative and positve values with threshold, so i feel its better if it stays",True
@saurabhdaund8475,2021-08-11T17:30:57Z,0,So then how to use VIF and for what VIF is used?,True
@teenamadhu7883,2021-08-09T14:49:26Z,0,How to get the name of the column which is highly correlated to the given column. Please help,True
@christianbargraser2403,2021-07-18T21:59:33Z,0,"Here is a numerical example for why you would need abs:  By removing correlated features, you are essentially getting rid of redundant data. It is redundant in the sense that they tell you the same thing (if you know one variable, you know the value for the other variable). Let's say that you have 2 features, x an y.     x = [1, 2, 3, 4, 5] y = [2, 4, 6, 8 , 10]    When x is 1, y is 2 y = x*2 y = 1*2 y = 2    When x is 2, y is 4 y = x*2 y = 2*2 y = 4    When x is 3, y is 6 y = x*2 y = 3*2 y = 6    Since we know that there is a strong direct relationship (strong positive correlation) between x and y, we don't need to include them both in our feature set. If we know x, we know y, so why include them both in the feature set? If we include x and then include y, y is not adding any new, useful information.   The same is true when there is a strong inverse relationship (strong negative correlation) between x and y. Let's assume the following values for x and y:    x = [1, 2, 3, 4, 5] y = [-2, -4, -6, -8 , -10]    When x is 1, y is 2 y = x*-2 y = 1*-2 y = -2    When x is 2, y is 4 y = x*-2 y = 2*-2 y = -4    When x is 3, y is 6 y = x*-2 y = 3*-2 y = -6    Since we know that there is a strong inverse relationship (strong negative correlation) between x and y, we don't need to include them both in our feature set. If we know x, we know y, so why include them both in the feature set? If we include x and then include y, y is not adding any new, useful information.",True
@RandevMars4,2021-06-24T15:22:12Z,0,Well explained. Really great work sir. Thank you very much,True
@pratikjadhav1242,2021-06-16T18:07:59Z,1,We cheak the correlation between inputs and the output so why you drop output column and then cheak correlation we use a VIF (variance inflection factor) to cheak the relationship between  inputs and the threshold value is preffer 4.,True
@oladosuoladimeji370,2021-06-01T16:23:02Z,0,How can correlated features be selected for a multi label learning task especially in images,True
@waatchit,2021-06-01T00:53:46Z,0,Thank you for such a nice explanation. Does having 'abs' preserve the negative correlation ??,True
@aritratalapatra8452,2021-05-24T14:04:00Z,0,"If I have 3 correlated columns, I should drop 2 out of 3 right ? why do you drop all correlated features from training and testing set ?",True
@mgfg22,2021-05-24T09:41:16Z,0,"Why you don't use corr_features = correlation( X , 0.7 ) instead of X_train. (Please look at 08:22)",True
@Gsaurabh98,2021-05-11T19:29:48Z,0,"Hey Krish, how would you select one feature out of two if they have same correlation coefficient wrt target ? lets say: x1 = 0.89 and x2=0.89 which one to choose here?",True
@rhevathivijay2913,2021-05-05T10:26:56Z,1,"Being in a teaching profession ,I assure this is the best explanation about Pearson correlation.. Please make more likes.",True
@mrunalsrivastava2015,2021-05-05T05:28:33Z,0,can we compute correlation between two rows of a single matrix??,True
@kalvinwei19,2021-04-22T08:29:49Z,0,"Thank you man, good for my assignment",True
@marijatosic217,2021-04-15T12:35:04Z,0,"What do you think about feature reduction using PCA, looking for a correlation between each feature and principal components, and then using those who have the most number of correlation that is great than 50% (or any other)?",True
@anikashetty3001,2021-04-08T14:39:52Z,0,"am getting a error while i defined corelation  def correlation(dataset,threshold):     col_corr=set()     corr_matrix=dataset.corr()     for i in range(len(corr_matrix.columns)):         for j in range(i):             if (corr_matrix.iloc[i,j]) > threshold:                 colname=corr_matrix.columns[i]                 col_corr.add(colname)     return col_cor     It ran successfully  corr_features=correlation(X,0.7) len(set(corr_features))--> for this am getting --------------------------------------------------------------------------- NameError                                 Traceback (most recent call last) <ipython-input-48-a82e63e22ba0> in <module> ----> 1 corr_features=correlation(X,0.7)       2 len(set(corr_features))  <ipython-input-46-ea0616a56e68> in correlation(dataset, threshold)       7                 colname=corr_matrix.columns[i]       8                 col_corr.add(colname) ----> 9     return col_cor  NameError: name 'col_cor' is not defined",True
@alphoncemutabuzi6949,2021-04-05T08:26:39Z,19,I think the abs is important since it's like having two rows one being the opposite of the other,True
@antonyjoy5494,2021-03-14T08:14:46Z,0,"Sir, I have a query regarding this...features which are highly correlated gives same information right or they are duplicate feature. where does it in this code remove only the duplicate features?? From the code I feel like  this is removing all the features which shoiws a value above threshold..",True
@ActionBackers,2021-03-02T05:56:33Z,1,This was incredibly helpful; thank you for the great content!,True
@levon9,2021-02-28T21:06:51Z,0,"Two quick questions:  (1) Why not remove redundant features, ie highly correlated variables, from X before splitting it into training and test? What would be wrong with this approach? (2) If one features variable is correlated with a value of 1 and another variable with a value of -1 with regard to a given feature, are these also considered redundant?",True
@ankitmahajan3674,2021-02-19T17:58:24Z,0,Hi Krish while removing the correlated features we haven't checked that the independent variable is corelated to dependent variable. As you said in staring we should not remove the features that are highly correlated to dependent variables  so while generating the heatmap should we include the dependent variable also ? let me know if my understanding is correct?,True
@Egor-sm4bl,2021-02-16T10:15:18Z,0,Perfect defence on 3rd place!,True
@tron_town,2021-02-15T19:20:36Z,0,default for corr( ) is pearson which requires normal distribution.  Does it matter to check if all these columns are normally distributed before using the heatmap to figure out which features to drop?,True
@siddhantpathak6289,2021-01-31T16:11:45Z,1,"Hi Krish, I checked it somewhere and I think if the dataset has perfectly positive or negative attributes then in either case there is a high chance that the performance of the model will be impacted by Multicollinearity.",True
@kjayeshnaidu6012,2021-01-29T14:10:39Z,0,Sir before dropping we should also check the correlation of that feature with the target value as well ?? Anyone please help with this,True
@laveylukose5350,2021-01-27T15:00:01Z,0,How will u do feature selection for categorical input data?,True
@conceptsamplified9332,2021-01-22T15:43:09Z,0,"Of the highly correlated columns, Should we not keep one of the columns in our X_train dataset?",True
@amitmodi7882,2021-01-21T10:36:49Z,0,Wonderful explanantion. Krish as mentioned in video you said you upload 5-6 videos for feature selection. Can you please share the link for rest of them.,True
@siddharthdedhia11,2021-01-17T01:05:45Z,1,Sir you are dropping all the correlated features. Shouldn't you keep 1 of them?,True
@omi_naik,2021-01-11T10:38:05Z,0,Great explanation :),True
@venkatk1591,2021-01-09T18:55:00Z,0,Do we need use the entire datasets for correlation testing. Are we not missing something by considering the train set only?,True
@raghavkhandelwal1094,2020-12-27T17:07:41Z,0,waiting for more videos in the playlist,True
@KnowledgeAmplifier1,2020-12-26T09:27:11Z,7,"I want to point out a veryyy important concept which is missing in this video discussion: Suppose 2 input features are highly correlated then it's not like that , I can drop any between those 2 , then I have to check which feature between those 2 has weaker correlation with output variable , that one has to be dropped.",True
@souvikghosh6509,2020-12-24T05:09:45Z,0,"Sir, kindly make a video on embedded methods of feature selection..",True
@phyuphyuthwe670,2020-12-09T16:41:43Z,0,"Dear teacher, May I ask a question? In my case, I want to predict sale of 4 products with weather forecast information, season and public holiday one week ahead. So, do I need to organize weekly based data? When we use SPSS, we need to organize weekly data, how about Machine Learning? I feel confused for that. In my understanding, ML will train the data with respect to weather information. So, we don't need to organize weekly data because we don't use time series data. Is it correct? Please kindly give me a comment.",True
@chineduezeofor2481,2020-12-09T03:31:28Z,0,Another great video!!!,True
@tigjuli,2020-11-28T21:45:37Z,0,Nice! please upload more on this topic!! thank you!,True
@keshavsharma-pq4vc,2020-11-28T09:11:45Z,0,Sir when you will Upload Next video of this playlist (Feature Selection),True
@sukanyabag6134,2020-11-12T05:27:24Z,1,"Sir, the videos you uploaded on feature selection helped a lot ! , Please upload the rest tutorials and methods too! Eagerly waiting for it !",True
@ashishkulkarni8140,2020-11-05T07:53:12Z,13,"Sir, could you please upload more videos on feature selection to this playlist? It is very amazing. I followed all the videos from feature engineering playlist. You are doing a great work. Thank you.üôèüèª",True
@kvsubbaiahsetty9834,2020-10-26T13:58:04Z,2,"The highly correlated negatives also need to be removed, we not all removing all negative, but only highly related negative ones similar to highly positive related ones . so the abs should be used.",True
@arjundev4908,2020-10-20T14:26:04Z,0,"Hi folks,    The below code for finding multicolinearity using variance inflation factor.. if this is of any help to anybody..!!   Train_X_Copy = Train_X.copy() High_VIF_Col_Names = [] counter =1 Max_VIF = 10  while (Max_VIF >= 10):     print(counter)          VIF_Df = pd.DataFrame()     VIF_Df['VIF'] = [variance_inflation_factor(Train_X_Copy.values,i) for i           in range(Train_X_Copy.shape[1])]     VIF_Df['Column_Name'] = Train_X_Copy.columns          Max_VIF = max(VIF_Df['VIF'])     Temp_Column_Name = VIF_Df.loc[VIF_Df['VIF'] == Max_VIF,'Column_Name']     print(Max_VIF,"":"", Temp_Column_Name)          Train_X_Copy.drop(Temp_Column_Name,axis =1,inplace =True)     High_VIF_Col_Names.extend(Temp_Column_Name)          counter = counter + 1 1 85.36380606700601 : 10    PTRATIO Name: Column_Name, dtype: object 2 77.50601814846476 : 4    NOX Name: Column_Name, dtype: object 3 54.17276515454995 : 8    TAX Name: Column_Name, dtype: object 4 38.20240177407496 : 4    RM Name: Column_Name, dtype: object 5 14.288969852667897 : 4    AGE Name: Column_Name, dtype: object 6 9.92439187811757 : 6    B Name: Column_Name, dtype: object  High_VIF_Col_Names Out[456]: ['PTRATIO', 'NOX', 'TAX', 'RM', 'AGE', 'B']",True
@androixde,2020-10-18T04:48:14Z,0,good video my friend <1+2,True
@skfaizannasir4834,2020-10-12T16:21:51Z,0,You could have done the feature selection first and then split it into train and test data. Would've required less effort. :),True
@vivekkumargoel2676,2020-10-09T17:04:34Z,0,sir when the remaining videos on features selection are coming,True
@bishwasarkarbishwaranjansarkar,2020-10-09T03:36:25Z,0,Hello Krishna thanks for your video but along with please explain real life use as well. Where can we use in real life.,True
@yashkhant5874,2020-10-08T17:19:39Z,0,GREAT CONTRIBUTION SIR....  THIS CHENNAL SHOULD 20M SUBSCRIBERü§òü§ò,True
@anandruparelia8970,2020-10-07T07:01:07Z,0,"If anybody can answer my doubt, We checked the independent features which are highly correlated with each other, We got a list of those! But Why are we removing all of them? Like if there are X highly correlated independent features, we need atleast 1 of them right for prediction purpose?",True
@amarkumar-ox7gj,2020-10-07T05:02:11Z,0,"If idea is to remove highly correlated features, then both highly positive and negative correlation should be considered!!",True
@parms1191,2020-10-02T20:26:57Z,4,"I write the threshold code simply like [df.corr()>0.7 OR df.corr()<-0.7]. Also, I sometimes feel 0.5 is a good threshold but again that's subjective. And would be great to see other Correlations explained as well.",True
@ireneashamoses4209,2020-10-02T17:36:46Z,1,Great video!! Thank you!üëçüëçüíñ,True
@rukmanisaptharishi6638,2020-10-02T16:49:11Z,32,"If you are transporting ice-cream in a vehicle, the number of ice-cream sticks that reach the destination is inversely proportional to temperature, higher the temperature, lesser are the sticks.  If you want to effectively model the temperature of the vehicle's cooler and make it optimal, you need to consider this negatively correlated features, outside air temperature and number of ice-cream sticks at the destination.",True
@deepanknautiyal5725,2020-10-02T16:04:50Z,1,Hi krish please a make a video on complete logistic regression for Interview preparation,True
@prabhusantoshpanda5259,2020-10-02T15:13:26Z,0,"While dropping the columns using the list of all corelated columns arent we deleting all of them and not even retaining the ones we actually want.  for example, suppose we get 3 corelated columns in the list. and then apply, corelated_columns=[f1,f2,f3]    : corr>0.8  for e.g x_train=x_train.drop(corelated_columns,axis=1) then all 3 are getting dropped whereas we want only 2 to drop and retain one??  Please clarify.",True
@yasharthsingh805,2020-10-02T14:33:30Z,2,"Sir , can you please tell which website should I refer if I want to start reading white papers.... Please please do reply....I follow all ur videos!!",True
@aayushdadhich4840,2020-10-02T12:38:09Z,1,"Should i practice by writing my own full code including the hypothesis functions, cost functions, gradient descent or fully use sklearn?",True
@gurdeepsinghbhatia2875,2020-10-02T11:03:17Z,8,"I think it all depends on domain that whether to involve the neg corr or not , or we can train two diff models and compare their scores , Thanks Sir",True
@piyushdandagawhal8843,2020-10-02T11:02:59Z,1,"Instead of doing X_train , x_test split, if we find correlation of the whole data and then we compare correlated column's correlation with the dependent feature and then drop only those features among the correlated columns which are less correlated?....does my question makes sense? if it does, would it affect the model?",True
@adiuke1666,2020-10-02T10:02:39Z,1,"At 4:14, Krish Sir tried to get naughty by saying 12""inches""....",True
@prakash564,2020-10-02T09:13:55Z,19,Sir your channel is  a perfect combination of sentdex and statquest. You are doing a great work üôåmore power to you!!,True
@abdelazizhimmi5244,2020-10-02T08:45:54Z,1,"Hey Krish, is the iNeuron Platform available just in India cuz i'm from Morocco and i've already subscribed but no answer from you or your team , what should i do ?  thanks for help",True
@shubhambhardwaj3643,2020-10-02T08:21:23Z,3,Any word is not sufficient to thank you for your work sir ....üôèüôè,True
@megalaramu,2020-10-02T07:27:52Z,1,"Hi kris, in multicollinearity conceps we have both corrlation matrix as well as VIF to remove the collinearity. Which method is best or does that depend upon data",True
@amitrout5419,2020-10-02T07:13:38Z,1,Sir what if we compute the correlation before we split the data and drop the features ?,True
@vijaytogla2614,2020-10-02T07:10:28Z,1,Plzzz help sir..,True
@suneel8480,2020-10-02T07:10:15Z,3,Sir make video on how to select features for clustering?,True
@vijaytogla2614,2020-10-02T07:10:01Z,1,Sir i m in lot of trouble..how i can talk to u...help me out..üôèüèºüôèüèºüôèüèº,True
@vijaytogla2614,2020-10-02T07:09:17Z,1,1 st comment..,True
