author,updated_at,like_count,text,public
@AyanChowdhury01,2022-09-02T18:54:13Z,0,"In my opinion,  Kmeans clustering uses number of clusters on the k value. It is just formation of non overlapping clusters which are similar to one another. Hence test data can be predicted.  But for agronomic clustering, it divides the data into bottom up or top down way. That is single cluster is created and natural hierarchy is formed as set of tree. On predicting test data the labels on which the tree or hierarchy can be formed in unknown.  Similar thing goes in DBScan clustering, it creates clusters on the theme of reachability and connectivity which is a density function. Here epsilon is very important in finding the clusters. If we take the same labels for predicting test data it may include all noises and would be highly ineffective.",True
@zeviersunilseema5502,2022-09-02T18:42:48Z,1,"I think it is because When you use k-means clustering you define the k i.e. the number of centroids i.e. also the number of clusters you want your k-means model to divide your dataset into.  So the number of clusters the k-means model divides the X_train dataset into is equal to the number of clusters the k-means model divides the X_test dataset into as the number of clusters/centroids (k) is given by us to the model. This is the reason why we are able to predict when using k-means clustering. Now when it comes to dbscan or agglomerative clustering It is the model/algorithm that gives us the numbers of clusters it has  divided our dataset into. So the number of clusters the model divides the X_train dataset into may or may not be equal to the number of clusters the model divides the X_test  dataset into as both of these dataset i.e. X_train and X_test are two different datasets. And this is the reason why we are not able to predict when using dbscan and agglomerative clustering. So to conclude, the reason that we are able to predict in k-means clustering is that the number of clusters formed in the train dataset is equal to the number of clusters formed in the test dataset when used k-means clustering  And the reason we are not able to predict in dbscan and agglomerative clustering is that the number of clusters formed in the train dataset may or may not be equal to the number of clusters formed in the test dataset when used dbscan or agglomerative clustering.",True
@the_man_without_fear__,2022-09-02T17:18:45Z,0,"Sir, which will be better to me in terms of Salary and opportunities? Software developer v/s Data Scientist? I am third year Engineering student from tier-3 college...",True
@souvikadhikary214,2022-09-02T17:09:29Z,6,"In my opinion The way of computing clusters makes all the difference: 1. Firstly in K-means, a data-point is assigned to the cluster with the closest centroid and then the centroids are updated afterwards. So, predicting in the K-means is actually assigning the datapoints without updating the clusters, given that the prediction data are from the same distribution of data as the training set. 2. DBSCAN on the other hand finds the cluster based on the high-density areas of the dataset. These densities are parametrized by parameters epsilon and min-points. This is done by computing point properties i.e whether the point is a core, border and noise or outlier. Now during predicting if the new points are close to, say a noise and core point, the point could mess up the previous clustering results and make the noise point a border or within epsilon. This could also be the case with (min-points-1) new points added within the epsilon of noise, which again will recompute the cluster. 3. Same as DBSCAN, Agglomerative (bottom-up approach) clustering iteratively builds the cluster starting from single data points and successively agglomerates pairs of cluster until all the clusters are merged into a single cluster according to a linkage measure. Let the linkage measure be ""single"", which merges the clusters if all the points between two cluster are less than a threshold. Now adding a new point during prediction could trigger a merge between two clusters that would have been separated otherwise. Thus predicting here also requires recomputing.",True
@nadmaanfazeel2691,2022-09-02T16:38:31Z,0,"DBSCAN doesn't initialize centers because there is no centers in DBSCAN ( it works on connectivity of points).  Kmeans family perform classification( like KNN algorithm) using the previous cluster center and then update the centers.   Solution to this problem :-  In R, there is a predict function under DBSCAN class , first it will cluster the data then we have the dataset of data points vs label , here it becomes a classification problem when new points come we will assign a label based on our classification model.   Drawbacks of kmeans  1. Can't deal with periodic data ( angle, hours etc)  2.prone to outlier  3. Require prior cluster information  4. Performed badly on high dimensional data  5. Clusters are always in spherical form and not of arbitrary shape.",True
@abhishekchatterjee4805,2022-09-02T16:11:08Z,10,"In case of Kmeans it uses distance from centroid as a measure of similarity and due to this metrics it can compute distance of a unknown data point to the closest centroid and assign a label to it.  For hierarchical clustering algos, the problem statement is to figure out all possible sets of cluster which it does in a interative manner either starting from top / bottom and dividing / fusing data points using affinity and linkage method. So for a new data point it can not predict which cluster as it does not have a measure to do so other than recomputing entire clustering which is again the fit method.  Same goes for spatial ones as they measure similarity / disimilarity using density estimation so it can not predict a label for a single data point hence no predict method.",True
@ayushanchal,2022-09-02T15:59:48Z,0,"In k-means, new data point is assigned to the closest centroid  But in hierarchical clustering, it builds clusters iteratively starting from single data point,  if a new data point comes it will recompute the clusters an hence can entirely modify the final cluster.",True
@harshmaheshwari8850,2022-09-02T15:44:27Z,0,for agglomerative it must be recomputational approach as top to down  which consume lots of work for algorithm.........,True
@jeetpranav2304,2022-09-02T15:40:37Z,2,"""Dibbe me dibba, dibbe me cake Hamare Krish bhai laakho me ek "" ðŸ”¥ðŸ”¥ ðŸ”¥",True
@harshmaheshwari8850,2022-09-02T15:40:30Z,0,in case of dbscan....it do it with density approach and single data point cannot be the estimator of density ...that's why prediction is not the case,True
@Victory-Vindaloo,2022-09-02T15:37:36Z,1,Sir can you please tell a method in which data is provided in text files one by one like all the features have their name in rows and only one value for each feature is present in text file and 1000 of text files like this are there so how to merge all the text files in one csv file with features as columns,True
@DataLift,2022-09-02T15:25:32Z,0,Main interview questions are do all these things without sklearn or other libraries do these thing with only python code ðŸ˜…,True
