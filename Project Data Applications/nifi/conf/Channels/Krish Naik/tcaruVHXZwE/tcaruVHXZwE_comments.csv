author,updated_at,like_count,text,public
@priyankabanda7562,2024-05-12T03:15:19Z,0,exact question asked in an interview,True
@yajingli4384,2023-08-26T22:26:21Z,0,Ridge & Lasso regression tutorial: https://www.youtube.com/watch?v=9lRv01HDU0s&ab_channel=KrishNaik,True
@venkivtz9961,2022-07-01T10:17:58Z,0,PCA is the best in some cases of multicollinearity problems,True
@harvinnation3027,2022-02-20T15:21:50Z,0,i didnt know post malone is into data analysis !!,True
@K-mk6pc,2021-11-26T17:44:35Z,0,Stoic Gradient Descent is a type where the Feature Values are Taken randomly Unlike the other Type of Gradient Descent where the global minima is found out  after training the Entire Model.,True
@Arjun147gtk,2021-10-31T13:29:31Z,0,is it recommended to remove highly negative  correlated.,True
@BhanudaySharma506,2021-10-17T17:54:45Z,0,"In case of multicollinearity, why there is no mention of PCA?",True
@louerleseigneur4532,2021-08-15T11:13:03Z,0,Thanks Krish,True
@surajshivakumar5124,2021-07-28T16:18:04Z,0,We can just use variance inflation factor right?,True
@AnotherproblemOn,2021-01-28T01:15:57Z,0,"You're simply the best, love you",True
@rahuldey6369,2021-01-09T14:21:43Z,1,"@2:26 could you please explain what disadvantage can it cause to model performance? I mean, what if I remove correlated features,will my model performance increase or stays the same?",True
@haneulkim4902,2020-12-04T16:27:16Z,0,"When u are using small dataset and x1,x2 are highly correlated, drop which one?",True
@sridhar6358,2020-11-23T10:03:16Z,0,"Lasso and Ridge Regression - precondition is that there should not be multicollinearity, if we see linear relationship between the independent variables like how we see it with dependent and independent variables we call it multicollinearity which is not the same as correlation",True
@ElonTusk889,2020-10-20T06:26:02Z,0,Actually,True
@swatisawant7632,2020-10-19T14:34:46Z,0,Very nicely explained!!!,True
@GauravSharma-ui4yd,2020-09-17T10:17:57Z,1,Ever wondered what is the effect of multicollinearity on estimates and predictions of linear regression. This notebook will walk you through 4 experiments showing and explaining the same.  https://www.kaggle.com/gauravsharma99/effect-of-multicollinearity-on-linear-regression,True
@sathwickreddymora8767,2020-08-19T13:41:36Z,0,"Let's assume we are use a MSE cost function Gradient Descent -> It takes all the points into account for computing the derivatives of the cost function w.r.t each feature which tells the right direction to move. It is not productive if we have a large number of data points. SGD -> It computes the derivatives of the cost function w.r.t each feature based a single or some subset of data points and moves in that direction pretending it was the right direction. So, it decreases much of the computational complexity.",True
@Sunilgayakawad,2020-08-16T16:31:17Z,0,Why multicollinearity reduces after standardization?,True
@ganeshprabhakaran9316,2020-07-17T13:29:08Z,0,That was an clear explanation ..Thanks Krish.. Small request can you make a video for feature selection using atleast 15-20 variables based on multicollinearity for better understanding by practice..,True
@ashum6612,2020-05-12T13:13:19Z,0,Multicollinearity can be completely removed from the model. (True/False). Give reasons.,True
@alipaloda9571,2020-03-24T09:59:17Z,0,How about to use box cox technique,True
@brahimaksasse2577,2020-03-22T09:59:53Z,1,GD algorithm uses all data for updating weights when optimising loss function in BP algorithm. However SGD uses a sample data at each iteration.,True
@dragonhead48,2020-03-22T04:08:01Z,2,@krishNaik  you can add the links for lasso and ridge regularization techniques in this current video.  That would be helpful and beneficial for both parties as well  I think.,True
@gaugogoi,2020-03-21T16:47:23Z,1,"Along with correlation heatmap and lasso & ridge regression ,can VIF is another option to figure out multicollinearity ?",True
@thunder440v3,2020-03-21T11:18:38Z,0,So helpful video ðŸ™â˜ºï¸,True
@thunder440v3,2020-03-21T11:15:38Z,0,So helpful video ðŸ™â˜ºï¸,True
@shoraygoel,2020-03-21T10:20:11Z,0,Why can't we remove features using correlation when there are many features?,True
@ShivShankarDutta1,2020-03-21T09:19:25Z,10,"GD: Run all samples in training to do a single update for all params in a specific iteration SGD: Only one or subset of training sample  from training set to update parameter in a specific iteration GD: If sample/features are larger  it takes much time in updating the values SGD: It is faster because there is one training sample SGD conveges faster than GD.",True
@prathmeshbusa2195,2020-03-21T05:28:17Z,0,hello i am not able to do payment of 59rupees to join u r channel i tried with all the possible bank card but it always fails,True
@swethakulkarni3563,2020-03-21T02:05:07Z,1,Can you make a video for Naive Bayes in detail?,True
@bharathjc4700,2020-03-20T17:52:55Z,7,"multicollinearity may not be a problem every time. The need to fix multicollinearity depends primarily on the below reasons:  When you care more about how much each individual feature rather than a group of features affects the target variable, then removing multicollinearity may be a good option If multicollinearity is not present in the features you are interested in, then multicollinearity may not be a problem.",True
@bharathjc4700,2020-03-20T17:49:31Z,2,"In batch gradient descent, you compute the gradient over the entire dataset, averaging over potentially a vast amount of information.  It takes lots of memory to do that. But the real handicap is the batch gradient trajectory land you in a bad spot (saddle point).  In pure SGD, on the other hand, you update your parameters by adding (minus sign) the gradient computed on a single instance of the dataset.  Since it's based on one random data point, it's very noisy and may go off in a direction far from the batch gradient.  However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minima GD theoretically minimizes the error function better than SGD. However, SGD converges much faster once the dataset becomes large.That means GD is preferable for small datasets while SGD is preferable for larger ones..",True
@ShubhanshuAnand,2020-03-20T17:44:03Z,0,SGD: Picks k random sample from n samples in each iteration whereas GD considers all n samples.,True
@DionysusEleutherios,2020-03-20T17:14:09Z,2,"If you have a a large feature space that contains multicollinearity, you could also try running a PCA and use only the first n components in your model (where n is the number of components that collectively explain at least 80% of the variance), since they are by definition orthogonal to each other.",True
@sarveshmankar7272,2020-03-20T17:01:39Z,2,Sir  can we use pca to reduce multicollinarity if we have suppose more than 200 columns??,True
@ManishKumar-qs1fm,2020-03-20T16:59:36Z,0,Sir plz upload eigenvalues & eigenvector video,True
@adityay525125,2020-03-20T16:46:08Z,0,"SGD uses a variable learning rate and hence is better imo. I do not know the answer though, still a noob.",True
@sahubiswajit1996,2020-03-20T16:17:55Z,0,"Stochastic Gradient Descent (SGD): It means we are sending ONLY ONE DATA POINT (Only one row) for the training phase.  Gradient Descent (GD): It means we are sending ALL DATA POINTS (all rows) for the training phase.  Mini Batch SGD: It means we are sending SOME PORTION DATA POINTS (let us consider each 50 data points from 50K data points, it means 5000 epoch) for the training phase.  Sir is this correct?",True
@charlottedsouza274,2020-03-20T15:46:49Z,2,"In addition, can you create a separate playlist for interview question so that it is all in one place?",True
@ManishKumar-qs1fm,2020-03-20T15:46:29Z,0,Nice Sir,True
@jagannadhareddykalagotla624,2020-03-20T15:46:09Z,0,Stochastic gradient for globel minima Gradient desecent some times shows local minima,True
@charlottedsouza274,2020-03-20T15:40:29Z,4,"Hi Krish...Thanks for such clear explanation. For large datasets, for regression problems we have ridge and lasso. What about classification problem..How to deal with multi collinearity for large datasets?",True
@mahender1440,2020-03-20T15:28:03Z,3,"Hi, Krish Gradient descent : on big volume of data it takes more number of iterations,for each iteration it works with entire data so casuses High latency and more computing power, Solution : batch gradient Batch gradient : data is splitted into multiple batches,on each batch gradient will be applied separately,for each batch separate minimum loss is achieved,it considers finally the weight matrix of global minimum loss Problem with batch gradient : each batch contains few patterns the entire data,that means missing other patterns,model couldn't learn all patterns from the data",True
@harshstrum,2020-03-20T15:05:14Z,51,"Diff 1 --->Gradient descent takes all the data point into consideration to update the weight during back propagation to minimize the loss function..................whereas stochastic gradient descent considers only one data point at a time for weight updation.   Diff 2 ----> In gradient descent convergence towards the minima is fast..............where as in stochastic gradient descent convergence is slow.   Diff3-------> Since in gradient descent whole data points are loaded and use for calculation, computation get slow.........where as stochastic gradient descent is comparatively fast.",True
@cutyoopsmoments2800,2020-03-20T15:00:06Z,5,"Sir, kindly make all the videos of feature engineering and Feature selection which is present in your Github Link..  please..",True
@cutyoopsmoments2800,2020-03-20T14:58:32Z,2,"Sir, I am great fan of yours",True
@sushilchauhan2586,2020-03-20T14:50:51Z,4,"thanks krishna!...krishna i m from that person who like your videos first then i watch your videos ....  i yesterday said you about it and u explained it...thank you.. so you r saying after applying regularization  ... there will be no multicolinearity stochastic gradient descent : stochastic gradient descent is almost similar to gradient descent only difference is : if i have ""n"" no. of points in training data then it will randomly pick ""k"" no. of points where k<n and k is indirectly proportional to iteration  can you cover loss function of logistic  and hinge loss ...i generally get confused..pls explain it krish Bhai",True
