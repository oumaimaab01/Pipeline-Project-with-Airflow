author,updated_at,like_count,text,public
@krishnaik06,2021-06-10T18:54:28Z,10,Here is the answer to the question https://www.youtube.com/watch?v=5aIFgrrTqOw,True
@muditdubey6067,2023-02-22T11:09:43Z,0,"Both entropy and Gini impurity are commonly used metrics for measuring the impurity of a set of samples in a decision tree algorithm. ENTROPY: if the data set contains rare classes, entropy may be a better measure of impurity GINI IMPURITY:  if the data set has many classes of similar proportions, Gini impurity may be a more efficient measure.",True
@alakeshkalita9797,2023-01-03T16:08:56Z,1,"People are mentioning that when dataset is small we will use entropy.Areee if gini is computationaly less expensive the we will use it in every situations be it small dataset or large dataset.  The advantage of entropy comes when the dataset is imbalanced.Say there are two possible outcomes A and B P(A)=0.99 P(B)=0.01 In gini we square them ,hence effect of P(B) is neglected. But in entropy we have P(B)logP(B) Whose value is muchmore than P(B)^2. Hence its value has some effect. This is the only scenario we use entropy For feature selection or splitting",True
@ankitchoudhary5585,2021-09-04T18:28:24Z,1,Good to see the healthy contribution of the community...thnx Krish..great work.,True
@murumathi4307,2021-07-05T18:39:58Z,0,Regression tree used entropy... Classifier tree used Gini .. Why?..Gini impurity is probability besed calculate the split tha tree ...but entropy is  how much take energy tha split tree ...,True
@903vishnu,2021-06-13T17:16:10Z,0,Y hat is nothing but calculated value,True
@Yashgupta-dd6eq,2021-06-12T19:49:10Z,1,"While finding the entropy log computation is been done so, it will be time consuming while in gini impurity we don't have any log or any other function which is computational expensive. Gini impurity is used when we work with large dataset, while for small dataset we can use entropy it will not make any much of a difference.",True
@anmolagarwal8373,2021-06-11T13:12:42Z,1,Maximum value of gini impurity is 0.5 while Entropy has maximum value of 1. Entropy takes more computational time as compared with gini impurity.,True
@akshatpant9758,2021-06-11T09:37:00Z,7,"For smaller dataset we can use Entropy For larger dataset it is better to use Gini impurity In case of Entropy, calculations involved logarithmic computations whereas in case of Gini impurity its calculation only involves squaring operations.. So in case of Gini impurity calculations will be much easier as compared to that of Entropy.. So if using larger dataset it is better to use Gini impurity as it will give quick and efficient results.. But for smaller datasets we can use Entropy also",True
@shubhamchoudhary5461,2021-06-11T08:40:47Z,6,"entropy ranges between (0-1). gini-Impurity ranges between (0-0.5). gini = 1 - sum(p) entropy = yes log (yes) - no log(no) here - yes& no are the , probability of classes .ðŸ”¥",True
@shubhamchoudhary5461,2021-06-11T08:35:22Z,1,Whenever we have complex data with large number of columns then gini-Impurity selection criteria is good on other hand for smaller pr medium scale dataset we use entropy.. The entropy fromula exist log term which is computationaly costly as compared to the Gini hence we use Gini for large dataset.. Information gain is the method of selection of best root node by taking entropy as a fuel.. higher the Info-gain better will be the split & vice versa..the feature having highest info -Gain is selected as root node.. The feature having lowest gini-Impurity is considered as root node.. Thanks!!,True
@kabyabasu,2021-06-11T07:47:05Z,16,"If we see the formula for entropy it is, Hs = - p1 log p1 - p2 log p2, and for gini = 1 - ((p1) square+ (p2)square )  Calculating 'log' is computationally intensive, hence if the dataset is large calculation of entropy will not be an good idea, in case of large data we can use gini. In case of small dataset we can use entropy.",True
@NitishKumar-zd9ej,2021-06-11T07:46:41Z,2,"Gini impurity and Information Gain Entropy are pretty much the same. And people do use the values interchangeably. Given a choice, I would use the Gini impurity, as it doesn't require me to compute logarithmic functions, which is computationally intensive.  The most important remarks are : â€¢	It only matters in 2% of the cases whether you use gini impurity or entropy. â€¢	Entropy might be a little slower to compute (because it makes use of the logarithm).  The internal working of both methods is very similar and both are used for computing the feature/split after every new splitting. But if we compare both the methods then Gini Impurity is more efficient than entropy in terms of computing power. As you can see in the graph for entropy, it first increases up to 1 and then starts decreasing, but in the case of Gini impurity it only goes up to 0.5 and then it starts decreasing, hence it requires less computational power. The range of Entropy lies in between 0 to 1 and the range of Gini Impurity lies in between 0 to 0.5. Hence we can conclude that Gini Impurity is better as compared to entropy for selecting the best features.",True
@shashipaul6279,2021-06-11T07:41:33Z,4,"The major difference in both is, gini impurity is computationally efficient as compared to entropy. It is not ideal to use entropy when we have continuous feature values as it will take more time.  On the other hand, it is efficient to use Gini impurity as it takes less time to do the computation.",True
@maulana6969,2021-06-11T07:15:37Z,4,Ans -1  - Formula difference as you know - gini impurity is reduced scale of entropy. - entropy has log calculation where gini is simple algebric calculation  Ans -2 - use of gini impurity is faster many times than entropy. People use gini impurity- - when people need speedy DT. - when DT depth is high. -when feature categories and feature dimensions are high.  Ans-3 DT on regression is based on highest reduction of MSE or MAD from one level to another level.,True
@rajeshayanuru6739,2021-06-11T06:50:02Z,0,Can you explain the when we should we use entropy and when we should use gini impurity krish,True
@ajaykushwaha4233,2021-06-11T05:50:52Z,0,"Entropy is use for small size data because calcutong Entropy is time consuming/slow as compare to Gini index. Calculating gini index is faster so we use for large data. Actually large data small data is for understanding purposes I told but exact answer of guru ji question is, gini calculation is faster than entropy. Nishkarsh ye hai ki entropy slow kaam kerta hai to log small data bolte hain aur gini fast calculate hota hai to huge data bolte hain. Ab mere paas i3 processor hai wo bhi 2nd gen to bhaiya Hm to gini use karenge phir bhi mera beda garak hi hoga, ab jiske paas i7 hai wo kuch bhi use kare.",True
@akashdeb9185,2021-06-11T05:25:05Z,6,"Entropy vs gini impurities Gini impurities tell us the probability of misclassification of an observation Entropy is the measure of randomness in the information gain being processed.higher the entropy,the harder is to draw conclusions from that information. Hence this is the two criterion for calculating information gain  One major difference is that the range ofÂ entropy Â lies in between 0 to 1Â andÂ the range ofÂ gini impurities lies in between 0 to 0.5. Hence we can conclude thatÂ gini impuritiesÂ is better as compared toÂ entropyÂ for selecting the best features",True
@rajkir2852,2021-06-11T05:22:38Z,0,"Sir, i want to know about area under the curve. I know most of the things about differentiation. Like its a slope of the tangent. Its a Particular slope on that  point on the whole curve(line). But someone says area under the curve is like total slope of the curve= area under the curve. And also how its related to probability distribution.",True
@rajkir2852,2021-06-11T05:17:35Z,0,Lowest entropy is best and highest information gain is best,True
@rajkir2852,2021-06-11T05:16:09Z,1,Information gain is a criteria used to select decision tree. Entropy : measure of impurity in a system or in a node of decision tree.,True
@ajaykushwaha4233,2021-06-11T05:06:33Z,1,Guru ji aap ki jitni tareef ki jaye wo kum hai.,True
