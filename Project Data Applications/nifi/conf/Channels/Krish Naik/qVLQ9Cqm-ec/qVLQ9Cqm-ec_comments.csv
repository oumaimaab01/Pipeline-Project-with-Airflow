author,updated_at,like_count,text,public
@ramakrishnayellela7455,2024-03-29T17:39:43Z,0,"Sir,Can you send notebook pdf",True
@satwindersingh1797,2023-12-27T14:18:57Z,0,U r ultra best teacher Sir ..  Ultimate and even better...,True
@anjubhagat8257,2023-10-26T10:42:18Z,0,"Sir,  you have explained very well. Thank you. I have the data from the year 1987-88 to 2021-22 of mango production and I want to apply ANN. so please give me code for Rstudio and also tell me which activation function, learning rate and how many hidden layers are best for the problem.",True
@the-ghost-in-the-machine1108,2023-09-02T11:24:48Z,0,"27:45 No, the Swish activation function is not zero-centered",True
@gamingultimate7489,2023-07-06T09:45:05Z,0,Fooling layer in nlp,True
@deepaklonare9497,2023-06-20T04:14:15Z,0,Can you please share the Github link for this document?,True
@sachinborgave8094,2023-06-02T05:30:45Z,0,"Thanks Krish, Can you please share this document ?",True
@borgavejs4379,2023-06-02T05:29:47Z,0,"Thanks Krish, Can you please share this document ?",True
@himanshugoel4573,2023-05-25T22:14:51Z,0,Hello Krish I am trying to follow your complete Deep Learning playlist but can you share a link to the Jupyter notebook or the documentation that you used in the video to explain everything? It would be a great help to read those notes showin in the vieo. Thanks in advance !,True
@NikhilaRaoLakku,2023-04-17T21:52:42Z,0,"Hello Sir, Can  you please share the notes that you are explaining from?",True
@RehmanKhan-gb6sk,2023-04-09T15:06:50Z,0,can you share this file please.,True
@shaikrahamatulla-jy2cg,2023-02-25T15:37:34Z,0,please provide jupyter notebook,True
@smarttaurian30,2023-01-18T19:41:50Z,1,You should have made single video for each activation function as it is difficult to get  from this video. You are going fast and bit confused in understanding,True
@smarttaurian30,2023-01-18T18:59:19Z,0,If ReLU has zero or one output then why we don't value step function?,True
@ganjirao7094,2023-01-02T14:49:33Z,0,Good,True
@vipnirala,2022-11-19T05:35:56Z,0,Great content. Thank you.,True
@rageadigaming8162,2022-10-12T14:12:56Z,0,share the  notes sir >>>>>>,True
@vivekkumarshaw6495,2022-10-11T08:31:11Z,0,Where can i get these slides,True
@shynie4986,2022-10-05T13:56:40Z,0,I was wondering how to access the notes in your video.,True
@vimalgupta86,2022-08-04T06:50:23Z,0,"Hi Krish, can you please provide the link for this notebook?",True
@shubhibansal7016,2022-08-02T07:17:24Z,1,"Hi, where can we find jupyter notebooks or the notes for the videos?",True
@arijitmukherjee8293,2022-07-03T09:08:25Z,1,"Hi Krish, thanks for the lovely explanation. I have one question. Why does zero centered data converge faster. Can anyone explain this?",True
@bcinerd,2022-06-23T13:51:19Z,0,could you please upload this notebook in video description?,True
@mdmynuddin1888,2022-06-20T04:57:48Z,0,Can i get the notebook?,True
@user-qb5gw8xg9m,2022-06-19T13:37:10Z,0,thx Krish! can I have access to your notebook?,True
@akashm1027,2022-06-06T11:50:21Z,0,Is there a notebook link to refer?,True
@MrKB_SSJ2,2022-04-03T14:30:21Z,0,Thanks a lot ðŸ˜Š,True
@priyankachore1617,2022-03-23T12:41:48Z,0,Thank you sir!!!,True
@chitramethwani4758,2022-03-14T18:22:55Z,3,hi Krish.. can you pls provide the link for this notebook? Great content and nice explanation.. :),True
@chinmaybhat9636,2022-03-02T07:20:35Z,0,@Krish Naik Sir Can you share the Jupyter Notebook for this in the github ???,True
@hariharans9408,2022-02-15T08:04:24Z,2,"Such a Amazing sir, Please put a link for this notebook in the discription that will help us to revise more about this",True
@nazishiqbal1046,2022-02-14T08:20:34Z,0,Great work! From where I can get this notebok?,True
@rafibasha1840,2022-01-29T06:37:35Z,0,"Hi Krish,please share the notebook you are using",True
@rafibasha1840,2022-01-18T08:16:52Z,0,@15:00 when we multiply negative value with .01 how it became positive  value,True
@lamis_18,2021-12-26T11:23:13Z,1,Can we have this amazing ipynp  file?????????????????????????????????????,True
@ArunKumar-sg6jf,2021-12-13T06:55:19Z,0,Bro unavailable join u telegram group,True
@nagamanid8926,2021-11-24T11:03:41Z,0,Thank you Sir. My question is-Can we use both SVM and Softmax simultaneously in CNN for classification,True
@ivavrtaric,2021-10-12T07:08:55Z,1,"every sentence you say two times, good lecture but so much repeating is annoying.",True
@vishaljhaveri7565,2021-10-07T06:38:04Z,1,Sir please share the jupyter notebook with all of us. Thank you so much. I hope you reply us with the notebook.,True
@techsavy5669,2021-10-02T23:46:12Z,1,How can i get the ipynb file that you described here ! Thank you.,True
@guddubhagat7854,2021-09-24T12:02:40Z,1,Link to this notebook file please ?,True
@dr.ratnapatil9272,2021-08-04T03:31:47Z,0,Wonderful session.,True
@pritamH,2021-06-21T07:24:12Z,1,Make theoretical video on white board..because most of the people familiar with that..Than you krish..big fan of youðŸ–¤ðŸ–¤,True
@vaibhavyaramwar,2021-06-01T16:45:46Z,2,"This one is the artical where Activation Function Explanation is given along with Diagrams :  https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#:~:text=delta(x)%20!-,ELU,to%20RELU%20except%20negative%20inputs.",True
@gurucharank5491,2021-05-17T12:13:46Z,0,Very nice video sir kindly share the notebook,True
@devmaharaj1,2021-05-14T13:40:13Z,0,"Super , Liked a LOTTTTTTTTTT !!!!!!!!!!!",True
@louerleseigneur4532,2021-05-12T13:30:27Z,0,Thanks krish,True
@kavuluridattasriharsha,2021-05-10T08:07:34Z,5,Hi Krish  Can you please provide the Activation functions notebook for our reference.,True
@KK-rh6cd,2021-05-06T15:37:40Z,1,"sir, please share this notebook",True
@Dragon48OO,2021-04-19T15:44:15Z,1,Naik  Sir can you make  fully explained vedio on YOLO algorithm with its  working program,True
@classictremonti7997,2021-04-17T15:46:32Z,0,"Hello...When you say the model has ""2"" or ""3"" output layers...are you really saying that the ""output layer has 2 or 3 neurons""?  Just trying to keep the semantics clear in my mind.  Thank you!",True
@koustavdutta5317,2021-04-13T11:09:30Z,8,"@Krish Naik, sir please kindly share the notebook. Its very much required for self revision and from notes point of view.",True
@balasaraswathiyugandher3176,2021-04-01T14:34:56Z,1,16:35 ELU,True
@hashimhafeez21,2021-03-14T12:57:20Z,0,thank you brother for such a nice explanation.,True
@datasciencewallah9620,2021-03-11T09:14:52Z,0,"I don't understand why you are saying we are not trying to find derivative of 0. Why can't we find derivative of zero in ELu, ReLu, Leaky ReLu function",True
@AbhishekMishra-nl6by,2021-03-11T07:58:08Z,1,from which repository I can find this file in your GITHUB,True
@shreesapkota,2021-03-03T06:57:51Z,0,Good explanation. Thank you.,True
@muhammedcansoy1434,2021-02-21T15:12:19Z,0,Thank you so much,True
@imranriaz9752,2021-01-29T10:46:18Z,1,I am very thankful to you dear sir for uploading such nice and well explained videos. Engr. Imran Riaz Lecturer Department of Electrical Engineering MUST Mirpur A.K. Pakistan,True
@rimanshumangal3517,2021-01-17T04:01:54Z,0,"HI krish, could you please provide link for github where this notebook uploaded and others also.",True
@AbcAbc-kx3xm,2021-01-11T11:27:50Z,0,Great!,True
@sukumarroychowdhury4122,2021-01-08T03:13:47Z,8,"Dear Krish: we all love you, your energy, your enthusiasm.   One point about derivative of ReLU activation Function at zero.   To express it properly, derivative of ReLU as x tends to zero does not exist because the derivative is a step function and at zero, it is discontinuous. And limit approaching from left is not equal to limit approaching from right.   The ReLU function is continuous, not-bounded and not zero-centered. At x = 0, the left hand derivative of ReLU is zero while right hand derivative is 1. Since the left hand derivative and the right hand derivative are not equal at x =0, ReLU function is not differentiable at x = 0.  Derivative of Leaky ReLU at zero is still discontinuous. Hence, it is still not differentiable at zero.   Generally, in Geophysics, we use Leaky ReLU  as follows:  f(x) = 0.1*x if x<0 f(x) = x if x>0   Else, I am your Bhakt and I want to start your (iNeuron) - 3 courses as soon as I reach India. Master Machine Learning Masters Deep Learning Masters NLP.   Congratulations for your new position as CTO at iNeuron.   Cheers, Roy",True
@laxmankusuma8994,2021-01-03T07:37:04Z,4,playback speed 1.5,True
@rahuldey6369,2020-12-22T13:58:08Z,4,33:58 just a question- Do we have mutually exclusive results when apply sigmoid like softmax? Because the 60% 40% scenario will be there when we have mutually exclusive results where total probability sums to zero as you've mentioned,True
@randhirpratapsingh9795,2020-12-18T09:36:59Z,2,Hi Krish.... Well explained... Could you please help me with the Jupyter notebook for this activation functions script....,True
@dineshjayakumar6349,2020-12-17T12:28:39Z,1,"sir , can you please share this activation funtion jupyter notebook ?",True
@pratibhasawant9349,2020-12-06T22:07:19Z,1,"Hi Krish Naik, Your videos are as usual excellent. Could you help me to know what kind of activation function is this?   Ïˆ (x) = (1        0 â‰¤ x                 âˆ’1      x < 0 the operator is chosen as Ïˆt (x) = âˆ’1 + r (x + t) /|t|  âˆ’ r (x âˆ’ t)/ |t|",True
@ratulghosh3849,2020-12-05T08:49:15Z,3,Thank you sir you really made the concepts related to different activation functions so clear.,True
@abhinaykumar8875,2020-12-05T07:40:08Z,2,Very informative video sir Please can you share the link to notebook,True
@divyanshchahar843,2020-12-04T11:17:12Z,0,"Hello Krish,                     Really helpful video. But I have the following question   If a function is not zero centered can the gradient shift to one direction ? If yes, then how ?  I read this article and am bit confused (https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253 )",True
@anandvamsi1993,2020-12-02T04:14:00Z,1,"Hi Krish, Why can't we use mod x (|x|) as an activation function? It will ensure neurons are not completely deactivated + take care of the vanishing gradient issue.",True
@saruaralam2723,2020-11-30T15:42:26Z,0,"Nice video, Krish, btw where is the video of loss functions, couldn't find it",True
@sudipnarayanchoudhury1112,2020-11-26T06:00:45Z,0,@Krish .. Please share the notebook. or Github link atleast.,True
@Sudeepdas313,2020-11-25T18:32:33Z,5,Thank you so much sir for taking out time and effort to put out such great content!,True
@saurabhtripathi62,2020-11-17T06:35:01Z,0,thanks for updating.,True
@ameenali1837,2020-11-16T14:44:30Z,0,Sir ko nahi pta h kya ki derivative of 0 is 0 itself?,True
@arjyabasu1311,2020-11-15T21:43:34Z,3,Sir please upload the notebook !!,True
@laykefindley6604,2020-11-13T07:19:58Z,0,"Holy moly, you jump around more than a locust after snorting the entire amount of cocaine Charlie Sheen snorted before each episode of Two and a Half Men.",True
@meedoremee9622,2020-11-07T13:26:32Z,2,Can you download your machine learning that you used in your teaching?,True
@rohanyewale3184,2020-11-06T22:33:34Z,6,Really very Helpful !!! Can I get this notebook?,True
@shailendra9292,2020-11-06T11:15:50Z,6,Hi Krish ! Thanks for clear explaination. Can you please share github link for this notebook ?,True
@dr.pushpalathamanagement1276,2020-11-05T13:00:58Z,0,very clearly explained .,True
@arpanghosh3801,2020-11-03T15:56:56Z,9,can you share the github link for the code,True
@suneel8480,2020-11-03T10:42:28Z,1,Can you name the tool with which you are writing on the screen?,True
@DeepakSaini-sg3pq,2020-11-02T16:38:44Z,18,Thank you sir for making this video really very helpful and sir can you please provide us this notebook.,True
@nguyenngocly1484,2020-11-02T03:02:59Z,1,f(x)=x is connect. f(x)=0 is disconnect. ReLU is then a switch. A ReLU neural net is a switched system of dot products. Fast transforms like the FFT and fast Walsh Hadamard transform are fixed systems of dot products that you are free to mix in.,True
@anandhiselvi3174,2020-11-01T17:02:01Z,5,Please do video on svm kernels,True
