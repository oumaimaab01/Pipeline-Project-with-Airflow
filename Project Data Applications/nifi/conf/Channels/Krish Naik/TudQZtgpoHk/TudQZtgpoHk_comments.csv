author,updated_at,like_count,text,public
@hemamaliniveeranarayanan9901,2024-05-27T10:00:20Z,0,nice explanation Krish sir... wonderfully explained all the optimizers.,True
@RajeshRajesh-sh7zj,2024-04-25T05:42:18Z,0,"Ryt from first video I uv been watching this deep learning course,,, u not talked abt bias in the previous videos,, but in this long video suddenly u r saying abt Bias.  What is it ??",True
@alibastami,2024-04-14T11:25:57Z,0,"Hands down the best explanation of optimizers on planet earth till to this day. whenever I want a refresh I simply go back to this video, nothing better has ever came out since this video was launched. thank you Krish deep from my heart.",True
@mohamedbadi8875,2024-04-08T00:52:11Z,0,"Thank you Mr Krish, your work  inspired me,now i understand optimizers",True
@ramakrishnayellela7455,2024-03-31T11:07:01Z,0,Is weights gets updated for every iteration or for every epoch.,True
@littletiger1228,2024-01-30T21:30:03Z,0,beautiful,True
@ArunKumar-sg6jf,2024-01-29T02:51:00Z,0,Alpha t is wrong u giving wt but it shold wt-1 for adgrad,True
@sriramvaidyanathan5094,2023-11-17T11:42:54Z,0,can give me a small hint on what is the beta exactly,True
@sumaiyachoudhury7091,2023-11-17T03:20:44Z,0,"at 1:32:27 , it should be V_db (not w)",True
@gauravkamble336,2023-11-16T18:32:13Z,1,Video should be small,True
@LakshmiDevi_jul31,2023-10-24T11:57:13Z,0,Hello Krish I'm a Research Scholar and I was looking for some good explanations and luckily found your video and you made it clear in the first shot Forget the formulas why one after the other algorithms came  into picture was really much clear with math intuitions.  Will save your video for future reference.  Thank you Kish . Appreciate your works.,True
@VasanthBattula,2023-10-21T18:27:13Z,0,Hello sir let me know  your free time to discuss about my problem. Please share your contact details.,True
@shantanu556,2023-10-13T05:14:09Z,0,"Thanks sir, helped me.",True
@shantanu556,2023-10-13T05:14:08Z,0,"Thanks sir, helped me.",True
@levelupmonk18,2023-10-10T22:45:25Z,0,thanks. well taught : ),True
@santhoshkumarmatlapudi2851,2023-09-18T08:24:24Z,0,In SGD with momentum what will be the previous iteration (t-1) for first weight means (t),True
@nazaninayareh5008,2023-08-24T05:22:02Z,0,Thank you Krish. You make it seem easy to grasp.,True
@mahamaher1826,2023-08-01T16:19:09Z,0,ÿ¥Ÿà ÿßŸÉŸà ÿ¨ÿ±ÿµ ŸäÿØŸÉ ÿ®ÿßŸÑŸÅŸäÿØŸäŸà ÿ¥ŸÜŸà ŸÅÿ±ÿµÿ© ŸÜÿ∑ŸÑÿπ ŸÖŸÜ ÿßŸÑŸÅŸäÿØŸäŸà,True
@user-nm4hm5gk6w,2023-07-29T15:05:33Z,1,you are the best teacher . i have seen many videos' but no one explain concepts so deep and clearly.,True
@pratiek8s,2023-07-10T07:48:16Z,0,1:04:00,True
@SwethaNandyala-sf9lt,2023-07-06T03:50:26Z,0,Thank you for your efforts krish.... Your videos are incredible!!!!,True
@muhammedshalan5781,2023-05-28T08:14:02Z,0,hey can you please explain NAG using the given momentum equation as a continuation of sgd with momentum,True
@swetamoharana3553,2023-05-12T21:22:22Z,0,Sir can you help me doing a comparative analysis between all the optimizer for a particular data set to see which can use better in performance? It's really urgent sir please help,True
@IndustrialAI,2023-04-19T14:50:50Z,0,Can anyone share the link of the deep learning notes of Krish sir?,True
@meenalpande,2023-04-02T02:29:43Z,0,Really very great effort ..  Thank you sir,True
@KOTESWARARAOMAKKENAPHD,2023-03-15T06:06:19Z,0,"sir i watched SGD with Momentum,Adagrad,Adadelta,RMSprop,Adam Optimizers videos, but i need two more topics -Nesterov accelerated GD, Nadam videos",True
@SouhardyaDasChowdhury,2023-03-08T12:00:02Z,0,Why  the value of alpha (that square term) would skyrocket ?? Asking in terms of adagrad(learning rate decay one),True
@gokulnath4297,2023-02-15T16:52:02Z,0,"10k records, 1000 samples in one batch , ie)10 iterations(epoch/batch size)",True
@gokulnath4297,2023-02-15T16:51:17Z,0,"EPOCH-->ITERATION-->BATCH, that is like the heirarchy is..",True
@francisegah6115,2023-01-30T12:22:39Z,0,Krish  is fastly becoming my favorite teacher,True
@bipulsingh6232,2023-01-18T15:02:41Z,0,tq so much in one viedeo my total unit finished,True
@daur_e_jaun9201,2022-12-18T16:43:54Z,0,Thank you so much Sir for this,True
@vadimshatov9935,2022-11-25T09:40:59Z,0,"@krishnaik06 Great channel. Thank you. Could you please turn on automatic subtitles? I am not a native English speaker.""",True
@aayushguptaaa,2022-11-23T18:07:49Z,0,Yes,True
@akashkumar-ni9ec,2022-11-22T17:40:52Z,0,what an effort! stunning <3,True
@benbabu9404,2022-11-03T18:57:27Z,0,"in the equation of alpha_t in adagrad, is the limit of i from 0 to t ? If so, it means for each iteration value of LR varies from initial value to a minimum in each iteration. Isn't is logical for the LR to vary for each epoch?",True
@ayushtulsyan4695,2022-10-15T21:48:40Z,0,Orgasm aa gaya bro,True
@scienceandmathbyankitsir6403,2022-10-08T09:44:16Z,0,"Plz explain Nadam, ftrl etc too",True
@lukafarkas420,2022-09-13T09:42:45Z,0,"good stuff mr Krish, simple and step by step, helped me a lot.",True
@Manojprapagar,2022-09-13T04:59:42Z,1,start watching from 19:05,True
@deelipvenkat5161,2022-09-12T17:36:12Z,0,very well explained üëç,True
@marijatosic217,2022-08-23T15:28:36Z,0,"Thank you for the video, it's beyond helpful!   One question, when talking about the implementation on 01:23:00 would we have 2 different learning rates, one for updating weights and the other for updating the bias, since the calculation of exp weight average would be different since one depends on the derivative of Loss w.r.t. weights, and the other of derivative of Loss w.r.t. bias. ? :) Thank you! :)",True
@merv893,2022-08-20T08:33:51Z,0,"This guys great, repeats himself so you can‚Äôt forget",True
@kagglefire6545,2022-07-31T18:55:06Z,0,"Thanks so much. Recently, I have been asked about the comparison between (SGD and ADAM), And I didn't know the intuition behind adam. But now everything is clear. Thanks so much again.   And I have a question now. I want to build a custom optimizer in Keras. So, is there a good resource in this?  And I know we can manually take the derivatives w.r.t the vector of matrix multiplication vector. But is it possible to manually take the derivative w.r.t matrix? For example,  taking the gradient w.r.t weights. I know it is done numerically with auto diff, but could I solve it manually?",True
@mohamedyassinehaouam8956,2022-07-31T08:36:07Z,0,very interesting,True
@mhadnanali,2022-07-25T08:23:46Z,0,"I think you are wrong about SGD. Stochastic stands for random so it means, it will choose random inputs and perform GD on them. so it will converge faster. it does not mean it is iterate one by one.",True
@srijanshovit844,2022-07-14T09:57:34Z,0,Awesommeee,True
@skvali3810,2022-07-01T10:19:24Z,0,This is all for Deep Learning are can we use it in machine leaning algos,True
@pravalikadas5496,2022-06-29T12:04:54Z,0,You are a fantastic teacher Krish. Simple.,True
@lekkalanaveenkumarreddy1539,2022-06-04T19:01:21Z,1,nice explanation.thanks a lot,True
@gourab469,2022-05-18T18:52:43Z,1,Beautifully explained and taught. Hats off!,True
@shraddhaagrahari7519,2022-05-18T17:26:06Z,3,I am becoming a fan of you .... I am very lazy person ...  Never want to study but after Watching your videos . It feel good to learn .... Thank You  Krish Sir....,True
@md.nazrulislam6739,2022-05-15T05:14:54Z,1,"I am a data scientist working in startup company in Bangladesh. Thanks you so much for preparing such wonderful videos.  You are really great teacher, I really learned a lot from you.",True
@hamedmajidian4451,2022-05-07T18:01:13Z,0,"However, I'm not sure is the definition of SGD in your stream is correct.",True
@hamedmajidian4451,2022-05-03T18:14:37Z,0,you rock!!!,True
@stipepavic843,2022-04-28T20:15:55Z,0,respect!!!! subbed,True
@rohitray1118,2022-04-27T20:55:13Z,0,sir kindly share the notes also,True
@hichamkalkha5847,2022-04-16T08:29:10Z,0,could you plz share the research paper you mentionned in rmsprop?,True
@hichamkalkha5847,2022-04-16T06:47:41Z,0,Thank u bro!  Question :  should i retain that : GD=>One epoch leads to underfitting. SGD => require more resources RAM etc. (comp. explos.) ?,True
@kcihtrakd,2022-04-15T17:34:21Z,0,Really nice way of teaching Krish. Thank you so mcuh,True
@DHGokul,2022-04-02T08:06:14Z,0,Wonderful way of teaching ! Krish Rockzzzz,True
@haripandey5276,2022-04-01T21:54:49Z,0,Awesome,True
@jaisvarghese7304,2022-03-31T10:55:24Z,0,"These equationd are from geeks of geeks, im totally confused when comparing with research paper, cos equation seems like somewhat lacking ...",True
@faizkounain768,2022-03-23T06:59:16Z,0,Adds in every 5 mins,True
@tagoreji2143,2022-03-09T11:15:38Z,0,Thank you so much Sir for this Educative Video. VERY VERY VERY EDUCATIVE. THANKS  A LOT Sir üôè Not even my Course faculty had taught like this And the words u spoke @56:00 increased my Respect towards You Sir That's üíØ% true.We should be respectful to the Researchers and Every one behind what we Learning,True
@anuragmishra2032,2022-03-08T08:15:50Z,0,What happens if we apply the momentum gd again after first smoothing once. is it viable? does it take more comoutation time? will it make the path more smooth? @krishnaik,True
@sumitkumarsharma4004,2022-03-05T17:30:14Z,0,This is called amazing. I went through paper .I was not able to grap the concept but your teaching skill is amazing. Thanks for this video. I request for yogi algorithm video.,True
@wilsvenleong96,2022-02-22T15:51:13Z,0,"For the last point on bias correction, could you or anyone please explain its purpose. Thank you!",True
@rafibasha1840,2022-01-26T08:09:19Z,0,What‚Äôs the disadvantage of RMSprop krish,True
@sumitagarwal2335,2022-01-13T01:04:04Z,0,earlier i was literally struggling with concept of optimizer but after i watched your video it became very easy to understand. a very simple way of explaining even complex topic.,True
@yashmishra12,2022-01-07T00:36:49Z,0,Find Nachiketa Hebbar's video on SGD with momentum to get a feel for the topic. Krish has.....written the formulas.,True
@pavankumarpk9013,2021-12-30T03:37:53Z,0,"Hey Krish,  1:32:26 it should be Vdb instead of Vdw right since we are calculating with respect to Bias ?  Thank you for this interesting story of Optimizers, really helped a lot  : )",True
@Rahul_Singh_Rajput_04,2021-12-21T11:03:45Z,0,"hi sir first of all thankyou for providing  such a valuable education , sir where we can get this notes.",True
@BMESparshJain,2021-12-12T15:38:23Z,0,Wonderful session sir üî•üî•,True
@IrfanKhan-oh7kb,2021-12-11T06:54:43Z,0,1:28:12 üòÇ,True
@Stenkyedits,2021-12-09T09:19:45Z,0,wouldnt make more sense if adagrad alfa_t = sum(i=0->t)(dL/dw_(t-i)) ? since N will be decreasing more smoothly according to the W change in each iteration?,True
@liudreamer8403,2021-11-26T14:46:31Z,0,"Shocked, so clear explaination!",True
@sivabalaram4962,2021-11-19T04:18:55Z,0,"Try to watch full video, that would be better for understanding every optimizer... thank you so much krish Naik Ji üëçüëçüëç",True
@ashishanand1466,2021-10-19T03:59:02Z,0,"Bias correction is done to reduce the velocity and squared velocity component in lower iteration t but for higher iteration, it won't make much of a difference.",True
@DenorJanes,2021-09-22T11:17:11Z,0,"Hello Krish, everything in the video is very well explained, but I didn't understand why do we need bais correction for the Adam formula and why it uses b1 and b2, which depend on the timestamp t? b1 and b2 seemed to be constant values through out your explanations, so it doesn't make sense to me why would they depend on time in that formual... But just in case, b1(t) and b2(t) equal to b1 and b2, then the end formual would look like: vdw = vdw * (b1/(1-b1)) + dl/dw. Where in case of b1 = 0.95, we would get: vdw = vdw * 19 + dl/dw, so it looks like a scaled version of the original fromula, where all variables are given an additional weight. Could you please comment on that?",True
@BalaguruGupta,2021-09-14T15:05:11Z,0,This is a well explained video to understand Optimizers. Thanks a lot Krish!,True
@nishitgala2861,2021-09-13T17:03:27Z,0,Thank you for the wonderful intuition and for clearing the concepts.,True
@nhactrutinh6201,2021-09-04T03:52:01Z,1,"I mean to change learning rate, we should use a scalar value to decay it after each iteration.",True
@nhactrutinh6201,2021-09-04T03:50:28Z,0,I think AdaDelta or RMSProp does not change learning rate as in the video. It adjusts dLoss_dw of each neron of each weight. S is the same size with Weight matrix so this AdaDelta or RMSProp does not change learning rate.,True
@nhactrutinh6201,2021-09-03T18:52:41Z,0,There are one matrix dLoss_dw at each layer. So there are many layers. ADAM and other optimizer occurs at each layer?,True
@nhactrutinh6201,2021-09-03T18:50:33Z,0,dLoss_dw is a matrix. So in dLoss_dw^2: how to convert the matrix to scalar to get S and sqrt(S)?,True
@Bunny-yy6fo,2021-08-21T06:33:39Z,0,"Hi, can anyone explain maths behind SGD with momentum for the graph to become smooth, means how the graph is getting smooth if we apply SGD with momentum?",True
@thepresistence5935,2021-08-16T13:43:07Z,0,"SGD with momentum formula changes(old video) to (new video) confused but, I got we do exponential moving average in sgd with momentum.",True
@wahabali828,2021-08-09T12:12:55Z,1,Sir in live session you specify  Sdw in rmsprop and in recorded videos you specify Wavg both are same or different?,True
@dr.ratnapatil9272,2021-08-03T17:32:27Z,1,Awesome explanation,True
@raghvendrapal1762,2021-07-30T18:40:28Z,0,"Very nice video, one doubt here, I want know in each epoch, are we using weights of last completed epoch or just randomly generating it in each epoch?",True
@ishantsingh3366,2021-07-27T19:32:57Z,0,You're too awesome to exist ! Thanks a lot man !!,True
@uonliaquat7957,2021-07-23T21:55:33Z,0,Would you mind providing this white board sheet¬†?,True
@solomonaiyede8499,2021-07-20T04:27:03Z,3,You are a very good teacher. I have been following your video since 2020. It has helped me to understand so many concepts in Machine and deep learning. I like your simplicity with respect to your teachings. Thanks a lot. Writing from Nigeria.,True
@AnilVeni,2021-07-12T11:44:00Z,0,Sir please can you make the video on sailfish optimization,True
@alexmash1353,2021-07-04T14:35:06Z,0,Excellent video! Understood things I had problems with.,True
@kishlayraj4219,2021-06-27T06:09:14Z,1,"I liked the video, it was excellent. The only problem is that it had a lot of ads and really got frustrated at a point.",True
@jameswestbrook5709,2021-06-21T19:27:32Z,0,you are the best at explaining,True
@vikashdas1852,2021-06-12T05:31:01Z,1,I have always been confused with Optimizers in NN however this was the best resource available on internet that gave me an end to end clarity. Hatts off to Krish Sir.,True
@bhuwanacharya6275,2021-06-09T06:47:17Z,0,very good lecture thanku you  .  I am watching from Nepal,True
@sharanpreetsandhu3215,2021-06-06T06:36:39Z,0,Amazing explanation Krish. You teach in a very simple manner. I respect your skills. You made deep learning concepts so easy. Keep doing the good work. Thank you so much and All the Best.,True
@voidknown2338,2021-05-24T17:54:15Z,0,Very good one by one from basic to advance and one related to other gradually increasing to the best optimizer Adam‚ù§Ô∏è,True
@voidknown2338,2021-05-24T15:21:46Z,1,Thora ads kam lagaya kare sir please ‚ù§Ô∏è,True
@godse54,2021-05-22T17:08:06Z,0,Is there any difference between rmsprop and adadelta,True
@sur_yt805,2021-05-21T12:00:38Z,0,One of best method to teach thanks alot so simple so concise and every point is important,True
@sweetisah735,2021-05-18T07:57:03Z,0,"After seeing this video, m getting dizzy. u taught very well but my mind is dancing with fear after seeing so much.",True
@louerleseigneur4532,2021-05-14T13:00:16Z,0,Thanks Krish,True
@miraagarwal632,2021-04-25T16:51:59Z,0,Lot of noise in the form of advertisement  in ur video. Please try to apply gradient descent or batch gradient descent there,True
@alihaiderabdi9939,2021-04-25T10:02:36Z,0,"can we say that in adagrad due to high value of alpha, vanishing gradient problem occurs?",True
@shauryadixit7775,2021-04-24T20:06:08Z,0,"some kind suggestions: kindly remove previous videos so that we don't waste our time there and directly land here try to explain more, and improve your teaching skills.  kindly also state that the videos further uploaded have the right content or not or you gonna make a separate video to get more views.",True
@ommprakash4431,2021-04-18T06:20:26Z,0,"Hlw sir, Please provide the notes you have presented in video ....",True
@sivachaitanya6330,2021-04-15T14:39:19Z,0,where can i find the practical code for this class..........?,True
@tesla1772,2021-04-12T18:50:33Z,28,This video cleared many doubts. I would suggest everyone to watch even if you have watched previous videos.,True
@pruattea0302,2021-04-07T16:26:09Z,0,"Respect Sir, such an amazing explanation with super crystal clear. Super Thank you",True
@masthanjinostra2981,2021-04-05T14:40:45Z,0,Include few theory concepts,True
@yathishs1895,2021-04-03T07:46:35Z,1,@krish Naik ok then the previous video on sgd with momentum was wrong and this explanation is correct?,True
@shubhamchoudhary5461,2021-03-29T11:02:05Z,0,Thank you sir....you are amazing,True
@harshkhandelwal2974,2021-03-24T19:55:37Z,0,"I don't know what to say, so I subscribed!!  nice video :)",True
@hashimhafeez21,2021-03-18T16:14:14Z,0,Thank you so much sir..,True
@hashimhafeez21,2021-03-18T16:14:02Z,0,We are understanding beacuse you teach brilliantly..,True
@InovateTechVerse,2021-03-16T05:27:32Z,0,You are my guru.,True
@Hari-xr7ob,2021-03-02T11:48:27Z,0,"Time taken to update the weights is the same in all the three cases (GD, SGD and Mini Batch SGD). ONly forward propagation will take different times in these three cases",True
@gopikrishnabs7159,2021-03-01T09:44:51Z,0,"Hello sir, i doubt that for alpha(t) in Ada grad it should be i=1 to t-1 cuz we dont know that yet and you are using w(t) before it is even calculated?? is it right? r wrong?.",True
@MrAyandebnath,2021-02-18T21:56:59Z,0,Very informative and best video on youtube to understand details of all optimization techniques. Thanks @Krish Naik..I became your admirer,True
@richadhiman585,2021-02-17T09:40:29Z,0,Thank you sir...you have explained everything very nicely... excellent work..üôè‚ù§Ô∏è,True
@anusuiyatiwari1800,2021-02-15T12:40:21Z,0,Thank u so much sir...,True
@srishtikumari6664,2021-02-12T07:01:33Z,0,very nice video!!!,True
@nehabalani7290,2021-02-11T20:16:51Z,16,"I am amazed by the improvement in the quality, clarity and depth of  intuition in your recent videos. keep up the great work. I have watched most of your Deep learning videos and I must say you make learning very easy.",True
@priyabratasahoo8535,2021-02-11T05:21:25Z,3,"thanks, Krish for this great explanation. I have understood now .. lastly I was not able to follow so thanks again Krish",True
@krishnachaitanyavaddepally2539,2021-02-01T07:03:45Z,0,please make a video on L- BFGS-B optimizer,True
@joelbraganza3819,2021-01-30T16:16:06Z,0,Thanks for explaining it simply and easily.,True
@yogeshkadam8160,2021-01-21T09:55:56Z,2,‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§æ‡§ö‡§æ ‡§µ‡§ø‡§°‡§ø‡§ì ‡§∏‡§∞..‚ù§,True
@arjunsubramaniyan1675,2021-01-07T12:55:46Z,2,"About Bias correction in Adam, Just wanted to write about the need for it, So when we have B1 and B2 (Beta) , for the first iteration Both Momentum parameter and learning rate parameter will be zero so Sdw(1) will end up being very small and since Sdw will be in the denominator while updating the new weight w1, this will give a really huge change for the initial iterations and in the paper it has been mentioned that due to this bias of having zero (For the first iteration), the loss might not reduce over time so, we the authors proposed a bias correction, where we do a weighted average instead of a simple moving average   Sdw(t)corrected = Sdw(t)/1-(B2^t) where t is the number of iterations so if you notice, for the first few iterations the value of bias corrected Sdw(t) is different from Sdw(t) but as t increases Sdw(t)corrected is equal to Sdw(t) as the denominator becomes 1, thus this correction removes the bias created by using Vdw(0)=0 and Sdw(0)=0",True
@monalisameena103,2021-01-05T07:53:15Z,1,"Krish, you have taught it very nicely, it became simple to learn, it is like story, thanks a lot for making NN and optimizer very easy to learn.",True
@ADESHKUMAR-yz2el,2021-01-02T18:45:11Z,1,"only thing i got from this lecture is that ""Adam is the best optimizer and we gotta use it without thinking twice""üòÇ and for the rest of the video god knows üòÇ",True
@asiftandel8750,2020-12-28T16:54:03Z,1,Great Video.,True
@Areeva2407,2020-12-27T13:48:43Z,0,Excellent,True
@ppsheth91,2020-12-25T07:21:42Z,0,Amazing video krish sir..!,True
@RajeshGupta-wx5qd,2020-12-10T20:40:26Z,4,In the loss formula for stochastic gradient for n records you are dividing by 2 after calculation of sum of squares of loss. I did not understand the reason by dividing by 2. Should it not be division by n? I am referring to the loss formula written on the extreme right hand side at time interval 8:35 of video clip,True
@rajak7410,2020-12-09T07:27:54Z,0,"Understood 100% , I got it in a single time. Excellent explanation",True
@ManishSharma-tp3eb,2020-12-06T14:03:46Z,0,best,True
@Amankumar-by9ed,2020-12-02T03:35:38Z,0,One of the best video about optimization algorithm.‚ù§Ô∏è,True
@Ajaychauhan-gt3fq,2020-12-01T08:16:07Z,0,very useful,True
@rasikai102,2020-12-01T00:48:04Z,1,"Sir, thank you so much for this story. It has cleared all my doubts. Maths behind this all is so interesting.  Sir, but you have not explained RMS Prop anywhere. Not on this mega video of optimizers and nor on this link Tutorial 16- AdaDelta and RMSprop optimizer. Can you please walk us through RMS once or a short video on it? Even in this video you have directly stated about RMSProp but we dunno why RMS was introduced like we know that for other optimizers. Looking forward to this. Also, sir in this Tutorial 16- AdaDelta and RMSprop optimizer:""gamma is taken and terminology is Weighted average (Wavg), wherein this current mega optimizer video you are saying """"beta and Sdw (replaced by Wavg) "". We are learning sir this will confuse us all the more. Please use same signs/terminology all over the videos.",True
@anupsahoo8561,2020-11-28T12:54:12Z,1,This is really awesome video. The maths is explained really in details. Thanks,True
@khushalkhan6745,2020-11-28T09:41:50Z,2,you explained each and every topic differ than this  video it is confusing and i think you did mistakes in previous videos,True
@vishalvanpariya1466,2020-11-23T13:14:24Z,0,What a video! I never ever learn maths this much properly in my whole life. I was learning Deep learning I saw lots of videos about optimizers but I never understand correctly. I watched Coursera and Udemy courses but did not understand this much correctness. loved your teaching and storytelling skill. only one doubt here where beta1 and beta2 come from in ADAM optimizer?,True
@ManishSharma-tp3eb,2020-11-15T13:40:19Z,0,Very Nice,True
@arjyabasu1311,2020-11-14T01:47:35Z,0,Thank you so much for this live session !!,True
@oss1996,2020-11-11T18:13:15Z,0,"Great brother, doing excellent work üëçüëç",True
@theshishir24,2020-11-06T16:28:43Z,1,"I have a doubt, suppose we have a simple neural network and I have 1000 records (data), now lets say for 1st record, during training time (backpropagation), weights will be updated for minimum loss (global minima for GD), and training will go on till 1000 records.   Now my question is in this way we will get 1000 respective weights for each nodes after training, then how the final weights are calculated?,  Please also correct me if i am wrong in my understanding.",True
@kanhataak1269,2020-11-05T06:39:14Z,2,Amazing video Sir.,True
@RAZZKIRAN,2020-11-04T14:58:48Z,1,can we use PSO ?,True
@tejas260,2020-11-04T09:23:03Z,1,"For those who are concerned, the equation of adom had beta1, beta2, beta, it won't be beta there, it will be beta1 for vt equations, and beta2 for st equations. So without bias correction we'll have just 2 hyper-parameters, beta1 and beta2.",True
@rajputjay9856,2020-11-04T05:36:32Z,7,"Mini-batch Gradient Descent. at each step, instead of computing the gradients based on the full train‚Äê ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini- batch GD computes the gradients on small random sets of instances called mini- batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs",True
@rajputjay9856,2020-11-04T05:34:28Z,56,"The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (SGD can be implemented as an out-of-core algorithm.) On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on aver‚Äê age. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down . So once the algo‚Äê rithm stops, the final parameter values are good, but not optimal.",True
@ehtishamkhan421,2020-11-04T04:41:12Z,1,Sir i  would suggest that you should include a github tutorials aswell in 5 months program.,True
@shahrukhsharif9382,2020-11-04T04:25:38Z,9,"hi sir, you made a great video. But in the Sgd with momentum equation, some error is there.  in the video, you explained this equation w_t = w_t-1 - (learning_rate)*dw_t  dw_t = B* dw_t-1  + (1-B)*dl/dw_t-1 but in this eqaution it's sholud not be dl/dw_t-1. it will be dl/dw_t  So correct equation is dw_t = B* dw_t-1  + (1-B)*dl/dw_t",True
@senthilkumara3653,2020-11-03T19:48:47Z,3,"Hi Krish, your live data science project demos are very useful. Interested to know when can we see new projects further? Requesting for projects with stock market applications.",True
@sreeramsaravanan8132,2020-11-03T17:00:03Z,1,Krish make a tutorial on Kivy.It is used for deploying ML models in Mobile apps,True
@GauravSharma-kb9np,2020-11-03T16:14:34Z,1,Great Video Sir !!!,True
@OmkarYadavDhudi,2020-11-03T15:32:14Z,3,"Hi Krish, can do same thing for ML techniques",True
@anandhiselvi3174,2020-11-03T05:28:13Z,5,Sir please do video on svm kernels,True
