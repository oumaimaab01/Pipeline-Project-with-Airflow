author,updated_at,like_count,text,public
@krishnaik06,2020-10-12T16:04:13Z,74,We are near 250k. Please do subscribe my channel and share with all your friends. :),True
@arshaachu6351,2024-02-17T18:12:50Z,0,Is there any detailed videos about Adaboost regressor and gradient boosting classifier? Please help me,True
@debasishcharanbehera1437,2024-02-14T18:55:48Z,0,Hoere we have -0.5. why we do 1-0.5. it is ideally 1-(-0.5) right,True
@jamalnuman,2024-01-21T19:10:04Z,0,great,True
@tarabalam9962,2024-01-17T09:00:49Z,0,Please upload a video on Light GBM.,True
@sheikhshah2593,2023-11-14T14:48:44Z,0,Great sirüî•üî•,True
@kolusumounika4367,2023-11-12T06:19:27Z,0,"then gain is 0.19 right ,could u pls recheck @krishnaik06",True
@iftikhar58,2023-10-30T07:40:51Z,0,Mistake at 14 mints,True
@annusrivastava4425,2023-08-23T14:00:19Z,3,"hi, have one doubt, for p(1-p) + lambda in denominator to calculate similarity weight, if the residual is -0.5 it should be 0.5(1-(-0.5))= .75? or the negative sign does not matter?",True
@Shakhaiiuccse,2023-07-09T18:17:08Z,0,You didn't add the lamda. Why?,True
@yogendrashinde473,2023-06-22T18:36:23Z,0,@krish,True
@suridianpratama7855,2023-05-14T11:03:34Z,0,how xgboost work in multiclass?,True
@mihirjha1486,2023-03-27T04:54:12Z,0,Loved It. Thank You!,True
@edwinokwaro9944,2023-03-02T09:48:37Z,0,is the formula for similarity score of the root node correct? since this is a classification problem?,True
@KOTESWARARAOMAKKENAPHD,2023-02-28T07:16:52Z,0,what is the need of LOG(odd) function,True
@KOTESWARARAOMAKKENAPHD,2023-02-28T07:15:41Z,0,"why we split G,N into one but not separately",True
@KOTESWARARAOMAKKENAPHD,2023-02-28T07:14:52Z,0,is any other value except 0 as a hyperparameter in XGboost algorithm,True
@mohamedgaal5340,2023-02-24T19:01:20Z,0,"Thank You, Krish. Well explained!",True
@durjoybhattacharya250,2023-02-24T09:58:34Z,0,How do you decide on the Learning Rate parameter?,True
@swethanandyala,2023-02-16T08:51:57Z,0,Hi sir @Krish Naik. What will be the initial probability when there are multiple classes....if anyone knows the answer please share...,True
@RakaAdli,2023-01-16T09:31:21Z,0,gg bro,True
@REHAN-ANSARI-,2022-11-27T07:22:32Z,0,XG-Boost is the secret of my energy,True
@thinhlevan6913,2022-11-21T14:32:18Z,0,Can you put subtitles for the video?,True
@sachinjaisar5776,2022-11-10T05:32:53Z,0,Shouldn't your similarity weight be 1? Residuals must be squared first before adding up.,True
@mohittahilramani9956,2022-11-07T20:52:14Z,0,Seriously thank u so much,True
@cynthiac2174,2022-08-27T14:52:25Z,0,can someone please help me clearing this ... why negative sign has not been considered while calculating similarity weight?,True
@belxismarquez4447,2022-08-10T21:34:02Z,1,Please subtitle the videos in Spanish. There is a community that speaks Spanish and listens to your videos,True
@moindalvs,2022-07-20T10:26:43Z,5,"Thanks a lot, for eveyrthing you do. You did turn off the fan so that it doesn't interrupt the audio, you were sweating and breathing heavily with all this trouble and hardship you deserve more. I wish you success in life and a healthy and a prosperous life.",True
@dvp1678,2022-07-17T03:01:28Z,0,11:09 0.33-0.14 is 0.19 not 0.21,True
@adityarajora7219,2022-06-28T20:53:22Z,1,How is Pr gonna change please explain!!!!,True
@saptarshisanyal4869,2022-06-26T14:38:22Z,0,Statquest Light !!!! Fantastic effort though.,True
@mohana4179,2022-05-30T03:16:16Z,0,Please put lgbm mathematical explanation sir,True
@ayanmullick9202,2022-05-23T02:07:01Z,0,You are legend sir.,True
@bayazjafarli3867,2022-04-19T10:45:52Z,1,"Hi, thank you very much for this explanation! Great video! But I have one question. In 19:39 you first wrote 0 which is the probability of first row then you added learning rate*similarity weight. My question is instead of 0 shouldn't we write 0.5 which is the average probability of first (base model). 0.5+learning rate*similarity. Please correct me if I am wrong.",True
@accentureprep1092,2022-04-10T12:56:53Z,0,Hi @krish  First of all kudos to you Great video Can you tell me how xgboost is different from Aprori alogrithm or does it cover every combination as in Aprori cover ( ie it's covers all the combination while creating tree as Aprori will cover for same problem statement)  Thanks and love your work Keep rocking,True
@Acumentutorial,2022-03-24T06:30:10Z,0,Wht is the role of lambda in the similarity weight here.,True
@shaelanderchauhan1963,2022-03-15T03:13:49Z,0,You did not explain the Cover value concept in detail!,True
@felixzhao9070,2022-01-26T05:35:24Z,2,This is pure gold! Thanks for the tutorial!,True
@yashsethi2402,2022-01-20T19:11:35Z,0,"0-0.6 = -0.6, not 0.4",True
@yashsethi2402,2022-01-20T18:58:38Z,0,"Information gain, It's 0.19 instead of 0.21",True
@divakarkeshri564,2021-12-25T17:23:50Z,0,.33-.14=.19,True
@IamGaneshSingh,2021-12-09T19:06:43Z,0,"This video is ""pretty much important!""",True
@amitupadhyay6511,2021-11-19T22:01:14Z,0,"its tough to understand in first attempt ,but thanks for giving the outline so clearly, I will watch it untill I understand I implement it from scratch .",True
@brunojosebertora7935,2021-10-26T21:57:08Z,0,"Krish, I have a question: when you compute the output value you are catching the similarity weighted. I think it is incorrect for classification, isn't it?  To compute the output you shouldn't square the residuals.  THANKS for the video!!",True
@davidd2702,2021-10-16T02:08:29Z,0,Thank you for your fabulous video! I enjoy it and understand well!  Could you tell me if the output from the xgb classifier gives 'confidence' in a specific output (allowing you to assign a class) ? or is this functionally equivalent to statistical probability of an event occuring?,True
@chiranjivikumar3690,2021-10-09T11:20:22Z,0,What's is the use ?,True
@saimanohar3363,2021-08-31T19:10:33Z,0,"Grt teacher. Just a doubt, can't we take the credit as first node?",True
@yimmidisettibhargav3304,2021-07-25T11:19:22Z,0,isnt .33 -.14 = .19 ? am I missing something here?,True
@murumathi4307,2021-07-10T08:21:59Z,0,Lambda value not calculate why?,True
@biplabroy1406,2021-07-04T05:19:47Z,0,someone please explain 21:34 . 0-0.6 = -0.6.. then how it is converging to actual value.,True
@modhua4497,2021-06-20T15:16:23Z,2,Good! Could you make a video explain the difference between XGB and Gradients Boosting? Thanks,True
@jainitafulwadwa8181,2021-06-06T11:36:01Z,0,"The similarity score is not the output value, there is a different formula for calculating the output based on residuals, you just have to remove the square in the numerator of the similarity score function.",True
@vishnukv6537,2021-06-01T16:56:30Z,0,Sir you are too pleasant and amazing in teaching,True
@mayurpardeshi395,2021-05-23T08:28:17Z,0,how krish calculating gain ??,True
@dulangikanchana8237,2021-05-19T04:48:45Z,0,can you do a video difference between statistical models and machine learning models,True
@mainakray6452,2021-04-11T11:03:19Z,0,"the max_depth in xgboost for each tree is 2? plz answer ,",True
@muhammadsaqib2961,2021-03-29T19:20:31Z,0,Quite amazing and clear explanation,True
@shashwattiwari4346,2021-03-22T13:34:31Z,2,"""Day 1 or 1 Day your Choice"" Thanks a lot Krish!",True
@bhavikdudhrejiya852,2021-03-12T19:30:51Z,69,"Great video. Understood in depth I have jotted down the processing steps from this video: 1. We have a Data 2. Constructing base leaner 3. Base learner takes probability 0.5 & computing residual 4. Constructing Decision as per below        Computing Similarity Weights: ‚àë(Residual)^2 / ‚àëP(1-P) + lambda     - Computing Similarity Weight of Root Node     - Computing Similarity Weight of left side decision node & its leaf node     - Computing Similarity Weight of right side decision node & its leaf node        Computing Gain = Leaf1 Similarity W + Leaf2 Similarity W - Root Node Similarity W     - Computing Gain of Root Node & left side of decision node and its leaf node     - Computing Gain of Root Node & right side of decision node and its leaf node     - Computing Gain of other combination of features of decision node and its leaf node     - Selecting the Root Node, Decision node and leaf node have high information gain 5. Predicting the probability = Sigmoid(log(odd) of Prediction of Base Learner + learning rate(Prediction of Decision Tree)) 6. Predicting residual = Previous residual - Predicted Probability 7. Running the iteration from point 2 to 6 and at the end of the iteration, The residual will be the minimal. 8. Test Prediction on the model of iteration have minimal residual",True
@hackernova1532,2021-03-03T09:45:05Z,0,What is lambda in similarity weight formula ...pls some one answer,True
@joeljoseph26,2021-03-01T21:48:25Z,5,"Guys, please watch for the mistake. There is a mistake made at 16:10 i.e. For credit >50 (G,B) = {-0.5,0.5} its not three, there is only two. The information gain for the right side is 0.67. However, you chose the right node.   Btw, your teaching very simple and understandable. Keep doing more videos. Love your content.",True
@yourvibe2844,2021-02-20T13:48:26Z,0,you've not taken user-defined gamma subtraction after calculating gain in order to prune.,True
@deepsarkar2003,2021-02-14T08:55:28Z,1,Can anyone explain to me the video during 21:38 Mins ( 0-0.6)=-0.6 right not 0.4  right? or did I get it wrong Please Advise,True
@RahulKumar-hb8cl,2021-02-14T06:30:12Z,2,"Sir, How will the Prob value( 0.5 for the base tree ) be updated in each tree?",True
@ashwanikumar-zh1mq,2021-02-13T11:24:18Z,0,When I training data first calculate residual and create dt but here we are not able to see how it classified the point and in this it say when new data point is come I am confused in this,True
@mohitjoshi4209,2021-02-12T15:03:56Z,2,"So much to learn from a single video, hats off to you sir",True
@gardeninglessons3949,2021-02-09T12:03:15Z,0,"sir please make a video on differences in all the boosting techniques , they are elaborate and couldn't find out the exact differences",True
@hemantsharma7986,2021-02-09T10:43:18Z,1,isnt gradient boosting and xgboost same with miner difference?,True
@Amansingh-tr1cf,2021-02-06T10:15:18Z,0,the most awaited video,True
@ManoharKumar-cw3ed,2021-02-03T09:05:23Z,0,Thank you sir! I have a question in this how we predict the probability value at the begging  from 0-1,True
@nandangupta727,2021-01-18T13:13:29Z,0,Thank you so much for such a step to step explanation. but I have a quick question what would we do if we have continuous variable than categorical. would we proceed as we do in decision tree for continuous features? or it's not recommended to use XGBoost in case of continuous features?,True
@nukulkhadse5253,2021-01-14T12:23:09Z,2,"Hey Krish, you should also have a video about Similarity Based Modelling (SBM) and Multivariate State Estimation Technique (MSET). They are actually widely used in the industries since 90s. There are many research papers to validate that. They also calculate similarity weight and residuals.",True
@ajayrana4296,2021-01-12T09:02:52Z,0,what is similarity weight why we use it what is its advantage what is the intution behind it,True
@ramnareshraghuwanshi4737,2021-01-10T18:40:22Z,0,Dude!! 3.29 residual =  actual - probability? how come?,True
@nitinahlawat2479,2020-12-30T15:43:03Z,0,Really Data science Bisham Pitamaüôè Respect you a lotüëç,True
@Abhishek-jd9qk,2020-12-29T05:35:45Z,0,"I did not feel this algorithm properly , it looks like may  be Krish is also not so confident about some parts.",True
@stabgan,2020-12-28T17:46:02Z,0,"23:00 that's lambda not alpha, please correct that",True
@adireddy694,2020-12-25T18:20:56Z,0,Alpha or lamda ?,True
@adireddy694,2020-12-25T18:05:14Z,0,How you have calculated the probability ?? How you have got 0.5 ??,True
@pratikbhansali4086,2020-12-25T14:06:52Z,0,U didn't upload gradient boosting classification videos i. e part 3 and part 4 of gradient boosting,True
@shivanshsingh5555,2020-12-18T05:30:13Z,0,Can anyone tell me whether 'Pr' and 'Prob' in the denominator is the same thing?,True
@ajiths1689,2020-12-13T10:44:58Z,0,what should be the new probability value we need to consider when we are considering the second decision tree?,True
@subhodipgiri2924,2020-12-05T14:44:00Z,0,how can we subtract probability of a value from that value. if suppose i take approvals in terms of Y and N then also their probability remains same at 0.5. but we cannot subtract 0.5 from Y or N. I did not get your concept of subtracting the probability from value.,True
@sandipansarkar9211,2020-12-01T09:02:03Z,1,Very very important to crack in product based companies.Great explantion too.Thanks,True
@marijatosic217,2020-11-02T16:25:13Z,3,"This was amazing, I literally feel like I'm sitting in your class at a Uni.",True
@gulzarahmedbutt7213,2020-11-02T06:29:38Z,3,I've learned a lot from Mr.Krish. You're doing great and Keep up the good work. You make people love Machine Learning. Hats Off to you! Love from Pakistan.,True
@nothing8919,2020-10-26T10:35:58Z,0,"thank you alot sir, you are my best teacher",True
@frozen1860,2020-10-23T04:27:42Z,0,Sir the way you teaching us is more better than any varsity classes.  pls do a practical implementation on XGBoost. sir pls it will be very helpful for us...,True
@naveenvinayak1088,2020-10-23T02:28:49Z,0,Krish How do u stay so focused,True
@sohinimitra7559,2020-10-19T09:13:04Z,3,Can you please do a video on feature selection approaches? Especially the use of Mutual Information. Thanks. Great videos!!,True
@amitshende5161,2020-10-18T03:59:05Z,0,"It's lambda as hyper parameter, which u mentioned as alpha...",True
@dheerendrasinghbhadauria9798,2020-10-16T13:44:48Z,0,How he is taking probability = 0.5 in the whole process. What is the calculation of that probability??,True
@titangamezone4379,2020-10-15T06:09:38Z,0,sir please make a video on gradient boosting for classification problem,True
@seniorprog9144,2020-10-15T01:24:58Z,0,"Sir . krish  Do you have  a  code  that  deal   with   more than one target    (  y1,y2,..   Y is   2 columns  or  3 columns  .   (two target  , three  target  )",True
@ArunKumar-sg6jf,2020-10-14T06:34:10Z,1,How u determine value of pr in base model,True
@johnnyfry2,2020-10-14T06:24:29Z,10,"Great work Krish.  Don't ever lose your passion for teaching, you're a natural.  I appreciate how you simplify the details.",True
@SRAVANAM_KEERTHANAM_SMARANAM,2020-10-14T04:26:28Z,1,"Dear Krish,  We have a course on machine learning. Around 40000 people subscribe to this course. But since they dont understand many of them will drop out in the middle. Why dont you start creating videos parallel to what is taught in the class and make a playlist for it. So that you can easily many views with one shot. Are u interested in this.",True
@olfchandan,2020-10-14T02:32:18Z,1,Why does it work?,True
@yashkhandelwal3877,2020-10-13T10:55:38Z,13,"Hats off to you Krish for doing so much hardwork so that we can learn each and every concept of ML, DataScience!",True
@antonym9744,2020-10-13T07:39:31Z,2,Amazing !!!,True
@vishaldas6346,2020-10-13T07:05:01Z,1,"Hi Krish, I have a doubt, can you please confirm if XGBOOST is a part of ensemble technique or not as while importing from the library we are doing it separately not from sklearn library.",True
@amitsahoo1989,2020-10-13T06:32:47Z,10,"Hi krish, i have been watching ur videos for  the last few months and it has helped me a lot in my interviews. A special thanks from my end. In this video, at 10:54 min 0.33 -  0.14 should be 0.19.",True
@alokranjanthakur5746,2020-10-13T04:25:03Z,4,Sir can you refer some NLP projects using python. I mean with live implementation,True
@sajidchoudhary1165,2020-10-13T02:50:46Z,2,i am most happiest person to see this videos thank you,True
@animeshsharma7332,2020-10-13T02:18:34Z,91,"Man, this guy is now coming in my dreams. Who else have been binge watching his channel for months?",True
@narendradamodardasmodi3286,2020-10-13T02:06:53Z,8,"Thanks, Krish for building the nation Towards AI Journey.",True
@abhishek_maity,2020-10-13T01:08:23Z,2,Great.... Clear explanation !! Thanks a lot üòÑ,True
@govind1706,2020-10-13T00:57:25Z,2,Finally !!!!,True
@ashwinkrishnan4285,2020-10-12T19:47:20Z,1,"Hi Krish,  I have a doubt here. Here all the input features (salary, credit) are categorical. so we are making the decision tree easily based on the categories. Say suppose if we get the salary feature as continuous like 30k, 50k and not like <50k or >50k, how this split of decision tree will be done.",True
@nareshjadhav4962,2020-10-12T19:20:46Z,7,I was desparately waiting for this since last 7 months...now I will complete mashine learning playlistüí•   Than you Krish..god bless youüòÄ,True
@raneshmitra8156,2020-10-12T17:31:42Z,1,Super explanation,True
@yashkhant5874,2020-10-12T17:25:16Z,4,Great Explanation sir... keep contributing to the community. We love your videos and most importantly you are serving your experience is the best thing.,True
@VinodRS01,2020-10-12T16:41:40Z,2,Sir how does the model chooses which similarity weight should be multiplied with learning rate . Thank you sir  u r doing great  by helping usüôÇ,True
@ShahnawazKhan-xl6ij,2020-10-12T16:28:41Z,3,Great,True
@mrzaidivlogs,2020-10-12T16:13:57Z,37,"How do u stay so focused , strong and learn everything in a very efficient way?",True
@ishitachakraborty1362,2020-10-12T16:10:16Z,13,Please do a indepth maths intuition video on catboost,True
@satwikram2479,2020-10-12T16:09:02Z,2,Finally‚ù§,True
@dhruvenkalpeshkumarparvati4874,2020-10-12T16:07:19Z,7,Just what I was waiting for üî•,True
@krishnaik06,2020-10-12T16:04:13Z,74,We are near 250k. Please do subscribe my channel and share with all your friends. :),True
@datakube3053,2020-10-12T16:03:26Z,4,250k coming  soon,True
@datakube3053,2020-10-12T16:02:31Z,3,thank you so much,True
@ppersia18,2020-10-12T16:02:15Z,6,1st view 1st like krish sir op,True
