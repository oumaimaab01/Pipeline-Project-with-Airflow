author,updated_at,like_count,text,public
@anjalisharma6543,2022-09-24T17:07:09Z,0,"Naive Bayes, Linear Regression, logistic regression, and SVM, Yahh yes, DT also gets impacted by highly imbalanced datasets",True
@siyays1868,2022-08-04T06:04:47Z,0,"Thanku Krish so much for a valuable guidance, providing everything. Nice interview tips.",True
@anilshinde8025,2022-05-29T13:06:37Z,0,All classification related algorithms are affected by imbalanced datasets.,True
@mohitupadhayay1439,2022-02-27T14:20:36Z,0,"Assumption 1: Linear Relationship. Assumption 2: Independence. Assumption 3: Homoscedasticity. Assumption 4: Normality.",True
@reshusingh6005,2022-02-08T10:18:44Z,0,Where is the link?,True
@navinchowdary5954,2022-01-09T18:26:52Z,0,logistic regression  will be impacted by imbalanced data set,True
@aishwaryaandure6043,2021-10-21T11:57:37Z,0,please make a video on SVM loss function,True
@louerleseigneur4532,2021-07-31T05:37:48Z,0,Thanks Krish,True
@akshaygudiyawar3180,2021-07-17T18:59:25Z,1,How is SVM affected by outliers? The separating planes will depend on just support-vector points right?,True
@LuciaCasucci,2021-05-07T01:49:06Z,2,"Krish, you can tune the gamma to prevent the outliers to influence too much the SVM algorithm",True
@arrigocoen7856,2021-03-09T13:58:27Z,1,Great video! A tiny recommendation would be to add it to the playlist of Interview Prep.,True
@vijendersinghshekhawat6244,2021-01-08T09:27:00Z,0,"Hi Krish, The video should be reorderd at 3rd place in ML interview playlist.  Thanks for wonderful videos..",True
@kumareshbabu7951,2020-12-28T11:31:57Z,2,Can we use Principle Component Analysis along with SVM?,True
@basavarajbijali7342,2020-12-22T06:19:10Z,2,All ML algorithms deals with linear seperability and linear relationship are affect by imbalance dataset,True
@pavankumar4368,2020-12-11T07:07:52Z,3,"Except for the Ensemble Learning algorithms RF, XGboost, Gradient Boost, Ada Boost other algorithms may get affected by the Imbalanced dataset.    why only these not effected 1. Ensemble Learning is of creating N number of Week models to make strong learners in RF.   2. XGBoost is a combination of Bagging and boosting, it takes the weighted average of the many weak models and by focusing on weak predictions and iterating through the next models it reduces the error.   3. same with gradient boost it starts with a weak prediction as average and builds decision trees by adding learning rate times the outcome of the residual tree to the initial prediction and process repeated. 4. Ada boost gives more weight to the incorrectly classified samples in creating further stumps.  thank you guys hope you find the answer   Correct me If I am wrong by replying back",True
@anandhiselvi3174,2020-11-02T03:08:03Z,0,Please do video on svm kernel,True
@peddaboinatribhuvan7749,2020-10-27T19:36:04Z,8,I think If Imbalance is present in Dataset itself(Even if Intentional for Experiment)  No model would perform better as Bias is present in data itself .,True
@nothing8919,2020-10-27T19:31:14Z,0,i think all the algorithms of classification,True
@saibhaskar591,2020-10-20T09:31:02Z,0,Need a video on K means,True
@PiyushKumar-tv6dr,2020-10-17T14:33:32Z,0,sir i think that before this in a video of yours you said tht we don't need feature scaling in svm,True
@ayushichoudhary6989,2020-10-15T14:07:47Z,0,plz upload kernel videos,True
@tanmayvaidya8337,2020-10-08T07:39:12Z,1,Thanks Krish. This will surely help. Please upload similar interview preparation videos for other ML and DL algorithms. :),True
@prasanna271992,2020-10-06T04:48:13Z,0,Why no assumptions for SVM? Didn't get proper answer in the internet.,True
@barathkumarg9197,2020-10-02T05:28:44Z,0,sir please upload svm implementation video,True
@sahilraheja2491,2020-09-28T10:48:52Z,0,"SVM is prone to over-fitting as it is s sensitive to noise and the amount of training data, please can you recheck and suggest.",True
@shubhammarathe534,2020-09-26T18:10:21Z,0,Sir can u suggest me a good laptop under 70-80k for study purpose. Write now i am in last year of BscIT and doing research on email classification using SVM. I want a laptop which will last for 3-4 years for software development. Please suggest me,True
@nikhilkumarreddy5581,2020-09-26T13:11:26Z,1,where is day - 4 VIDEO??,True
@roshankumargupta46,2020-09-24T17:39:37Z,0,"Thank a lot sir. Can you make similar videos on XGBOOST, RANDOM FOREST?",True
@utkar1,2020-09-24T04:24:44Z,3,Algorithms effected by imbalanced dataset - Linear Regression - Logistic Regression - KNN - SVM etc,True
@mr.kapoor3803,2020-09-24T03:01:05Z,2,Again D,True
@sofluzik,2020-09-24T01:12:48Z,1,"Since your question is more about imbalanced dataset, I am assuming we can safely look at only supervised learning algorithms   In this , cart, random forest , baggingz boosting type of algorithms are fairly resistant to imbalanced data.  Logistic , svm , Ann,k-nn are sensitive .",True
@saidurgakameshkota1246,2020-09-23T16:39:09Z,1,"Logistic regression,knn,svm",True
@jeeveshkataria6439,2020-09-23T16:24:44Z,2,Sir  please provide hugging face bert model,True
@kalyankrishna5902,2020-09-23T15:28:22Z,14,"Svm, logistic regression,  random forest classifier, decision tree classifier, knn, naive Bayes are affected by imbalanced data set",True
@pinakimishra9057,2020-09-23T15:12:14Z,2,"support vector machine, Xgboost, LightGBM, KNN, regression Algos can be impacted by imbalance data set .",True
@anshusingh536,2020-09-23T13:46:22Z,2,krish sir SVM AND LOGISTIC REGRESSION are haviely  impacted by imbalance dataset,True
@teluguforyou3992,2020-09-23T13:44:46Z,3,"Hoo sir please make a video on how projects will be in product based companies(like how the tasks will be give to team, how is the deadline for task...)",True
@tarunbilla1900,2020-09-23T13:37:52Z,2,Please  make some videos on times series .,True
@souravmzdr,2020-09-23T13:07:26Z,3,"SVM can handle balanced and imbalanced dateset with the help of 'c' parameter. c is a parameter which can be though of how much strict we want our model to be or in other words how many data-points we allow the model to make mistake in. Therefore as in a imbalanced dateset, no of points from one of the classes is already low , so we cannot afford to allow the model to make higher number of  mistakes. So we have to make the model more strict , i.e choose a higher c value. I found this amazing article, hope it helps anyone looking for more information: https://towardsdatascience.com/support-vector-machines-imbalanced-data-feb3ecffbb0e",True
@kshitizomar6730,2020-09-23T12:41:59Z,12,"Classification Algorithms such as Logistic Regression and Linear SVM (kernel= 'linear') which try to find an optimal hyperplane are generally prone to imbalanced dataset.  The reason behind this is these linear classifiers are actually to solve their respective optimization problems in presence of a tradeoff, which is: Greater Generalization Power Vs Minimal Classification Error. Hence if the dataset itself is imbalanced, it will be only solving the optimization problem for the points of the majority class, given we haven't done any hyper-paramter tuning. Now for some aggressive hyper-parameter tuning, we get these classifiers to perform okayish for imbalanced dataset (tuning for minimising error over maximising generalization power), but then we will be sacrificing the Generalization Power for unseen datasets, which isn't desirable, also hyper-parameter tuning won't work for severely imbalanced datasets.  Now, for Similarity based algos such as KNN are also prone to imbalanced datasets, as they employ a majority voting at the end to make their classification decision.  Kernel SVM shouldn't be prone to imbalanced dataset, because it finds suitable optimal support vectors using the kernel itself, that is if the dataset is imbalanced, then their will be less number of support vectors for the minority class. (Please do correct if I am wrong on this one)  Naive Bayes isn't affected by the imbalanced as it calulates the likelihood of each (seen) features during runtime.  Tree based algorithms such as Decision Tress and Random Forest should also not be affected by imbalanced datasets, because they divide the given feature space into axis parallel hyperplanes. (Also, they try to maximise the information gain)  Also, I think that MLP based algos would overfit the training dataset if it is imbalanced, hence making them immune to imbalanced datasets. This is a guess, please correct if it's wrong.  I am certain that some of which I have said maybe rubbish, please correct if it really is! Great series Krish Sir!",True
@ritajyagupta4666,2020-09-23T12:31:24Z,10,"all top of the line supervised learning algorithms such as Linear ,Logistic,KNN,SVM are affected by imbalanced datasets.",True
@rohanbura8978,2020-09-23T12:25:23Z,3,"Logistic Regression, KNN affects most to the imbalanced data set",True
@SahilKhan-yu3oh,2020-09-23T11:34:11Z,1,Sir you are great  thanks you so much,True
@himanshuarora6822,2020-09-23T11:28:06Z,6,"Machine Learning Algorithms such as Logistic Regression, KNN,SVM (basically which includes Gradient Descent and Euclidean Distance Computation) are affected by Imbalanced Datasets. Please correct me if I am wrong. Thanks",True
@shahnawazkhan1636,2020-09-23T11:24:54Z,3,You are motivational like vivek bindra sir but in the domain of Data science.,True
@hardikvegad3508,2020-09-23T11:15:08Z,7,"All the ml algo which contains gradient descent.... Are impacted by Imbalanced dataset... Such as ANN, Logistic regression, Linear Regression... Etc and KNN also",True
@techspoc7442,2020-09-23T11:10:34Z,3,Algorithms which are not based on tree based algo impacted by imbalance datasets. Please correct me if I am wrong.,True
