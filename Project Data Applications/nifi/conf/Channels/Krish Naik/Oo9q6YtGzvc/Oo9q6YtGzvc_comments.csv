author,updated_at,like_count,text,public
@SreeramPeela,2024-05-02T11:54:39Z,0,should the learning rate be fixed ahead or change over interations?,True
@willwoodward4150,2024-04-23T10:00:12Z,0,How is gamma_m calculated in step 3 used in subsequent steps?,True
@subarnasubedi7938,2023-09-12T22:55:47Z,0,The last step of minimization is wrong because if you actually minimize you will get  y (hat)=y(bar)-60 which is 60-60=0,True
@DharmendraKumar-DS,2023-05-04T15:18:05Z,0,Great explanation...but is it necessary to remember all these formulas from interview point of view?...or having understanding of concepts is enough?,True
@sriramayeshwanth9789,2023-04-21T07:12:27Z,0,For the people who are looking for a solved example. Hope this clears all your doubts.  https://www.youtube.com/watch?v=0ARLObJJ3y4&t=812s,True
@jayanthkothapalli9.2,2023-03-13T13:10:39Z,0,sir without finding the residual(R2) values how you have find the updated model value output?,True
@ShashankMR,2022-12-17T14:16:30Z,0,will you start deep learning and neural network also,True
@karunamayiholisticinc,2022-12-10T01:59:32Z,1,Thanks to Professor Leonard's Calculus classes on here that I could understand this. Great explanation. I don't think it would be as easy to get this so fast from Wikipedia. Thanks for taking out time to explain the concepts. Keep up the good work!,True
@abhijeetjain8228,2022-10-05T19:48:11Z,0,thank you sir !,True
@davidzhang4825,2022-08-26T08:04:31Z,0,Nice video. What's the connection in Step2 between (2) Fit a base learner and (3) Calculate the gamma using the argmin sum function ?,True
@SuperRia33,2022-08-22T14:33:09Z,0,"Wikipedia scares me(with formulas) but Krish saves me ,Thank you for all your hardwork ,for simplifying complex things and keeping me motivated to learn!!",True
@akhandshahi3337,2022-08-19T10:28:35Z,0,If we are euclidean distance then you are the standardization. You make our calculations very easy.,True
@MrKishor3,2022-07-11T05:45:40Z,0,"hi krish, i've a doubt, you said d/dx(x^n ) is nx^n-1. so it will be d/dx(1/2(y-y^)^2)=2/2(y-y^)^2-1, but you are taking it to be 2/2(y-y^)*-1.please resolve my doubt.",True
@danishwais2701,2022-06-12T07:39:58Z,0,why is the loss function starts with 1/2.  n is the number of samples. are threre only 2 samples ?,True
@ravikiran1284,2022-06-11T18:15:01Z,0,I think this is for gradient boosting not for gradient boosting decision tree,True
@alliewicklund6192,2022-06-10T17:04:17Z,4,"You are brilliant, Krish! I've had trouble with the theory and intuition of data science before, but these videos make things so clear.",True
@talkswithRishabh,2022-06-08T12:43:16Z,0,thanks sir so much ðŸ˜€,True
@abhishekmaharia4837,2022-04-28T02:20:38Z,0,Thanks for the great explanation....my question is how do you select a loss function pertaining to a problem or is it like try different loss functions according to different ML models,True
@anirudhagrawal5044,2022-04-22T21:50:54Z,0,"hello krish , i have doubt regarding this video only as we use gradient descent technique and find the first order  derivative of y^ we equate the equation with zero to find the local minima value for y^ but as we know gradient descent technique is a greedy technique we will never be able to reach best solution or global minima how can we implement gradient descent also and have global minima at the same time?",True
@spicytuna08,2022-03-17T18:02:48Z,0,thanks. 11:30  - confusion between r and gamma.,True
@priyeshdave3799,2022-02-23T18:41:03Z,0,"Hi, Can anyone please explain me why we took 10 in the step 2.4 for model updation?  That is 60 - 0.1(10). As per my understanding 10 was the residual value.",True
@mainhashimh5017,2022-02-21T19:41:04Z,0,Krish man I'm so thankful for your work! Passionate and intelligent.,True
@sohailhosseini2266,2022-02-09T15:15:22Z,0,Great work!!!,True
@bhavyaasharma9920,2022-01-11T08:19:45Z,0,I am not getting the sequence which is to be followed. Is it repeat(1-2-3) and 4 or repeat(1-2-3-4).,True
@pawanthakur-df2yk,2021-12-27T17:42:13Z,0,12:45,True
@yashasvibhatt1951,2021-10-17T17:16:57Z,1,"In the third sub-step, according to the formula doesn't that always makes things to 0. Since yi will be your original values, y_hat will be the residual, Fm-1(xi) will be the base estimator's value. These values makes it, 1/2(50 - (60 + (-10))) which is apparently equals to 0 and it is not for a single sample, it is for all the sample. Correct me if I am wrong.",True
@RoamingHeera,2021-10-17T14:37:54Z,0,shouldn't it be Gamma multiplied by h(x) in equation 3 (equation 3 on bottom right)?,True
@rashidquamar,2021-10-12T11:29:19Z,0,"We need to step 2 for m --> 1 to M, what minimum M we should consider ?",True
@nivitus9037,2021-10-09T07:26:56Z,0,Before seeing this video I thought ML is all about sklearn and python only! ðŸ˜…ðŸ˜…ðŸ˜…,True
@himanshubhusanrath212,2021-09-05T01:32:28Z,0,Very beautifully explained Krish,True
@thepresistence5935,2021-08-14T05:30:26Z,0,nice explanation about this thankyou so much !,True
@abdulbasith7665,2021-07-25T18:03:39Z,0,Where did the 1/2 came from? If considering the loss function as MSE then it should be 1/n sum((y-y_hat)**2),True
@rkaveti,2021-07-23T02:57:18Z,0,I am taking this class cs109a at Harvard and I tell you what- you beat the professor any day. So clear!,True
@fatmamansour8606,2021-07-10T04:36:03Z,0,excellent video,True
@ankitgupta1808,2021-06-15T05:43:12Z,0,awesome krish you have  an amazing ability to describe complex things with ease,True
@utkarshsalaria3952,2021-06-10T02:55:11Z,1,Thanks a lot sIr for such clear explaination.!!!,True
@srinagabtechabs,2021-05-29T10:35:53Z,0,excellent teaching..thank u ..,True
@rupeshsingh4012,2021-05-21T12:10:40Z,0,Hats off to you sir ji,True
@pushpitkumar99,2021-05-05T08:29:36Z,0,Sir you make things look so simple. Really learnt a lot from you.,True
@akashanande6725,2021-04-21T05:14:03Z,0,In step no. 4 it is gamma m instead of alpha as per Wikipedia,True
@jaysoni7812,2021-04-10T21:00:49Z,0,"you said that there's part 2 will be come as gradient boosting classification, please make it bcz the classification of gradient boost is different compare to ada boost in ada boost it's easy but i found difficulty in gradient boost.",True
@dipkumarmallick8132,2021-03-24T12:19:46Z,0,Best lecture on the mathematics of gradient boosting regression. Thank you so much Krish Sir!,True
@sudheer596,2021-03-21T11:46:05Z,0,Where is part 3 and part 4 videos after this.,True
@rafsunahmad4855,2021-03-19T13:46:28Z,0,Is knowing  the math  behind algorithm  must or just knowing that how algorithms works is enough? please please please give a reply.,True
@urmishachatterjee5127,2021-03-13T06:31:01Z,2,Thank you for making this video on gradient boosting. I am getting a better understanding of ML from your videos. Thanks a lot,True
@jewaliddinshaik8255,2021-03-06T01:49:40Z,0,"Hi krish sir i am following all your videos ,easy explanations .....keep doing same ..thanks a lot sir",True
@amardeepsingh9001,2021-02-23T15:09:03Z,0,"It's a good explanation. Just one thing: at last (at step 4), what you are referring to as alpha (learning rate), is a gamma(m) actually. It's an obtained coefficient for minimum loss at step 3. However, we can add alpha in a multiple of gamma there, to perform regularization. Just tried to understand from wiki ;)",True
@uttasargasingh9911,2021-02-18T20:26:08Z,0,My brain exploded on 10:18!,True
@9604786070,2021-02-07T10:30:52Z,0,"In step 4, h(x) is simply r_m i.e. residual calculated for that DT. Then why use different notation h(x)? And there should be summation over i in last term of eq.4, right?",True
@satishbanka,2021-01-31T15:15:32Z,0,Very good explaination fo Gradient Boosting Complete Maths!,True
@ajayrana4296,2020-12-24T07:33:55Z,0,2-1(n-1)=1 not  -1  should it not like that or i m missing something,True
@Vishal-rj6bn,2020-12-17T09:53:06Z,0,"What i think is, learning rate is not the one in update model equation that is our multiplier gama(m). Learning rate is the one that we need to use while computing the multiplier. Since it is used to decide the rate at which we minimize the loss function.",True
@JalalUddin-xy7lf,2020-12-11T19:54:51Z,0,Excellent explansion,True
@helloworld7886,2020-12-08T10:09:34Z,1,It should be 1/n instead of 1/2 at time-stamp -3:40,True
@grantmiller8634,2020-12-07T18:54:08Z,0,Why did he do all this calculus for the base model instead of simply just taking the average of the y's?,True
@sandipansarkar9211,2020-12-01T08:31:01Z,4,watched it again today.Very important for interviews in product based companies,True
@sagarmunde3088,2020-11-27T17:49:11Z,1,"Hi Krish. All your videos are really well explained but, can you please upload how to implement the algorithms using code also. so, it will be helpful for everyone",True
@hiteshmalhotra183,2020-11-27T16:44:28Z,1,Thankyou sir for sharing your knowledge with us..<3,True
@tengliyuan1988,2020-11-16T13:52:25Z,1,"Thanks Krish, I cant tell how much I appreciate your sharing of the knowledge.",True
@roshankumargupta46,2020-10-22T06:40:39Z,7,3:25 Why 1/2 sir? Shouldn't it be 1/n?,True
@kmnm9463,2020-10-17T13:54:14Z,0,"Hi Krish,  Excellent math discussion on Gradient Descent. I have one clarification and an observation. Clarification : at the start  the loss function is defined as 1/2 summation y-y^.   Want to know where the 1/2 came from.?   Also in calculating the y(cap) in the first base model - it is also the direct average value of the initial dependent variables ( salary). This gives 60 ( the same as derivative route). Why to use derivative in the first step?    Regards KM",True
@srinathtripathy6664,2020-10-15T13:08:42Z,0,Thanks man . you have made my day ðŸ˜Š,True
@sushanbastola947,2020-10-07T07:16:11Z,13,15:32  The moment when your teacher caught you dozing!,True
@shivadumnawar7741,2020-09-29T05:25:58Z,0,Thanks krish,True
@sandipansarkar9211,2020-09-28T17:37:08Z,0,"Superb explanation Krish,Thanks",True
@1pmcoffee,2020-09-16T02:59:21Z,2,"Hello Krish, I have a doubt: at 14:00, you mentioned the previous value of the model as 60. But as calculated earlier in the video, the latest error was r11 that is -10. So, shouldn't we put -10 instead of 60?. A side note, I am enrolled in Applied AI course but couldn't understand this concept. You made it so much easier. Thank you so much.",True
@lijindurairaj2982,2020-09-11T06:53:15Z,0,"thank you, was very helpful",True
@anon44492,2020-09-10T20:20:25Z,1,amazing bro! i have been trying since months to get my head around this... thank you so much!,True
@suvarnadeore8810,2020-09-04T06:49:41Z,0,Thank you sir,True
@aminearbouch4764,2020-09-01T16:08:16Z,0,thank you my friend,True
@ashimmaity64,2020-08-31T17:27:06Z,0,the explanation is not clear...,True
@shashirajak9997,2020-08-29T03:55:58Z,0,"Hi krish. Just a request that whenever u make a video which is continuation of a video (part 2, part3 ) then plz put link of part 1 or last video related to it. This will really help . Thanks",True
@subhadipchakraborty8997,2020-08-27T05:48:13Z,0,Could you please explain the same with a classification problem,True
@chirodiplodhchoudhury7222,2020-08-17T10:47:41Z,1,Sir please makr the part 3 and part 4 of the Gradient Boosting series,True
@niladribiswas1211,2020-08-01T13:16:08Z,0,"what is the use of gamma(m) in 3 rd step because  later you changed the forth step to F(x)=Fm-1(x)+alpha*h(x),but in wiki it is gamma (multiplier) instead of alpha which makes quiet sense",True
@Prem-xj7zh,2020-07-27T20:30:53Z,0,Could you please make a video on XGBoost,True
@shindepratibha31,2020-07-23T04:23:21Z,0,Can anyone tell me the need of step 3? Why again minimizing loss function and calculating residual?,True
@amartyahatua,2020-07-15T17:06:14Z,0,Where are you using the gamma_m in the next step? Great tutorial.,True
@gsaravanan4u,2020-07-12T20:06:24Z,0,Y hat is nothing like mean?,True
@abhinav02111987,2020-07-12T04:19:50Z,0,Thank you Krish for helping us understand many complex algorithms.,True
@avishgoswami2141,2020-07-01T08:45:37Z,0,Fantastic !!!,True
@ArunKumar-sg6jf,2020-06-28T18:13:18Z,0,can u make video for light gbm maths intutiton please,True
@kalppanwala6439,2020-06-24T19:26:22Z,0,Wonderful !!! explained like an arrow ie. on point,True
@Arjit_IITH,2020-06-23T13:21:12Z,9,"I am enrolled in an ML online course, but I was unable to understand Gradient Boosting there, but everything is cleared after watching this video. Thank you Krish Naik",True
@SK-ww5zf,2020-06-19T14:19:43Z,2,"Krish -- Fantastic teaching! Thank you! You mention we first fit y to the independent variables, then fit the residual to the independent variables, and repeat that second step. When do we stop iterating? Will there be an iteration after which y-hat will start to deviate away from true y values, and how do we identify that?",True
@someshjaiswal545,2020-06-18T17:50:29Z,7,"Thanks for the explanation Krish. If  someone wonders why Gamma_m in step 4 changed to alpha at 15:32. It is because alpha is a hyperparameter (something whose value you set), set it between 0-1 and it remains fixed through all iteration from m = {1...M}. In this case however you dont need step 3.   If you dont want to set alpha by yourself, want to learn it from data itself, and that is adjusted automatically in each iteration for m={1..M}, use step 3, but from wikipedia link given in description.  Modification at 15:32 is done to make things look simple I believe. Awesome explanation. Thanks Again.",True
@ppsheth91,2020-06-18T05:53:19Z,1,"Hey Krish, Can u please upload the remaining videos for Gradient boosting.. Thanks..",True
@tapasbiswal6693,2020-06-14T16:22:53Z,1,How you gonna implement this equation into python.. Kindly explain,True
@hemantdas9546,2020-06-01T14:13:45Z,0,Great video,True
@shashankbajpai5659,2020-05-26T16:10:22Z,1,The explanation is strikingly similar to StatQuest's explanation on gradient boosting.,True
@TheR4Z0R996,2020-05-20T10:24:02Z,27,"Hey krish, I have a doubt, when we update the model shouldn't we multiply the base learner with gamma_m instead of the learning rate alpha? There is this little mismatch from your video and the wikipedia page. That being said, keep up the good work. You're such an amazing guy, 10x a lot.",True
@somalkant6452,2020-05-19T14:12:38Z,4,"Hey Krish, Thanks a lot for the awesome explaination, just love watching your videos like m watching a TV series, continuously i can watch for 2-3 hours :) One request if time permits can we have video on LGBM and Catboost. There are no good explaination available.",True
@Abhishekpandey-dl7me,2020-05-18T23:24:59Z,0,wonderful explanation. please upload a video on xgboost,True
@pranabjena4438,2020-05-18T16:38:44Z,0,Could you please make a video on xgboost algorithm.,True
@pramodkumargupta1824,2020-05-17T17:13:05Z,2,"wow Krish, you made math equation so easy to understand that it really motives to me to look at equation in different angle. Great Job.",True
@saichaitanya9613,2020-05-16T22:59:08Z,1,"Hi Krish, thanks for your explanation.   So the column r11 is the residual we got when we subtracted y^ with actual target value,but in your explanation you said it is the output of decision tree trained with r11 as target. I am bit off here, may be I might have understood in a wrong way. Anyone can correct me :)",True
@vijaymukkala27,2020-05-16T17:50:10Z,0,You are doing a great job .. This is bulletproof explanation of the whole algo .Actually you made me inspired in recording my own videos of my understanding which might help me in future ..,True
@ganeshkharad,2020-05-14T19:33:10Z,0,that was a good explanation....,True
@vipindube5439,2020-05-14T18:27:50Z,7,"IF you teach in this way people will become passionate about data science, thanks for your effort.",True
@anantvaid7606,2020-05-14T18:12:05Z,0,"Sir, could you make a video to explain which boosting algo is suitable to appropriate scenario?",True
@keerthi5006,2020-05-14T16:08:42Z,0,Awesome explanation. I want to know what is the better course to learn Python for data Science.,True
@chandrudp6567,2020-05-14T12:09:28Z,0,Super,True
@16876,2020-05-14T10:45:31Z,0,thanks a lot,True
@Vignesh0206,2020-05-14T10:42:17Z,0,Awesome sir ðŸ‘ŒâœŒï¸..also please do indepth videos for PCA too.. personally heard many people find it little difficult to understand. Please consider this as an humble request on behalf of all,True
@nareshjadhav4962,2020-05-14T09:25:48Z,1,Very nice explained krish!...can we expect  Xgboost after this or when?,True
@ThePKTutorial,2020-05-14T08:18:47Z,1,Nice video please keep it up,True
@khushboovyas5932,2020-05-14T07:27:13Z,2,Very informative.. thanks sir.. but i have one query here that how do we find optimal number of trees??,True
@punithraj5478,2020-05-14T06:12:24Z,1,Sir videos on NLP??,True
@tanmoybhowmick8230,2020-05-14T06:02:10Z,2,Sir can you please show a full video on model deployment....,True
@dheerendrasinghbhadauria9798,2020-05-14T04:51:13Z,1,"In India , no research happens during masters or PhD degrees . Masters or PhD degree in india is not of much use . In such a case what should indian students do to become data scientist ??",True
@dheerendrasinghbhadauria9798,2020-05-14T04:47:21Z,2,Is Data Structure and Algorithms same for data science field and software developer field ?? Are OOPs & DSA  of software developer field important for data science field as well ??,True
@ManishKumar-qs1fm,2020-05-14T04:41:25Z,2,U r doing well Sir Awesome ðŸ‘,True
@rajeshrajie1237,2020-05-14T04:34:07Z,0,Thanks Krish ...Good explanation ..Much Appreciated..,True
@priyabratamohanty3472,2020-05-14T04:27:22Z,1,Nice to see the gradinent boosting series,True
@parthsingh3473,2020-05-14T03:44:16Z,1,Hello I am first year btech student. How much maths  is needed  for ai. As I am average in mathematics should I choose Ai as my career option  Please tell sir,True
