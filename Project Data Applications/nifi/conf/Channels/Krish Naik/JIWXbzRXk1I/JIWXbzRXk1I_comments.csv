author,updated_at,like_count,text,public
@kumarpiyush2169,2020-08-11T15:26:10Z,117,"HI Krish.. dL/dW'11= should be  [dL/dO21. dO21/dO11. dO11/dW'11] +           [dL/dO21. dO21/dO12. dO12/dW'11] as per the last chain rule illustration. Please confirm",True
@amitdebnath2207,2024-05-07T17:06:26Z,0,Hats Off Brother,True
@calista-e,2024-03-11T07:23:59Z,0,0.20*0.02 = 0.004,True
@Haraharavlogs,2024-01-23T10:32:39Z,0,you are legend nayak sir,True
@adityashewale7983,2023-07-24T04:27:52Z,0,"hats off to you sir,Your explanation is top level, THnak you so much for guiding us...",True
@Thriver21,2023-06-10T05:45:32Z,0,nice explanation.,True
@naresh8198,2023-05-24T14:25:10Z,0,crystal clear explanation !,True
@sapnilpatel1645,2023-05-19T04:08:35Z,1,so far best explanation about vanishing gradient.,True
@Joe-tk8cx,2023-04-09T10:37:20Z,0,"Great video, one question, when you calculate the new weights using the old weight - learning rate x derivative of loss with respect to weight, the derivative of loss wrt weight is that the sigmoid function ?",True
@naimamushfika1167,2023-03-14T19:05:23Z,0,nice explanation,True
@gouthamkarakavalasa4267,2023-03-01T15:49:16Z,0,"Gradient Descent will be applied on Cost function right ?-1/m  Î£ (Y*log(y_pred) + (1-y)* log(1-y_pred))... in this case if they had applied on the activation function, how the algo will come to global minima.",True
@maheshsonawane8737,2023-02-05T08:40:49Z,0,Very nice now i understand why weights doesn't update in RNN. The main point is derivative of sigmoid is between 0 and 0.25. Vanishing gradient is associated with only sigmoid function. ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹ðŸ‘‹,True
@shaikhabuzar1364,2022-11-23T07:11:03Z,0,Think how W'11 will affect O22? It will not affect! hence plus part will not come. Final equation will be ... dL/dW'11= [dL/dO21. dO21/dO11. dO11/dW'11],True
@rhul0017,2022-10-24T03:11:56Z,0,"you talk about changing weights on back propagation by derivation and then suddenly talks about derivating activation function, i dont understand that part, does function triggered during back propagation?",True
@salimtheone,2022-10-15T00:55:00Z,0,very well explained 100/100,True
@UniversalRankingOfficial,2022-10-12T23:58:21Z,0,Indians are built differently. Just watching the first minutes of their tutorials you understand what your professors failed to make you understand in  several hours ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚,True
@dukesoni5477,2022-09-30T10:51:49Z,0,could you upload the notes all  this lectures,True
@elielberra2867,2022-08-31T21:07:15Z,0,"Thank you for all the effort you put into your explanations, they are very clear!",True
@rayyankhattak544,2022-08-26T04:00:16Z,0,Great-,True
@Vinay1272,2022-08-22T13:10:21Z,6,"I have been taking a well-known world-class course on AI and ML since the past 2 years and none of the lecturers have made me so interested in any topic as much as you have in this video. This is probably the first time I have sat through a 15-minute lecture without distracting myself. What I realise now is that I didn't lack motivation or interest, nor that I was lazy - I just did not have lecturers whose teaching inspired me enough to take interest in the topics, yours did.  You have explained the vanishing gradient problem so very well and clear. It shows how strong your concepts are and how knowledgeable you are.  Thank you for putting out your content here and sharing your knowledge with us. I am so glad I found your channel. Subscribed forever.",True
@nazgulzholmagambetova1198,2022-08-12T06:13:13Z,0,great video! thank you so much!,True
@nikunjlahoti9704,2022-08-07T11:45:47Z,0,Great Lecture,True
@aaryankangte6734,2022-07-13T06:58:59Z,0,Sir thank u for teaching us all the concepts from basics but just one request is that if there is a mistake in ur videos then pls rectify it as it confuses a lot of people who watch these videos as not everyone sees the comment section and they just blindly belive what u say. Therefore pls look into this.,True
@nola8028,2022-07-06T08:19:58Z,0,You just earned a +1 subscriber ^_^ Thank you very much for the clear and educative video,True
@faribataghinezhad3293,2022-07-05T15:43:09Z,0,Thank you sir for your amazing video. that was great for me.,True
@LazingOnSunday,2022-06-23T08:14:20Z,0,This video is really goooooddd! Can anyone help me understand why the derivate value decreases as we go backward @9:03? I am new to DL..!!,True
@FlyingTurtleLP,2022-06-13T14:31:14Z,0,What I didnt get: What can the values of the derivative of the sigmoid function be? I don't think you mentioned it in the video.,True
@gultengorhan2306,2022-04-17T16:22:51Z,1,You are teaching better than many other people in this field.,True
@shmoqe,2022-04-04T14:24:05Z,0,"Great explanation, Thank you!",True
@abdulqadar9580,2022-03-10T09:39:49Z,0,Great efforts Sir,True
@anujak786,2022-02-25T04:15:00Z,0,Even I have the same question,True
@vikrantchouhan9908,2022-02-19T10:38:16Z,2,Kudos to your genuine efforts. One needs sincere efforts to ensure that the viewers are able to understand things clearly and those efforts are visible in your videos. Kudos!!! :),True
@ashwinshetgaonkar6329,2022-02-07T10:08:46Z,0,"output O21 also depends upon O12,so its derivative should also be considered",True
@koraymelihyatagan8111,2022-01-29T15:30:11Z,0,"Thank you very much, I was wandering around the internet to find such an explanatory video.",True
@nabeelhasan6593,2022-01-10T05:05:49Z,0,"Very nice video sir , you explained very well the inner intricacies of this problem",True
@raj4624,2021-11-23T13:51:54Z,0,superb,True
@venkatshan4050,2021-11-23T12:02:32Z,1,Marana mass explanationðŸ”¥ðŸ”¥. Simple and very clearly said.,True
@spicytuna08,2021-10-30T21:42:21Z,0,you teach better than ivy league professors.  what a waste of money spending $$$ on  college.,True
@vishaljhaveri6176,2021-10-03T09:06:15Z,0,"Thank you, Krish SIr. Nice explanation.",True
@feeham,2021-09-30T05:01:52Z,0,Thank you !!,True
@skiran5129,2021-09-25T19:39:00Z,0,I'm lucky to see this wonderful class.. Tq..,True
@GunjanGrunge,2021-09-25T17:51:00Z,0,that was very well explained,True
@grownupgaming,2021-09-18T03:21:01Z,0,Why cant they just make the activation function curve aggressively spike up at x=0?,True
@himanshubhusanrath2492,2021-09-12T12:08:30Z,0,One of the best explanations of vanishing gradient problem. Thank you so much @KrishNaik,True
@benoitmialet9842,2021-09-06T21:25:37Z,1,"Thank you so much, great quality content.",True
@abhinavkaushik6817,2021-08-27T23:11:50Z,0,Thank you so much for this,True
@piyalikarmakar5979,2021-08-16T05:19:39Z,0,One of the best vedio on clarifying Vanishing Gradient problem..Thank you sir..,True
@AdarshSingh-nb2ql,2021-08-06T18:24:38Z,0,"I have one doubt, if we use sigmoid only in the last layer, due to multiple back and forth propagation, won't that minimize the derivative of loss function to 0 - 0.25",True
@krishj8011,2021-07-10T11:28:24Z,0,Very nice series... ðŸ‘,True
@MauiRivera,2021-06-22T13:24:12Z,0,"I like the way you explain things, making them easy to understand.",True
@ambreenfatimah194,2021-06-11T05:41:06Z,0,Helped a lot....thanks,True
@BalaguruGupta,2021-05-28T06:09:57Z,0,Thanks a lot sir for the wonderful explanation :),True
@tonnysaha7676,2021-05-25T06:44:19Z,0,Thank you thank you thank you sir infinite timesðŸ™.,True
@AnirbanDasgupta,2021-05-22T06:11:41Z,0,excellent video,True
@sowmyakavali2670,2021-05-15T18:25:50Z,1,"Hi krish  everyone says that  Wnew = Wold - n * dL/dWold  theoritically we know that dL/dWold means slope  where as in practical scenario    L is a single scalar value   Wold is also a single scalar value Then how dL/dWold is calculating ???  And also coming to the activation function , you are explaining it theoritically , can you explain it by taking practical values ? , and don't tell it by taking predefined function or module ,  bcz we know how to find a module and import it and how to use it , but we don't know practical",True
@YashSharma-es3lr,2021-05-12T06:55:09Z,0,very simple and nice explanation . I understand it in first time only,True
@louerleseigneur4532,2021-05-11T21:05:37Z,0,Thanks krish,True
@souravdey1227,2021-04-23T16:56:39Z,0,"Why is it called vanishing gradient, not vanishing weight. Not a big deal, but just curious.",True
@classictremonti7997,2021-04-13T22:58:37Z,0,Krish...you rock brother!!  Keep up the amazing work!,True
@classictremonti7997,2021-04-12T21:28:33Z,0,So happy I found this channel!  I would have cried if I found it and it was given in Hindi (or any other language than English)!!!!!,True
@nikhili9559,2021-03-07T12:01:58Z,0,"the derivative of Sigmoid ranges (0,0.25) but can never be equal to 0/0.25.",True
@khiderbillal9961,2021-03-05T13:33:06Z,1,so relu function is the best solution for deleting vanishing gradient descent,True
@khiderbillal9961,2021-03-05T13:30:31Z,1,thanks sir you really hepled me,True
@arunmeghani1667,2021-02-27T20:33:55Z,0,great video and great explanation,True
@suryagunda4038,2021-02-17T17:35:54Z,0,May god bless you ..,True
@manojthanu,2021-02-17T11:00:02Z,0,"Hello Sir , Your videos were awesome , could you please help me out in learning the concept of  deeplearning with R",True
@anusuiyatiwari1800,2021-02-10T18:46:40Z,0,Very interesting,True
@skviknesh,2021-01-07T08:10:18Z,1,I understood it. Thanks for the great tutorial!  My query is:  weight vanishes when respect to more layers. When new weight ~= old weight result becomes useless.  what would the O/P of that model look like (or) will we even achieve global minima??,True
@muhammadarslankahloon7519,2020-12-27T11:54:12Z,2,"Hello sir, why the chain rule explained in this video is different from the very last chain rule video. kindly clearly me and thanks for such an amazing series on deep learning.",True
@ayushprakash3890,2020-12-26T16:16:50Z,1,is this equation correct ?? (this equation is used in the starting of the video) dL / dw11 = dO21 / dO11 * dO11 / dw11 should it be :  dL / dw11 = dL / dO21 * dO21 / dO11 * dO11 / dw11,True
@melikad2768,2020-11-27T09:03:53Z,1,"Thank youuuu, its really great:)",True
@susmitvengurlekar,2020-11-21T15:59:52Z,0,"Understood completely! If weights hardly change, no point in training and training. But I have got a question, where can I use this knowledge and understanding I just acquired ?",True
@mittalparikh6252,2020-11-14T06:57:23Z,1,"Overall got the idea, that you are trying to convey. Great work",True
@marijatosic217,2020-11-08T21:48:39Z,3,I am amazed by the level of energy you have! Thank you :),True
@ilyoskhujayorov8498,2020-11-05T11:33:14Z,0,Thank you !,True
@lalithavanik5022,2020-10-23T15:00:04Z,0,Nice expalnation sir,True
@Xnaarkhoo,2020-10-16T13:11:17Z,14,"many years ago in the college I was enjoy watching videos from IIT - before the mooc area, India had and still have many good teachers ! It brings me joy to see that again. Seems Indians have a gene of pedagogy",True
@shubhangiagrawal336,2020-10-16T05:01:47Z,0,plz dont repeat the concepts again and again and plz clarify the formula of weight updation,True
@AA-yk8zi,2020-10-13T13:32:24Z,0,Thank you so much,True
@al3bda,2020-10-11T22:44:56Z,1,oh my god you are a good teacher i really fall in love how you explain and simplify things,True
@rahuldey6369,2020-10-06T13:11:44Z,0,"I'm a bit confused whether 'O's are weighted sums or activation of the weighted sums. If they are the activation of the weighted sums say 'a' and the weighted sums be 'z', then won't it be like- (dL/dw= dL/da * da/dz * dz/dw)",True
@niazmorshedulhaque4519,2020-10-02T12:47:17Z,0,Dear Sir Splendid tutorial indeed. Please share the reference book    link that you are following.,True
@mohsinkhan7470,2020-09-24T05:49:54Z,0,"Can anybody explain that what is ""X"" in the equation Z = sigma XW + b.       The equation is written at the top right corner of the white board in video. This is the only confusing point to me. Thanks",True
@varayush,2020-09-15T10:15:10Z,0,@krish: thanks for the wonderful lessons on the neural network. may I request you to correct the equation using some text box on the video as this will have intact information that you would like to pass on,True
@_jiwi2674,2020-09-09T01:32:55Z,0,"you meant that the derivative of the sigmoid is between 0 and 0.25, right? I wanted to clarify about that range written in red color. The sigmoid of z would be between 0 and 1, from what I understood. Any reply will be appreciated :)",True
@omernaeem1388,2020-09-08T21:43:40Z,0,Sir GANs ky bhi tutorials bnae plz,True
@praveen3779,2020-09-06T23:51:32Z,0,"An honest feedback, video content was good but due to consistent repeating of the same thing(like ""value of the sigmoid derivative is between 0 and 0.25"") the video was unnecessary long and became annoying after a point",True
@yoyomemory6825,2020-08-20T23:08:08Z,1,"Very clear explanation, thanks for the upload.. :)",True
@benvelloor,2020-08-02T16:49:57Z,1,Very well explained. I can't thank you enough for clearing all my doubts!,True
@leninmeher8043,2020-08-01T16:10:25Z,0,"In place of dL/dw, it should be dy(hat) /dw as dy(hat) =dO21 but dL is not equal to dO21",True
@naughtyrana4591,2020-07-27T08:52:14Z,0,Guruvar ko pranamðŸ™,True
@gowthamprabhu122,2020-07-22T11:55:17Z,1,Can someone please explain why the derivative of each parent layer reduces ? i.e why does layer two have lower derivative of O/P with respect to its I/P?,True
@swapwill,2020-07-17T06:20:53Z,0,The way you explain is just awesome,True
@ngelospapoutsis9389,2020-07-14T14:50:34Z,0,so if we have 2 layers and as we know 1 forward and back step is 1 epoch. If we now have 100 epochs the derivative is going to get smaller every time?  Or the vanishing problem is due to many hidden layers and not depended on the number of epochs?,True
@rushikeshmore8890,2020-07-13T12:50:19Z,0,"Kudos sir ,am working as data analyst read lots of blogs , watched videos but today i cleared the concept . Thanks for The all stuff",True
@chinmaybhat9636,2020-07-04T11:11:04Z,0,https://www.quora.com/Where-can-one-find-proof-that-the-derivative-of-a-sigmoid-function-ranges-between-0-and-0-25 Is this the correct Explaination of Derivative of Sigmoid Function ranges between 0 to 0.25 @KrishNaik Sir ??,True
@deepthic6336,2020-07-02T20:53:00Z,0,"I must say this, normally I am kinda person who prefers to study on own and crack it. Never used to listen to any of the lectures till date because I just don't understand and I dislike the way they explain without passion(not all though). But, you are a gem and I can see the passion in your lectures. You are the best Krish Naik.  I appreciate it and thank you.",True
@gaurawbhalekar2006,2020-07-02T14:27:53Z,0,excellent explanation sir,True
@RAZONEbe_sep_aiii_0819,2020-06-29T13:20:31Z,1,"There is a very big mistake at 4:14 sir, you didn't applied the chain rule correctly, check the equation.",True
@MrSmarthunky,2020-06-28T12:21:02Z,0,Krish.. You are earning a lot of Good Karmas by posting such excellent videos. Good work!,True
@surajpawar1812,2020-06-26T15:26:53Z,0,problem ocurrs when you initialize your weights badly which will result in exploding your gradients or vanishing gradients,True
@dhananjayrawat317,2020-06-25T17:10:52Z,0,best explanation.  Thanks man,True
@b0nnibell_,2020-06-24T05:39:48Z,0,you sir made neural network so much fun!,True
@aishwaryaharidas2100,2020-06-19T05:16:52Z,0,"Should we again add bias to the product of the output from the hidden layer O11, O12 and weights W4, W5?",True
@ArthurCor-ts2bg,2020-06-14T04:31:56Z,0,Excellent ðŸ‘Œ,True
@narsingh2801,2020-06-13T16:24:25Z,0,You are just amazing. Thnx,True
@honeycands1043,2020-06-13T01:25:59Z,1,What kind of job are you doing? Are you an employee in any Deep Learning based Industry ?,True
@sumeetseth22,2020-06-09T21:59:32Z,0,"Love your videos, I have watched and taken many courses but no one is as good as you",True
@manujakothiyal3745,2020-06-05T19:35:24Z,1,Thank you so much. The amount of effort you put is commendable.,True
@zoroXgamings,2020-06-05T05:16:37Z,0,"if u ask me which one is more understanding between andrew Ng and krish naik , i think this would be my choice",True
@ltoco4415,2020-06-03T19:08:02Z,6,Thank you sir for making this misleading concept crystal clear. Your knowledge is GOD level ðŸ™Œ,True
@prerakchoksi2379,2020-05-30T14:37:36Z,1,"I am doing deep learning specialization, feeling that this is much better than that",True
@yousufborno3875,2020-05-30T09:04:35Z,0,You should get Oscar for your teaching skills.,True
@rahul625k,2020-05-22T12:16:33Z,0,0.004,True
@kirankumarj8229,2020-05-17T15:11:00Z,0,"Hi Krish, Thanks for the good videos..God may bless you and your family.. Do we have merial for ML and DL.If you have how to get it.",True
@daniele5540,2020-05-16T15:37:35Z,1,Great tutorial man! Thank you!,True
@karth12399,2020-05-08T16:36:12Z,3,Sir you are saying derivative of sigmoid is 0 to 0.25. I understand it.  But how that will imply derivative of O21 /derivative of 011 should be less than 0.25.  Could you please help me understand that assumption,True
@sandipansarkar9211,2020-04-15T20:26:16Z,0,Thanks krish .Video was superb but I am having apprehension I might get lost somewhere .Please provide some reading reference regrading this topic considering as a beginner.Cheers,True
@vikasbhadoria6535,2020-04-10T14:14:49Z,0,Do we face Vanishing Gradient problem also in Relu?,True
@Adinasa2,2020-03-29T10:03:51Z,0,How does relu take care of vanishing gradient problem,True
@Adinasa2,2020-03-29T09:49:48Z,0,Why was relu function not invented,True
@ganeshkharad,2020-03-28T15:45:56Z,0,nice explaination,True
@QScientist,2020-03-23T11:01:33Z,0,What does optimizer mean?,True
@lekjov6170,2020-03-13T18:27:44Z,36,"I just want to add this mathematically, the derivative of the sigmoid function can be defined as: *derSigmoid = x * (1-x)* As Krish Naik well said, we have our maximum when *x=0.5*, giving us back: *derSigmoid = 0.5 * (1-0.5) --------> derSigmoid = 0.25* That's the reason the derivative of the sigmoid function can't be higher than 0.25",True
@juliepravin7960,2020-03-13T09:57:34Z,0,Sir please make this video in hindi...ðŸ˜Š,True
@magicalflute,2020-02-23T14:04:25Z,0,"Very well explained. Vanishing gradient problem as per my understanding is that, it is not able to perform the optimizer job (to reduce the loss) as old weight and new weights will be almost equal. Please correct me, if i am wrong. Thanks!!",True
@tosint,2020-02-21T01:03:13Z,11,"I hardly comment on videos, but this is a gem. One of the best videos explaining vanishing gradients problems.",True
@mahabir05,2020-02-15T09:26:14Z,34,"I like how you explain and end your class ""never give up "" It very encouraging",True
@abhisheksainani,2020-01-23T07:10:47Z,1,In red color you've written that sigmoid of z is between 0 and 0.25 whereas you meant to say that the derivative of sigmoid of z is between 0 and 0.25.,True
@PeyiOyelo,2020-01-19T13:56:09Z,43,"Sir or As my Indian Friends say, ""Sar"", you are a very good teacher and thank you for explaining this topic. It makes a lot of sense. I can also see that you're very passionate however, the passion kind of makes you speed up the explanation a bit making it a bit hard to understand sometimes. I am also very guilty of this when I try to explain things that I love. Regardless, thank you very much for this and the playlist. I'm subscribed âœ…",True
@shahidabbas9448,2020-01-11T03:39:38Z,1,Sir i'm really confusing about the actual y value please can you tell about that. i thought  it would be our input value but here input value is so many with one predicted  output,True
@satyadeepbehera2841,2020-01-06T15:48:30Z,3,"Appreciate your way of teaching which answers fundamental questions.. This ""derivative of sigmoid ranging from 0 to 0.25"" concept was nowhere mentioned.. thanks for clearing the basics...",True
@neelanshuchoudhary536,2019-12-29T17:15:00Z,1,"very nice explanation,,great :)",True
@bhavikdudhrejiya4478,2019-12-06T14:54:42Z,0,"Very nice way to explain. Learned from this video-  1. Getting the error (Actual Output - Model Output)^2 2. Now We have to reduce an error i.e Backpropagation, We have to find a new weight or a new variable  3.  Finding New Weight = Old weight  x Changes in the weight 4. Change in the Weight = Learning rate x d(error / old weight) 5. After getting a new weight is as equals to old weight due to derivate of Sigmoid ranging between 0 to 0.25 so there is no update in a new weight 6. This is a vanishing gradient",True
@anandemani5472,2019-11-22T11:51:53Z,0,"Hi Krish,Can we declare convergence when the weights are decreasing less than 0.0001?",True
@kamal6762,2019-11-13T20:34:15Z,0,In this particular case if we take the Learning rate value very less like about .001 then the weights difference is going to very less.,True
@gopalakrishna9510,2019-11-10T14:47:57Z,0,on what basis  no's of hiden layers will be create ?,True
@aidenaslam5639,2019-11-05T22:56:35Z,3,Great stuff! Finally understand this. Also loved it when you dropped the board eraser,True
@nirmalroy1738,2019-10-10T03:22:23Z,0,super video...extremely well explained.,True
@sunnysavita9071,2019-10-07T17:09:06Z,0,very good explanation.,True
@vasupatel6932,2019-10-06T04:05:43Z,2,0.2*0.02!=0.4,True
@sunnysavita9071,2019-09-15T14:44:46Z,0,"your videos are very helpful ,good job and good work keep it up...",True
@thanikainathants9535,2019-08-27T11:27:28Z,1,"Krish, Quick Query..I am representing derivative as D in the below sentence. Per the chain block video, D loss/D  w11= D loss/D O21 * D O21/D 011 *D011/Dw11. Please correct me.",True
@gautam1940,2019-08-18T03:04:04Z,3,This is an interesting fact to know.  Makes me curious to see how ReLU overcame this problem,True
@narayanjha3488,2019-08-09T08:12:39Z,1,This video is amazing and you are amazing teacher thanks for sharing such amazing information Btw where are you from banglore?,True
@MsRAJDIP,2019-08-08T14:32:23Z,2,"Tommorow I have interview, clearing all my doubts from all your videos ðŸ˜Š",True
@winviki123,2019-08-08T12:35:13Z,2,Could you please explain why bias is needed in neural networks along with weights?,True
@sekharpink,2019-07-22T19:55:18Z,32,"Derivative of loss with respect to w11 dash you specified incorrectly, u missed derivative of loss with respect to o21 in the equation. Please correct me if iam wrong.",True
@manikosuru5712,2019-07-22T18:01:51Z,4,As usual extremely good outstanding...  And a small request can expect this DP in coding(python) in future??,True
@meanuj1,2019-07-22T14:40:46Z,4,Nice presentation..so much helpful...,True
@hiteshyerekar9810,2019-07-22T14:22:20Z,4,"Nice video Krish.Please make practicle based video on gradient decent,CNN,RNN.",True
@hokapokas,2019-07-22T14:17:25Z,4,Good job bro as usual... Keep up the good work.. I had a request of making a video on  implementing back propagation. Please make a video for it.,True
