author,updated_at,like_count,text,public
@rahulbhardwaj3843,2022-08-04T11:26:19Z,0,"Krish , Why not xgboost algorithm , because its also loading the completed dataset and doing parallel processing of data using cache aware access.",True
@pavankumarpotta4565,2021-06-11T07:41:25Z,1,there is a library called dask which does the parallel processing of the data (ML),True
@manishgurunule2189,2021-06-09T18:16:34Z,1,set chunk_size,True
@True_Feelingsss...,2021-06-07T14:39:52Z,0,Basic ML interview question: Why/When ML model stops training after calling fit method ??,True
@aswathrao2644,2021-06-07T11:23:35Z,0,Can we use something like distributed systems to solve these kind of problems ?,True
@sauravojha6345,2021-06-05T06:56:56Z,2,"In deep learning we have to reduce batch size.. In ML we need to batch train our model, means, we load part of our dataset, train and load the next one.",True
@anilchaudhry804,2021-06-05T06:16:24Z,0,can such questions be asked for freshers also??,True
@Abhishek-jd9qk,2021-06-05T03:56:27Z,1,"Krish , cant we have similar batch technique for ML models too ?",True
@rookiedrummer6838,2021-06-05T01:35:19Z,0,"For deep learning we need to use , generators , reduced batch size and less deeper model. If it's a CNN than we can even reduce image size  For ML model we will do stratify sampling the data.",True
@harishg7371,2021-06-04T17:14:39Z,1,How do you split 12gb dataset to six  2gb ones?,True
@raviyadav2552,2021-06-04T16:50:03Z,2,may we can train the deep learning model by providing the data into batches such that our data in one batch will be less than the 4GB. no idea about ML.,True
@parameswararao3973,2021-06-04T15:41:05Z,5,What about bagging techniques where we can use chunks of dataset by row and column sampling? We can bootstrap and aggregate them later.,True
@divyajansari4986,2021-06-04T15:36:28Z,1,Please upload more videos on Transformers and Bert Implementations,True
@AshitDebdas,2021-06-04T15:00:45Z,4,Similar kind of question I got today.. But I was confused... ðŸ˜­,True
@raneshmitra8156,2021-06-04T14:15:37Z,2,Please make a detailed video on partial fit,True
@shubhangisakarkar9532,2021-06-04T13:59:04Z,1,When read file using read_csv we can put low memory partner true which will lead less memory utilization. But data is 12gb initial preprocessing like removing of ids column and columns which does not provide any info can dropped.there are lot of parameters itself in read_csv which can help in less memory utilization. Other alternative can be using google colab.,True
@sanjubollineni9877,2021-06-04T13:57:39Z,2,"Krish in ML instead of using partial_fit, can't we use Dask, Modin libraries which are available in python for paralleling??",True
@menakask6050,2021-06-04T13:47:49Z,5,divide the 12gb datasets into 3 4gb datasets and train the model with  2 4gb datasets and the remaining 1 4gb dataset if for testing.(Incase of Machine Learning),True
@suriyab8143,2021-06-04T13:38:25Z,1,"@krish Sir..   Kindly post ur email I'd here Sir.  Regards,   Dr. SURIYA BEGUM",True
@rakeshsekar3840,2021-06-04T13:12:11Z,2,Hi,True
