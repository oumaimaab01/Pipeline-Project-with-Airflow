author,updated_at,like_count,text,public
@vasudevrv7417,2024-05-09T07:45:44Z,0,"Great work Sudhanshu, I can see your passion while teaching",True
@aritradutta9538,2024-02-07T04:54:16Z,1,nobody on internet explained attention better than sudhansu sir,True
@voldemort7585,2023-09-03T21:05:27Z,0,44,True
@avinashanand2163,2023-07-27T14:54:02Z,0,"Bhai he isn't explaining what the video has intended  to explain rather so much here and there, and bakwaas. Either come straight on the point or don't waste time bro.",True
@vishnusit1,2023-07-12T10:19:46Z,1,"Video timestamp. Input feeding is confusing..i think This is correct way.  Suppose we have a bidirectional RNN with LSTM cells and an input sequence consisting of the following words: ""I"", ""love"", ""to"", ""eat"", ""pizza"".  Input Representation:  Each word is typically represented as a word embedding vector. Let's assume we have pre-trained word embeddings, where each word is represented by a 100-dimensional vector. Forward RNN:  The forward RNN processes the input sequence from the beginning to the end. At each time step, the forward RNN receives the input word embedding vector for that time step. Time step 1: Input word embedding vector = ""I"" embedding Time step 2: Input word embedding vector = ""love"" embedding Time step 3: Input word embedding vector = ""to"" embedding Time step 4: Input word embedding vector = ""eat"" embedding Time step 5: Input word embedding vector = ""pizza"" embedding  Backward RNN:  The backward RNN processes the input sequence in reverse, from the end to the beginning. At each time step, the backward RNN receives the input word embedding vector for that time step. Time step 1: Input word embedding vector = ""pizza"" embedding Time step 2: Input word embedding vector = ""eat"" embedding Time step 3: Input word embedding vector = ""to"" embedding Time step 4: Input word embedding vector = ""love"" embedding Time step 5: Input word embedding vector = ""I"" embedding  Combining Outputs:  The outputs from the forward and backward RNNs can be combined in different ways, such as concatenation or addition, depending on the specific architecture with alpha weight. Let's assume we concatenate the forward and backward outputs at each time step. Time step 1: Concatenated output = [forward output at time step 1; backward output at time step 1] Time step 2: Concatenated output = [forward output at time step 2; backward output at time step 2] Time step 3: Concatenated output = [forward output at time step 3; backward output at time step 3] Time step 4: Concatenated output = [forward output at time step 4; backward output at time step 4] Time step 5: Concatenated output = [forward output at time step 5; backward output at time step 5]  In this way, the input sequence ""I love to eat pizza"" is fed into the bidirectional RNN. At each time step, the forward and backward RNNs receive the input word embedding vector, process it, and produce their respective outputs. The outputs from both directions can be combined for further processing or used separately depending on the specific task or architecture.",True
@messiisthebest,2023-06-16T16:36:09Z,0,best lecture,True
@avikdey7400,2023-04-03T05:21:25Z,0,It is different from the attention that is used in bert or transformer.,True
@thatsfantastic313,2023-02-25T12:19:02Z,0,brilliant teacher.,True
@tanb13,2022-11-15T09:12:59Z,0,üòïüòïüòïconfusing....,True
@maaz_dev,2022-09-03T20:46:15Z,0,"Amazing session, can't believe I'm getting latest research level stuff for free at such ease.üéâ",True
@dhirendra2.073,2022-06-03T04:28:09Z,0,Superb Explanation of such a complex subject,True
@ravish5387,2022-05-28T15:52:36Z,0,Superb explanation,True
@VistaVagabond,2022-05-27T14:36:18Z,3,I have gone through multiple videos of attention model but the concept was not clear. Seeing Sudhanshu Sir's explanation all my doubts are cleared. This is the best ever explanation of attention model. The way of teaching and making us understand the concept is really the best. Thank you Krish sir for bringing these type of sessions.,True
@thepresistence5935,2022-04-01T07:06:18Z,0,I am here after 6 months :),True
@manalisingh1128,2022-02-11T10:00:38Z,0,So far this is the best video on Attention. Very rich in information and everything is explained very very clearly and to the point. Simply wow.  I watched his previous videoon encoder decoder  and wasn't very happy but thiss? It's a gem üíé,True
@raj4624,2022-01-22T07:32:40Z,1,gem video,True
@shekharkumar1902,2022-01-01T14:02:38Z,0,Sudhanshu Ji .. if you write 'used to'  ...it will translate correctly. Krish tried to help you but you moved forward.,True
@rakeshkumarmallik1545,2021-12-16T11:30:33Z,0,"where is the third session, any link ?",True
@himanshubhusanrath212,2021-09-30T15:28:38Z,2,Exceptionally excellent explanation. What a simplified way of explaining the internal structure & working of the Attention-based model !! Thank you so much brother Sudhanshu !!!,True
@thepresistence5935,2021-09-30T07:36:56Z,0,Whenever I seeing video second time I am getting lot of knowledge,True
@thepresistence5935,2021-09-28T10:59:31Z,0,Fact: These last 2 videos recalling all the 50 videos of krish naik.,True
@rajm5349,2021-07-30T11:43:08Z,2,what i learned from this video is 1. the output from both directions is considered as h value. 2. the context is taking the output from the encoder into the decoder 3.w-- we get this value from the attention model  and the overall context is attention model is unable to do for long sentences-- if i understood in wrong way anyone can correct me.  thank you krish and sudh sir,True
@louerleseigneur4532,2021-06-05T21:42:22Z,0,Thanks Sudhanshu and Krish,True
@anmolnarang7606,2021-05-01T12:20:06Z,0,Please also discuss bert encoder architecture,True
@sandipansarkar9211,2021-04-30T14:57:39Z,0,great explanation,True
@manishbolbanda9872,2021-04-21T21:46:48Z,0,"1:16:10  you have used the word ""use"" instead of ""used"" that the reason traslator saying ""upyog karta hai""",True
@manishbolbanda9872,2021-04-21T21:34:58Z,0,please make a video on Disentangled Variational AutoEncoder,True
@ayushichoudhary6989,2021-03-19T18:04:41Z,0,Please upload the part-3 on bert,True
@ayushichoudhary6989,2021-03-19T18:04:22Z,0,Nicely explained,True
@RishSandMagic,2021-03-18T19:32:10Z,0,Not a single minute wasted,True
@vinitkothari8502,2021-03-17T11:51:27Z,0,please add video on BERT,True
@Want_to_escape,2021-03-12T11:37:42Z,12,"Sudhanshu sir is a blessed teacher. A rare human created by the god almighty. He knows really in depth about what he is teaching. I am really surprised by seeing this. Even though there are other videos saying about Attention Models, it is not easy to understand like this. God bless you. Thank you Krish sir for bringing him to define this concept for us.",True
@hardikvegad3508,2021-02-13T09:19:16Z,0,There are no hidden layer in ANN which is between encoder and decoder??,True
@pratikpratik8495,2021-01-30T11:18:51Z,0,want more session with Sudhansu ....nicely explain,True
@prakashkafle454,2021-01-23T19:14:18Z,1,Where is bert here ??? I can't understand anything ü§£,True
@theip7556,2021-01-17T07:44:02Z,1,I didn't find the part-3 of this series with supposed to discuss the Transformer and BERT. Any idea if its recorded at all ?,True
@abhisheksainani,2020-12-02T10:15:33Z,0,Can we use the same set of sentences in both pretraining (same sentences without labels) and fine-tuning (same sentences with labels) of BERT or is that a bad practice and does that lead to bias?,True
@gulabmeetyou,2020-10-14T08:45:46Z,5,"The topic you mentioned ""Attention Models, Transformers And Bert In depth Intuition Deep Learning- Part 2"" but you covered only Attention model then why you mentioned other topics?",True
@khalifadaiict7715,2020-09-12T16:48:34Z,0,"It will be great if you can involve speech recognition task also, please",True
@magelauditore333,2020-09-12T10:56:58Z,0,great session liking it very much. Want more sessions and discussion on research paper,True
@ajaybhatt6820,2020-09-12T07:12:24Z,1,sir please upload its implementation vedio asap üôèüôè,True
@abilashsagar8150,2020-09-10T02:24:54Z,1,Can we apply transfer learning here to the concept of encoders and decoders along with attention layer. Assuming Decoder ( and/or attention) as the top layer? Use case could be same English training sentences are translated to both French and Spanish. One encoder to Many Decoders.,True
@abhishek307100,2020-09-01T12:26:40Z,0,"So, the window is not the moving window in this case??",True
@laxmipreethi,2020-09-01T11:42:16Z,0,"Hi Sudhanshu and Krish, Thanks a ton for making such amazing lecture videos. I was able to get most of the attention model functionality  from part 2 video. Could you please tell me when Part 3 will be released? I am currently working on Transformer model and I need to understand the functionality of it and each component usage in detail. Please advice. Thanks once again. Stay safe.",True
@ShortVine,2020-09-01T09:40:28Z,0,When is the next session  on Transformers ? please reply.,True
@harikunamneni5163,2020-08-31T15:49:22Z,0,Has monday transformer already started?,True
@gurdeepsinghbhatia2875,2020-08-30T20:40:27Z,0,"****How this is possible that j will always start with 1 because if this is so then at every attention it will always pick j=1 to j=tx ,, and this means that it will always choose first 4 h's for all ANN so formed during attention ?????????Please reply, if anyone knows****",True
@chetanmundhe8619,2020-08-29T06:11:47Z,0,"Very good session sir, thanks to both of u, i have one question- for speech to text which model we should go with?",True
@pratheeeeeesh4839,2020-08-28T17:40:32Z,0,"Sir, as you said we are passing the hidden information of the previous timestep of the decoder and the the output of the encoder to the attention function(a) , but what does that attention function do to calculate alpha(i,j) value",True
@mehuljain4920,2020-08-27T16:40:15Z,1,Very informative session can‚Äôt wait to see the transformer video with mathematical equations explanation thanks a lot for bringing it up,True
@samarafroz9852,2020-08-27T10:23:01Z,2,Awesome tutorial sir. But I'd loved your tutorial in this topic. Way you explained the topic is best in whole YouTube,True
@vihaanrajput8082,2020-08-27T10:20:00Z,1,"Hey krish, i want to know how the red cell and the blue cell connected to eachother, video time (22:02)",True
@programming_hut,2020-08-27T03:03:57Z,3,i was eagerly waiting for this thanks a lot,True
@smilebig3884,2020-08-26T22:53:49Z,7,Please have more sessions like these. And also add coding sessions as well. Thumbs up for sudhanshu,True
@ranjiranju7759,2020-08-26T17:42:41Z,1,"Wat does that ""guaranteed internship"" mean krish in ur video  description? Thr vil b interview held before joining as an intern right",True
@dsbupadrasta2385,2020-08-26T16:48:17Z,3,Thanks for the video. Your content is just awesome,True
