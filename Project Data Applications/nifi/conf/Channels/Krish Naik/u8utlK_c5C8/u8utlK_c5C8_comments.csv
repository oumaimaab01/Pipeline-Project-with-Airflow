author,updated_at,like_count,text,public
@mohammedfahadkhan,2024-05-30T01:44:19Z,0,Sir w dash and w one are same?,True
@rafipatel5020,2024-05-15T21:14:43Z,0,"You are a blessing,  I usually come here, when my highly talented professors in London, could not make me understand this simply.",True
@usmanyousaaf,2024-01-21T10:05:43Z,0,Their is little bit confusion in w-1 and w dash,True
@katyhessni,2024-01-14T16:19:06Z,0,Thanks,True
@deepknowledge2505,2024-01-10T19:09:52Z,0,"Backprobagation with example, very simple  https://youtu.be/EQPjZwyBa1g",True
@dipayanghosh1159,2024-01-10T04:46:45Z,0,Wrong concept,True
@aj_actuarial_ca,2023-10-28T11:00:36Z,0,Thank you so much for the very wonderful explanation!! It is only now that I understood.,True
@AbrarKh1,2023-09-21T23:47:38Z,0,"Dude you're love, the way you made it so simple. Respect",True
@xXmayank.kumarXx,2023-09-17T17:42:30Z,0,why w1 and w' are different?,True
@manujkumarjoshi9342,2023-09-16T07:28:33Z,0,I love the energy,True
@pratiksha5748,2023-07-12T12:37:54Z,0,so basically the number of neurons in RNN is always the same right????,True
@shreya6107,2023-03-21T12:00:10Z,0,very detailed explanation! Awesome,True
@sandhyaranidash2429,2023-02-05T16:59:52Z,0,"This is the best explanation for how RNN architecture works. I have read multiple blogs and watched animations, but I was not clear on how the feedback on the RNN block works. Thank you so much for such a passionate presentation.",True
@bhanukarthik1803,2023-01-10T10:04:43Z,0,Why is Loss calculated as yhat-y instead of y-yhat?,True
@sanyuktabaluni4608,2022-12-13T04:30:02Z,0,"""Pretty much simple""",True
@adelmahfooz4041,2022-10-08T17:30:17Z,1,I feel so motivated when watching his videos. God Bless you and you have huge respect Sir.,True
@Andrew-fz8fb,2022-09-10T19:27:15Z,0,Thanks a lot for the video. Looking forward to seeing ‚Äúlogic behind CRNN for text recognition‚Äù video someday :),True
@jayasreechaganti9382,2022-08-26T16:54:08Z,0,Sir can you do a video of Rnn example by giving numerical values,True
@noumanahmad562,2022-05-31T05:44:23Z,0,"Why we are using softmax activation function, because it is use in multiclass classification. Should we use sigmoid activation function.",True
@omi_naik,2022-05-19T07:21:52Z,0,07:58 sigmoid at the last layer,True
@durrotunnashihin5480,2022-04-28T10:55:45Z,1,"Question please, why do you differently pronounce the W1 of the hidden state? Do they have different number? You call W1 (W one) at O1 & O3, but you call W1(W dash)  at O2",True
@tawfikborgi1339,2022-04-19T03:21:49Z,0,Merci¬†!,True
@i_amanrajput,2022-03-26T16:59:32Z,0,Awesome,True
@jackshaak,2022-03-15T07:10:03Z,0,"Thanks for this great video, Krish. You''re awesome! I've liked and subscribed.",True
@fernandofischer3725,2022-03-11T20:22:43Z,1,I love how you go in to depth and show the mathematical expression of what is happening in each step. Definitely one of the best channels on deep learning,True
@namansharma9490,2022-02-09T19:20:17Z,0,hi I have a doubt regarding the topic pls see though it  1st . when we find the weight w then we have to find it only once ?? (since weight is shared ) 2nd can u pls tell how we can calculate/update the weight w' pls reply thanks,True
@sumithrap3155,2022-01-30T15:57:06Z,0,weight w and output weight w' is same weight are different ?,True
@hasibkhan5129,2022-01-28T19:44:00Z,0,I am following Krish for the last 3 years and he is one of my Favourite Instructor.,True
@praveenvarma1,2021-11-17T11:33:56Z,0,can anyone explain why bias is not calculated in RNN?,True
@nullvoid7543,2021-10-30T18:41:40Z,0,I liked the code basics one more!!,True
@debatradas9268,2021-10-26T06:43:21Z,0,thanks,True
@lifechamp007,2021-09-27T04:42:31Z,0,You are such a great teacher - thank you so much !! #StayBlessednHappy,True
@raghavendra2096,2021-09-21T16:58:16Z,11,"I promise you nobody!!! i repeat again nobody!!!! can explain anything better than Krish sir and that too RNN explanation was the most simplest yet the best explanation i have ever seen ,  just  so excited to see LSTM as well.   All your content is like ruby's, emerald's and gold given for free and anybody can take, that's how i see your videos.  no words to thank you sir!!!",True
@MK-wb3oq,2021-09-14T15:57:40Z,4,"I wanted to thank you for your time and effort making this video. Your series is just amazing and your explanations are simply beautiful. I learned alot from you, so thanks again for sharing your understanding <3",True
@dawoodabuswai9594,2021-09-06T14:10:39Z,1,That annoying fly XD Great explanation thank you,True
@mohammedoucif217,2021-08-30T10:52:01Z,0,"Hello sir first of all, thanks for all the videos you provided for us but can you just update the playlist with the videos about neural network that we need to watch in order to start with RNN.  Thank you.",True
@premranjan4440,2021-08-26T16:30:39Z,3,Shouldn't the last layer function should be a sigmoid as we are classifying only between 0 and 1?,True
@yonatannisenboym2548,2021-08-24T14:09:03Z,0,"Thanks , great video , helped me a lot.",True
@sanjeevkumar-sf4zo,2021-08-23T18:44:59Z,0,great explanation sir.....,True
@thepresistence5935,2021-08-21T04:40:21Z,0,"1-time seeing the video - nothing understood, 2-time seeing the video - slightly understood, 3-time seeing the video - pretty clear, 4-time seeing the video - pretty much good. Thanks, Krish bro!",True
@shahrinnakkhatra2857,2021-08-17T15:28:53Z,0,"Can you please explain why all the weights for the inputs x11, x12...are initialized the same (w) and  why all the weights for the hidden layer to output layer (w1) the same for all the connections? I mean can't we initialize all the weights as different random values so that those are not the same? (What I mean is, say we initialize weight w1_ for the first input, w2_ for the second and so on) and also initialize weight w1 for the first output, w2 for the second and so on. What could possibly be different? (Or would it give us actually similar outputs as the backward propagation will fix that accordingly anyways?)",True
@AkMyriadVentures,2021-07-15T18:14:58Z,0,Well explained video. Thank you for sharing.,True
@tejusmodhe7629,2021-07-08T01:36:28Z,0,A Grand Salute to you Manüëç,True
@johnhhu2137,2021-06-28T11:19:41Z,0,thank you  sir.,True
@pravalipravali9989,2021-06-23T19:54:50Z,0,"If you know how forward propagation works in ANN, this makes perfect sense. Great explanation as usual.",True
@narendrasompalli5536,2021-06-22T10:21:16Z,0,"Sir,  we  have to add bais haven't us!",True
@zrmsraggot,2021-06-08T13:01:42Z,0,"Hello, I wonder if it's possible to output a vector like [0, 1, 0] at the end .. thanks",True
@sourodeepdas,2021-06-04T11:17:31Z,0,"The amount of Ads I get while watching your videos, clearly states the Monopoly of your videos in ML/DS. Thanks you for the videos. Your Videos helps a lot.",True
@ashishsangwan5925,2021-05-24T05:50:08Z,0,@Krish Naik ----- Why weight is different at output of time stamp t4. W'',True
@louerleseigneur4532,2021-05-20T21:50:13Z,0,Thanks Krish,True
@newone2489,2021-05-06T17:52:39Z,0,Sigmoid function should be used instead of Softmax,True
@suvarnadeore8810,2021-04-21T10:12:49Z,1,Thank you sir,True
@casseypaputungan3217,2021-03-30T05:14:12Z,0,Good explanation!  Is that circle thing inside the hidden layer is a RNN neuron?,True
@rahulsoni412,2021-03-07T09:54:25Z,2,"at 8:02, it should be sigmoid as you mentioned the classification is for 0 and 1 .",True
@Strugglingdreams,2021-02-28T08:05:03Z,4,"For binary classification, the final activation function is sigmoid, and not softmax",True
@aqibfayyaz1619,2021-02-17T08:23:06Z,0,Great.,True
@rhythemjain1067,2021-02-11T22:14:44Z,0,Is w' value for all the layers identical?,True
@atulj962,2021-01-19T13:19:17Z,0,WOW. Great. I never seen a video in you tube clearing explaining the concept. Thank u,True
@divyagupta5218,2021-01-18T07:56:17Z,0,Why you didn‚Äôt consider the bias of every hidden layer of network in forward propagation?,True
@ruslan124,2021-01-10T17:15:29Z,7,By far the best explanation of the workings of an RNN I have experienced.,True
@sagnikmukherjee8954,2021-01-09T05:03:12Z,0,"Are the weights associated with each preceding output (w') also initialized and fixed and same for all like we have with the weights for each Xi1 input ?? In simpler words, like w is same for all X, is w' also same for all O ??",True
@ArunKumar-sg6jf,2020-12-31T03:41:13Z,0,time start 0 or 1 in rnn,True
@maheshbisht2967,2020-12-30T15:13:01Z,0,BOW and TfIdf are  vector representation of word ? I think it's not true. They represent sentence into vector . Correct me if I m wrong .  And please made me clear how we input the data in RNN. Please anyone .,True
@hamzanaeem4838,2020-12-20T09:57:21Z,0,Is it a single hidden layer or 4 hidden layers ?,True
@shreyasb.s3819,2020-12-08T14:14:18Z,0,Nice explained,True
@prasadjoshi8213,2020-11-17T07:54:09Z,0,"krish thanks a lot for ur videos on ML, DeepLearing and now NLP. Very helpful and vital. I have two question: (1) How to create PoS unigrams , PoS bigrams or PoS trigrams?? (2) How to train ML using these PoS unigrams, bigrams and trigrams??  If u could answer it will be  very very helpful , Thanku in advance",True
@vijaypalmanit,2020-11-02T15:15:14Z,0,"Superb! Finally I could understand the intuition behind RNN, great work, thanks Krish ! üëç",True
@sunaaldua90,2020-10-19T02:43:29Z,0,are you on linkedin Krish?,True
@dfordeveloper1629,2020-10-16T12:54:58Z,2,"sir , don't mind but you have some misconception regarding RNN . loss function calculated every word neural network",True
@rekeshwardhanani920,2020-10-03T17:00:07Z,0,"I just throw a like on your videos, without even seeing it first, coz I know the effort behind.",True
@sarangak.mahanta6168,2020-09-29T10:36:53Z,0,"The last activation function should be sigmoid not softmax,  since it's basic sentiment analysis (only two possible outputs)",True
@az84239,2020-09-17T02:32:29Z,0,Thank You So Much!!!!,True
@KuldeepSingh-cm3oe,2020-09-11T11:44:25Z,1,"My question is, this diagram was in context to one hidden layer only. How the input will move if we have more than 1 hidden layer?",True
@chaitanyakulkarni6012,2020-09-10T15:31:44Z,0,i think the weights of the previous info is constant W and weights of current inputs is altered in back propagation ...Krish please confirm,True
@Virtualexist,2020-09-03T18:12:37Z,0,He is the AndrewNG of India.,True
@VaishnaviChaturvedi,2020-08-16T11:42:31Z,1,Your list of these videos is amazing! Thanks!,True
@gurdeepsinghbhatia2875,2020-08-06T18:26:04Z,0,sir what will be the ouput dimanension of O1,True
@TheMehrdadIE,2020-08-04T07:33:45Z,0,"Krish, Good job man.  Would you please make a video for BERT? Thanks!",True
@gopikumar7888,2020-07-31T12:38:52Z,1,"I have one doubt regarding input to the hidden layer . Krish had said  first text data was converted into vector then feed to the RNN. SO ,  while converting into vector, sequential informed may be lost(as krish said in previous video). My question - why  are we converting into vector?..   Also suggest some best technique which are here used to convert text data into vector. Correct me if i am wrong",True
@pranjalgupta9427,2020-07-27T03:29:00Z,1,Awesome explanation üëçüëèüòä,True
@rameezusmani1294,2020-07-26T20:52:26Z,0,cant thank you enough. Love from Pakistan,True
@DeepakYadav-gx2pm,2020-07-10T17:54:20Z,29,"I think in calculation of o2, o4 it should be w' in place of w1",True
@vgaurav3011,2020-07-06T06:51:50Z,0,Amazing explanation! Seeing this after completing the Udacity course and found this to be more intuitive,True
@souradeepdas,2020-07-02T15:59:13Z,0,Very Helpful Sir,True
@Official-tk3nc,2020-06-26T07:36:26Z,70,"if you are watching this in lockdown you are one of the rare species on the earth . many students are wasting their time on facebook, youtube, twitter, netflix, watching movies playing pubg, but you are working hard to achieve something . ALL the best ...nitj student here",True
@SaptarsiGoswami,2020-06-17T15:50:04Z,1,"Brilliant to say the list Krish.  As you explained this is for t=4,  in your corpus you may have different size, how do you handle that ? Will watch your next videos to watch it.",True
@antunmodrusan828,2020-06-15T16:59:38Z,3,"I've watched 30 of your DL videos today and would like to thank you for them because they were very helpful in explaining these complicated topics. In some of the videos you've made some simple typing errors which might confuse people, so I would suggest you to watch the videos afterwards and put annotations correcting the typing errors. Keep up the great work! :)",True
@ayushpatel4710,2020-06-15T10:24:07Z,0,You mean in a hidden layer output of 1 neuron is the input of next nuronü§î,True
@samuelsumbo7693,2020-06-15T09:04:46Z,0,"lol, @ 10 :53 , thought the bug was on my screen",True
@sumeetseth22,2020-06-11T14:11:28Z,0,claps to you!! you are amazing!!,True
@aajanquail4196,2020-06-08T15:14:39Z,0,"@Krish Naik do the weights that are applied to each output layer (w1, w', w'') have to be different? If this is the case, it seems inefficient - then the network would have to update many weights for a sentence with many words.",True
@aartiarora4987,2020-06-05T12:28:09Z,0,Hlo sir Could you please make video on differentiate btw rnn and cnn,True
@keithfetterly4111,2020-05-24T13:12:23Z,1,"Outstanding!  Had you answered Shahriar's question in your video, I would say you made the perfect RNN video.  Great job!  Keep up the outstanding work.",True
@rakeshguduru4389,2020-04-23T20:28:15Z,31,Anyone else tried to remove the fly off the wall from their screen?! :'D. Great video BTW :),True
@sindhujapateel6455,2020-04-06T11:29:09Z,0,I can‚Äôt see where nlp videos are please any one can help,True
@shahriarrahman8425,2020-03-30T18:39:47Z,3,"Great content! One question: Is w' getting updated during forward propagation? Like is w' different at t=1, 2, or 3, or is it the same value all along with the forward propagation?",True
@RECKED_NOXIN,2020-02-15T20:52:56Z,0,please share more link for more depth knowledge,True
@ankurgupta2806,2020-02-08T21:27:17Z,8,"SIr, Just a suggestion, you could also give some links for further or supplementary reads.",True
@omkarr8282,2020-01-02T09:22:20Z,0,"Hey if possible can you also tell if  the number of hidden layers is related to the sentence length? You were very consise on the other aspects which I really appreciate. I just had that doubt, if possible do clarify.Thanks for the amazing video again",True
@sayelichakraborty7222,2020-01-01T05:14:20Z,1,waiting for the rnn next vedio.. please uploead....its great,True
@sameerkumargupta6058,2020-01-01T03:42:01Z,1,"Sir , please make video on how to identify plant disease using neural networks",True
@teetanrobotics5363,2019-12-30T19:34:54Z,9,One of the best AI professors and Data Scientist in the country,True
@winviki123,2019-12-30T06:54:43Z,2,Sir please explain how is W different from W' ??? You have used W and W' alternatively while calculating the output at each time step..why is that???,True
@omkashyap5109,2019-12-29T03:01:54Z,2,"-: Plz can you make a video to explain how probability and statistics is useful in machine, deep learning and A.I ?  -: What part of statistics and probability is most important which we need to study for this ?  -: plz give some elaborative examples to make us understand.   -: How can it be implemented in these learnings and A.I ?. Please make a video on that.  by the way love your videosüòòüòò",True
@dipayanroy8357,2019-12-28T18:39:30Z,5,Great tutorial. Keep up the good work! Eagerly waiting for the next lesson on RNN,True
@ramakotisakha4067,2019-12-28T16:11:41Z,2,sir can you make a playlist of text to speech with CNN from the scratch,True
@pardeepsaini1634,2019-12-28T07:07:25Z,0,Hlo sir review this course because this is in hindi https://www.youtube.com/playlist?list=PLtCBuHKmdxOd8HyfVuAwioPiE1ZkG5IFR,True
@kishanlal676,2019-12-28T06:26:21Z,1,Shouldn't we use 'sigmoid' as activation function at the last layer? or 'sigmoid' can aslo be used ??,True
@pranaymathur997,2019-12-27T18:56:04Z,9,Sir what about the bias term???,True
@maheshvardhan1851,2019-12-27T18:40:40Z,5,"omg !!!!!  one of the best and simplified explaination of RNN in the internet ,tq sir.",True
@mohammedfaisal6714,2019-12-27T18:16:19Z,2,Awesome Work Sir Thanks a ton,True
@kushshri05,2019-12-27T18:08:27Z,3,I was watching the previous video right nowüòÉ thanks for next video,True
