author,updated_at,like_count,text,public
@joeljoseph26,2023-12-09T15:58:42Z,0,Minkowski distance = (Manhattan) and (Euclidean),True
@nelsondelarosa5490,2023-11-09T14:10:25Z,1,"This is in fact well explained, defining every term, and assuming no previous knowledge. Thanks so much!",True
@krish3486,2023-07-16T07:48:42Z,0,Sir why there we check only 1 to 40 neighbours only in the for loop,True
@ManashreeKorgaonkar,2023-01-17T15:10:50Z,0,Thank you so much for sharing this information. I'd just one doubt sir if we will scale before train_test_split wont it be lead to data leakage? as during scaling process during fit when it consider average of all the data points it also take the value of test data set so my model will already have some hint regarding it??,True
@Kishor-ai,2022-12-12T11:09:56Z,0,Thanks for this video krish naik sirü§©,True
@tagheuer001,2022-10-15T17:20:41Z,0,Why do repeat phrases so many times?,True
@sazeebulbashar5686,2022-05-27T18:09:52Z,0,Thank You Naik...... This is a very helpful video,True
@sivareddynagireddy56,2022-04-05T15:56:37Z,0,"No words about u r explanation sir,simple lucid way explanation !!!!!",True
@shreeyajoshi9771,2022-02-21T16:15:38Z,0,Thank you very much for this video. Helped a lot!,True
@atifaaruiyachi4125,2022-01-22T23:22:21Z,0,for my assignment i am not allowed to use import packages for knn but I have to write it myself. do you have a code without the imported knn method?,True
@colabwork1910,2021-12-17T11:28:21Z,0,Awesome,True
@ArunKumar-yb2jn,2021-11-23T11:12:08Z,0,"Krish - This seems to be a repeat of over a thousand similar videos on the internet, barring a few. What new insight have you brought here? You didn't define what that Y and X were and simply jumped into drawing X marks on the chart. Why do we need intuition of KNN? Why can't we really understand what it IS? This sort of explanation 'appears' to be clear, but in fact it really doesn't add to a student's understanding. Please take some actual data points and run the algorithm.",True
@programmingpurpose1329,2021-11-12T15:56:44Z,2,This explaination is one of the most precise explanation that I have seen on Internet.,True
@shubhamsongire6712,2021-11-07T18:17:41Z,0,Thank you so much Krish for this great playlist. You are gem,True
@ramu7762,2021-09-14T11:20:49Z,0,spot on. thank you.,True
@vaibhavchaudhary1569,2021-08-28T20:16:28Z,9,Feature scaling (StandardScalar) should be applied after train test split. As it will not lead to information leak.,True
@appiahdennis2725,2021-08-25T19:30:22Z,0,Respect Krish Naik,True
@deshduniya360scan7,2021-08-15T04:48:08Z,0,"Explain like a pro,thank you",True
@dragolov,2021-07-16T12:50:27Z,0,These are 2 musical (jazz) solos generated using K Nearest Neighbor classifier: https://youtu.be/zt3oZ1U5ADo https://youtu.be/Shetz_3KWks,True
@louerleseigneur4532,2021-07-16T08:57:35Z,0,Thanks Krish,True
@mdmynuddin1888,2021-07-04T04:49:20Z,0,"if both category same neighbor point , than which category belongs to new data point?",True
@indrajitbanerjee5131,2021-06-27T15:30:16Z,0,Nicely explained.,True
@mahikhan5716,2021-06-21T18:18:35Z,1,how can we choose optimal value of k by KNN ?,True
@903vishnu,2021-06-17T16:12:59Z,1,"really its good... but you mentioned K=150, as per my knowledge we are not supposed to take even number. there might be chance of equal number of classes got selected nearest neighbor... algorithm may not be able to estimate the class for new record...",True
@rambaldotra2221,2021-05-04T19:39:53Z,0,Grateful Sir ‚ú®‚ú®Thanks A lot.,True
@makrandrastogi5588,2021-04-22T07:33:55Z,0,can anybody tell why in most of the cases we use euclidian distance and not manhattan distance ?,True
@codyphillippi8831,2021-04-19T20:40:18Z,4,"This is awesome! Thank you so much. I am working on a project at work for lead segmentation to help us find our ""ideal lead"" for our sales reps with a lot of very messy data. This is a great starting point. Quick question (might be a loaded question ha) - after we find these clusters, how do we go about seeing the ""cluster profiles""? Or what all data points make up these clusters (in categorical form)",True
@scifimoviesinparts3837,2021-04-03T13:47:00Z,0,"At 18:52, you said larger value of K will lead to overfitting, which is not true. Smaller value of K leads to overfitting. I think, if there are 2 K-values giving same error, we choose the one that is bigger because it is less impacted by outliers.",True
@vignesh7687,2021-03-31T14:00:01Z,2,Sooper Explanation Krish. I have a doubt here.. When do we need to use MinMaxScaler() and when do we use StandardScaler()? Is there any difference? or we have to try using both and see which gives better results? Please clarify,True
@parammehta3559,2021-03-31T06:34:20Z,0,Sir is it normal that sometimes as the value of n_neighbors is increasing the error rate is also increasing?,True
@adhvaithstudio6412,2021-03-30T12:55:44Z,0,can you exlplain how hyper parameters will helps in what scenarious,True
@vibhutigoyal769,2021-03-23T08:48:46Z,0,Is knn non- linear algorithm???,True
@asawanted,2021-03-06T23:34:09Z,0,What if we choose a K value and hit a local optima? How would we know if I should stop at that K value or proceed to a higher value in search of global optima?,True
@Anubhav_Rajput07007,2021-02-15T05:25:04Z,0,"#Hi Krish, hope you are doing well. i trying to find the best value for K. but the code is not execute.. its running last 20 mint.",True
@birendrasingh7133,2021-02-14T05:30:17Z,0,Awesome üòÅ,True
@sunilkumarkatta9062,2021-02-05T07:00:56Z,0,How we will get error value to calculate accurate k valueüòÖ,True
@abdelrhmandameen2215,2021-01-11T02:32:12Z,1,"Great work, thank you",True
@laxmiagarwal3285,2020-12-03T19:30:20Z,0,This is very nice video.. But I'm having one doubt..what value u are taking for calculating the mean of error rate as prediceted values are in terms of 0 and 1,True
@madeye1258,2020-10-29T13:54:07Z,0,"5.03 , if we are classifying the points based on the number of points next to it, then why we need to calculate the distance in step 2",True
@vishalaaa1,2020-09-25T20:56:29Z,0,Thank you,True
@sandipansarkar9211,2020-09-22T18:50:23Z,7,Cool. Also finished my practice in Jupyter notebook. Thanks,True
@sandipansarkar9211,2020-09-20T21:19:25Z,0,great explanation Krish.,True
@kiruthigan2014,2020-09-09T14:21:42Z,7,Loved Ur videos and Ur taste in music..kadhal vanthale in the bookmark üòÇ‚ù§Ô∏èüî•,True
@DeepakSharma-od5ym,2020-08-25T11:16:28Z,1,"error_rate = [] for i in (1,40):     knn = KNeighborsClassifier(n_neighbors=i)     knn.fit(X_train,y_train)     pred_i = knn.predict(X_test)     error_rate.append(np.mean(pred_i != y_test))  plt.figure(figsize = (10,6)) plt.plot(range(1,40), error_rate, color = 'blue', linstyle = 'dashed', marker = 'o' ) plt.xlabel('k') plt.ylabel('error rate')  My above code giving error ""x and y must have same first dimension, but have shapes (39,) and (2,)""  Please suggest",True
@mayukhdifferent,2020-07-08T07:58:54Z,0,"So, shud K always be an Odd number?",True
@devinpython5555,2020-05-29T08:13:14Z,1,Could you please explain to me why fit and transform is done for the x values (in the above example leaving target column rest data is x values),True
@sanyamjain7416,2020-05-03T18:13:37Z,0,Can't we use gridsearchcv here?,True
@uchuynguyen9927,2020-04-21T14:36:06Z,0,"where you taking np.mean(pred_i != y_test), i think it should be pred_i = knn.predict(y_test) so then we will compare the predict y_test to actual y_test, then we''ll find the errors. If i wrong can somebody explain, thank you!",True
@yathishl9973,2020-04-12T17:37:14Z,3,"Hi Krish, you are really amazing. I learn many things from you. I have a doubt, what measures should I take if the error rate increases with K-Value,  please advice",True
@MaanVihaan,2020-03-19T09:31:59Z,0,Very nice sir ur explanation and coding technique is very nice.... I am new learner of data science please keep uploading such video and new techniques of different kinds of algorithms which help us make easy to understand to deal with different kinds of datasets.,True
@shaz-z506,2020-01-25T06:55:28Z,0,"Hi Krish, Just wanna verify since you've said at 5:10 that model is ready, but KNN is instance-based learning with no parameter assumption then I don't think so that it creates any model out of the algorithm. Please let me know I'm wrong as I need some clarity.",True
@shyam15287,2019-11-22T07:01:33Z,1,All the best Superb Explanation  you are a superb resource u will reach great heights continue ur good work,True
@manusharma8527,2019-11-17T18:27:10Z,0,i am not getting any any Classified Data csv file on Keggal.Please can you tell me the real name of that csv file,True
@shayanshafiqahmad,2019-11-05T18:54:13Z,0,What is the reason for taking pred_i !=y_test?,True
@sensei-guide,2019-10-30T04:33:07Z,0,As my k value increase my error rate also increasing bro,True
@TechnoSparkBigData,2019-10-02T14:29:25Z,1,Sir you are great inspiration to me. Thanks a lot for making every complex problem simpler.,True
@aditisrivastava7079,2019-08-25T15:25:36Z,0,Very well explained,True
@Tapsthequant,2019-08-06T05:29:25Z,5,"Thank you, you asked a question I had in my head, looking forward to applying the suggested solution, about imbalanced dataset...",True
@shaz-z506,2019-06-19T19:53:46Z,0,"Hi Krish, In what scenario we'll use manhattan over euclidean.",True
@chaithanyar9961,2019-04-11T06:17:28Z,1,"Hello sir , will this code work in tensor flow?? any changes to be made if I want excecute it in tf",True
@ijeffking,2019-02-15T11:40:19Z,5,Very well explained again. Thank you so much.,True
@sriramswar,2019-02-12T17:08:10Z,0,"Hi Krish, unable to open ipynb file in Jupiter noteboox.  Getting the below error: Error loading notebook Unreadable Notebook: C:\Users\Srira\01_K_Nearest_Neighbors.ipynb NotJSONError('Notebook does not appear to be JSON: \'<!DOCTYPE html>\\n<html lang=""en"">\\n <h...'). Iam able to open it by clicking the link, but when I downloaded it and tried to open locally, it is giving the above error.  Can you please check?   BTW, the video is very informative and thanks a ton!",True
