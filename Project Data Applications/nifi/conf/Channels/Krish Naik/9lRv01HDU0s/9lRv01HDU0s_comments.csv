author,updated_at,like_count,text,public
@SahanPradeepthaThilakaratne,2024-05-28T18:54:53Z,0,Your explanations are superbbb!,True
@Amir-English,2024-04-06T09:54:24Z,0,You made it so simple! Thank you.,True
@muhammednihas2218,2024-04-02T12:04:56Z,0,thank you ! good  explanation,True
@thespeeddemon7832,2024-03-31T16:32:19Z,0,thank you so much for this explaination â˜º,True
@adinathshelke5827,2024-03-04T13:54:22Z,0,"perfect explanationnnnnnnn. WAs wandering around for whole day. And at the end of the day, found this one.",True
@dianafarhat9479,2024-03-02T18:45:00Z,0,"Amazing explanation, thank you!",True
@prateekcaire4193,2024-02-04T19:24:41Z,0,maybe higher slope has low variance across all data set points including unknown test and validation datasets. why would we like to penalize the higher slope. Maybe unknown test datasets aligns more with steeper slope and has less residual square error?,True
@ajithsdevadiga1603,2024-02-02T15:07:57Z,0,"Thank you so much for this wonderful explanation,  truly appreciate your efforts in helping  the data science community.",True
@taruchitgoyal3735,2024-02-01T10:49:38Z,0,"Hello Sir, As I understand, we apply Regularization [Ridge and Lasso] to reduce features or reduce contribution of features. As you also shared we took sum of squared residuals and then added the regularization constant to it.  Thus, we get cost value which is equal to sum of squared residuals and regularization penalty and we get a numerical result as the output of the computation. At this point, How do we perform the feature selection or control contribution of each input variable in feature space? Thank you",True
@PraveenN-fp5jy,2024-01-30T08:48:40Z,0,Excellent sir ğŸ˜Š,True
@user-eb3th4hb3d,2024-01-05T14:39:07Z,0,"What if so as to reduce overfitting I'll need to increase the steepness rather than decreasing it (We can't even have Lambda as a negative integer, then how can we fix it)?",True
@Zainiology,2023-12-08T16:50:26Z,0,Is LASSO the sum of the magnitude of the m's or the magnitude of the sum of the m's.,True
@Markmania510,2023-10-26T13:21:07Z,0,What a great video thank you so much for posting. Can anyone tell me what happens if youâ€™re overfitting with a smaller slope? I.e. you need to increase the slope to generalise the model more.  Is this when you need to make the lambda parameter negative?,True
@surenkhalsa,2023-10-16T12:23:38Z,0,"But, are we not Fudging with actual data relationship by forecefully reducing slope ?",True
@ZubairAzamRawalakot,2023-10-16T10:29:42Z,0,Very informative lecture dear. You explained with maximum detail. thanks,True
@Jais-yu7rz,2023-10-13T04:38:04Z,0,â¤â¤ ml means krish Naik Krish naik means ml,True
@tenzinlhakpa1672,2023-10-05T15:34:25Z,0,Better than my professor,True
@sarahlila,2023-10-04T18:57:30Z,0,thank you,True
@nano7586,2023-09-27T15:18:19Z,0,"Also, is optimizing Lambda as easy as performing cross-validation many times until an optimal Lambda will give me the best testing results?",True
@nano7586,2023-09-27T15:15:24Z,0,"I'm confused. But what if the testing data is actually higher than the training data. In that case, wouldn't a steeper slope be beneficial? Would you then just put a ""-"" (minus) in front of your ridge regression term of the loss function?",True
@nano7586,2023-09-27T15:02:36Z,0,"So regularization is only helpful when your sample size is low, right? Are there any drawbacks when applying regularization for a high sample size dataset? Will it introduce a bias? I'm an engineer and to be honest, regularization to me sounds like you're manipulating your data and basically adjusting the regression manually. If the linear regression model was a rope then that would mean rotating that rope clock- or counterclockwise. I don't feel super confident using any of these techniques because I can't explain what exactly I've done. My superiors will ask me what I've done and I'll be able to explain what this means technically, but I won't be able to justify it. Are my worries justified?",True
@user-cy9zf4oz2i,2023-09-22T02:20:11Z,0,"i think you need to emphasize on the high variance toward the test data, and low variance toward the training data, the problem with overfitting is that this low variance on the training data comes at the expense of high variance on the test data. When the model is exposed to new, unseen data (the test data), it struggles to generalize because it has essentially memorized the noise and intricacies of the training data. This results in a significant difference between the model's predictions and the true values on the test data, indicating high variance on the test data.",True
@ishphanda9782,2023-09-07T05:18:05Z,0,Phenomenal!,True
@vaish6859,2023-09-05T02:56:46Z,1,You are helping many of the ML enthusiasts free of cost... Thank you,True
@jalpapatel1370,2023-09-02T08:28:59Z,0,but every lambda will give different set of coefficient,True
@k.s.vkanakadurga5900,2023-08-29T04:32:26Z,0,"Hi Krish, thanks for details.small doubt here. When peanaluze the  slope  term in ridge regression,  there is a possibility  in increase the ols error which was zero in the 1st iteration in your exmple .How do we handle this",True
@sardarjalal7155,2023-08-25T07:35:55Z,0,â¤â¤â¤â¤,True
@dollysiharath4205,2023-07-23T17:44:24Z,1,You're the best trainer!! Thank you!,True
@52nevil36,2023-07-03T07:57:40Z,0,why lambda leads to zero? please explain anyone,True
@suman.michael,2023-06-20T07:48:39Z,0,"Great work Krish @krishnaik06 ...!   Just curious, How do we relate overfitting in linear regression to high slope?  Consider a simple scenario, A model got ""overfitted"" to the the training distribution with slope 1. What if the test distribution lies above the line such a way that a higher slope (>1) would predict the test distribution much better.  Putting it again, The model is overfitted (memorised the train data with slope 1). The core idea of Regularisation (at least what's discussed here) is to bring down the slope value by penalising it, Right? So, How do I increase the slope and generalise it with these Regularisation techniques?   Could a lambda < 0 work here?",True
@robertasampong8086,2023-06-19T11:46:23Z,0,Absolutely excellent explanation!,True
@shahhassan9561,2023-06-11T07:28:19Z,0,"Great Sir, Sir I need slides for the given topic please share if you have.. Thanks in advance",True
@debrupdey7948,2023-06-07T17:00:46Z,0,crazy,True
@shantanuyadav732,2023-05-27T14:26:30Z,0,Great Explainantion,True
@shravanshukla5352,2023-04-24T06:11:00Z,0,There is no meaning of this hard work. Only perception in Ridge and lasso regression is true and follow everyone,True
@abhijeetsingh1573,2023-04-19T17:50:34Z,0,Thanks I will get 10 marks I think,True
@mithunmiranda,2023-04-18T09:23:59Z,1,"I wish I could like his videos multiple times. You are a great teacher, Kind Sir.",True
@Bedivine777angelprayer,2023-03-24T18:51:29Z,0,Thanks is there articles i can refer any blogs you recommend thanks again great content,True
@aravindvasudev7921,2023-03-20T11:43:04Z,0,Thank you. Now I got a clear idea on both these regression techniques.,True
@DivyaMaske-zi5rl,2023-03-20T11:13:22Z,0,When is ridger regression favourable and lasso regression ?mcq ans  plz ans me this question,True
@spider279,2023-03-18T14:49:31Z,0,"what a coincidence , i wear exactly same color polo than him when i watched this video",True
@moe45673,2023-03-13T00:53:30Z,0,Thank you! I thought this was a great explanation (as someone who has listened to a bunch of different ones trying to nail my understanding of this),True
@satyamjha2804,2023-02-26T13:30:21Z,0,nice explanation,True
@cyborg69420,2023-01-31T13:47:44Z,0,just wanted to say that I absolutely loved the video,True
@AMITABHADAS-mv1fo,2023-01-13T10:48:27Z,1,In Ridge regression why always we have to reduce the slope to overcome overfitting? What will happen if most of my test dataset points are lying on the left side of the best fitted line?,True
@BipinYadav-wn1pm,2022-12-31T11:08:06Z,0,"after going through tons of videos, finally found the best one, thnx!!",True
@johnhardingz5204,2022-12-07T15:56:48Z,0,Dude I wich your were mentor through university,True
@vladimirkirichenko1972,2022-12-03T16:44:16Z,0,This man has a gift.,True
@tasgaonkar.vaibhavbtech2012,2022-11-28T15:54:37Z,0,Best video for ridge and lasso ğŸ†ğŸ†ğŸ†,True
@rohith646,2022-11-20T06:14:37Z,0,awesome man,True
@mustafizurrahman5699,2022-11-19T05:14:24Z,0,What is training data and test dat?,True
@jitendrasoni7793,2022-11-18T14:25:13Z,0,Bhaiya aaapne bacha liya ğŸ˜,True
@shadiyapp5552,2022-11-05T15:58:57Z,0,Thank you â™¥ï¸,True
@abhishekvashistha2398,2022-11-02T12:29:22Z,0,thank you,True
@sincerelysilvia,2022-10-18T03:00:19Z,0,This is the most clearest and best explanation about this topic on youtube. I can't express how thankful I am for this video for finally understanding the concept,True
@saitcanbaskol9897,2022-10-16T14:25:24Z,0,Amazing explanations.,True
@datafuturelab_ssb4433,2022-10-12T19:11:42Z,1,Best explanation on lasso n ridge regression ever on YouTube... Thanks krish... You nailed it...,True
@calewang3713,2022-10-07T00:48:33Z,0,But why does one shrinks to zero and the other gets all the way to zero...?,True
@AmineSdour,2022-10-06T12:44:37Z,0,â¤,True
@lavanyasenthilkumar4814,2022-09-24T20:34:31Z,1,"Krish,Thanks for the clear explanation. I have a doubt :  if the test data points are below the best fit line we have selected earlier, this is fine. What if the test data points are above the best fit lines ? in that case , applying ridge regression , would still try to reduce the slope and we may not end up getting the best fit line. can i get a help in this scenario. Thanks in Advance!",True
@saurabhsaraswat6015,2022-09-24T05:37:30Z,0,Just curious...shouldn't you add some validation points before calling the model overfitted? In fact this model should be best fit based on the data provided.,True
@ling6701,2022-09-22T19:30:19Z,0,"Thanks. Very well explained (I needed this). I have a question though. For ridge regression, we're trying to penalize steep slopes. Ok, but why is that ? Why don't we try to penalize flat slopes instead (imagine the flat slope is flat, and the real data points are above the best fit line, so that there is still overfitting)? I mean: don't we want to penalize any type of difference between the actual data points and the best fit line? Why only penalize the difference in one direction?",True
@curiousmind3916,2022-09-18T21:56:38Z,0,Thank you so much,True
@MohsinKhan-rv7jj,2022-09-03T20:14:26Z,0,The kind of explanation is truly inspirational. I am truly overfitted by knowledge after seeing your video.â¤,True
@abdulnafihkt4245,2022-08-12T07:30:44Z,1,Best best best bestttttt class...hats off maaan,True
@iamfavoured9142,2022-08-09T07:35:42Z,4,100 years of blessing for you. You just gained a subscriber!,True
@DamafiakingzDMK,2022-07-26T18:58:02Z,1,what the fk dude u are good,True
@bhuvaraga,2022-06-30T17:28:43Z,0,"Loved your energy sir and your conviction to explain and make it clear to your students.  I know it is hard to look at the camera and talk - you nailed it.   This video really helped me to understand the overall concept.  My two cents, 1) Keep the camera focus on the white board I think it is autofocussing between you and the white board and maybe that is why you get that change in brightness also.",True
@koderr100,2022-06-10T16:38:08Z,1,Now I finally got about key L2 and L3 difference. Thanks a lot!,True
@askpioneer,2022-06-06T04:37:14Z,0,well explained krish. thank you for creating . great work,True
@maasahebbiustad8514,2022-06-04T09:59:53Z,0,if we can use PCA for over fitting why this Ridge?,True
@manyavarshney4399,2022-05-28T05:14:48Z,0,Thank you so much sirğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ™ğŸ™ğŸ™ğŸ™ğŸ™,True
@adijambhulkar1742,2022-05-26T15:37:31Z,0,Hats off... What a way... What a way to explain man... Clear...all doubts,True
@harinainyarpillai5496,2022-05-15T05:10:08Z,0,"hi, Sir, can u say how did u draw the best fit line i.e second best-fit line based on the assumption of value u got as 1.69or? can u say on that",True
@amitshukla7827,2022-05-02T13:12:07Z,0,Ridge Regression seems to be same as regularisation of the standard Linear Regression. Nice Content !,True
@yamika.,2022-04-20T04:15:24Z,0,thank you for this! finally understood the topic,True
@gagangayari5981,2022-03-04T07:31:53Z,0,"Doubt !!.. In this scenario , while doing RidgeRegression, we are penalizing the model when the slope is higher. But how do we know that slope should be lower. It might be even higher if we see the whole data. We are assuming that the slope should go lower. Why is that assumption ?",True
@R_SinghRajput,2022-03-01T13:13:51Z,0,Beautifully explained sir â˜ºï¸,True
@yitbarekmirete6098,2022-02-26T22:45:41Z,0,"you are awesome, better than our professors in explaining such complex topics.",True
@VikashKumar-je6fb,2022-02-26T06:41:21Z,0,@krish how Lasso shrink to zero..,True
@weebwholesome1811,2022-02-21T12:05:11Z,0,"points to be noted L1REGULARIZATION-LASSO Regression,  L2REGULARIZATION-SVM,RIDGE Regression",True
@NEDIKIMED,2022-02-18T13:01:22Z,0,"thanks sir,is when i am learnning , hopping i will be good in 3-6 months to come , 2 hours a day",True
@divyatejadadi6898,2022-02-16T05:21:11Z,0,"Anyone, who watch his 4 to 5 videos, on linear regression, will get an overall picture.",True
@aish_waryaaa,2022-02-08T13:15:18Z,4,"Krish Sir you are saving my masters literally,up to date explanation,and the efforts you are putting to help us understand,Thank You so Much Sir.ğŸ˜‡ğŸ¥°",True
@shahzebchoudry6175,2022-02-06T18:39:25Z,0,Outclass,True
@nv3796,2022-02-04T14:50:57Z,0,5 STAR intuition video,True
@teblu9974,2022-02-01T06:30:45Z,0,"Sir , you single handledy have created more jobs then GOI . Thank you so much",True
@Mars7822,2022-01-27T04:58:09Z,0,only one word = brilliance,True
@tsrnihar,2022-01-22T10:42:55Z,12,"Small correction - For Lasso regression, it is sum of mod of coefficients multiplied by the regularization parameter. You wrote it as mod of sum of coefficients multiplied by the regularization  It is lambda*(|m1| + |m2| + ..) and not lambda*(|m1+ m2 + ...)",True
@belllamoisiere8877,2022-01-19T08:22:43Z,1,"Hello from MÃ©xico. Thank you for your tutorials, they are as if one of my class mates was explaining concepts to me in simple words. A suggestion, please include a short tutorial on ablation of Deep Learning Models.",True
@MsMaliniSiva,2022-01-16T06:40:52Z,0,Thank u so...much,True
@gunjanagrawal8626,2022-01-14T13:11:31Z,0,Very well explained!ğŸ™Œ,True
@hussameldinrabah5018,2022-01-09T13:59:19Z,0,"so far you are the best Ä±n explaining it and really appreciated! But, Im still confused, like how this term (lambda * slope) can actually change the steepness of the slope? what is the intuition behind it actually?",True
@rachadlakis1,2022-01-01T21:05:25Z,0,How reducing slopes can help in overfitting?,True
@therawkei,2021-12-12T03:13:28Z,0,"this is the best , thank you so much",True
@themightyquinn100,2021-12-10T22:33:00Z,0,what if a higher slope actually results in lower variance? How does ridge regression help in that situation?,True
@AMITKUMAR-gw3di,2021-12-10T16:13:18Z,0,really helpful  sir,True
@veradesyatnikova2931,2021-12-10T14:03:41Z,0,Thank you for the clear and intuitive explanation! Will surely come in handy for my exam,True
@brijudemel9891,2021-12-09T17:02:04Z,0,U saved my day,True
@ashinkajay,2021-11-25T11:40:36Z,0,Thank you so much !!!,True
@misscutecat888,2021-11-23T08:25:24Z,0,awesome,True
@ArunKumar-yb2jn,2021-11-23T03:00:59Z,0,"At 8:15 you say ""a steep slope will always lead to overfitting case, why? I will just tell you now..."" But I couldn't find where later on you have explained this.",True
@juozapasjurksa1400,2021-11-10T16:17:12Z,2,Your explanations are sooo clear!,True
@fratcetinkaya8538,2021-11-09T08:41:02Z,0,"Here is where I understood that damn issue. Iâ€™m appreciated too much, thanks my dear friend :)",True
@somnathpatnaik2277,2021-11-06T11:52:27Z,0,"i have tried 4 very reputed organizations for doing courses all claim faculty from IIT and xyz high profile name. My feedback is if you are from IIT then that doesnt mean you are a good teacher, for teaching they should have passion like you had.   When i see your lectures i enjoy learnings. Thank you",True
@iidtxbc,2021-11-06T05:47:55Z,0,"So, ridge regression relaxes the steepness of the slope?",True
@ChandanBehera-jp2me,2021-11-03T06:31:58Z,1,i found your free videos better than some other paid tutorials...thanx for ur work,True
@kshitizkhandelwal879,2021-10-29T10:14:29Z,0,can someone explain what Lamda does?,True
@creatorsayanb,2021-10-21T03:52:09Z,0,In ridge we're taking sum of squares of the slopes but in lasso we are talking absolute of sum of the slopes. Why not Sum of absolute?,True
@creatorsayanb,2021-10-21T03:45:53Z,1,11:34 there is a symbolic error. It is > instead of <. Thank you for clearing my doubt by detailed explanation.,True
@gandhalijoshi9242,2021-10-14T05:08:40Z,0,Very nice explanation. I have started watching your videos and your teaching style is very nice .  Very nice you tube channel for understanding data science-Hats Off!!,True
@haziq7885,2021-10-13T16:21:19Z,3,"Thanks krish! Just a question, why are we penalising high slope functions? Are there any situations where the model would benefit from a higher slope (as compared to the linear regression slope) ?",True
@pjanjanam,2021-10-04T06:32:59Z,0,"Hi Krish, Just curious, are there any general guidelines when to use which technique? Should we use Lasso regression in favour of Feature selection objective? or Should we experiment the best of the both?   Thanks for a crystal clear explanation of this topic, you made it so easy to visualize this concept of regularisation.",True
@MsGeetha123,2021-09-29T18:25:55Z,0,Excellent video!!! Thanks for a very good explanation.,True
@JEEVANKUMAR-hf4ex,2021-09-24T18:44:52Z,0,good explanation without touching any complex maths derivations.,True
@kanavsharma9562,2021-09-24T17:05:11Z,0,I have watched more than 8 videos and 2-3 articles but didn't get how lambda value effect the slope ur video explain it best. Thanks,True
@RinkiSingh-ph6oo,2021-09-22T12:04:30Z,0,wonderful explanation daer,True
@loganwalker454,2021-09-15T05:27:59Z,0,"Regularization was a very abstruse and knotty topic. However, after watching this video; it is a piece of cake Thank you, Krish",True
@316geek,2021-09-11T17:40:54Z,0,"you make it look so easy, kudos to you Krish!!! <3",True
@yadikishameer9587,2021-09-10T06:56:42Z,4,I never watched your videos but after watching this video I regret for ignoring your channel. You are a worthy teacher and a data scientist.,True
@mayurgupta4004,2021-09-08T12:11:12Z,0,i am confused can somebody can cleat it out. We use OLS method from stast api package for linear regression  and also we use sklearn for linear regression   does both linear regression from stats api and sklearn uses Ordinary least square method in backend  or sklearn package for linear regression uses gradient descent mehtod for calculation of best fit line,True
@ankitchoudhary5585,2021-09-04T16:12:05Z,0,"@Krish Naik These penalties making the slope smooth not zero..these slopes are nothing but coefficients of the features(x1,x2,x3...) and we are trying to reduce the impact of a features which have high value of coefficient with respect to other coefficients(in case of multicollinearity some feature which are highly correlated will tend to confuse the gradient decent while it minimize the error function and which results in high value of coefficients of correlated features and eventually the higher values of coefficients tells the models that these are the most important deciding features for the target column, but it is a wrong prediction ...there  are many reasons for the high values of coefficients of some features with respect to the other coefficients ) So by adding the new parameter(lambdaÃ—sum of squares of coefficients) to the error function (which will eventually minimized by gradient decent) we are telling the  gradient decent to take care of the coefficients which are very high even if we have to loose some training accuracy (we want generalized model not model which shows best training accuracy but not a  good  predictor for new unseen data ??)  https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/",True
@siddharthsks,2021-08-07T11:37:36Z,0,"Very bad example, overfitting could have happened if the slope wasn't steep enough. This is a misleading.",True
@anandhasrivi,2021-08-02T02:58:38Z,0,This seems incorrect. Over fitting happens at any slope. Not necessarly at higher slope,True
@aritrabanerjee5918,2021-07-31T18:46:49Z,0,I wish i had 100 of google accounts so that i could have subcribe your channel from all of those id's.,True
@satyamuralidharpeddireddi6192,2021-07-26T17:51:23Z,0,Sir   Incase we have underfitting how to overcome this solution,True
@anildhage,2021-07-17T08:03:14Z,0,"Hey Krish, you uploaded this video twice .... just an FYI",True
@balavivek123,2021-07-12T14:38:23Z,0,nice one sir !,True
@ashishveera3431,2021-07-09T11:48:21Z,0,amazing explanation.. thank you!,True
@raufurrahmankhan1284,2021-07-06T21:04:20Z,0,Can we use lasso for feature selection in classification problems?,True
@yogeshbabu3106,2021-07-01T10:00:46Z,0,bro how do you find the equation (bo+b1) after find the cost function is high,True
@ENGMESkandaSVaidya,2021-06-27T05:39:12Z,0,Lasso regression does feature selection which is an extra thing done when compared to ridge regression,True
@shaurabhsinha4121,2021-06-26T22:55:32Z,0,"Krish,but equation of a line with best generalized fit,eg y=Mx+c,can be possible with M being high-->AS ACTUAL DATAPOINT CAN BE closer and crowded near Y axis.So,steep slope can't be criteria.",True
@ALIPNATH,2021-06-26T22:11:04Z,0,"well explained Krish. One question though, can we apply LAsso & RIdge on imbalanced dataset?",True
@sakshargupta875,2021-06-26T04:44:53Z,0,May be you could use different colour for highlighting the text before or after editing. That would be helpful and easy to grasp,True
@sakshargupta875,2021-06-26T04:41:06Z,0,"Content is great but  Genuine Feedback: your video camera is adjusting light again and again, when you turn your face towards the board. And it is very distracting. Please try to resolve this issue.",True
@parthkoul9441,2021-06-22T21:38:24Z,0,"Hi, Thank you for the videos . I have a question- In Lasso regression- it is said You take magnitude of m1+m2+m3.... why not take m1 square +m2squaare+m3 square to maximize the effect of the idea of cancelling features that are less relevant.",True
@anupamdewan7828,2021-06-19T04:58:51Z,0,u just put wrong inequality sign at 11:43..,True
@KalyanGk0,2021-06-19T04:41:33Z,0,Great explanation,True
@TheOntheskies,2021-06-16T13:20:10Z,1,"Thank you, for the crystal clear explanation. Now I will remember Ridge and Lasso.",True
@MuhammadAhmad-bx2rw,2021-06-12T15:12:42Z,1,Extraordinary talented Sir,True
@bhanuteja9408,2021-05-26T16:32:47Z,0,"Am i wrong to use ridge and lasso instead of linear, as i have additional benefit of overfitting..always...? May become underfit right.",True
@_cestd9727,2021-05-24T22:13:41Z,0,"super clear, thanks for the video!",True
@arjungoud3450,2021-05-23T17:36:52Z,0,"Thank you for the wonderful lecture. Does Ridge cause data leaking, as we are taking data from test dataset/.pls/ So instead of Ridge can we direct jump to K-fold instead of train_test_split.",True
@MrLoker121,2021-05-23T14:17:08Z,0,"Good video for beginners,  a couple of pointers though: 1. The lasso regression would lead to |m1|+|m2|+|m3| +.... and not |m1+m2+m3+m4....| 2. The explanation why coefficients in L1 regularization would go to zero and not for L2 is missing. Probably can expand upon it theoretically.",True
@jingyangyu9482,2021-05-19T19:30:53Z,0,"Thank you Mr.Naik. It helped me a lot. Btw, I enjoyed you passion for teaching very much and I also learned from you on this. haha",True
@heplaysguitar1090,2021-05-18T04:16:02Z,0,"Just one word, Fantastic.",True
@arjunch1257,2021-05-10T12:02:07Z,0,"Hi Krish,  If the magnitude is reducing to zero in Lasso, the square of a smaller number will also be reducing to zero right, which is used in Rigid. Why does Rigid regression not help in feature selection, then?",True
@challakarthik4469,2021-05-06T06:47:17Z,0,thanks :),True
@praveenbhatt3127,2021-05-03T19:06:03Z,0,"as in the example considered in this video, our test data was below the predicted line and we were trying to reduce the overfitting by decreasing the slope of the line because the test data was below the predicted line, but what if our test data lies above the predicted line then decreasing the slope of predicted line cause the problem of underfitting. Some clear me out, please.",True
@muhammadhamzakhan1541,2021-05-02T13:46:13Z,0,Sir love from Pakistan <3,True
@tenvillagesahead4192,2021-04-26T23:28:27Z,1,Brilliant. I searched all over the net but couldn't find such an easy yet detailed explanation of Regularization. Thank you very much! Very much considering joining the membership,True
@SatyendraJaiswalsattu,2021-04-26T04:29:37Z,0,Hey Krish ..how to select the lambda value??,True
@Captain_Cool_007,2021-04-24T13:48:58Z,1,Sir Your explaination is absolutely Phenomenal !!!!,True
@ganeshrao405,2021-04-23T09:18:49Z,6,"Thank you soo much Krish, Linear regression + Ridge + Lasso cleared my concepts with your videos.",True
@ahmedaj2000,2021-04-15T07:52:15Z,0,THANK YOU SO MUCH!!!!!!! great explanation!,True
@muhammadsalmanhassan7544,2021-04-13T08:10:07Z,0,Magnitude or norm for lasso slope? please tell me,True
@mvcutube,2021-04-06T08:10:55Z,0,Nice explanation,True
@taqihussain4595,2021-04-03T23:06:52Z,0,Bro thank you so much this was amazing seriously thanks,True
@muzmmilpathan7781,2021-04-01T15:37:58Z,0,salute your knowladge love you sir awsom awsom ooooooo,True
@sireeshjasti,2021-03-30T13:42:39Z,0,But in the linear regression math intuition video took 1/2m in loss function.  Why?,True
@MDIBRAHIM-ep2xd,2021-03-26T15:50:40Z,0,thank u so much sir god bless uğŸ‘ŒğŸ’–,True
@bhooshan25,2021-03-20T15:43:42Z,0,understood the whole video very well,True
@alexanderbrandmayr7408,2021-03-19T20:53:00Z,0,Good Job,True
@deepthiponnam4210,2021-03-14T22:57:49Z,1,do we have any documentation/notes that you can share with us for the statistical concepts.,True
@sameerpandey5561,2021-03-05T16:15:49Z,0,Superbly explained,True
@rahul281981,2021-02-24T10:57:00Z,1,"Very nicely explained, thank God I found your posts on YouTube while searching the stuffğŸ‘",True
@mubasheerkhan822,2021-02-24T01:38:28Z,0,Too good bro,True
@goutamdas641,2021-02-22T11:00:11Z,1,"You are a great explainer. Thank you. I have one query. Lasso regression provide feature selection. Now say, in my data set there are 50 variables. After Lasso regression 20 out of them become zero. Now there are 30 non-zero predictors. If I want to select 10 most valuable predictors then on what basis I can choose it?",True
@vishalkottu4792,2021-02-21T20:48:27Z,0,for any concept I watch videos from two or three sources. this time it is not required. TQ,True
@pranayp1950,2021-02-20T09:32:53Z,0,Question : So if we want to reduce overfitting by penalizing slope. Can't we just improve our training set so that there is less variance between training and test set ?,True
@ganeshkameswaran7123,2021-02-16T03:25:56Z,0,very good,True
@shashanktripathi3034,2021-02-14T13:40:14Z,0,Sir at 16:19 you said as we increase the lambda value the value of slope will decrease sir how is the slope of the line dependent on lambda?,True
@nehabalani7290,2021-02-06T08:10:42Z,1,Too good and short for ppl clear with basic modeling concepts,True
@abhinavm9685,2021-02-05T16:00:37Z,1,"Hey, one small doubt... we can do feature selection even in ridge regression right...all we have to do is incorporate all the slopes.  could you please correct me if i am wrong...thanks..keep it up...cheers!",True
@BRIJESHKUMAR-zq4rt,2021-01-27T09:00:43Z,0,great sir !,True
@rupayan21,2021-01-27T02:47:55Z,0,"Hello Krish, how do we deal with the following two scenarios? 1. The opposite of what you explained - training data provides an overfitting condition, however, the slope is nearly horizontal (slope is ~0), whereas the test data points are much above the slope. Penalizing the slope won't help in this scenario. 2. Training data as well as test data have high slopes - in such a scenario, penalizing a slope by Ridge/Lasso regression would result in an inappropriate model.  Would appreciate if you can explain this and share the link  With regards Rupayan",True
@shriharimutalik3231,2021-01-26T23:27:34Z,0,"he is explaining nothing but Regularization term , that is a part of Linear Regression I think , there we will use the values of co-coffients in polynomial regression  we have  a model such y = b  + m1x1+m2x2 .........+mnxn  then the value of the slope reduces as along with m in case of stocastic gradiesnt Descent , I dont know much about batch GD",True
@siddhinagare5210,2021-01-23T07:17:22Z,0,what about the bias ? when we getting the slope less steeper???,True
@ashwanikumar-zh1mq,2021-01-16T07:50:18Z,0,Ridge and lasso use can be use other algo for reduse overfitting please tell me sir,True
@georginjacob2984,2021-01-12T14:45:13Z,0,why in the cost function there is no term as 1 by 2m,True
@ibrahimibrahim6735,2021-01-10T01:44:51Z,0,"Thanks, Krish,   I want to correct one thing here, the motivation behind the penalty is not to change the slop; it is to reduce the model's complexity.   For example, consider the flowing tow models: f1:  x + y + z + 2*x^2 + 5y^2 + z^2 =10 f2: 2*x^2 + 5y^2 + z^2 =15  f1 is more complicated than f2. Clearly, a complicated model has a higher chance of overfitting.     By increasing lambda (the complexity factor), it is more likely to have a simpler model.   Another example:  f1: x + 2y + 10z + 5h + 30g = 100 f2: 10z + 30g = 120  f2 is simpler than f1. If both models have the same performance on the training data, we would like to use f2 as our model. Because it is a simpler model and a simpler model has less chance for overfitting.",True
@claymore9112,2021-01-07T13:12:08Z,0,"What happens to Ridge regression if regularisation parameter approaches inï¬nity?",True
@rebornat2582,2020-12-24T11:57:27Z,0,I don't think our aim is to make the slope closer to zero but rather the whole equation close to  zero.,True
@godse54,2020-12-24T11:10:16Z,0,But is it always good for a model that our best fit line  slope moves near to 0,True
@SabyasachiMoitra,2020-12-23T13:29:08Z,0,What is the meaning of penalizing here?,True
@dhananjaykimothi2429,2020-12-23T04:59:45Z,0,1. For lasso it is not the modulus of sum of the coefficient rather it is sum of modulus of the coefficients. 2. Why LASSO will help in selecting features is rushed through and not explained well,True
@sheikhshahbaz2848,2020-12-22T08:20:37Z,0,Just . ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜˜. Awesome . Krish,True
@gavin8535,2020-12-20T08:20:08Z,0,"If the slope is zero, how come square it not making it close to zero?  0^2 = 0 isn't it?",True
@nitishranjanghosh7346,2020-12-19T14:58:57Z,1,"HI Krish, In the linear reg video the cost function formula is different. CF=1/2N(sigma(y^ - y)^2) and in this video it is sigma(y^ - y)^2 . Which is to be used?",True
@aravindmolugu3308,2020-12-04T15:49:03Z,0,"Hi Krish, Great video.. Thanks a lot. However, I have  one small concern. through out the duration of 20 mins I was interrupted with close to 10 advert breaks and that is disturbing the flow. Please see is you can control the no of adverts. It is annoying. But the content and your way of explanation is what has kept me hooked.",True
@satyamchatterjee1074,2020-12-03T16:35:26Z,2,"Sir,  what if the best fit line connecting two initial train points is already at a lower slope ,say [ (1,2) and (9,3)] ? Then we will need to increase the slope  to avoid overfitting. How the penalizing concept helps in such condition ?",True
@litonpaul6133,2020-11-18T04:10:36Z,2,"Krish it is a great video to understand Ridge and Lasso. You have shown here the example of test data which is below the best fit line. In this case we need to reduce the slope of the best fit line. It is fine. But if the test data is populated on the upper side (above) of the best fit line, then what we will do. Will we reduce the slope or increase the slope of the best fit line to overcome the overfitting and underfitting?",True
@ak-su3qt,2020-11-13T20:29:41Z,0,"Watching it second time to revise the concepts. However Sir, slope can't be more than 1....",True
@AkalntDas,2020-11-13T08:24:01Z,0,"what is lamda here?i mean logically, in the graph?",True
@Ramesh-rp6jq,2020-11-06T06:03:12Z,0,What is the use of lambda. Please explain in layman's terms,True
@prabhamelady1680,2020-11-05T09:48:46Z,0,Very clearly explained..  thanks sir..,True
@towardspeace3719,2020-11-01T15:26:31Z,1,"Hi Krish, What if the test data requires a more steeper line! Is the Ridge regression still applicable in that scenario?",True
@towardspeace3719,2020-11-01T15:24:05Z,1,"Hi Krish, I was wondering , what if we include all the test data into the training data and regenerate the model with Linear Regression. Wont that give a better prediction than Ridge Regression?",True
@bahaansari7201,2020-10-29T03:14:40Z,1,this is great! thank you!,True
@wasimahmad2490,2020-10-26T21:20:19Z,0,"Thank you so much for nice lecture on Ridge and Lasso regression. I am still not clear on the point in Lasso regression, how the features are removed when there slope value are low?",True
@litonpaul6133,2020-10-17T17:09:18Z,0,"Hi Krish, here you explained overfitting when the slope is steep, but what if the slope is less steep for regression line.",True
@mohit10singh,2020-10-15T20:07:29Z,0,Very nicely explained. awesome Sir. keep up this good work.,True
@Raveendr1191,2020-10-11T07:32:45Z,1,can an one explain why cost function  formula he changed in liner regression and ridge regression -> 1/2m sumof(y-y^)2 to sumof(y-y^)2,True
@dhirajkumarsahu999,2020-10-06T15:09:52Z,3,Thank you for the efforts ğŸ™ you are like a gift,True
@maheshurkude4007,2020-10-06T04:42:47Z,1,thanks for explaining Buddy!,True
@abhishekchatterjee9503,2020-10-04T20:07:23Z,1,You did a great job sir.... It helped me a lot in understanding this concept. In 20min I understood the basic of this concept. Thank youğŸ’¯ğŸ’¯,True
@ujjawalsharma8363,2020-10-02T18:30:00Z,0,Thanks ğŸ˜Š,True
@thulasirao9139,2020-09-30T14:16:13Z,0,You are doing awesome job. Thank you so much,True
@vishalrai2859,2020-09-29T16:40:28Z,0,best sir,True
@samahirrao,2020-09-27T11:40:25Z,0,"We can just standardize and reduce the slope, in a single step. Why all this is needed?",True
@auroshisray9140,2020-09-24T08:00:30Z,3,Hats offf...grateful for valuable content at 0 cost,True
@rachitsarin6706,2020-09-21T15:25:22Z,13,"Dear, i would like to understand one thing that if my data requires my slope to be steeper How this concept will work in that case?",True
@vishalaaa1,2020-09-19T19:26:07Z,0,This naik is excellent. He is solving every ones problem.,True
@depon1024,2020-09-19T01:59:08Z,1,Great explanation! But it scares me when i'm on fullscreen. thought my PC brightness got error it went up and down,True
@sandipansarkar9211,2020-09-18T18:00:21Z,0,Great explanation Krish.I think I a understanding a little bit about L1 andL2 regression.Thanks,True
@vivekkumaryadav2305,2020-09-17T10:13:10Z,0,Amazing explanation,True
@firta_banjara,2020-09-11T03:34:26Z,0,"Just a Doubt Krish, cost function also has (1/2m) multipied by the sum of squared residuals right, in the video there is no (1/2m).",True
@nehasrivastava8927,2020-09-09T12:54:37Z,0,best tutorials for machine learning with indepth intuition...i think there is no tutorial on utube like this...Thankuu sir..,True
@shubhamkohli2535,2020-08-31T22:27:16Z,62,Only person who is providing this level of knowledge at free of cost. Really appreciate it .,True
,2020-08-31T17:03:50Z,0,"In 17:55, I believe you mean (in a multiple regression model) the sum of the modules of the coefficients instead the module of the sum of the coefficients, right?!",True
@chetankumarnaik9293,2020-08-29T15:21:57Z,0,We need at least three data points to build a linear regression model  because there is no degree of freedom when we use two data point.,True
@Zizou_2014,2020-08-27T09:43:53Z,1,Brilliantly done! Thanks Krish,True
@vijaypalmanit,2020-08-22T12:09:02Z,0,"at 10:28, why would slope of line will decrease only ? it can increase also as there could be another fitting line with higher slope and  can give the same error as line with higher slope, basically there could be 2 lines with different slope but still giving the same cost/loss for those two points, so while explaining why did you assume that another line would be the one only with lower slope ?",True
@indrasenareddyadulla8490,2020-08-20T09:45:09Z,2,"Sir, you have mentioned in your lecture this concept is complicated but never I felt it is so. you have explained very excellent.ğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘Œ",True
@GauravSharma-kb9np,2020-08-18T19:19:46Z,0,"Great Video sir, you explained each and every step very well.",True
@saiteja118,2020-08-14T02:14:31Z,0,when we are changing best fit line for calculation of  (lambda).(slope^2).value of difference in y and y cap might change or not sir...??,True
@mohammedmunavarbsa573,2020-08-11T12:36:56Z,0,super explanation,True
@siddhant997,2020-08-11T11:06:24Z,0,Thanks a ton!,True
@simransingh8042,2020-08-04T17:08:14Z,1,"In ridge regression why cant the slop be zero, just like in lasso regression?",True
@educationalaccountrn1626,2020-08-03T08:45:31Z,2,<3 thank you,True
@sambhavjain1887,2020-07-21T22:46:28Z,0,Sir can you please make a video on how to solve lasso regression problem,True
@ayeshajyoti2446,2020-07-19T05:59:24Z,0,He make this topic sooo easyğŸ˜,True
@prashanths4455,2020-07-19T05:35:14Z,2,Krish An excellent explanation. Thank you so much for this wonderful in-depth intuition.,True
@vidhitandon8176,2020-07-12T20:44:42Z,3,I aim to do my first data science project in another 1 month. If I am successful I will owe it to you ğŸ™,True
@akhila5597,2020-07-11T11:16:14Z,0,wonderful explanantion!,True
@asmersoy4111,2020-07-01T08:50:20Z,0,This is helpful. Thanks,True
@gerardogutierrez4911,2020-06-30T18:05:30Z,37,"if you pause the video and just watch his facial movements and body movements, he looks like hes trying his best to convince you to stay with him during a break up. Then you turn on the audio and its like hes yelling at you to get you to understand something.   Clearly, this man is passionate about teaching Ridge regression and knows a lot. I think its easier to follow when hes like checking up on you by saying, you need to understand this, and repeats words and uses his voice to emphasize concepts. I wish he could explain other things to me besides data science.",True
@erikahorn5007,2020-06-28T17:51:03Z,0,you are very good!! thanks a lot,True
@RinkiKumari-us4ej,2020-06-26T04:34:27Z,1,Best explanation sir.,True
@rrrprogram4704,2020-06-25T07:17:55Z,0,"appreciate the effort... BUT, what is the practical application..... In praction when do we chosse 2 data points as data selection ??? what kind of model is that to have 2 data points???....   Is ridge regression on for high slope lines ?? ....  listening to all this theory part is OK but unable to see the practical application",True
@subramanyasagarmylavarapu5286,2020-06-24T17:40:16Z,0,"Hi Krish, very well explained. It really helps me to understand. Thank you.",True
@saeedkhan246,2020-06-24T08:21:30Z,0,amazing explanation,True
@deepayanbardhan911,2020-06-23T05:41:02Z,0,"Why are we trying to reduce ALWAYS? Maybe we select the two points with the least slope instead of taking the top 2 points - for that example, we might need to increase the slope to reach a better regression line. How is that case handled w.r.t the explanation given?",True
@VishalPatel-cd1hq,2020-06-20T12:34:42Z,0,Hi Krish here we are adding the regularization term to our loss function and this regularization will always positive as lambda value is greater then 1 and slope value we will do square or taking mode then how we can say that it is penalizing . Infact here it is adding some positive value in the loss .,True
@anshulmangal2755,2020-06-19T09:29:43Z,0,Sir great channel on YouTube for machine learning,True
@iamjinse,2020-06-17T10:42:45Z,0,amazing ...,True
@cherryandchanu8632,2020-06-14T12:14:43Z,0,In the last vedio cost function is given like variance / sample? Can you please tell why it is rwmoved here?,True
@karanparadkar5509,2020-06-13T21:31:37Z,0,Very helpful.,True
@HammadMalik,2020-06-13T21:26:16Z,4,Thanks Krish for explaining the intuition behind Ridge and Lasso regression. Very helpful.,True
@marijatosic217,2020-06-13T20:37:17Z,6,Great video! I appreciate how hard his effort is to help us really understand the material!,True
@rainfeedermusic,2020-06-12T07:07:26Z,0,How to choose between Ridge and Lasso?,True
@dayakarreddy9266,2020-06-10T01:40:41Z,0,what if the test data is above the best fit line ? will it still work ?,True
@sidduhedaginal,2020-06-06T10:04:38Z,0,Just an awesome explanation. concepts  are very clearly explained ...thanks for your true effort,True
@phungtruong6698,2020-06-05T09:47:28Z,0,Thank you sir <3,True
@kiran082,2020-06-05T03:07:14Z,0,Thank you Krish very detailed explanation,True
@andrewwilliam2209,2020-06-04T11:56:29Z,0,"Hey Krish, is regularization only for linear regression?",True
@faizanzahid490,2020-06-02T12:28:47Z,0,What if in case of test data slope actually increases. Why everybody take this case of reducing slope? I mean residuals can also be increased if slope is increased even more to introduce bias deliberately for better results for the test data,True
@hafizmfadli,2020-06-01T18:43:08Z,0,"16:19 if lambda goes bigger than slope tend toward to zero ? why?, i think if lambda goes bigger then the slope will be greater too",True
@Sumta555,2020-05-27T04:46:04Z,5,18:35 How the features get removed where |slope| is very very less? Hats off for this fantastic clarity on the topic.,True
@chavianddavid,2020-05-22T05:29:20Z,0,"If you are always adjusting the training model until it results in the best model for the test data, why isn't that the same as just creating a model from the test data?",True
@habilmohammed5127,2020-05-21T23:17:02Z,1,"Good video on ridge and lasso..  Just a clarification, if our variables are indeed highly correlated in general, wouldn't performing these techniques increase the errors on test data as well.",True
@Sksahu_123,2020-05-21T04:24:28Z,0,great,True
@arunxavier502,2020-05-21T01:18:08Z,0,Hi...in this video..u always mention that Ridge regression is trying to penalise lines with higher slope..but what if in the test data the points are on the left side of the steep slope line...i.e. the slop still need to be steeper for best fit line?,True
@shakyasarkar7143,2020-05-19T22:17:50Z,1,"Sir, you are a pure genius in explaining concepts. I follow almost all your ML topics.  But one thing that's constantly creating clouds of questions is that, how are the features exactly getting eliminated..you said that the slope for those features will get to zero..but that will also happen for the other features also right?  If the Â penalty i.e: Î»(|w_1|+|w_2|+|w_3|+...|w_N|) is getting zero, then all the slopes for those features will be also getting zero right?  Can you please give me a detailed explanation, Sir?  I am a huge fan of your playlists and your explanations. And I am working very hard to get these ML concepts crisp and crystal clear. Please help me out with this Sir.  You can also suggest me some articles if needed..but please make me understand this, Sir.  Thank you.",True
@mdishtiyaquehussain9564,2020-05-19T16:35:45Z,1,"@krish naik sir ,  when you make  slope steaper  then  how  the  summation  term will be  remaining  zero..??? because  at that time trainning data is  not lying  on the best fit line so, it not will be zero ..",True
@csprusty,2020-05-18T14:27:57Z,0,"but you didn't say why in lasso, the slope = 0",True
@dv5686,2020-05-10T12:27:50Z,0,nice expmlanation,True
@berberbergsma3811,2020-05-07T18:34:20Z,1,Thank you!,True
@binnypatel7061,2020-05-04T21:15:02Z,1,Awesome job.....keep up with the good work!,True
@pponvannan,2020-05-04T17:01:52Z,0,well explained,True
@alexjohnston6847,2020-05-03T21:28:06Z,0,"At 18:00, you are writing lambda * |m1 + m2 + m3 + m4|... but shouldn't it be lambda * (|m1| + |m2| + |m3| + |m4|...)? Otherwise, the individual slopes could partially or entirely cancel eachother out. Say if m1 = +2, m2 = -2, m3 = +1, m4 = -1, then the |sum(all slopes)| = 0, but sum(|each individual slope|) = 6.",True
@sahilzele2142,2020-05-02T12:04:50Z,11,"so the basic idea is:   1)steeper slope leads to overfitting @8:16 (what he basically means is that the overfitting line we have has a steeper slope which does not justify his statement on the contrary)     2)adding lambda*(slope)^2 will increase the value of cost function for the overfitted line, which will lead to reduction of slopes or 'thetas' or m's (there are all the same things) @10:03   3)now that the value of cost function for overfitted line is not minimum, another best line is selected by reducing the slopes or 'thetas' or m's which will also reflect in addition of lambda*(slope)^2 ,just this time slope added will be less. @13:45    4)doing this will overcome overfitting as the new best fit line will have less variance(more successful for training data) and less bias than our previous line @14:10 , the bias maybe more because it was 0 for overfitted line ,then it will be a bit more for the new line   5)lambda can be also called as scaling factor or inflation rate to manipulate the regularization. as for the question ,what happens if we have overfitted line with less steeper slope?,  then i think we'll find the best fit line with even less steep slope(maybe close to slope~0 but !=0) @16:30   and tadaa!!!! we have reduced overfititng successfully!! please correct me if anything's wrong",True
@user-sw9kd9pv4n,2020-05-02T08:11:30Z,2,@6:20 If u understand linear regression start here,True
@abhi9raj776,2020-05-01T09:03:15Z,0,perfect explanation!!! thank you sir !,True
@deepak0417,2020-04-30T21:52:46Z,1,How do we solve the under fitted equation resulted from training data set ? Even I see the slop is not steeper. Should we introduce more steepness in this case?,True
@kanhataak1269,2020-04-26T04:31:26Z,0,After watching this lecture is not complicated... good teaching sir,True
@rajk58,2020-04-25T23:44:37Z,1,"You sir, are amazing!!! Hats off to you!!",True
@JoseAntonio-gu2fx,2020-04-22T15:59:16Z,3,Muchas gracias por compartir. Se agradece mucho el esfuerzo por aclarar los conceptos que es la base de partida para la resoluciÃ³n. Saludos desde EspaÃ±a!,True
@monikasingla295,2020-04-18T13:27:15Z,0,thanks for the video butÂ Â how to get the value of lambda,True
@susaedu6971,2020-04-14T16:59:49Z,0,"Salute sir, nice explanation",True
@arjyabasu1311,2020-04-13T11:36:25Z,0,Is alpha or the learning rate and lamda same??,True
@mumtahinhabib4314,2020-04-11T17:55:19Z,0,This is where I have found the best explanation of ridge regression after searching a lot of videos and documentations. thank you sir,True
@AnkitSingh-rc9bb,2020-04-10T15:48:36Z,0,how to get the value of lambda ?? from cross validation,True
@sriramp8478,2020-04-05T19:23:57Z,0,I think lasso will be slope square,True
@saurabhtiwari2541,2020-04-05T14:22:34Z,0,Awesome tutorial with clear concepts,True
@callppatel1,2020-04-03T19:34:45Z,0,Very clearly explained..,True
@TheR4Z0R996,2020-04-03T12:37:37Z,34,"Keep up the good work, blessing from italy My friend :)",True
@partheshsoni6421,2020-03-31T09:03:29Z,0,Nice explanation. Thanks a lot!,True
@143balug,2020-03-30T10:46:53Z,1,"Hi Krish,  Your are making  our confidence more on data scince with the clear explanations",True
@AnkJyotishAaman,2020-03-29T12:21:13Z,22,"This guy is legit !!  Hat's off for the explanation!!  Loved it sir, Thanks",True
@KARAB1NAS,2020-03-21T21:24:21Z,0,"in your example, you're explaining a situation of UNDERfitting not OVERfitting",True
@dragonhead48,2020-03-13T03:20:24Z,0,How red bull or coke do you do before each video?,True
@kanuparthisailikhith,2020-03-03T19:32:53Z,1,The best tutorial I have seen till date on this topic. Thanks so much for clarity,True
@walete,2020-02-27T02:15:18Z,0,"thank you sir, great explanation",True
@cvshukla,2020-02-26T12:22:53Z,0,you said to calculate lambda we use cross-validation. Is it right or we should use GridesearchCV. Please correct and explain how cross val do it.,True
@hipraneth,2020-02-23T15:04:57Z,234,Lucid explanation at free of cost . Your passion to make the concept crystal clear is very much evident in your eyes...Hats Off!!!,True
@aelitata9662,2020-02-19T23:57:52Z,27,I'm in crisis to learn this topic and all I know is y=mx+c. I think this is the clearest one I've watched on youtube. Thank you sooooo much and love your enthusiasm when you tried to explain the confusing parts <3,True
@mrjigeeshu,2020-02-19T23:41:34Z,0,Good one !,True
@huashi8893,2020-02-17T06:15:04Z,0,Excellent,True
@antonyraja9902,2020-02-16T11:21:13Z,1,Amazing ğŸ‘Œ Great explanation ğŸ‘ Thanks and keep doing videos like thisğŸ”¥,True
@srtvenkat215,2020-02-12T16:55:51Z,2,Could you please share the link to the full playlist?,True
@maYYidtS,2020-01-31T13:31:24Z,0,what is the difference between steeper and homoscedasticity?,True
@saurabhtripathi62,2020-01-31T04:48:31Z,0,sir it should be ycap /pred- y/actual as per your last video which one is right?,True
@parikhgoyal5506,2020-01-29T17:26:26Z,1,"Thank you very much sir, I found very few useful resources for ridge regression and your's one is definetly good",True
@t-ranosaurierruhl9920,2020-01-22T20:28:40Z,0,You are great!! Thanks a lot,True
@akashgayakwad9550,2020-01-17T16:01:15Z,0,"What if line with lower slope overfit the training data,and showing high variance for testing data, please someone resolve my this doubt????",True
@BoyClassicall,2020-01-15T18:23:38Z,2,Concept well explained. I've watch a lot of videos on Ridge regression but most well explained has shown mathematically the effect of lambda on slope.,True
@Raja-tt4ll,2020-01-11T13:40:48Z,1,very nice explanation. Thanks :),True
@shrikantlandage7305,2020-01-09T05:47:42Z,0,Thank You,True
@thananjayachakravarthys4092,2020-01-06T08:51:37Z,1,"(y^ - y) **2, this is the expression you explained in the in-depth math intuition for linear regression. But, why (y-y^)**2 in this video sir?",True
@pulkitmehta1795,2020-01-01T11:58:15Z,0,very well explained ...,True
@dheerajkumark2268,2019-12-29T15:26:16Z,0,Sir plz upload Elastic net regression,True
@adityasinha4022,2019-12-21T14:48:14Z,0,what is lambda there?,True
@harshstrum,2019-12-20T02:41:44Z,30,Thank You bhaiya. It feels like every mroning when I watch your videos my career slope will increase. Thank you for this explaination.,True
@akshaygupta6321,2019-12-17T06:32:20Z,0,Do ridge and lasso are applicable for non-linear regression models?,True
@aruningreenvalley,2019-12-14T15:46:28Z,0,"After doing my multiple linear regression and using the convergence theorem to reduce the cost function, how will I know that I should go for ridge or lasso regression?",True
@smlekhashree3599,2019-12-13T15:18:32Z,1,Thank you sir.. It's clear explanation..,True
@palashmoon3808,2019-12-13T09:33:58Z,0,"Hi Krish,  Can you tell when to use Lasso and when to Use Ridge? or we can use anyone depending on our choice.",True
@dineshpramanik2571,2019-12-12T15:06:10Z,1,Excellent explanation sir...thanks,True
@rishisankhla3124,2019-12-11T17:14:51Z,1,Use hashtag in your title ending like - #regularization .... Then you will get more views,True
@swaruppanda2842,2019-12-11T11:18:33Z,1,Thanks this was quite helpful,True
@veerendraammu,2019-12-11T10:44:22Z,1,Sir can please make video on web scraping,True
@adarshmamidi334,2019-12-11T08:04:19Z,1,Reply in Insta,True
@subhammal9667,2019-12-11T07:42:29Z,1,"good morning Sir.. I want to know something from you sir. I am trying to learn - AI, so which book should i buy for AI.",True
@krishnamishra8598,2019-12-11T04:53:53Z,1,"Hello sir, Just one clearity needed regarding bias-variance tread-off .. When we say high bias doesn't it mean of very good fit in training & bad fit in testing ? Also as per my understanding when we have high bias there will be low variance & vice-versa, isn't it ?:)",True
@pranavmotarwar1428,2019-12-11T04:53:15Z,1,"Hello, I have a doubt in that heroku  deployment concept. My API is showing internal error. Where you have to upload the dataset actually in github?",True
@vikrambankar4663,2019-12-11T04:33:55Z,1,Hi krish will you pls show us how we can code this equation.,True
@yashodhansatellite1,2019-12-11T03:51:42Z,1,Blessings,True
@dudee420,2019-12-11T00:10:47Z,2,"hello, can you make video with example also",True
@saddamshaikh9285,2019-12-10T21:52:04Z,1,Which one is best tech. Ridge or lasso?,True
@VenuGopal-qz2oj,2019-12-10T20:54:37Z,1,Do u have program which shows the above usage ridge and laso,True
@saddamshaikh9285,2019-12-10T20:23:21Z,1,In cost function actual minus predicted value but why use square? and also same in regularisation tech. Ridge and lasso use square? most of the time asked question in interview..,True
@mohammedfaisal6714,2019-12-10T20:02:12Z,1,Thanks a lot for your Support,True
@akashpawar9058,2019-12-10T20:02:06Z,2,Doing well  in this! Field,True
@codingquiz,2019-12-10T19:57:17Z,3,thanks bro,True
