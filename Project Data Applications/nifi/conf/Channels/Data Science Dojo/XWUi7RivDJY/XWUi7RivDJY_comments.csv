author,updated_at,like_count,text,public
@kobeoncount,2023-09-04T23:32:36Z,0,"Hi. Thank you for the tutorial but there is a problem with the code: > test.tokens.dfm <- dfm_select(test.tokens.dfm, pattern = train.tokens.dfm,  selection = ""keep"") Error: dfm cannot be used as pattern; use 'dfm_match' instead  I hope you will help!",True
@gabrieleruggeri2248,2021-05-28T10:05:29Z,3,"UPDATE: line 729 doesn't work to me; I add to go with test.tokens.dfm = dfm_match(test.tokens.dfm, features = featnames(train.tokens.dfm))",True
@163ii,2021-05-01T21:15:48Z,0,Good Job!,True
@egecant,2021-03-07T20:24:58Z,0,You clean data and produce new features for 6+ hours and all you get is overfitted model... Thats data science lol,True
@prashu25925,2021-01-18T19:37:49Z,0,Hi Dave data science dojo  There is no parameter called 'features' in 'dfm_select' package.  Where can I get it ? Please help. Stuck there since long time,True
@prashantpanchal2022,2021-01-14T19:46:42Z,0,"I have a query, please help me with this  # ENSURE THE TEST DFM HAS THE SAME N-GRAMS AS THE TRAINING DFM. test.tokens.dfm <- dfm_select(test.tokens.dfm, features = train.tokens.dfm)    There is no ""features"" parameter in dfm_select.  How to match up with the training data's features count? Plz Plz help",True
@johnpeterson3913,2021-01-05T01:43:26Z,0,"I am running into a problem that a few couple other viewers have already mentioned: When I run test.tokens.dfm<-dfm_select(test.tokens.dfm,features=train.tokens.dfm) I get an error message saying  ""unused argument (features = train.tokens.dfm)"". When I used the help function, I got the following warning: ""pattern = dfm is deprecated; use dfm_match() instead"" Before using this, I queried ""?dfm_match"" and found that ""features"" is an element in that function. But when I ran: test.tokens.dfm<-dfm_match(test.tokens.dfm,features=train.tokens.dfm) I got another error: ""features must be a character vector"".   Any thoughts on what I could do to resolve this problem? The videos have been superb, so thank you for the help.",True
@fadedmachine,2020-09-29T05:14:39Z,1,"Quick question for @DataScienceDojo: Is there a function within an R package that would allow me to replicate the code at line 747 (test.tokens.tfidf = apply(test.tokens.df, 2, tf.idf, idf = train.tokens.idf)? More specifically, I am asking is there a tf-idf function you would recommend that would allow me to reference the idf values from my training data when pre-processing my test data without creating functions of my own or copying your work? Appreciate any feedback!  Awesome series! I'm applying what I have learned here to a new data set for independent practice!",True
@argonaut2004,2020-06-05T02:55:10Z,0,I'm not sure I exactly understand why you use the training idf.  Why are you inverse weighting each column (ie token) using on the training dataset and not the testing set?  Should you inverse weight it based on the testing set?,True
@shivangsharma54,2020-01-02T10:27:28Z,0,"Hello Everyone,   While performing below mentioned code :   test.svd.raw <- t(sigma.inverse * u.transpose %*% t(test.tokens.tfidf)   I'm getting ""Error in u.transpose %*% t(test.tokens.tfidf) : non-conformable arguments""   Please help me out.   Regards",True
@shivangsharma54,2020-01-02T09:42:14Z,0,"Hello Dave,   While performing below mentioned code :   test.svd.raw <- t(sigma.inverse * u.transpose %*% t(test.tokens.tfidf)   I'm getting ""Error in u.transpose %*% t(test.tokens.tfidf) : non-conformable arguments""   Please help.   Regards",True
@sunil7162,2019-03-22T05:16:13Z,0,"On line# 729, I understand that features have to be same as in the training for the test set, however, isn't it like you are enforcing the features of training set on to test even though some features of training set may not be present in the test data set. vice versa some features of testing dataset many not be present in training.... may be possible that some features of test set is more significance .... similar to a situation of production data..... I have seen all videos till end, though the accuracy is little down by removing the cosine feature... I feel hypothetically the train.dfm is stale when we look from the test.df perspective",True
@DnyaneshwarPanchaldsp,2019-01-27T01:27:33Z,0,Blurred screen......,True
@nizindia,2018-11-25T09:43:42Z,0,"Hi Dave,  Excellent video tutorials. Really enjoyed it. But I have a question @row# 778, why you have used the train set spam.indexes and appended to the test set. In my opinion, you have to extract the spam.indexes out of the test set and use them to append to test data set. Pls clarify.",True
@user-zy7yv8mp8y,2018-07-26T17:40:51Z,0,test ! text test  test! lol,True
@GiedriusBlazys,2018-05-29T17:41:58Z,3,"Hi. Great tutorial. Just a quick note on Session 11: when creating cosine similarities with spam message feature on training data you should exclude the observation itself from the spam messages list:  # cosine similarities with spam messages and vice versa! spam.indexes <- which(train$Label == ""spam"") train.svd$SpamSimilarity <- rep(0.0, nrow(train.svd)) for(i in 1:nrow(train.svd)) {     spam.indexesCV <- setdiff(spam.indexes,i)     train.svd$SpamSimilarity[i] <- mean(train.similarities[i, spam.indexesCV]) }  This solves the data leakage problem leading to over-fitting. The RF results on test data with updated feature are much better:   # Drill-in on results  confusionMatrix(preds, test.svd$Label) Confusion Matrix and Statistics            Reference Prediction  ham spam       ham  1445   32       spam    2  192                                                       Accuracy : 0.98                            95% CI : (0.972, 0.986)     No Information Rate : 0.866              P-Value [Acc > NIR] : < 2e-16                                                                   Kappa : 0.907           Mcnemar's Test P-Value : 0.000000658                                                         Sensitivity : 0.999                      Specificity : 0.857                   Pos Pred Value : 0.978                   Neg Pred Value : 0.990                       Prevalence : 0.866                   Detection Rate : 0.865             Detection Prevalence : 0.884                Balanced Accuracy : 0.928                                                          'Positive' Class : ham",True
@PCI208,2018-03-25T15:43:27Z,0,"hi Dave, can you teach me about precision, recall, f-1 measure ?  Thanks",True
@preeyank5,2018-03-04T08:55:25Z,0,"Hi Dave,  If I have 200 different target(response) variable instead of only two, ham and spam, then should I include the cosine similarity step in my model. Additionally, would any of my model development steps be affected by the presence of such a large number of target variable.",True
@toastersman217,2018-01-28T21:41:27Z,0,"After calculating rf.cv.3 on my machine, I obtained the same results in your last video. However, my confusion matrix is the following for this video:           Reference Prediction  ham spam       ham  1445   35       spam    2  189",True
@fayburns0,2017-11-26T12:08:05Z,0,A real thriller .... thank you :-),True
@ajaymahara483,2017-11-09T19:42:43Z,0,"even after applying the dfm_select and feature = train.data.df . but even then the pridict function  is giving the error -  Error in eval(predvars, data, env) : object 'next.' not found any help ?",True
@rajeshwaran001,2017-08-26T09:43:39Z,0,"Hello , would like to understand better what does test.tokens.dfm <- dfm_select(test.tokens.dfm , features=train.tokens.dfm) do ?  I understand that both train and test should have the same features to apply prediction. But would like to understand, that after i run this piece of code , what would be the values in the cell under the columns in test.tokens.dfm ?",True
@jackcornwall352,2017-08-24T10:17:35Z,1,You meantion that if we decide to use a package we should choose one that allows us to maintain the TFIDF training data over time. Does quantidads tfidf function allow you to do this? If not can you recommend any that do?,True
@nicholascanova4250,2017-08-23T19:14:36Z,3,"I believe line 784 should be:   test.svd$SpamSimilarity[i] <- mean(test.similarities[i, spam.cols])  however, after making this correction, the preds output from line 790 causes an error when input as a parameter to the confusionMatrix() function in line 793. you'll notice length(preds) now equals 1665, which is not the desired 1671. So correcting the error in line 784 causes a new error when making predictions. digging into it further, i notice that sum(is.na(test.svd$SpamSimilarity)) now equals 6, so there are 6 NaN values for the spam similarity, and therefore 6 predictions cannot be made.",True
@viswacr,2017-08-19T10:12:21Z,1,"Great series, thank you",True
@hemanthkumar-qg3fy,2017-08-17T06:56:10Z,1,"Hi Dave, Thank you for this awesome series! My question for you is related to text clustering. What are some of the techniques discussed here, that would help me cluster text given features (such as keywords describing the document)?",True
@junaideffendi4860,2017-08-15T05:37:58Z,1,"I have a question Dave, since you are performing the same steps as you did in train data set, what if you have split the dataset after doing all the processing?  Thanks Junaid",True
@mannysingh2133,2017-08-14T19:36:47Z,1,"Great video! please make some on Neural Nets, Markov Chains, SVM etc :)",True
@ShoaibSaadat,2017-08-14T17:54:11Z,1,great job Dave. love it 👍,True
