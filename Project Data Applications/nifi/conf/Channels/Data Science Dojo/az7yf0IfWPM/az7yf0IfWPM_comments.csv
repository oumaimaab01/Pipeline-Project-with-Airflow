author,updated_at,like_count,text,public
@rafaelfonseca7942,2023-11-15T13:35:43Z,0,"Good teaching, so far thanks sir!",True
@kanika1,2022-01-19T06:18:08Z,0,Loved all your videos. I highly recommend these videos to any beginners.,True
@maryamsi5090,2021-05-03T16:00:46Z,1,"what does ""Text"" represent?",True
@163ii,2021-05-01T20:01:01Z,0,Thanks for your contribution to expand the data science knowledge!,True
@dr.javidiqbal265,2020-07-06T11:41:59Z,0,"Hello. I am just wondering to calculate tf.idf and your video looks great. However, if you could help me understand that I want dictionary specific terms to find from corpus of 500 annual reports and then calculating tf.idf. Is it possible to do it.? In addition, I want identity of annual reports rather mixing it into only one document.",True
@sureshnaik2943,2020-05-17T18:34:24Z,0,I have a bunch of Text files in my folder . And I want answers from all those text files. This should happen like me coding on R . Like when I type some question in R for example what is earth . I need answers extrcated  from all those in sentences.,True
@vaz.felipe,2020-01-28T23:55:48Z,0,"Hi guys from Data Science Dojo. Please, in my case I have just one document. So, dont I need to implement the TF-IDF method? Or, thinking like the documents are the number of respondents that I had in my csv file. Which one may I follow?",True
@danielencinasz,2020-01-16T19:17:51Z,0,"Hi Dave, I am experiencing this problem and I cannot create the matrix needed: Error in asMethod(object) :    Cholmod error 'problem too large' at file ../Core/cholmod_dense.c, line 105",True
@juanmauricioarrietalopez2395,2019-11-27T07:48:05Z,2,"All of these videos, all of this playlist has helped me into my thesis. Thanks a lot!",True
@cikatheresia,2019-08-24T12:49:43Z,1,"Hi Dave, thanks for your video! learned a lot today. Unfortunately, I'm not blessed with the number of documents you have, in fact, I only have one document to analyze! Therefore, using your log(N/count(t)) would not be rational because a lot of them will hit the score 0. What do I do? Bet on domain knowledge and manually flag certain terms as stopwords?",True
@williamjohnson928,2019-02-25T21:27:56Z,0,I am having memory issues with 8 GB of memory. Is that to be expected? (It is on a work machine... unsure if I can request more memory) Is there a file or two that I could eliminate to continue the work?,True
@Fealivren,2018-12-02T15:50:05Z,1,"Hi Dave, First of all i'd like to say your videos are great, simple to digest and follow. So thank you for that! I'm occuring an odd error, stating: 'Error: cannot allocate vector of size 45 Kb' while running the TF function.  Would you have any insight of solving the problem? I'd be very greatful! Regards.",True
@TheBebwa,2018-09-04T12:46:45Z,0,This is awesome.... thanks,True
@slimoueslati9479,2018-07-05T18:48:26Z,1,Or simply you can use weightTfIdf fom tm package !!,True
@phamngocminh4907,2018-06-05T11:16:31Z,0,"Dear Dave, I really love your tutorials. Thank you very much for all your effort in making these videos and sharing your insights and experience in data science. Besides the Titanic and these text analytics tutorials, do you have any others? I want to check them all out :).  Also, I want to ask TF (text frequency) you mentioned in this tutorial is the count of a term over the total number of terms in ONE document or ALL documents in a matrix/ data.frame. Besides, why don't we use 'scale' to normalize our data?",True
@hussainhatimi4858,2018-05-08T06:47:21Z,0,"Really Helpful Videos  but when run this line of code it gives me an error: > train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency) Error: cannot allocate vector of size 45 Kb   I have an i7 4th gen machine with 8gb RAM and it can't allocate 45kb? doesn't make any sense to me",True
@PCI208,2018-03-27T12:24:23Z,0,"hey can you help me?  when i run this code  train.tokens.tfidf <-  apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)  i got a message ""Error: cannot allocate vector of size 171.0 Mb"" what should i do to fix this?",True
@93jackjoe,2018-02-17T18:10:26Z,0,Thanks yet again!,True
@xixicy,2018-01-16T06:24:23Z,1,"Dave, love your lecturing!",True
@akrsrivastava,2018-01-06T10:21:04Z,1,Thanks. A good explanatory video on text mining with R was missing on youtube. I have followed your other videos on youtube and found them helpful. One query. Quanteda has inbuilt functions for calculating TF IDF. Any specific reason why you wrote the code yourself?,True
@juwlsss,2017-12-13T16:17:22Z,0,"Dear Dave.   First of all, thank you for the great introductory course!   I've worked through the first 5 videos of this course on the spam dataset, and in parallel I am running the code on my dataset which is slightly larger. In particular I have about 1,6 mln observations each one of them containing just about the same amount of text as in the spam observations.   When I created my train.tokens.dfm, I got a large dfm object of 200 bln elements, 420 mb.  Then I proceeded to the next step to create a matrix, here is when I encounter memory problems.  I've tried (as per your instruction) train.tokens.matrix = as.matrix(train.tokens.dfm) or train.tokens.df = cbind(Label = train$Label, as.data.frame(train.tokens.dfm))  both of these function return the following mistake: Cholmod error 'problem too large' at file ../Core/cholmod_dense.c, line 105  I can imagine what size the matrix must be created with these formulas, but I really do want to use tokenization in my approach.  Is there any way around it?  Thank you in advance! P.S. I'm running a Macbook Pro core i7, 16 GB RAM P.S.S. Your caret xgboost tutorial is awesome!",True
@sebastianhuppmann2017,2017-10-14T19:34:57Z,1,Thanks for your videos! Learned a lot!,True
@sidhantamaharana3813,2017-08-14T04:27:23Z,1,"Probably this is my first youtube video comment, because i couldn't resist myself appreciating you for this wonderful video. Thank you and kudos to you for such a wonderful explanation. :D",True
@prateek2191,2017-08-13T11:51:50Z,2,"Hi, I am following your tutorial from the start and trying to build model for my problem that includes more than 2 class labels. I tried with the bag of words model like in your fourth tutorial with random forest, it is giving maximum of about 40% of accuracy. Any advice so that i can improve my model accuracy?",True
@deepaksingh9318,2017-08-11T10:23:58Z,3,"Hi,  . One question on calculation of tfidf here..   As per the coded formula uh r taking documents which have > 0 occurance.  But it should be all the documents right?   Is that correct?",True
@Jayl__,2017-07-16T17:21:11Z,1,Should I be concerned with this results when I run my model:  1: closing unused connection 7 (<-myname:11218)  2: closing unused connection 6 (<-myname:11218)  3: closing unused connection 5 (<-myname:11218)  >,True
@bhupen008,2017-07-13T12:52:19Z,3,U r a great teacher! I will follow ur each and every video from now on. Thank you so much. More power to you. Regards,True
@mainestouen,2017-07-07T07:46:06Z,1,"Hi Dave. I'm trying to use this approach but instead of training the model to predict text that results in ham or spam, I'm trying to predict which text leads to better email open rates. I have split my corpus into quartiles ranked by open rate and am training the model against these quartiles. Is this a valid adaptation of your approach outlined in this video series?",True
@MasoudPaydar,2017-07-04T17:44:09Z,3,Thanks a lot. Learned a lot today.,True
@avishekkumar9341,2017-07-03T23:32:24Z,2,Thanks Dave. Really appreciate your style of teaching. Very helpful.,True
