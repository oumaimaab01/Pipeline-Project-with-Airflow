author,updated_at,like_count,text,public
@syedtanzeelrabani1704,2022-06-30T16:59:39Z,0,Hey.. It was a pleasure to learn from your exceptional video.  Can anyone explain a little bit about the range of values in the cells after applying the SVD. How to interpret those values.,True
@jayverdetorres7684,2021-10-27T14:39:39Z,1,Is computation of sigma inverse correct? I think you should use solve() function instead of getting of 1/matrix,True
@KayYesYouTuber,2019-10-04T21:38:34Z,1,Thank you for a wonderful video. Between 18:00 and 20:00 you explain relationship between document.hat and the v matrix. How is this established? Where can I see more math about this?,True
@vivekv73,2019-05-30T12:58:24Z,2,Thank you for this tutorial on Text Analytics. Excellent teaching and really enjoyed the series.,True
@yashvsiter,2019-04-23T11:52:17Z,0,"Hi , I am getting the following error with confusion matrix:- Warning message: In Ops.factor(predictedScores, threshold) : ‘<’ not meaningful for factors Couldn't come across any satisfactory resolution through google. how should I go about resolving this. Thanks",True
@jdlopez131,2019-01-27T04:20:04Z,0,I'm 4 min into the video and we are not passed the part where we loaded the library ... ZzzZZZzz,True
@PCI208,2018-11-27T05:39:45Z,0,"in 7.38, dave said the research said the value of nv 300 is the best, can you provide the research source? This is for my final project  thanks",True
@WorldAroundWe,2018-11-14T13:57:53Z,0,"Hi Dave, i am getting stack overflow error as the matrix has exploded what is the best way to do SVD , meaning use irlba package if the matrix is huge",True
@farhanamim3068,2018-09-28T07:10:38Z,0,"In the github, that couldn't open please tell how you create the ( rf.cv.1.RData) file please reply me. #Data_science _dojo",True
@indrajitsaha3808,2018-06-22T15:31:30Z,1,"Hey Dave, First of all this text analytics series is too good! Thank you for sharing the content online. I have a question : can we choose Principal Component Analysis instead of Singular Value Decomposition? What I understood that we need to reduce the feature space so that can be done using PCA also. But why are we using SVD? There should be some kind of intuition behind this. I saw you have used both kind of matrices like document and term for the right and left singular vectors. Anyway it would be great if you let me know the intuition behind using SVD and not using PCA. Thanks in advance!",True
@sreenathreddy7093,2018-02-12T11:49:49Z,0,"Hello Dave,    When I am executing the below statement    train.irlba = irlba(t(train.tokens.tfidf), nv = 300, maxit = 600)   I am getting error as below.  Error in irlba(t(train.tokens.tfidf), nv = 300, maxit = 600) :    BLAS/LAPACK routine 'DLASCL' gave error code -4  Could you please help me how to resolve this error?",True
@acornheadmoron,2017-12-22T16:07:09Z,0,"Hello, really enjoying this video series so far.  I have been following along with my own data set.  After running the SVD on the transposed tfidf matrix I have been seeing the error ""You're computing too large a percentage of total singular values, use a standard SVD instead"".  So far, my dataset is smaller than the one used in the training videos.  Any ideas on how to address?  Thanks,",True
@terrybrooks9514,2017-10-10T19:22:39Z,2,"Great videos. Learning a lot and quickly. Btw, what are “grinzies”?",True
@anuragsharma5208,2017-10-08T18:51:44Z,1,"Entire series is awesome for text analytics, but can I expect something where we dont have labels to classify the documents. Instead we just have only documents (imagine a single columns in Excel which contains thousands of text cells) and we want to cluster or classify them in fewer groups.",True
@rajeshwaran001,2017-08-14T08:57:26Z,1,Some of the viewers might not have the luxury of the computing power that might be required to run some of these pieces of code. Would be helpful if a Markdown version of this be published so that viewers who do not have this luxury can view the code and results without having to execute them.,True
@rajeshwaran001,2017-08-14T08:54:26Z,1,"Hello, Thank you for making these tutorials. These are wonderful.  Up until now, we are seeing that the accuracy is being computed only using the training data set . Shouldn't this be applied to the test data set that has been held back ?",True
@walterhoekstra2312,2017-08-07T09:35:01Z,2,"hi, thanks a ton for these tutorials. I have a bit of experience with general data mining, but am quite new to using text as features - decided to look for tutorials when I ran into the Curse of Dimensionality. This series really eased me into it, and I'm now well on my way to solve a business problem!",True
@kokweikhong5974,2017-08-02T17:52:06Z,1,Is there a way to know what the column V stands for in the SVDs? Like what were the key terms that has been decomposed?,True
@mihinduperera8688,2017-07-26T10:56:44Z,1,"Hey Dave,   Ran my test data set through the same pipeline as the train data set. Did the same TF-IDF, SVD and so on.Then used the predict command to predict ham and spam messages, using the model built by randomForest and test data.  test.pred.rf<- predict(rf.cv1, test.svd)  confusionMatrix(test.pred.rf , test.svd$label)    Shows a 100% accuracy!!!!!  Freaking out, what's the answer to this problem? What have I done wrong?   Is it probable that the 300 predictors of the train.svd data set used on by the randomForest model do not correspond to the test.svd data set, or is that not possible?",True
@deepaksingh9318,2017-07-25T09:53:58Z,1,Are we gonna see more videos posted continuing this series?  Or is it over,True
