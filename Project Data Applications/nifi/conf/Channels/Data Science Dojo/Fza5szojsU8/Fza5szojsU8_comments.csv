author,updated_at,like_count,text,public
@pedrofernandosalgadoalvare772,2021-02-03T00:31:07Z,0,Dave! You're a genius! Thanks a lot.,True
@r_pydatascience,2020-05-21T20:49:37Z,0,"What a great teaching skills. Thank you for these helpful videos. I was trying to reproduce your codes with different data. I have the following error after running the following code. Pls help.  rpart.cv.1 <- train(Include ~ ., data = train.tokens.df, method = ""rpart"",  trControl = cv.cntrl, tuneLength = 10) Error in terms.formula(formula, data = data) :    duplicated name 'document' in data frame using '.'",True
@the_crypto_gorgu,2019-05-30T13:54:08Z,0,What about NGD is it similar? (NGD-> normalized google distance),True
@kristyburns2363,2019-04-03T21:16:20Z,3,Feels like we are skipping things because they are repeated but now I am confused and not sure what to do with my model,True
@jimbobbillybob,2019-03-29T01:33:05Z,0,whoo knoo? 15:03,True
@dezenaamvergeetiknie,2019-01-03T21:50:57Z,0,Linear algebra is not my strong suit so I have a question. Suppose I have performed the SVD computation but I want to add another feature to the original feature space. Does this mean I have to redo the entire SVD computation or is there some efficient way to update the SVD?,True
@aakashchugh9,2018-05-17T10:40:43Z,3,Hi Dave.. i was going through an article Dimensionality reduction for bag-of-words models: PCA vs LSA by Benjamin Fayyazuddin Ljungberg and the results are PCA performs better than LSA for dimentionally reduction .. Is it generalised or trial and error?,True
@soheilmohajerjasbi9900,2018-05-08T14:08:12Z,0,"Never mind my earlier question! I see that it has to do with the notation used in the lecture compared with available resources on the net. I still hold on to my ""Excellent lecture"" remark!",True
@soheilmohajerjasbi9900,2018-05-08T14:04:49Z,0,"Excellent lecture! However, is it possible that there is a typo on slide 8? Should term correlation be shown as X_trans * X, and document correlation be shown as X * X_trans? Please refer to earlier slides 6 and 7. Thanks!",True
@pariamolayemvand3343,2018-05-06T18:09:35Z,0,Hi. I can't open github anymore. it gives 404 error,True
@TomerBenDavid,2018-01-20T14:48:24Z,0,Extraordinar!  This is one of the best if not the best teacher I have ever met.,True
@krbkll,2017-12-16T21:57:07Z,4,"Dave,Nobody can ever explain it better than how you have done it here. The 12 sessions you have on Text Analytics should be made mandatory for any curriculum for Text Analytics. Same goes for your Titanic sessions. Thanks for your efforts in making this as flawless as one can imagine.",True
@dilshadkhanum6953,2017-10-11T12:22:44Z,1,Thank you soo much Dave,True
@nicholascanova4250,2017-08-22T20:26:44Z,1,"These videos are great, going to subscribe to your channel. Keep it up!",True
@cauliflower78,2017-08-17T23:09:18Z,1,"I am getting an error on this line:  rpart.cv.3 <- train(Label ~ ., data = train.tokens.tfidf.df, method = ""rpart"",                      trControl = cv.cntrl, tuneLength = 7)  #Error: protect(): protection stack overflow  I am running in an ec2 instance (m4*10Xlarge) with 40 vCPUs, 160 GB RAM. How do I overcome this? ( Stucked at this point :(  )",True
@jonimatix,2017-08-12T15:13:44Z,2,Great video with interesting concepts explained well!,True
@TheShekhar91,2017-08-06T07:42:47Z,1,"@ Dave, according to this video,  LSA reduces the dimensionality problem. But if we refer to a wikipedia page ( https://en.wikipedia.org/wiki/Singular_value_decomposition ) and follow the example given of a matrix M at the botton we can see that the matrices used in SVD have the following dimentions :   ( I am just using the dimensions here, please refer the link for the actual matrices)  M -  4 X 5 U -   4 X 4 Sigma - 4 X 5 and  V* -  5 X 5  Multiplying U and Sigma gives us a matrix of 4 X 5 dimensionality, and finally multiplying this result with V* gives us a 4 X 5 dimensionality matrix. Which is similar in size to the original matrix M.  how then is the dimensionality problem being handled by SVD?  Please help me understand this.",True
@kokweikhong5974,2017-08-02T11:53:41Z,1,"in 29:33, is the V that contains the eighvectors of the document correlations, XXt while the U is XtX?",True
@sumitdargan3288,2017-07-23T06:49:52Z,1,"Hey Dave,  Got an error in part 6. Have posted there the error in the comments section of part 6. Get back soon as soon as possible.  Thanks !",True
@suyashpandey830,2017-07-21T06:31:59Z,1,Wow..concepts so beautifully explained all through out the series. Waiting eagerly for the next video.,True
@murraystaff568,2017-07-18T20:35:16Z,1,So freaking cool! Can't wait for the next video. Please do a course in Australia!,True
