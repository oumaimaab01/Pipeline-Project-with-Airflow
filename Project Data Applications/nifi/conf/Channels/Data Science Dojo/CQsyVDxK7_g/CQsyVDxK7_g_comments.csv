author,updated_at,like_count,text,public
@mimisjimenez1617,2023-11-22T05:06:17Z,0,"Hi Dave! I'm following this course for a project I'm working on! It seems like the tokens( ) function has changed a bit. It no longer supports the argument ""remove_hyphens"" like you have shown in this video. Do you have suggestions that I could include in my code that would give me similar results?  Thanks!",True
@sergiochavezlazo5362,2023-03-12T10:10:12Z,0,"Thank u so much for ur video! I am still struggling everytime I tried to convert the data into dfm. The following error appear: Error in if (...length() && any(...names() == ""Dimnames"")) .Object@Dimnames <- fixupDN(.Object@Dimnames) :    missing value where TRUE/FALSE needed I did not find help regarding this problem but it seems that is very common in the comments bellow",True
@neguinerezaii3221,2022-05-18T12:29:11Z,0,An excellent series. Are there any tutorials on how to get the dump file of large texts: how to extract all e.g. wikipedia text files?,True
@sreejithkayes,2022-03-03T17:21:45Z,0,cannot load quanteda,True
@luisr.alcantara5119,2022-01-24T03:52:45Z,0,"Hello, I have a questions, Can I remove Spanish stop word using the same quantira package in R? Thanks in advance!",True
@YourIMGguru,2021-12-03T09:02:32Z,0,"Hi there! when I try converting my data in to matrix I get "" train.tokens.matrix= as.matrix(traintokens) >            view(train.tokens.matrix[1:20,1:100]) Error in as.data.frame.default(x) :    cannot coerce class ‘""tokens""’ to a data.frame In addition: Warning message: In if (drop_docid) x[[""docid_""]] <- droplevels(x[[""docid_""]]) :   the condition has length > 1 and only the first element will be used > dim(train.tokens.matrix)            [1] 3903    1"" this error. Can someone tell me what is wrong with this code?",True
@163ii,2021-05-01T18:49:37Z,0,I understood everything. Thanks for the good work.,True
@JOHNSMITH-ve3rq,2021-03-17T14:23:28Z,0,"Dave - man, please put that phone on silent during the talks! the constant vibrating is really annoying.",True
@heatherwells6245,2020-09-24T19:43:26Z,0,"Is it best practice to pre-process text for your training sample and test sample separately? Or, would it be acceptable to pre-process text for your entire data set first, and then split it into training and test samples?",True
@theodenking2420,2020-03-23T13:32:12Z,0,"at 13:00 when i use token it can't transform http://www.bubbletext.com to ""http"", ""www.bubbletext.com"", how can i fix it ?",True
@andreasmueller8023,2019-09-13T16:55:29Z,1,Can someone recommend good literature on this topic? Thx in advance.,True
@AlexanderAndradeDj,2019-04-13T15:51:33Z,0,Thanks for your job. You are great!,True
@vijaypalmanit,2019-01-26T07:14:46Z,1,can someone tell me at 16:00 why there are two 'your' even after converting the token into lowercase ?,True
@phunqdaphied,2018-08-10T12:29:23Z,0,"I would've expected remove_punct=TRUE to break the URL into four words.  Instead the www....com remained together. It's pretty cool that quanteda appears to differentiate between a period and a ""dot"" based on context.",True
@davidcurrie2528,2018-07-23T17:53:54Z,2,"At around 17:50 - I think stopwords were removed from ""quanteda"" (recently?) - I installed the ""tm"" package and ran it with quanteda and everything worked great.",True
@jean-mariemudry5830,2018-06-18T10:07:51Z,1,Really excellent and didactic to be recommended highly!tks,True
@kylenash4112,2018-05-21T15:09:37Z,0,"hi, can we use word doc instead of Excel sheet as a data file?",True
@aashwinsinghal4329,2018-02-22T09:11:16Z,0,"Hi Dave - first of all great tutorial.  I just had one doubt - at step where we find the dim' of the matrix - after following all the code as you wrote in your video - I am getting no' of col's as 5742 - can you think of any reason that this would happen?  Find below the code that I have used - I have written some extra comments for my personal use -    #installing all required packages install.packages(c(""quanteda"", ""ggplot2"", ""e1071"", ""caret"", ""irlba"", ""randomForest""))  # setting up wd setwd(""C:/Mydata/dellstudio/US_Stuff/wrk-study/Study/Data science study/R study/text analytics with data sceince dojo"")  # load up the .csv data and explore in RStudio. spam.raw <- read.csv(""spam.csv"", stringsAsFactors = FALSE) # View(spam.raw)  # Clean up the dataframe spam.raw <- spam.raw[,1:2] names(spam.raw) <- c(""Label"", ""Text"") # View(spam.raw)    # check data to see if there are missin values # Before starting any data analysis - we should know if our data is complete or it has any missing values that we # need to account for. length(which(!complete.cases(spam.raw))) # finding the no. of rows which are not complete  # Convert our class label into a factor spam.raw$Label <- as.factor(spam.raw$Label)   # next most important step is to explore the data # For classification problems - Find out if there is any skewness in the data.  # So, Let's take a look at distribution of the class labels (i.e., ham vs spam).  prop.table(table(spam.raw$Label))  # Next up, let's get a feel for the distribution of text lengths of the SMS # messages by adding a new fearure for the length of each message. # we are doing this as we can see in the data that most short messages are ham and  # on average most long messages are spam - so to test this hypothesis - we are engeneering a new feature.  spam.raw$TextLength <- nchar(spam.raw$Text) summary(spam.raw$TextLength)  # as can be seen from the results - there is presence of certain kind of skewness in the data # the min is 2 and max is 910.  # Now lets try and visulize the skewness using a histogram  library(ggplot2)  ggplot(spam.raw, aes(x = TextLength, fill = Label)) +   theme_bw() +   geom_histogram(binwidth = 5) +   labs(y = ""Text Count"", x = ""Length of Text"",        title = ""Distribution of Text Lengths with class Labels"")   # now as can be seen from the histogram - our hypo is right - the SMS' with less data # is normally ham and on average SMS' with more characters are spam. # also as it can be seen that till some length of the data almost all the sms' are ham and  # also those at the extreme end's are hams vs the data in the middle which is invariable spam # this could help us in future in engeneering new feature to help in prediction.  # Lecture 2 #----------------------------------------#-------------------------------#---------------------------#  #Currently we are splitting are data into two - training set and test set. # In a true project we would want to use a three way split of  # training, validation, and test.  # Also as we know our data has non-trivial class imbalance, we'll use # the mighty caret package to create a random train and test split # that ensures the correct ham/spam class label proportions # using caret to do a random stratefied split library(caret)  # using caret to create 70/30 stratified split  # also setting seed for reproducibility. set.seed(32984) indexes <- createDataPartition(spam.raw$Label, times = 1, p = 0.7 , list = FALSE) train <- spam.raw[indexes,] test <- spam.raw[-indexes,]  #verify proportions prop.table(table(train$Label)) prop.table(table(test$Label))   # Lecture 3 #----------------------------------------#-------------------------------#---------------------------#  # Basic Data exploration  # HTML -escaped ampersand character. train$Text[21]  # [1] ""I'm back &amp; we're packing the car now, I'll let you know if there's room""  # as can be seen above the '&amp;' is just '&' in actual message - but when we get the  # raw data - it gets converted to a different mix of symbols - therefore in text analytics  # when using the bag of words model - we have to deal with such instances as well - and according  # to situation we have to decide how to deal with it.  # same situation happens for train$Text[38] & URL example train$Text[357]   library(quanteda)  # Tokenize SMS text messages - Tokenisation is the process of breaking a document into individual words or tokens.  train.tokens <- tokens(train$Text, what = ""word"",                        remove_numbers = TRUE, remove_punct = TRUE,                        remove_symbols = TRUE, remove_hyphens = TRUE) # Take a look at a specific SMS message and see how it transforms train.tokens[[357]]  # Lower case the tokens. train.tokens <- tokens_tolower(train.tokens) train.tokens[[357]]  # use quanteda's built-in stopword list for english. # NOTE - one should always inspect stopword lists for aplicability  #        to your problem domain.  train.tokens <- tokens_select(train.tokens, stopwords(),                                selection = ""remove"") train.tokens[[357]]  #perfrom stemming on the tokens train.tokens <- tokens_wordstem(train.tokens, language = ""english"") train.tokens[[357]]  # create our first bag of words model. train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)  # transforming to a matrix and inspecting. train.tokens.matrix <- as.matrix(train.tokens.dfm) View(train.tokens.matrix[1:20,1:100]) dim(train.tokens.matrix)",True
@elicesroman7500,2018-01-27T19:19:33Z,0,I've got this error msg when installing this package. any idea on how to circumvent this Warning in install.packages :  package ‘quanteda’ is not available (for R version 3.3.2),True
@TomerBenDavid,2018-01-19T12:52:00Z,0,"So well clearly presented, amazing work.  just wish there was a version with spark+scala/java.",True
@Jawadislamian,2018-01-18T16:35:03Z,0,"train.tokens <- tokens ( ---) statement generated the following error. Error in UseMethod(""tokens"") :  no applicable method for 'tokens' applied to an object of class ""factor"", plz help me",True
@swapnaramesh6295,2017-12-11T23:20:19Z,0,"Hello, I have so thoroughly enjoyed this series. I am starting to run the scripts myself and I am getting this error  train.tokens.dfm <- dfm(train.tokens, toLower = FALSE)  Creating a dfm from a tokenizedTexts object ...    ... indexing documents: 3,901 documents    ... indexing features: 6,262 feature types Error in checkAtAssignment(""dfmSparse"", ""ngrams"", ""NULL"") :    assignment of an object of class “NULL” is not valid for @‘ngrams’ in an object of class “dfmSparse”; is(value, ""integer"") is not TRUE",True
@remamuditvaish9877,2017-11-29T11:01:30Z,0,"Hi Team.. please provide pdf for Video part 3 and 4 in github collection..after 2nd video's pdf,there is directly  pdf for for 5th video.",True
@amit898,2017-11-15T12:09:02Z,2,"This is excellent work! The pace of the videos is little on the slower side, but I completely appreciate that the author is trying to cater to aspirants with proficiency levels across the spectrum.",True
@DualFixMusic,2017-10-29T12:36:25Z,3,"Great video!  Do you have tips on dealing with very large datasets?  I have a dataset with 80,000 observations and 40,0000 tokens (3.2 billion elements).  When  I try to convert it to a dataframe (or matrix), I get ""Cholmod error 'problem too large' at file"" ...and the problem persists even when running additional doSNOW clusters.  Any help would be much appreciated!",True
@shubhamchauhan219,2017-10-16T18:10:30Z,1,"Not able to convert the dfm into matrix, as my data is little bit large. Any alternative please.",True
@kmocordoba,2017-09-29T01:41:58Z,2,"Hi, I would like to know why you use tokenization instead of working with the tm package and a Corpus",True
@AntonMudrak,2017-09-14T22:09:58Z,2,Thank you for the video!   I got a quesiton - is it possible to define your own stopwords list to work with other languages?,True
@nicholascanova4250,2017-08-19T00:31:21Z,1,What is this cryptic conversation at train$Text[38]...  A GRAM OF WHAT,True
@bpca2s029,2017-07-30T17:55:04Z,1,"Hi Dave!  First off, thank you very much for all the videos.  I however feel that there have been some changes to the Quanteda package because I am not able to use functions such as tokens(),tokens_tolower and tokens_select() to name a few. Do you know if this is the case.   For your reference, i am using R studio Version 1.0.153 and Quanteda_0.9.0 -1.  Thanks in advance!",True
@nickw22689,2017-07-24T17:14:55Z,1,"Dave, I can't get the quanteda package to work. I've installed it to the same location as the other packages, yet when I try to run it I get this:  > library(quanteda) Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :    there is no package called ‘ca’  Error: package or namespace load failed for ‘quanteda’  Any reason you can think of why it's failed? Thanks in advance!",True
@laurafosci,2017-07-16T15:48:36Z,1,This function tokens is the same than the str_split from the stringr library which creates lists of lists,True
@go_learners,2017-07-14T16:07:56Z,2,"Also when I am running the line code # 135 , it is giving train.tokens as large token in values but it is not giving me large dfmSparse, How can I fix it?",True
@go_learners,2017-07-14T15:56:50Z,1,"code in line 162: train.tokens.dfm<- dfm(train.tokens, tolower = FALSE)  # is not working it is giving me the following error: Error in validObject(.Object) :    invalid class “dfmSparse” object: superclass ""replValueSp"" not defined in the environment of the object's class Can you please help to find out why I am getting this error, Thanks",True
@pablomoreno8379,2017-07-08T20:35:42Z,1,Thanks for doing this great tutorial. I have learned more here that many other docs and tutorials just by going to the basics,True
@94fuckmylife,2017-06-24T19:37:59Z,1,When you used tokens function on the train set we get www.bubbletext.com as a single token. Shouldnt the period symbol(.) be removed since remove_punct = TRUE,True
@anughosh,2017-06-24T08:24:31Z,1,"I enjoyed the videos, well explained. Please upload some more TeXt analysis example. NB: In line 144,( train.tokens <- tokens_tolower(train$tokens)), the $ sign is missing.",True
@fernandobarrancor9882,2017-06-20T16:29:58Z,3,"Excellent videos, I really enjoy and appreciate your work!  What happens if I'm working on a non-english project, how can I load ""stopwords()"" from another language?",True
