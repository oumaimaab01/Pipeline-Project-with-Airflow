author,updated_at,like_count,text,public
@sonnyobrien,2023-04-28T08:51:09Z,0,Hi what is a function??,True
@capt_ndereya,2023-01-18T09:43:02Z,0,Website forbidden. how do i go about that,True
@artborovik,2022-11-05T19:16:10Z,0,10:00 I have zero result on py 3.9 w11 console cld anb hlp me? LVL UP,True
@drearystate,2022-11-02T05:53:27Z,0,This is a good tutorial,True
@michaeltillcock3864,2022-10-27T10:54:28Z,0,Very good video - do you have one that (i) goes through pagination (ii) shows you how to load  multiple different URLs to scrape - e.g. say you wanted to scrape all harry potter characters on Wikipedia; how do you get the script to load up each URL? (say i had a list of characters' wikipedia URLs in excel),True
@venkateshsubramani8889,2022-10-07T06:10:26Z,0,https://youtu.be/Lrl8JzS_BNs,True
@aishwaryamurali450,2022-09-20T02:35:31Z,0,"Is this still working  ? I tried to grab the h1 elements by executing page_soup.h1 , but it didnt seem to return anything !",True
@neokennyc,2022-09-16T14:18:18Z,0,Am I the only beginner who‚Äôs totally lost?,True
@billzhang7700,2022-07-28T19:54:12Z,0,"I ran the python code and got an error: IndexError: list index out of range.  It occurred when at:  brand = make_rating_sp[0].img[""title""].title().   How do I fix this?",True
@user-oe3wt9cz4z,2022-07-24T04:41:49Z,0,the Shift + Right Click doesn't work in Windows 11. sad,True
@foudiarchi7241,2022-06-07T00:15:29Z,1,"Really I m so match happy, thanks for you",True
@ryanfadden9585,2022-05-21T16:49:42Z,0,"Why do we create the ""containers"" variable? Couldn't we just use ""page_soup"" as the main area to search, since it contains all of the code?",True
@Dr.Cosmar,2022-05-19T02:24:17Z,0,"Bout halfway through the video, started losing steam...half of the video is you saying, ""Okay"" and repeating yourself about REALLY simplistic things.  Things you shouldn't be confused on after hearing it and seeing it once if you are trying to write code.",True
@jamiemorrissey2858,2022-05-10T23:27:29Z,0,Nice Video. great! Helped me a lot!!!,True
@arianaromero9552,2022-05-10T12:39:49Z,0,"Excelent, I have a question, I need the images, how can i get the images in format JPG?",True
@mohamedezawi6815,2022-05-03T23:15:27Z,0,how do you avoid being blocked by those websites since you're scraping quite often?,True
@Winterbear009,2022-04-12T11:01:50Z,1,I am from commerce background. I have zero knowledge of all the programming language. I found your video and explanation so good that at least now I can start my journey into scrapping and coding. I am so thankful at the moment. Love your channel. Thank you so much.,True
@Victor-dt1uq,2022-04-06T06:51:21Z,1,"6:28 - The goold old times where a mid-upper graphics card (GeForce GTX 1070) could be bought under 400$  :')  Great video, thx!",True
@Datasciencedojo,2022-03-17T19:21:39Z,5,"Hello everyone, find the updated version of this tutorial here: https://www.youtube.com/watch?v=rlR0f4zZKvc",True
@Datasciencedojo,2022-03-16T18:48:31Z,0,Watch the Livestream for Web Scraping with Python and BeautifulSoup by Arham Noman now on https://youtu.be/rlR0f4zZKvc,True
@idealsketch3778,2022-03-13T13:30:45Z,1,"Fantastic video dude, much more helpful than others I've seen on Youtube",True
@panic5306,2022-03-07T01:01:59Z,0,is there a windows 11 tutorial because whatever hes doing my computer aint having none of it,True
@adamhemeon734,2022-02-06T20:54:16Z,2,"Two years into a web program and a year working in the field and never bothered to learn how to do this. Great video, I followed along 5 years later in 2022 with Python 3.7.8 and it still works.",True
@MuhammadIsmail-jj1kk,2022-01-24T16:43:47Z,0,"Hi there, today I make this code but the code didn't scrape all products. Please help me",True
@victorpisarev7768,2022-01-16T15:56:18Z,1,Such a great video!,True
@brahimalali6693,2022-01-05T14:43:44Z,0,"Beautiful, clean, right speen and consise contenet , thank you very  much",True
@Mahdi_gh_sh,2022-01-04T20:35:02Z,0,"that was really fantastic, quick and usefull",True
@TechInsightswithAsma,2022-01-02T17:52:08Z,0,"Awesome video, it cleared my concepts of Bs4 beautifully...Thanks a lot for making easier for new learners like me.",True
@cheenle8815,2021-12-28T11:22:08Z,0,the best video. thanks!,True
@kamogelomekazi6769,2021-12-21T01:03:32Z,0,I'm struggling to create brand. I get an error working on the for loop. Please try to put code at the centre of screen rather than where the subtitles are... or remove subtitles,True
@fanico.digitalmng9133,2021-12-02T19:46:48Z,0,"I have question , for each command I created , there is an error ( not found) , how I can solve this?",True
@kianmesgouchi1379,2021-11-20T13:28:29Z,1,GREAT !,True
@larryhawkins3519,2021-11-18T03:43:55Z,0,"So, I see Superman is alive and wellüòä. GOOD JOB!",True
@idodo329,2021-11-17T21:01:26Z,1,"that is the best tutorial iv'e ever seen, keep it up, you are the best !!!!!!",True
@-raiwzo,2021-11-16T11:39:37Z,0,"Bro use an IDE, that's cringy little bit",True
@raoulzachary4855,2021-11-14T12:48:31Z,1,Thanks! Idid a nice chart!,True
@TheSibyjohn,2021-11-12T18:49:40Z,0,brilliant.... no wonder u guys lead in IT,True
@nileshvarpe1190,2021-10-30T18:27:04Z,0,thanks alot,True
@h1ghpower,2021-10-26T12:14:28Z,0,"dude, you are literally saving lives with this type of videos... I can't wait to digest all this precious info.....  You save people so much time with this!! you are magic!! <3 Thank you!!!!!!!!!!",True
@MrCaglar1993,2021-10-25T11:24:52Z,0,"When ƒ± watch this, all ƒ± am hearing is m'key from southpark. 2 words, m'kay... 2 words, a'right...",True
@johnehrenreich6543,2021-10-18T00:52:28Z,0,I‚Äôm using a Mac to scape data from a html page that I‚Äôm needing to bring into a Honeywell Tridium Jace. Can you show how to do that?,True
@mhau5176,2021-10-16T19:07:13Z,0,"Great vid, once I get to 6:52 after I've named the URL, I get an error stating 'urllib.error.HTTPError: HTTP Error 403: Forbidden. '  Any way to get around this?",True
@ericvenom,2021-10-16T03:29:24Z,1,"That is awesome. Thank, man it was really helpful",True
@nicksonmwangi5088,2021-10-11T10:00:12Z,1,Really good explanation on web scraping. Greate content,True
@abhijitzarekar9509,2021-10-04T04:22:59Z,0,Thanks for the video! You made our day by sharing knowledge! I learned with my friend and she too says Thank you!,True
@amor9125,2021-10-02T23:41:25Z,0,such an easy to follow tutorial THANK YOU SO MUCH God Bless!!!!,True
@funnystories3765,2021-10-02T09:51:48Z,1,Great Video <3,True
@hsoley,2021-09-28T12:59:39Z,0,Amazing! Love the content,True
@faustopf-.,2021-09-26T19:33:14Z,1,Could my IP get banned if I try this in other websites? Great tutorial,True
@baaaaaaaaaaaaaaaaaaaaaaam,2021-09-25T13:23:14Z,0,403 HTTP forbidden :(,True
@joulng3971,2021-09-22T11:47:39Z,0,"I went through this problem. Does anybody have any idea, thanks in advance!  Traceback (most recent call last):   File ""C:\Users\NQG\Downloads\webscrape\webscraping.py"", line 18, in <module>     brand = container.div.a.img[""title""][1:] TypeError: 'NoneType' object is not subscriptable",True
@niranjanparamkusam3743,2021-09-12T18:00:24Z,0,Simple and Clear,True
@potatofuryy,2021-09-08T13:56:28Z,2,My findAll functions keep returning empty brackets for some reason.,True
@aluuusch,2021-09-07T19:45:36Z,1,"I know, 1.3 million viewers do not lie, but for me the video would work better if the code would be visible throughout the video - as half screen view.",True
@aluuusch,2021-09-07T15:27:53Z,2,"Ok, I followed through and reached that magical moment when I'd put in the name of my script in the console, AND:  Error messages all over the place....",True
@TrillB3ast,2021-09-06T02:44:32Z,2,The solver mind is amazing ü§£,True
@julianarikmark1932,2021-09-01T15:08:24Z,0,"""w"" - Write - will overwrite any existing content  f = open(out_filename, ""w"")   and inside the loop we said  f.write(brand + "", "" + etc...)  wouldn't it overwrite the file every time an iteration is completed?",True
@nprstyle7724,2021-08-31T13:30:57Z,1,"dude was being chased by a cheetah, great content btw thanks a lot",True
@__-yk3zp,2021-08-24T01:04:42Z,0,thanks man this was very useful,True
@duysmith,2021-08-23T21:24:29Z,0,Brooo this is so sick thank you!!!,True
@rolandszirmai3922,2021-08-19T06:54:03Z,1,"Mate, this is just perfect! I learned so much by doing this with you. Now I'm ready to tackle other websites!!! You're a legend!",True
@mohammadrashedulhaque588,2021-08-16T07:49:38Z,0,"Can anyone help me out! I did not get the urllib package but got urllib3 and Request module on that. Now the problem is, there is no urlrequest function in tha module. What should I do. Thanks",True
@nsppreethika403,2021-08-13T10:34:40Z,0,sir any chance for instagram daily posts directly importing to our html site with any tool,True
@robertnichols2673,2021-08-12T02:55:00Z,1,"Everyone learns in a different way, and absorbs information through different methods!  This informal, laid back 'talk & walkthrough' (almost like sitting together with a mate) fits my style sooo much!! for me probably the best python lesson ever!! Will be looking for many more - thanks :P legendary !!",True
@cristobalandresparadahill4508,2021-08-10T21:10:46Z,1,"I think the syntax your using is the old version of BeautifulSoup, try instead something like find_all",True
@qiilakami,2021-08-10T05:40:31Z,0,uReq(my_url) does not work.,True
@hicham_6544-_-,2021-08-07T11:57:57Z,0,"I offer this service at competitive prices, feel free to message me ‚ò∫Ô∏è",True
@willymastermachoka9391,2021-08-07T05:09:50Z,0,"Good one , thanks",True
@ismailuwair187,2021-08-02T10:55:32Z,1,i liked the tutorial but you have kind of weird energy up and down. I would hope if u have a constant voice...,True
@beef2244,2021-07-28T23:11:05Z,0,Do you have a video that shows how to program a bot so I can scrape a webpage that's behind a login-esque wall.,True
@beef2244,2021-07-28T23:09:40Z,0,Could you show us something similar but with the data ending up in a google sheets file instead of an Excell file.,True
@flleeppyy9959,2021-07-27T00:44:29Z,0,why did github copilot make a comment linking to this video,True
@tochukwuejikeme7356,2021-07-17T03:16:25Z,1,Thanks for this,True
@redfeather22sa,2021-07-14T15:36:23Z,0,"it must have been a magic day when I saw this for the first time 1.5 years ago !!! its where i all started !!! Thanks! Best Video & Intro into webscraping for absolute begginers !! Thanks (notable mentions to Corey Shafer who I was watching a a few weeks earlier, who gave me the taste of it & how easy it could be to use/do). Thank you friends!! An amazing tool!!",True
@aaammm1888,2021-07-08T17:16:45Z,0,Thanks pal Just what i need can i do the same and save the data on a database,True
@akinyemisodiq6885,2021-07-07T14:36:15Z,1,awesome,True
@prajwalshrestha3500,2021-07-02T09:54:52Z,1,is this applicable in 2021? Because I learned how to do it with the request library not the urllib,True
@stratuxz,2021-06-21T18:43:45Z,0,This was so helpful,True
@ayeshaqamar111,2021-06-21T14:02:59Z,0,csv file is not showing contents yyyyyyyyyyy?,True
@ayeshaqamar111,2021-06-21T08:27:50Z,1,"containers = page_soup.find_all(""div"", {""Class"" : ""item-container""}) this is showing 0 item in this can some one help me out not getting any element",True
@SuperSb7,2021-06-07T11:37:26Z,0,Thanks a lot! This video was extremely helpful and straightforward.,True
@mandyronald8187,2021-06-06T11:33:26Z,0,indeed this is real soup,True
@danh4472,2021-05-28T16:17:20Z,1,"My program is not looping, any suggestions?",True
@bharaths7816,2021-05-28T13:11:13Z,1,So I have question whenever the sites update it's info will the code work and show us new results?,True
@lurkagurka,2021-05-28T00:08:52Z,0,uClient = uReq(my_url) gives: urllib.error.HTTPError: HTTP Error 403: Forbidden,True
@forstig,2021-05-26T14:03:53Z,0,"I was searching for a selenium alternative, this is what i was expecting to find. A webscraper which runs in background. Thanks for the tutorial really usefull!",True
@jackbird5839,2021-05-26T12:13:01Z,2,"Awesome tutorial thank you. but for a non-tech user it is quiet hard to do a workable scraper for my WooCommerce store. As a side solution i am using eCommerce scraper via ""ESCRAPER"" maybe it helps somebody too. But I am not giving up))) Thank you for your input!!!",True
@RS-Amsterdam,2021-05-24T10:55:56Z,0,Top Video Thanks for sharing,True
@dbenji3131,2021-05-16T22:47:37Z,1,"Thanks a lot for this video, I have been searching on internet for web scraping programs on python but I couldn't find one that worked ! Your video is perfect man !! I just have a question on a detail in the csv file: how do i do to sort the elements in different columns like yours because it's putting everything in one column. Thanks for you response, and continue making such useful videos !",True
@tanjakriko5190,2021-05-16T13:02:27Z,1,"I got an error in coding, IndexError: list index out of range I wonder why? can someone help",True
@user-rk8ew8fr3u,2021-05-14T18:45:13Z,0,Amazing tutorial! Thank you!,True
@jlambert12013,2021-05-11T07:36:35Z,0,and if it does error...,True
@sacroultima,2021-05-10T02:18:52Z,0,You are sooooo comfortable to listen to. Not because you have a perfect pronanciation and a seamless script you are gliding through. You are just talking but not constantly jumping back and forwards. Accurate tempo and personality in your voice.  New subscripion,True
@adrianreichert8215,2021-05-08T18:43:16Z,0,After Uclient its not downloading the site pls help:(,True
@adrianreichert8215,2021-05-08T18:39:14Z,0,the console doesnt copy the code into my sublime:( PLS HELP,True
@another_hindu,2021-05-08T16:17:27Z,0,Rely getting confused by the first terminal then sublime approach.,True
@curaviolence2277,2021-05-08T00:51:21Z,1,big thanks i can recomment this vid 10/10! youre a g,True
@panhwarsameer551,2021-05-06T21:48:47Z,0,Please make more data scraping tutorials using python,True
@thetravellingdream3480,2021-04-22T08:02:18Z,0,"Hello  Excellent video I have been following all the steps you have done but when I got to open a second command line to run the python file it gave me this error (python my_first_webscrape.py   File ""my_first_webscrape.py"", line 1     from urllib.request import urlopen as ureq     ^ IndentationError: unexpected indent)",True
@joaobarbosa5724,2021-04-16T14:37:58Z,1,"the black and green cmd with a massive font makes stuff harder to read and follow.  you probably want to make it easier to follow within a video, but it does the opposite",True
@oleksandrarsentiev7152,2021-04-06T11:15:36Z,0,"Amazing tutorial, however, I'm having one issue, when I save the csv file it looks good in excel but when I try to create a graph using matplotlib and pandas, it will give me a blank graph. I'm assuming this is data type issue as when I manually change the data to numeric values in excel it does work. Would you suggest any solution for this?",True
@johannes6009,2021-03-30T08:07:03Z,0,my urlopen('url.example') runs indefinitely and thus never executes uClient.read() - how can I solve this?,True
@kalidsherefuddin,2021-03-25T07:44:54Z,1,Please help me,True
@williamhendro9177,2021-03-20T11:29:16Z,0,"super cool, why not make more in depth ?",True
@magicalhats,2021-03-20T02:07:26Z,0,"Just heard of web scraping today, and this video made total sense on how it works. ty!",True
@gianmarcosalvi339,2021-03-18T20:12:08Z,1,"Pretty helpful, thanks alot dude",True
@blablawtf,2021-03-17T04:51:45Z,0,"I learned SOOOO MUCH from this video, you are the BEST!!!!!!! Your explanation was super simple and just spot on, thank you SO MUCH!!!!!!!!",True
@FAPfunchannel,2021-03-16T17:23:05Z,0,"keep going mr Phuc, l√†m video d·∫°y m√¨nh c√°ch l·∫•y coupons and deal c·ªßa c√°c site lazada, shopee, tiki ƒëi anh, tks a",True
@FAPfunchannel,2021-03-16T17:16:10Z,0,"glad to know you are VietNamese, tieng anh gioi lam bro :)",True
@Navor277,2021-03-10T09:00:03Z,1,make an updated version,True
@laszlomattern,2021-03-09T16:28:50Z,1,"How can I use the second item of an HTML typer after I defined the container (20:00). On the site I am scraping on there are three H4, I need the second one but also need to get one layer deeper afterwards.... Any ideas?",True
@May16Joe,2021-03-08T06:24:34Z,1,"I recommend checking Corey Schafer's web scraping tutorial first and then come back to this project, you'll finish it quicker.",True
@orgotmata9370,2021-03-04T03:26:17Z,0,What about bypassing Captchas on websites?,True
@sravanag8122,2021-03-02T13:44:01Z,0,"Its awesome  tutorial bro. scrapping is a difficult task for me untill i watch this video .thanks a lot.and i request to do a video on pipelines in data science if possible,",True
@tomadamclarke4509,2021-03-01T19:01:21Z,0,I got lost at downloading the program,True
@ardytanega,2021-02-26T15:58:14Z,0,"What can i do if i got hit with this error , urllib.error.HTTPError: HTTP Error 403: Forbidden?",True
@thankgodudeh4082,2021-02-22T11:48:22Z,0,"I am trying to scrape a website with but the ""url"" i copied from the site is not returning any item, what could be the cause. (' https://www.konga.com/category/macbooks-5249 ') That's the web_url.. Thanks",True
@dariahiguchi5890,2021-02-18T10:22:32Z,0,"Thanks for a great video!  It looks like not every website allows scrapping, how do I know if it does? Check robot.txt file? Not sure what to look for",True
@neelusingh2467,2021-02-18T09:25:05Z,0,you are so fast,True
@ygriksunknow,2021-02-14T19:38:05Z,0,"I would give you trillion likes, i was making huge data scraping and one part of code wasnt working in many req. and you de god of scraping make my week.",True
@kebravany6952,2021-02-13T21:09:25Z,0,AMAZING!!!!,True
@yaboymoey9964,2021-02-11T22:16:06Z,0,"""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 649, in http_error_default  raise HTTPError(req.full_url, code, msg, hdrs, fp).  urllib.error.HTTPError: HTTP Error 403: Forbiddenurllib.error.HTTPError: HTTP Error 403: Forbidden what the hell does this mean?",True
@arvenebinny,2021-02-10T14:33:53Z,0,Beautiful man.. loved it.,True
@kwasiappiah-kubi7686,2021-02-08T18:04:23Z,0,Does anyone know how to download beautiful soup or python requests,True
@afhamfardeen2448,2021-02-05T14:36:46Z,1,"containers=page_soup.findAll(""div"",{""class"": ""item-info""})   print(len(containers))  these two lines are not giving back the list of item, print statement is giving 0 instead of 12. why is that..???",True
@carlombo4192,2021-02-04T23:21:25Z,0,"I can't thank you enough for this tutorial. Im currently working in course project of creating a database extracting data from the web, and this tutorial was a big insight for me.",True
@Lado93,2021-02-04T19:47:05Z,2,What if I want to scrape multiple pages of graphics card? How do I loop through all pages?,True
@alexanthony5642,2021-02-01T06:25:18Z,0,"Thank you so much! This was so easy to follow, you've earned a subscriber!",True
@chakrabmonoj,2021-01-31T14:43:43Z,0,"Hi...thanks for the excellent tutorial. Really appreciate the way you have taken time to break down and explain each line of code. I am trying to scrape Linkedin, in order to sort my connections by the number of likes/comments their posts garner. This is to ensure that I am latching on to the right posts. Is this something possible to do with Python - if yes, could you help me with contouring the code or direct me to a source that can? Thanks",True
@valentinow3137,2021-01-25T20:26:36Z,1,"When I open excel it puts all information in one row, how do I solve this?",True
@domnicsatish,2021-01-16T08:37:35Z,0,Was helpful. thank you!,True
@kellywilbert4585,2021-01-15T00:42:33Z,0,Is my is my mother goose club out.,True
@ryanjim4366,2021-01-14T16:17:40Z,0,Very nice‚Ä¶.,True
@lloydcraig8263,2021-01-14T13:52:34Z,0,if you want God to forgive you dont commet here just go to the church and pray.,True
@Minecraft-hb1su,2021-01-14T05:53:57Z,0,"this was so much easier than i thought, thank u",True
@MeMe-ux1jm,2021-01-14T01:16:07Z,0,Can somebody tell me how we do to apply this to multiple pages ??   It finishes with:  ?page=2  And I want to do it until page 50,True
@graymatter7125,2021-01-12T11:05:52Z,0,"There's no span tag after body on that site anymore. How did y'all ""iT's ThE bEsT tuToRiAl eVer"" repeat after him?",True
@SomeGuy-ne3yl,2021-01-04T16:04:55Z,0,"do not use anaconda it literally rendered my python installation unusable, even after several deinstalls/reinstalls. everyone that i know that used anaconda, ran into problems. fuckin filth-ware.",True
@aakankshachoudhary8532,2021-01-04T02:29:07Z,0,Does anyone else get this error: `urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate` while running the `uClient = uReq(my_url)` command?,True
@juanelprogramador,2021-01-04T01:32:14Z,0,Best tutorial ever!,True
@camerond9290,2021-01-01T20:46:27Z,0,"I learned a lot in this video, however, your erratic back and forth, short-handing stuff, and glossing over things quickly made it very hard to follow. I can't tell you how many times I had to pause the video and back up a few seconds to see what you did.   For example, in the first section on sublime text, you state you type in ""set syntax python"" and before you finish typing it out, you backspace and start writing something different and hit enter before you finish typing it. You were just short-handing the statement to ""Syntax Pyt"" but I had to back up and read what it was saying because your actions were different than what you were actually saying you were doing.   When you were copying and pasting things and quickly going back and forth from the console to sublime text and it was confusing to follow you that quickly and make sure I had everything written correctly.  Otherwise, keep up the good work. Thanks!",True
@elliottttt,2021-01-01T20:30:53Z,0,"Hello, with my project that I am working on, there are only <div> tags, so it makes navigating the tags difficult. The data that I want is located between two <span> tags, and I am unsure of how to collect that data. Once again, after I write ```container.div.div``` it cannot go any farther into the HTML code, despite there being more <div> tags embedded in it. any help or resources would be greatly appreciated.",True
@GiggleNuggetsZone,2021-01-01T08:54:52Z,0,"Anaconda: I am a pretty big file, 500MB.  My Offline Spotify: 27 GB",True
@nhuymien4171,2021-01-01T02:29:11Z,0,Omg.,True
@sanhanh1056,2020-12-31T06:29:49Z,0,Ya Allah ampunilah dosa kami semua yang setuju like.,True
@autumnq3641,2020-12-30T18:07:41Z,0,"Why do i keep getting ""  File ""<stdin>"", line 1, in <module> TypeError: 'NoneType' object is not callable""  for  page_soup.findALL(""div"",{""class"":""item-container""})?",True
@lamtrien9575,2020-12-30T02:28:48Z,0,Pongan m√°s videos de porno es lo que m√°s estusiasma.,True
@divyeshmalhotra7649,2020-12-28T09:00:22Z,0,who's here trying to buy a RTX 3000 card,True
@TheAssrocket,2020-12-18T14:15:05Z,0,"""No module named 'bs4 '"" what should i do?",True
@malialik,2020-12-17T17:01:48Z,0,"Hi, I'm trying to scrape something that requires account access. How can I do something like login to the site before scraping?",True
@edenhoward2053,2020-12-17T12:29:17Z,22,UPDATE/SUGGESTION  The findALL function has been renamed to the find_all function in Bs4 version 4.9.3,True
@oke3166,2020-12-16T21:49:28Z,0,17:45,True
@sebastianbitsch,2020-12-16T12:23:04Z,0,"For people in some countries the standard delimiter in CSV-files is a semicolon, so the entries will have to be separated by a semicolon if you dont want to have to change your system settings. just a FYI some of you might find useful",True
@auzanwidhatama9421,2020-12-15T19:16:03Z,0,"On 26:29,  why those return and new line shows up, but when I tried it myself it didn't?  There's no \r and \n in the html but it shows up in his command line. I'm a beginner so I might missed something",True
@Ebusky,2020-12-15T12:57:52Z,0,"Hey is it possible to scrape more than one url in a single script, or do i need to make new scripts for each url",True
@patrickbateman7665,2020-12-15T04:05:42Z,0,URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)>   I can not open the URL üòûüòî,True
@quinnriver4783,2020-12-14T23:53:12Z,0,This was so fun!!!! Dzamn,True
@RegiiPad,2020-12-14T08:17:24Z,1,Great!,True
@antman7673,2020-12-14T00:42:13Z,0,"I would really like to get into a bit of data collection to do some buying and selling of tcg cards. See how often a card is mentioned in videos and how it relates to card reveals and how prices change. Combine it into an algorithm added with some small manual inputs for insights beyond data, like how big its potential is due to an effect relation to another card.  Could make me some good money. The market is definitely easy to predict, it is only a matter of having enough data to do predictions.  The only reasons I didn‚Äôt want to engage in such attempts is, that this would increase card prices for everyone and drain money from the community, whilst also having a carbon footprint for shipping/reshipping of cards. So morally I don‚Äôt like the idea too much, but it is an exciting idea.",True
@Papiii_benz,2020-12-12T20:41:47Z,0,What if it's on a different page? Do I have to run it again,True
@andreauda925,2020-12-12T17:23:02Z,1,"WOW, thanks man, wonderful",True
@bilalmajeed40,2020-12-11T07:49:42Z,0,"We provide high quality data scraping service, If the data exists anywhere we can get it for you! All we need from you is the data source, fields to extract, and desired output format. We can deliver extensive data outputs in a short time frame. Contact us at: https://aitomation.com/data-web-scraping/",True
@rycaz5565,2020-12-10T11:42:20Z,2,why do I always get an empty list when doing the findAll function? Even though im trying it on the same Website? I just don't get it,True
@mianali5664,2020-12-10T10:35:35Z,0,"We are providing services Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. Usually, such software programs simulate human exploration of the World Wide Web by either implementing low-level Hypertext Transfer Protocol (HTTP), or embedding a fully-fledged web browser, such as Internet Explorer, Google Chrome and Mozilla Firefox   Contact Us https://aitomation.com/data-web-scraping/",True
@thiccchungus588,2020-12-03T05:02:32Z,0,users/phuc/downloads.... weird ass kink bruh,True
@megantrotman2974,2020-12-01T02:32:28Z,0,Really nice tutorial!,True
@hutzimbl,2020-11-28T15:27:43Z,0,"Suggestion: From 28.11.2020: When I tried to replicate the code, I noticed that the website now contains ""ads""- which are basically graphic cards sold by other sites. The problem occurs when setting the brand name which in the video is:       brand = contain.div.div.a.img[""title""]  For the ads, divs are ordered differently. My workaround takes the full title of the graphic card (Which is the same for ads and regular entries) and extracts only the first Word, which fortunatly is always the brand name:  brand = container.find(""img"")[""alt""].split()[0]  Cheers, and thanks a lot for the great guide!",True
@ahmedshahriarsakib5728,2020-11-28T11:29:26Z,0,"Open growing community on web scraping   Join here, guys -  Web Scraping & Automation (Python/Javascript)  https://www.facebook.com/groups/348468202946051",True
@spring802007,2020-11-28T06:42:43Z,0,Its good but too complicated !!,True
@godac420,2020-11-27T22:30:35Z,1,"Awesome!  this was pretty valid in 2020,     I had to change findALL()  to find_all(""div"", class_=""item-container"")",True
@brendensong8000,2020-11-26T05:30:18Z,0,"As of Nov 2020, I went through the whole thing without any issue!  I used a different product name, but everything worked so well!   Everything worked so perfectly! I learned so much from this video!  this is awesome!!!!  Thank you!!!!",True
@megasilviana872,2020-11-25T10:40:09Z,0,Wow thank you very much,True
@nelsonramos8704,2020-11-25T03:23:00Z,1,"best video ever! but i got an error in the loop with the name of the brand, the program gave me this error: brand = container.div.div.a.img[""title""] TypeError: 'NoneType' object is not subscriptable The error appears only in the third loop, it gave me the first two brands but it stoped at the third with that message. But I fixed  it by putting a Try / except into the brand name and it run well but it skiped a few brands like ''cobra computers'' and ''Corn electronics'', so I dont understand why it happens. If someone knows, let me know pls.",True
@LuanNguyen-pl2wf,2020-11-23T06:30:37Z,0,"very useful, thanks so much !",True
@lucaskelbe,2020-11-22T03:16:08Z,0,I enjoyed this video A lot I also Liked how much you explained it,True
@annatinaschnegg5936,2020-11-17T11:02:16Z,0,"I really liked the tone, rythm and clarity of this tutorial! I‚Äòm not a total beginner with python anymore and so was able to listen and (mostly?) understand while preparing lunch for my kids. (I‚Äòll rewatch to try and do it later)",True
@nandervilarcastellar5772,2020-11-17T07:30:06Z,0,Awesome tutorial man! exactly what I was looking for,True
@kamaleshpramanik7645,2020-11-10T19:49:50Z,0,"Yes Sir, learned a lot. Thank you very very much.",True
@Ismahan-jawhar_223,2020-11-10T13:47:15Z,0,great,True
@VaibhavShewale,2020-11-09T13:44:16Z,1,so we can't scrape image from the website and save it in excel?,True
@AchmadBadra,2020-11-09T08:21:32Z,0,"I just use lxml, because more faster and stable.",True
@NicksStuff,2020-11-08T19:53:27Z,0,"""See price in cart"" <- can you add it to the cart and get the price there? In Python, I mean. Better: can you add 999 to the cart and read the error message to know how many they have in stock?",True
@danieldimitrov7067,2020-11-05T08:16:46Z,0,"The best tutorial on web scrapping, I've ever seen! Great work!",True
@lexborodai1639,2020-11-04T08:03:31Z,0,Here he does it twice in a row  https://youtu.be/XQgXKtPSzUI?t=1620,True
@santisantillan8088,2020-11-03T07:24:49Z,0,"I have this piece of code to get the price of each grapics card, may i know how i can parse this to get the price value (e.g. ‚Ç±259.99)  >>> price_container = container.findAll(""li"", {""class"":""price-current""}) >>> price_container [<li class=""price-current""><span class=""price-current-label""></span>‚Ç±<strong>259</strong><sup>.99</sup>¬†<a class=""price-current-num"" href=""https://www.newegg.com/global/ph-en/msi-radeon-rx-5600-xt-rx-5600-xt-gaming-mx/p/N82E16814137571?Item=N82E16814137571&amp;Description=graphics+card&amp;buyingoptions=New"">(4 Offers)</a><span class=""price-current-range""><abbr title=""to"">‚Äì</abbr></span></li>]",True
@AbuAhmedAlsudani,2020-11-02T21:07:44Z,0,"It is amazing and helpful, it exactly what I looking for. but I have a question: Is that possible to requests multiple webpages at once and read those links for that webpage from a txt file? Thanks in Advance üíïüíï",True
@AMC10000,2020-11-02T20:08:08Z,0,"Excellent , thank you.   Content, instruction  and enthusiasm terrific",True
@alienaccount2624,2020-11-02T18:07:13Z,0,"very helpful but I was wondering how you could clear words after a certain phrase. For example in the video you used a method to wipe out the spaces before and after, what if I wanted to wipe out a whole phrase and only took a selected keyword?",True
@Sadboy80629,2020-11-02T07:14:49Z,0,Wish you would make more content like this . There are other youtubers like codeforfreeacademy and the big ones but would love to find more perspectives from other average people however i very much enjoy this content and got to learn a few things I did not know.,True
@anonymosranger4759,2020-11-01T13:29:24Z,0,Since when do anacondas like pythons?,True
@Axeltreman,2020-10-31T05:20:05Z,0,"Amazing, thanks for sharing!",True
@Desleiden,2020-10-29T17:07:13Z,0,I enjoyed this too much. Learned something now today! thank you so much.,True
@deepamgupta8011,2020-10-28T22:55:17Z,0,I like the way of your teaching.,True
@xitcode,2020-10-28T15:40:08Z,0,Really enjoyed the video. Very helpful.,True
@RossittoS,2020-10-27T10:35:38Z,0,Excelent content! Thanks!,True
@sushilgurjar4010,2020-10-27T06:15:14Z,0,I get an error when I am typing len(containers) it is showing 0 instead of 12 .,True
@dipeto9110,2020-10-26T07:44:03Z,0,Which key combination did you use at 27:45?,True
@dylanthomas9366,2020-10-25T19:56:17Z,0,"thank you my brother. Very simple tutorial! As others mentioned.. use find instead of finall and then there is no need for [0] in the following line. best,",True
@TheJacklwilliams,2020-10-25T19:36:42Z,1,"Killing me. I can‚Äôt tell you how many vid‚Äôs I‚Äôve watched in the last 24 hours on Raspberry Pi, Linux, etc... where the narrator / tuber, slowly covered things like ‚ÄúThis is the box‚Äù, ‚ÄúThis is me opening the box‚Äù while I was screaming at the monitor, ‚ÄúSeriously dude, seriously?‚Äù... Then I choose a somewhat more complex topic, significantly, and I get ‚ÄúLarry The Coffee Head‚Äù speaking as fast as my daughter when she flips out, scrolling like a madman, flipping back and forth between command line and shell and web page, with hardly a breath in between the elements he‚Äôs describing. Dude, so far? Great stuff but MY GOD PLEASE SLOW THE HELL DOWN!  Thanks for the effort, I‚Äôm way ahead of where I was before I pressed play, and I‚Äôm glad you aren‚Äôt monotone going through insane details like, type this line, then hit enter. But there is a common ground in between. Please seek it. Thank you.",True
@firstnotefoundation,2020-10-25T19:06:31Z,0,"GETTING THIS ERROR:   Traceback (most recent call last):   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 1342, in do_open     h.request(req.get_method(), req.selector, req.data, headers,   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py"", line 1255, in request     self._send_request(method, url, body, headers, encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py"", line 1301, in _send_request     self.endheaders(body, encode_chunked=encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py"", line 1250, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py"", line 1010, in _send_output     self.send(msg)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py"", line 950, in send     self.connect()   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py"", line 1424, in connect     self.sock = self._context.wrap_socket(self.sock,   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py"", line 500, in wrap_socket     return self.sslsocket_class._create(   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py"", line 1040, in _create     self.do_handshake()   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py"", line 1309, in do_handshake     self._sslobj.do_handshake() ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1122)  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""/Users/emiliopereira/Documents/Python Projects/ross-market-bs.py"", line 5, in <module>     uClient = uReq(my_url)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 214, in urlopen     return opener.open(url, data, timeout)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 523, in open     response = meth(req, response)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 632, in http_response     response = self.parent.error(   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 555, in error     result = self._call_chain(*args)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 494, in _call_chain     result = func(*args)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 747, in http_error_302     return self.parent.open(new, timeout=req.timeout)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 517, in open     response = self._open(req, data)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 534, in _open     result = self._call_chain(self.handle_open, protocol, protocol +   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 494, in _call_chain     result = func(*args)   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 1385, in https_open     return self.do_open(http.client.HTTPSConnection, req,   File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py"", line 1345, in do_open     raise URLError(err) urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1122)>",True
@kamaleshpramanik7645,2020-10-25T00:05:05Z,0,This is just awesome video. Thank you very much.,True
@joshblannin6760,2020-10-22T12:53:14Z,0,Really great video! Only just starting with python and this was very helpful! Thanks üëå,True
@Yerp_To_Da_Skerp,2020-10-22T02:25:25Z,0,Ok so I'm following along but when I get to the calling the containers[0] to get the html when i run it in both my terminal or print it in my editor the html just comes out as one long line. Its not formatted at all,True
@iridium3342,2020-10-21T16:51:05Z,0,Thanks for the tutorial!,True
@HolyRamanRajya,2020-10-15T10:45:36Z,0,Its better to monitor xhr response than webpages. Many websites will render information and containers later. If the websites use JSON internally then it becomes far easier to parse that instead of xpathing.,True
@maxim25o2,2020-10-14T11:32:06Z,0,"Oh, why in c++ is not so easy :(",True
@estebiajero1670,2020-10-13T12:19:06Z,0,"didnt work even after copy pasting the code. first the html code wont parse and it onlz returns a big one string line of html and then when i try to write to the csv file it wont give me permission to write to csv. then, i changed to ""with open"" and tells me that it cannot be done as im trzing to add non descriptible objects",True
@estebiajero1670,2020-10-13T11:00:46Z,0,hey what do i do if the html code im obtaining is in one line string?,True
@TheMarcosutra,2020-10-12T15:12:34Z,0,for the love of god stop smacking your lips between every sentence. it makes this unwatachable,True
@ShirinEsm,2020-10-12T13:16:16Z,0,"i just can say ""thank you sooooooooo much""",True
@samyip3868,2020-10-10T06:03:57Z,0,That's really wonderful. Thank you very much for this video. I'll try it.,True
@zainahmed5231,2020-10-09T22:52:22Z,0,"why am I getting this error?  brand = cop.div.div.a.img[""title""] TypeError: 'NoneType' object is not subscriptable",True
@hamidoumaiga5733,2020-10-08T04:46:49Z,0,It would of been better if you took your time to explain. It‚Äôs so rushed!,True
@amruthasunilkumar6572,2020-10-07T18:31:55Z,0,Pls do web scraping on steam community reviews,True
@tikaoctridieni8029,2020-10-07T15:11:11Z,0,can i get code for scraping in multiple page? thanks in advance,True
@KeMmatey8,2020-10-06T14:47:57Z,0,"Could someone help me - I get 0 when finding len(members) for my page. The relevant line on the inspected page is¬† <tr role=""row"" class=""odd""....</tr>¬† or <div class=""member row"" data-member-id=""1"">",True
@caderzik,2020-10-06T14:13:30Z,0,why do you programm on your terminal?,True
@satriawira4358,2020-10-06T13:44:48Z,0,"Hi. I got an error message like this. Can anyone help me?  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""/opt/anaconda3/lib/python3.7/urllib/request.py"", line 222, in urlopen     return opener.open(url, data, timeout)   File ""/opt/anaconda3/lib/python3.7/urllib/request.py"", line 525, in open     response = self._open(req, data)   File ""/opt/anaconda3/lib/python3.7/urllib/request.py"", line 543, in _open     '_open', req)   File ""/opt/anaconda3/lib/python3.7/urllib/request.py"", line 503, in _call_chain     result = func(*args)   File ""/opt/anaconda3/lib/python3.7/urllib/request.py"", line 1362, in https_open     context=self._context, check_hostname=self._check_hostname)   File ""/opt/anaconda3/lib/python3.7/urllib/request.py"", line 1322, in do_open     r = h.getresponse()   File ""/opt/anaconda3/lib/python3.7/http/client.py"", line 1344, in getresponse     response.begin()   File ""/opt/anaconda3/lib/python3.7/http/client.py"", line 306, in begin     version, status, reason = self._read_status()   File ""/opt/anaconda3/lib/python3.7/http/client.py"", line 267, in _read_status     line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")   File ""/opt/anaconda3/lib/python3.7/socket.py"", line 589, in readinto     return self._sock.recv_into(b)   File ""/opt/anaconda3/lib/python3.7/ssl.py"", line 1071, in recv_into     return self.read(nbytes, buffer)   File ""/opt/anaconda3/lib/python3.7/ssl.py"", line 929, in read     return self._sslobj.read(len, buffer) TimeoutError: [Errno 60] Operation timed out >>>",True
@raj9101990,2020-10-05T01:28:22Z,0,"I usually don't comment but this video is really awesome and I really learned a lot. I also have a question: Can't I use ""REGEX"" to get to the details of the product",True
@susenka98,2020-10-04T17:48:18Z,0,"Thank you so much, this really helped me!",True
@anikyt7570,2020-10-03T19:14:04Z,0,awesome awesome video ...,True
@vidakhalili5552,2020-10-03T16:47:12Z,0,"Great, that was so helpful",True
@BikeshBudhathoki,2020-10-03T02:22:35Z,0,"loved it, though i m very beginner in data science and have zero knowledge in it, i watched the entire video and tried to grab everything possible discussed here",True
@preetik.4819,2020-09-30T15:54:45Z,0,thank you  so much for this video .. Actually i am assing. to submit .. and this is a huge help for me . I am a noob .. but this will help me a lot.. \,True
@TioUngaro,2020-09-29T16:40:20Z,0,"First tutorial, worked with web scraping and uses classes. # Big Thanks!  for UK users: shipping caused error for me, so used the price tag instead price_container = container.findAll(""li"", {""class"":""price-current""}) price = price_container[0].text[0:7] # [0:7] to get the range of it",True
@UnpluggedPerformance,2020-09-27T07:28:04Z,0,youre awesome!!!,True
@debghoshila,2020-09-26T13:09:42Z,0,Drinking Game : Take a shot everytime he says M'kay.   PS: Thanks for the great video <3,True
@angelgabrielortiz-rodrigue2937,2020-09-26T00:51:18Z,0,"Great video!  I  am having errors when I look for the brand. I even used the code avalaible in the description and still gave me the same error. Further analysis of the container, I saw that the ""class = item-branding""<a href>  division (the one that contains the img) was not included when I the page_soup.find_all(""div"", {""class': 'item-containers'} . I notice by printing container in the for loop. The error that it gives me is:  ""ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"" % key AttributeError: ResultSet object has no attribute 'img'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?   Need help :) Thanks",True
@9830762221,2020-09-25T19:37:25Z,0,how you check for duplicate records in your output. like I want to put only unique records in my csv ?,True
@JulianFerguson,2020-09-25T18:58:48Z,0,after i complete the following code: page_html = uClient.read()  i get the following error: ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host.  anyone know how to debug this?,True
@maheshk4405,2020-09-24T00:46:57Z,0,https://www.softenant.com/DATA-SCIENCE/,True
@rickneibauer1,2020-09-23T22:12:49Z,0,Thank you for this!  It's cool and important (to me) to see what one would actually do with python.,True
@ozkankurt449,2020-09-23T11:28:50Z,0,"Data Science Dojo √áok Ho≈ü Duruyor, Hemen Kendim Denemeliyim...",True
@yilu435,2020-09-22T15:42:27Z,0,u will get 503,True
@rohmatulhasanah9371,2020-09-19T02:53:24Z,0,Thanks for this üò≠üò≠,True
@vickyssworld,2020-09-18T12:36:49Z,0,I think we can also use the prettify() function instead of going to jsbeautiful.,True
@trungthanh4134,2020-09-16T09:04:17Z,0,Are u a vietnamese guy ?,True
@chaitanyareddy8037,2020-09-15T21:20:40Z,0,Very well explained. Thank you üòÄüíØüôå,True
@purpleturtledotcom,2020-09-15T12:40:08Z,0,Wouldn't it save some time to select the required tag using: Inspect > Right-click on tag > Copy > Selector/Xpath ? Just wondering,True
@christianloizou4463,2020-09-14T15:01:12Z,0,Thanks for this. I couldn't be bothered to create colour schemes by hand for an application I am writing so I wrote a program to scrape them from color-hex.com! Saved so much time and effort designing.,True
@charliecharless,2020-09-12T11:48:13Z,0,"dude you are amazing at teaching and at coding, I've been wanting to do this for so long... now I can, thank you really much",True
@harisfaraz5407,2020-09-11T17:52:47Z,0,where is captch then<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>><???????????????????,True
@nunomsilvestre8372,2020-09-10T12:17:09Z,0,Jesussss. ... all over the place.,True
@juliopaniagua8723,2020-09-10T03:40:32Z,0,"Hey! new guy here, so how would you write the 'if' condition in the loop for missing values?  The pseudocode I can think of is - if variable = blank then interpret variable as python None -",True
@tunneladdict5005,2020-09-07T22:52:55Z,0,https://5euros.com/profil/torifsaibou  Here is my 5euros profie. I can help you scrape datas from a facebook or a website.,True
@nonamed56,2020-09-06T15:07:00Z,0,19:26,True
@pratiharpooja,2020-09-05T19:41:24Z,0,perfect,True
@akhashs297,2020-09-03T16:30:05Z,0,11:38,True
@shehtabmasud9612,2020-09-03T01:55:09Z,0,can i use anaconda prompt instead of the command prompt,True
@pallavijog912,2020-09-02T11:30:15Z,0,"Doesnt this work with google.com search results? I wanted to work with google shopping link from where I could fetch images and data.. but its giving me error as ""HTTP Error 403: Forbidden""",True
@jasperb6191,2020-09-01T04:48:31Z,0,loved this tutorial and it was super easy to understand. Couldnt help but laugh when i saw rape.py at 31:57 haha,True
@nxxbnxjdhd8793,2020-09-01T01:00:22Z,0,Omg,True
@gduhfgsgdhgsudhg5595,2020-08-28T09:25:29Z,0,Yang milih allah like 40000 ya,True
@EverythingWeShouldKnow742,2020-08-25T13:36:33Z,0,I really dont know what's happeningüòÇüòÇbut still want to learn this kind of stuff.üòÇ,True
@ashikmahmud1404,2020-08-24T22:03:37Z,0,Thanks,True
@bikashhalder9511,2020-08-24T08:39:30Z,0,Spend a quality time,True
@laophan4591,2020-08-23T13:25:43Z,0,noted that your user's name is Phuc. Are you Vietnammese? Data Science Dojo,True
@johanr207,2020-08-21T04:52:49Z,0,"Great video! if you are watching in 2020 and using BS4, use the get() method to capture data from different fields within the <a block e.g. container.get('img')",True
@martin-xq7te,2020-08-19T15:47:31Z,0,Good training video. Pleased to see you added saving to a  csv.,True
@VIK2000GEV,2020-08-15T10:10:36Z,0,"Very high-quality tutorial. How to set up everything before running any code is very nice to include,  and timestamping it so people who already know it can quickly skip is just much appreciated. Keeping the tutorial example script and diverse is very welcome. Writing it from scratch just makes sooo useful for remembering what was where.  I wish other people made tutorials like this... Timestamping is so useful when you just want to look-up that one thing and don't really remember when it appeared.",True
@attendez1700,2020-08-14T04:35:21Z,0,oh no 500 megabytes so much data,True
@emilyfu8824,2020-08-14T04:30:03Z,0,"Hi, I couldn't get any result after entering page_soup.h1 or page_soup.anything. I've tried doing it in both terminal and jupyter notebook. Does anyone know what's the problem?",True
@khushalpathakkp,2020-08-11T10:26:43Z,1,https://www.youtube.com/watch?v=OOqt2duR-fs&t=564s,True
@nemanjam7917,2020-08-11T08:07:25Z,0,"–ì–æ–≤–æ—Ä—è—Ç EnotGlobal –≤–∑–ª–æ–º–∞–ª –≤—ã–¥–∞—á—É —è–Ω–¥–µ–∫—Å–∞, –≤–æ—Ç —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –±—ã–ª–æ –±—ã —É–∑–Ω–∞—Ç—å.. –∞ —ç—Ç–æ –æ—á–µ—Ä–µ–¥–Ω–æ–µ –∏ –Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ..",True
@LeonaMeyer,2020-08-11T06:51:39Z,0,"My script worked fine... but my output to the csv put everyting in column A, it did not space it into columns A, B and C... PS GREAT tutoria!",True
@techlaw997,2020-08-08T11:37:20Z,0,Great job. I liked how you explain everything really well.,True
@SRSujon-nn5fv,2020-08-07T12:17:38Z,0,"It is so cool but as a newbie scrapper it took me a while to get used to some new terms and difficulties. Thanks a lot, Bless you!",True
@yourhonor69,2020-08-05T03:56:30Z,0,Is that you Mr Mackey?,True
@komkohblllkvvcvv,2020-08-03T18:14:36Z,0,Beautiful work,True
@GhostAcademy,2020-08-03T11:48:45Z,0,This is so good. thanks for the video,True
@cubbybear8322,2020-08-03T11:25:45Z,0,"We loved it babe,thanks!",True
@shammiakterkeya1747,2020-08-02T07:59:55Z,0,"Hello! I was trying to follow your steps. But when i put the page_soup.h1, it is not getting the information from the webpage. Can you please tell what could be wrong?",True
@adnanzafar5385,2020-08-02T07:56:40Z,0,"Hi everyone. I have created Python Web Scraping WhatsApp group, You can join https://chat.whatsapp.com/HzX1DzO5IAg1x0FBB7mLUT",True
@wendikinglopez8842,2020-08-01T02:23:58Z,0,THANK YOU SO MUCH,True
@tommili4970,2020-07-30T19:57:36Z,0,Best Web scraping video on youtube! thank you for that,True
@avindx,2020-07-30T14:18:11Z,0,A big thumbs up man really helpful,True
@mukulvishwas6691,2020-07-28T07:27:49Z,0,i like your content.,True
@RePuLseHQKing,2020-07-26T19:11:46Z,1,"HELP: shipping_container = container.findAll(""li"", {""class"":""price-ship""})     print(shipping_container[0].text)  Traceback (most recent call last):   File ""scrapper2.py"", line 19, in <module>     print(shipping_container[0].text) IndexError: list index out of range  so there is no object 0? because when i do this:  shipping_container = container.findAll(""li"", {""class"":""price-ship""})     print(shipping_container)  ASUS TUF Gaming GeForce GTX 1660 SUPER Overclocked 6GB Edition HDMI DP DVI Gaming Graphics Card (TUF-GTX1660S-O6G-GAMING)  [<li class=""price-ship"">Free Shipping</li>]  i get that result. How can i acess the text inside the li tag ? Obviously i want: Free Shipping",True
@pnocti,2020-07-25T05:49:47Z,0,One of the best scraping tutorials good job,True
@AhmedAymanM,2020-07-23T13:58:09Z,0,"Liked it so much, Is there is a way to scrape a page that has a textbox searching and no direct items?",True
@jefflee9062,2020-07-23T11:42:27Z,0,Containers contain nurse,True
@bt201,2020-07-21T06:54:26Z,0,"I got a 403 Forbidden error, was wondering if anyone ran into the same thing?",True
@010270ma,2020-07-18T06:44:54Z,0,"What a clear  explaining,    thank you  man,,, i m learning... do you teaching on Udemy ?",True
@ssgoh4968,2020-07-16T11:28:55Z,0,Best Scrapping Video ever.  Thank you.,True
@wkowalski,2020-07-15T16:07:22Z,0,Thanks for this... it's amazing. You're a natural teacher. Is there any chance you could make a video about how to scrape a table and output the data into a spreadsheet? I'm having a hard time with this and there are really no videos out there as good as yours.,True
@Dana-gg3cj,2020-07-15T11:42:56Z,0,"Everything is somehow complicated, people who just went to find out about web scraping will definitely break their brains). If, like me, I didn‚Äôt quite figure it out, read here https://finddatalab.com/web-scraping/",True
@braydensmith6263,2020-07-15T04:30:04Z,0,Dawg your the best,True
@vegashands,2020-07-14T19:01:42Z,0,"Nice Tut as it goes, kudos.",True
@yoitsluan,2020-07-14T03:19:52Z,1,I'm getting AttributeError: 'tuple' object has no attribute 'findAll' help,True
@reginaldowusu3156,2020-07-14T00:02:26Z,0,Loving your coding skills. Was just about giving up on Web Scraping. Then BOOM!!! I found this. :),True
@fahimhossain165,2020-07-13T19:44:47Z,0,"Saving the file as ""My first web scrape"", what a lie!",True
@mockingspyhd6099,2020-07-13T13:53:59Z,0,thanks for everything! 2020,True
@brianknoll2410,2020-07-10T00:01:49Z,0,Unbelievably helpful! Thank you so much!,True
@printdaniel,2020-07-06T22:18:24Z,2,AttributeError: 'NoneType' object has no attribute 'a',True
@MubashirullahD,2020-07-05T19:57:19Z,0,That was so well done!  I like to learn Selenium from this guy.,True
@chadwong7051,2020-07-04T14:06:19Z,0,You're amazing!!!,True
@kiloAlpha9,2020-07-03T14:47:46Z,0,Brilliant! Thanks a bunch...,True
@felixkimutai8478,2020-07-02T20:45:45Z,0,"I have watched all the web scraping videos on YouTube but this one is the top, I learned a lot. Thank you.",True
@ravitejaw4604,2020-07-01T14:26:45Z,0,Can we use xpaths in soup?,True
@Strajaize,2020-06-29T00:57:49Z,0,"I expected this video to take me 30 minutes to do, because it takes 30 minutes. 10 hours later I HAVE MY FIRST WEBSCRAPER THANK YOU VERY MUCH! I still did not manage to get it to be an csv, but made a .txt and it is fine for now. Thank you so much again! tutorial from dataquest.io came in very handy as well!",True
@SeanKent69,2020-06-28T21:56:54Z,0,So unfortunately I'm getting the message HTTP Error 403: Forbidden According to RFC 7231: The 403 (Forbidden) status code indicates that the server understood the request but refuses to authorize it. Is there a way to get around this?,True
@franco521,2020-06-28T17:29:58Z,2,I expected something better than this. You dived into a very specific example without any introduction on the set of tools or functionality that this library supports. SOLID dislike.,True
@krishnagrandhe4409,2020-06-26T04:55:42Z,0,amazing video thank you,True
@edwardadams3727,2020-06-24T16:47:01Z,3,"brand = container.find(""a"", {""class"":""item-brand""}).img.get('title')  your welcome",True
@melisaliu5727,2020-06-23T10:41:21Z,1,¬øPuedes publicar un video sobre c√≥mo scraping web con Octoparse?  Creo que esta herramienta es m√°s adecuada para principiantes y no requiere python  https://www.octoparse.es/,True
@sebastianpeters2296,2020-06-22T21:25:09Z,0,"Hey there! this guide really helped me to create a tailored scraper for a pilot project. Even though I am at the very beginning stage of learning Python, I could manage to create the entire script, and even learned while the process. Amazing, really appreciate this!",True
@ProjectSage,2020-06-22T19:19:25Z,1,I loved this so muchhhhhh !!!! ^^,True
@ignaciolira4304,2020-06-22T18:16:35Z,0,YOU ARE A GOD,True
@SunnySharma-dc2nh,2020-06-22T11:06:02Z,0,"hey buddy, whenever i am giving this --> container.div.div.a, i get (AttributeError: 'NoneType' object has no attribute 'a') please help me..20:32",True
@alludeYT,2020-06-21T22:18:56Z,2,"""Sublime tect""",True
@collinsiwuchukwu4105,2020-06-20T12:43:10Z,0,"I love the video, make more",True
@dayashankarprajapati8686,2020-06-18T12:22:41Z,0,Oop!,True
@jeffreyyellow706,2020-06-18T05:20:41Z,0,good video,True
@clivestephenson2793,2020-06-17T23:54:46Z,0,You are the most concise teacher of python I have come across  Thanks  I will definitely give your other videos a view,True
@sk_4142,2020-06-17T02:11:48Z,3,"brand = make_rating_sp[0].img[""title""].title() TypeError: 'NoneType' object is not subscriptable [Finished in 3.074s] anyone know why this is happening? or how to fix this?",True
@ashfannm7674,2020-06-13T08:10:26Z,0,I Enjoyed watching it bro. do more,True
@amdenis,2020-06-12T05:06:51Z,0,"Very clear.  One thing, you might consider a modern all in one dev tool and environment like PyCharm, or at least VisStudio. It will save you a lot of time vs Sublime, site tools, CMI, etc.",True
@mohitnagarkoti4086,2020-06-11T09:04:50Z,0,"Awsome, Good, Excellent, Nice, Best. Hope Youtube's algorithm recommend this to every Scrapper Enthusiast.",True
@quickstorm8823,2020-06-10T19:56:06Z,0,great video! I raised the view count by 500 after spending hours scraping videos from youtube LOL,True
@tntcaptain9,2020-06-05T17:25:30Z,0,Saw many videos on web scraping but yours was probably the best one.,True
@8.shankercs254,2020-06-03T09:34:13Z,1,in the alternate universe 139132 Data gaming- Trolling Fortnite hacker with my hacking skills,True
@pratikjoshi677,2020-06-02T18:33:19Z,0,"I'm Getting 0 while printing the len function: Here is my code Thanks in advance  from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup my_url  =  'https://www.daraz.com.np/catalog/?_keyori=ss&from=input&page=3&q=headphones&spm=a2a0e.11779170.search.go.15bb2d2btaSWVj'  uClient = uReq(my_url) page_html = uClient.read() uClient.close()  page_soup = soup(page_html, ""html.parser"")  containers = page_soup.findAll(""div"", {""class"":""c2prKC""})  print(len(containers))",True
@richardmamaril4143,2020-05-31T03:59:42Z,0,"i cannot loop the variable  inside the container  any ideas? here's my code containers = page_soup.findAll(""div"",{""class"": ""item-container""}) container = containers  brand = container.find(""div"",{""class"": ""item-branding""}) <<<<---------------------------------- container is asking for [] , but how can i loop this model = brand.a.img[""title""]  print(model)",True
@kumardavesh,2020-05-29T19:56:48Z,0,i really enjoyed it ...thx ubro,True
@mdk5624,2020-05-29T09:59:35Z,0,"Nice video,  can you please teach us looping through multiple pages or urls",True
@nickmadew1127,2020-05-26T08:07:51Z,0,Awesome video. Just subscribed.,True
@andrewdannolfo5157,2020-05-24T17:56:03Z,0,Best tutorial I've seen. Thank you for saving me so much time!,True
@rafaTheHutt,2020-05-24T09:45:21Z,0,Grest video.  I can do it in pinterest to pin an image . Can you to learn how to do it? Thanks,True
@rattrap777,2020-05-24T09:10:51Z,0,Great video fam!,True
@codingjim3734,2020-05-23T01:09:26Z,0,do a shot whenever he says ok,True
@jdaregalario,2020-05-21T23:19:14Z,0,How do I data scrape a site that requires sign in first before you can access infos?,True
@jdaregalario,2020-05-21T19:42:38Z,0,Does it really have to be anaconda? Can't I use a regular python for data scraping?,True
@rayyanhashmi0,2020-05-21T08:39:00Z,1,it says python is net recognized as an external or internal command in the command prompt,True
@ryscarrillo,2020-05-21T01:25:17Z,0,"when i do the print, all the data puts at large on A, why is not printing on excel the data on list?? anyone knows??",True
@benvelloor,2020-05-19T19:46:20Z,0,Amazing Video!,True
@mahesheepuri1000,2020-05-19T12:13:04Z,0,"Thank you very much! The video was very clear to understand, it helped me to progress in learning python.",True
@ManpreetSingh-ro6xo,2020-05-19T08:22:33Z,0,"AttributeError: 'NoneType' object has no attribute 'div'  m getting this error while running the code.It would be great help if anyone could tell how to tackle this. P.s-  brand=container.div.div.div.a.img[""title""] I have used div 3 times because the title was present after 3 div.Correct me if I m wrong",True
@MrMehrd,2020-05-18T23:00:40Z,0,"Awesome,learn a a lot from one short tutorial !!!",True
@sandeepsingh-mo1uc,2020-05-18T19:49:56Z,0,"when i save the script- its giving me an error. for price shipping its saying - ""inconsistent use of tabs and spaces in indentation"" can someone pls help",True
@Badshahealam,2020-05-18T11:28:29Z,0,your website is very slow.,True
@sagenose1082,2020-05-16T09:22:01Z,0,"Hi Phuc, this is great - thank you! What if a page requires credentials> is there a way to program that in?",True
@vipulsharma8865,2020-05-15T14:41:18Z,0,Best tutorial,True
@fngonzo,2020-05-15T13:03:29Z,0,I have no idea what happened but I enjoyed it...maybe I should learn Python.,True
@jdaregalario,2020-05-15T05:55:56Z,0,"I already installed anaconda but when I type 'python' why does it says ""the term python is not recognized as the name of the cmdlet...""?",True
@oussamakaifouf7395,2020-05-14T20:40:50Z,2,This just saved my life <3,True
@sandeepsingh-mo1uc,2020-05-13T17:38:57Z,0,"I am getting this error when requesting a URL   >>> uclient = uReq(my_url) Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""C:\Users\Amros\anaconda3\lib\urllib\request.py"", line 222, in urlopen     return opener.open(url, data, timeout)   File ""C:\Users\Amros\anaconda3\lib\urllib\request.py"", line 525, in open     response = self._open(req, data)   File ""C:\Users\Amros\anaconda3\lib\urllib\request.py"", line 548, in _open     'unknown_open', req)   File ""C:\Users\Amros\anaconda3\lib\urllib\request.py"", line 503, in _call_chai n     result = func(*args)   File ""C:\Users\Amros\anaconda3\lib\urllib\request.py"", line 1389, in unknown_o pen     raise URLError('unknown url type: %s' % type) urllib.error.URLError: <urlopen error unknown url type: https>",True
@TheEducationWorldUS,2020-05-12T23:06:45Z,0,Anyone learning in this quarantine? ü§©,True
@npithia,2020-05-11T20:21:42Z,0,This is gold for someone learning python and seeing its application.,True
@icakinser,2020-05-11T17:09:33Z,0,Great video! You made everything crystal clear and understandable,True
@cwhizkid420,2020-05-10T20:21:20Z,0,Just one question: we are pulling data from all the items in the same page. What if there are others for the same list and in subsequent pages? Can we pull them also using url extensions? Thanks in advance,True
@cwhizkid420,2020-05-10T20:19:37Z,0,This is one of the most useful Web Scrapping videos I have ever come across. I could learn it from scratch. Thanks.,True
@butchinator1988,2020-05-10T15:41:17Z,0,"Im having trouble installing Beautiful soup. Its just giving me SyntaxError: invalid syntax >>> pip install bs4   File ""<stdin>"", line 1     pip install bs4",True
@JohirulIslam-lm2vz,2020-05-10T15:39:23Z,0,What just I saw!  This is really .. really helpful.  You deserve a big THANKS!  You present everything not just fine but out of the box! THANKS AGAIN...!!!,True
@hadistech466,2020-05-09T18:37:38Z,0,"hey, bro I still don't know how to scrape the rates, because as you say there is some products that don't have any rate so we need to add some if conditions but I don't know how to do it. please reply.",True
@Alex.T.H.,2020-05-09T17:27:33Z,0,–ó–∞ —Ç–∞–∫–æ–π –∫–æ–¥ –ø–∞–ª—å—Ü—ã –ª–æ–º–∞—Ç—å –Ω–∞–¥–æ %) –ø—Ä–æ –ø–æ–¥–∞—á—É –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –≤–æ–æ–±—â–µ –º–æ–ª—á—É,True
@leotruman240,2020-05-09T09:49:38Z,0,"This was one hell of video , thanks",True
@elsasuryana9111,2020-05-08T13:04:48Z,0,thank u!,True
@bestviewsreviews,2020-05-08T10:49:08Z,0,nice,True
@bmabma6830,2020-05-07T17:28:03Z,0,Is the file products.csv should be created prior to writing the code ?,True
@cameronbroad6271,2020-05-07T10:15:47Z,0,i don't have the option to open windows command file,True
@mitchellthescientist,2020-05-07T01:25:18Z,0,"I am using Ubuntu, and following along in my terminal. When I do uClient.read() into my terminal I keep getting a error? The error is:  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""/usr/lib/python3.8/http/client.py"", line 467, in read     s = self._safe_read(self.length)   File ""/usr/lib/python3.8/http/client.py"", line 608, in _safe_read     data = self.fp.read(amt)   File ""/usr/lib/python3.8/socket.py"", line 669, in readinto     return self._sock.recv_into(b)   File ""/usr/lib/python3.8/ssl.py"", line 1241, in recv_into     return self.read(nbytes, buffer)   File ""/usr/lib/python3.8/ssl.py"", line 1099, in read     return self._sslobj.read(len, buffer) ConnectionResetError: [Errno 104] Connection reset by peer   Where am I going wrong?",True
@wamiqueriyaz4689,2020-05-07T01:04:23Z,0,Absolutely Brilliant,True
@bielsampaio7976,2020-05-06T22:13:13Z,0,"My containers comes as empty , the website has some weird class names with spaces. I wonder if that's why i can't populate my containers.",True
@bielsampaio7976,2020-05-06T20:51:53Z,0,"My dude how on earth you have so little subscribers, your content is awesome",True
@hesborn6833,2020-05-06T12:16:21Z,0,Thanks a lot,True
@koladeadewoye5226,2020-05-05T23:29:47Z,2,"Hi, loved this tutorial but is there a way to automate the web scraper. For example how can i use it to scrape  data from a site every 24 hours assuming the content on the site changes every 24 hours",True
@VivekKumar-zd6vq,2020-05-05T20:41:40Z,0,"Yes, enjoyed and learnt a lot. Thankyou.",True
@TheL337trance,2020-05-02T17:57:01Z,0,"This is REALLY good content.  I know I'm not speaking for myself, but I would love to see a video of you doing this same thing but having it run once an hour and have it send an email if there is a change.",True
@waynechiechiengtsung5105,2020-04-30T17:28:12Z,0,Wow i didn't know you can scrape like this too. I've been using Turbo Marketer - Instagram Followers Scraper and it helps me scrape some info in instagram.,True
@warren5567,2020-04-30T04:41:55Z,0,"this video was okay and i learned some stuff, but it was a pain to follow overall. variables were way to similar and hard to keep track of, and video was difficult to follow along because some code didnt work properly. kinda annoyed",True
@sharkattack6423,2020-04-29T03:43:49Z,0,Fuck! I was hoping to just open note pad and get to it.,True
@farhadhossain8108,2020-04-28T09:30:19Z,0,"Scraper API handles proxies, browsers, and CAPTCHAs, so you can get the HTML from any web page with a simple API call!  Scraper API offers 1000 free API calls without Any credit card required.  One of the most frustrating parts of automated web scraping is constantly dealing with IP blocks and CAPTCHAs. Scraper API rotates IP addresses with each request, from a pool of millions of proxies across over a dozen ISPs, and automatically retries failed requests, so you will never be blocked. Scraper API also handles CAPTCHAs for you, so you can concentrate on turning websites into actionable data.  If you wish to try Scraper API (https://www.scraperapi.com?fpr=neiloy-hossain74) here is a 10% coupon for you to give it a try - apiscraping10",True
@Manuel-uz4qm,2020-04-26T19:58:47Z,0,"Man, you just saved my Python programming subject. Thank you alot",True
@ashutoshkumar4033,2020-04-26T10:39:28Z,0,u teach like shit..way too fast,True
@tsegalemhailu1211,2020-04-25T16:25:55Z,0,I was doing a project to collect comment and posts from Facebook. Is there anyway i can scrap from there. need your help.,True
@singhshobhit11,2020-04-25T13:01:39Z,0,Need Help: I am scraping a web-page for all the anchor tag links. I want to find the link at nth position from the first link on that webpage and then access the nth link. How do I do it?,True
@TABISHJAMAL2011,2020-04-24T15:18:17Z,0,bhagwan aapka bhalaa kare. God bless you :-),True
@youtraders,2020-04-22T18:09:31Z,0,Tanks brother,True
@zouhairbenazzouz2536,2020-04-22T15:59:31Z,0,thank you so much this is exactly what I was looking for my company. i dont think I have the time and skills to do it (even if you explain all of it) but at least I understand now what I can ask from someone competent in this.,True
@robertopicco1972,2020-04-22T15:02:48Z,0,"Your solution for brand doesn't work for me... Maybe this site has changes a bit... here's my solution for it   brand_container = container.findAll(""img"", {""class"": ""lazy-img""}) brand = brand_container[1].get('title')",True
@robertopicco1972,2020-04-22T13:17:52Z,0,"To bypass SSL error you might get on MacOS, use this piece of code:     import certifi import ssl   uClient = uReq(my_url, context=ssl.create_default_context(cafile=certifi.where()))     instead of the simple     uClient = uReq(my_url)   everything will work fine.",True
@zain9918,2020-04-21T04:03:41Z,0,thx bro,True
@brendanp9415,2020-04-20T21:19:26Z,1,"This is the best web scraping tutorial that I‚Äôve found. I‚Äôve been frustrated for hours trying to use other resources. Thank you for making this, your explanations are thorough and great!",True
@usmanabbasi143,2020-04-20T16:14:40Z,0,Great content ..I love your videos..it helps me a lot... I am using this data scraping tool to get data from google maps ..https://www.ahmadsoftware.com/115/google-map-extractor.html,True
@hendrickkone679,2020-04-19T23:09:57Z,0,"Thank you, great confidence booster in web scraping.",True
@yt-dman,2020-04-19T21:45:25Z,0,sorry it's just really hard to follow with you when your speaking so quick :/,True
@atoxorbaladze7216,2020-04-19T21:16:23Z,0,"Great video, Thank you!",True
@user-lf6fb9fj8j,2020-04-18T22:30:20Z,0,im only at 4:40 and I am getting an error 'ImportError: No module named request' im running python 2.7 in anaconda. Any suggestions?,True
@victorbigand1502,2020-04-17T12:43:42Z,0,"Is it normal that when I call uReq, for 90% of URLs it return an error?",True
@MrScienceOlympiad,2020-04-17T06:26:31Z,0,This is very well explained. I like this as a beginners guide!,True
@HarshSingh-zf7tw,2020-04-16T14:41:14Z,0,"what does container[0] do here exactly, why we take index as 0 not any other number?",True
@maendharreddy6317,2020-04-16T09:16:07Z,0,Very useful tutorial thank you for sharing knowledge,True
@martinkaspar5095,2020-04-15T23:19:26Z,0,can we run this code also today  -  is this doable!?,True
@vadisayamini5951,2020-04-15T04:13:47Z,0,while entering from urllib.request open urlopen as uReg itsxshowing error of no module named request,True
@abhimanyusharma7918,2020-04-14T08:10:11Z,0,"Maybe try to explain things a little bit more. By the way, great video!",True
@hasnain4260,2020-04-14T06:36:54Z,0,It's like magic. Thank you so much for this video.,True
@BachPhotography,2020-04-13T04:17:35Z,0,Great tutorial thank you. Why do I sometimes not receive any containers when I run the script? Is this an issue with late JS loading?,True
@nivishanwickramaratne6287,2020-04-12T05:29:13Z,0,Thank you very much. It is fantastic... !!!,True
@coolmonkey619,2020-04-11T21:34:29Z,0,You explained this amazingly. Hopefully can use python to its fullest,True
@fubrian2945,2020-04-10T19:25:15Z,0,"I might have missed something in the video. The variable container is not declared in your python file, how can you use it?",True
@minhajshovon9789,2020-04-10T10:07:11Z,0,Excellent video man! Just keep the terminal fonts smaller. That much large fonts distorts the format and hard to keep track.,True
@fubrian2945,2020-04-10T03:53:13Z,2,Bruh. Why do you only have less than 50K subs. You deserve millions. Thx dude!,True
@joeyzalman8254,2020-04-08T02:35:24Z,2,The explanation was super. Also thank you you for showing all those handy tools. Keep it up!,True
@teddyverdecia4855,2020-04-07T23:55:58Z,0,I love you,True
@umang20082008,2020-04-07T21:58:03Z,0,Learn Web Scraping from Scratch  https://www.youtube.com/watch?v=Ful6wfbavK0&list=PLfNXG56H-jTvDt_BrLT65hy-vUrs7vkT9,True
@huzailhassan4021,2020-04-07T17:04:19Z,0,Nothing different if I use Visual Studio Code to write the script?,True
@kevinzhang7228,2020-04-06T03:53:38Z,0,"Great tutorial, thanks!",True
@spencer5028,2020-04-06T01:36:20Z,0,"Great stuff.  I like python but I'm wondering about doing scrapping inside a PHP app like laravel,  are there any good packages?  I've heard of goutte but it would be cool to see a PHP based scraper that scrapes after X amount of time.",True
@a3igner,2020-04-04T20:06:02Z,0,Good Video. Question.  How do you do it if the website checks if you have javascript enabled?  Can you make a tutorial on how to do it? Take example:  www.marketwatch.com,True
@kingsleysimon8945,2020-04-03T10:40:14Z,0,GREAT. TUTORIAL.. can any give me a doc of the coding? please...im toooo lazy for that much coding hours....heeelllppp,True
@theworkflow19,2020-04-02T22:24:45Z,1,You are a blessing seriously! The first tutorial that actually made sense from start to finish. I was able to understand so much from this! Please Please Please Please upload more videos on Python Web Scraping with BeautifulSoup.     Thank you again for this blessing!,True
@haroonhassanqadir6275,2020-04-02T09:24:23Z,0,so what to do when you've got to scrape the whole website? i mean like all the pages? do u have to go to each page individually and scrape them?,True
@twlson49,2020-04-02T06:04:06Z,0,lol go faster,True
@TheUMESH34,2020-04-01T13:18:38Z,0,Please requests library instead of urllib if possible. Its easier I guess?,True
@willburke7969,2020-04-01T06:49:03Z,0,Awesome!,True
@yesankur,2020-04-01T04:16:23Z,0,"I want To Extract all url from a Webpage this.  https://www.kikiers.com/profile/view/714cc4b3d634734b5d15d75e233ea884e10b7cb1  but first you need to login on this website to access the given url.  Login :- https://www.kikiers.com/login Email :- info.achero+001@gmail.com password :- asdfghjkl  then goto this link¬†https://www.kikiers.com/profile/view/714cc4b3d634734b5d15d75e233ea884e10b7cb1¬†and extract all links for Example:-  Link 1 :-¬†https://www.kikiers.com/post/8a5078bf-90cb-4701-885e-df48861ba765/story  Link 2 :-¬†https://www.kikiers.com/post/96328dac-aa63-4a24-9a8c-cf94215f9f3d/story  link 3 :-¬†https://www.kikiers.com/post/8229b5cc-001c-41c8-9277-c7e05360ad5a/story  i already used many (chrome extension, firefox extension, Python selenium with chrome driver) but unable to success to extract those urls.  if you any suggestion or method pls send me.  i am very happy to follow your Method.  ~Ankur kumar Thanks",True
@pa4761,2020-03-31T19:31:49Z,0,"Request unsuccessful. Incapsula incident ID: 869001210139643433-308289561403137605</iframe></body></html>' I got this issue after    page_html = uClient.read() print(page_html)   If anyone could kindly help",True
@nadimhayat3412,2020-03-31T18:16:01Z,0,This is awesome. Thanks for the clearcut explanation and demonstration,True
@ShawneeUnion,2020-03-30T21:16:09Z,2,"So thankful for this, I was able to run it and scrape similar information off of a coding website. I had some trouble with installing BS4. Tip, I used pip3 to install BS4 to keep everything clean.    sudo pip3 install bs4",True
@MdBahauddinZakaria,2020-03-29T16:43:17Z,0,Prerequisite of this video?I only know basics of python.,True
@dksdbwls1,2020-03-29T07:23:16Z,0,"Why does my command promt doesn't recognize python and I looked further and I discovered cmd.exe promt inside anaconda where it runs python perfectly but when I do pip install bs4, it doesn't again recognize, it says File ""<stdin>"", line 1 pip install bs4 error invalid syntax. Please help :( I can't even start this whole tutorial",True
@armeljoelirie3797,2020-03-28T22:31:19Z,1,marque = produit.div.div.a.img['title']   GETTING THIS ERROR   AttributeError: 'NoneType' object has no attribute 'a',True
@martinkaspar5095,2020-03-28T19:25:48Z,0,"hello dear all good day -   first of all  - many many thanks for the great example  - i tired it out   freshly installed ATOM on MX-Linux versino 19.1 with Python version 3.7.x   i have to setup a lot. at the moment i have the question - where the data are stored. for some test i run the code - taken from here:   this is taken from a vid on youtube: https://www.youtube.com/watch?v=XQgXKtPSzUI see the code: https://code.datasciencedojo.com/datasciencedojo/tutorials/tree/master/Web%20Scraping%20with%20Python%20and%20BeautifulSoup Output  the following  Traceback (most recent call last):   File ""C:\Users\Kasper\Documents\_mk_\_dev_\python\_newegg_.py"", line 36, in <module>     brand = make_rating_sp[0].img[""title""].title() TypeError: 'NoneType' object is not subscriptable [Finished in 3.189s]    what can i do - what does that mean!? love to hear from you  _  many many thanks greetings martin",True
@waylus2897,2020-03-28T17:41:50Z,0,"wow,  i love this, wow thank you",True
@Jeff-pt8sw,2020-03-28T15:02:51Z,0,its so dumb to prototype things...,True
@JoaoPaulo-re5yz,2020-03-28T09:06:21Z,0,"Great video! If you are a dotnet developer, there is a tool that helps in extracting information from HTML pages, it is called DotNetExpose. You can install using Package Manager. Follow the links: Github: https://github.com/joao2391/DotNetExpose Nuget: https://www.nuget.org/packages/DotNetExpose/  #webscraping #csharp #dotnet #webcrawler #HtmlAgilityPack #DotNetExpose #python #datascientedojo",True
@terryliu3635,2020-03-28T03:34:24Z,0,Thanks. Just watched. Will try tomorrow!,True
@martinkaspar5095,2020-03-27T15:44:32Z,0,hi there - great stuff i am overwhelmed - one question - what if i run this code today - is this doable?,True
@newaccount4422,2020-03-23T17:19:35Z,0,2:29 Anyone else notice after users he spelled fuck as phuc???,True
@luisady8990,2020-03-23T02:09:09Z,0,"@20:40 If the ""container.div.div.a"" does not work, just do ""container.a"". it works the same.",True
@mauricenestler6559,2020-03-20T19:12:21Z,0,"Using , doesn't create a new column for me... What I get is a single column with ""brand, product_name, shipping""",True
@FredericBiondi,2020-03-20T09:49:42Z,0,"I learned so much, I can't believe it!!!",True
@samundraregmi8593,2020-03-20T03:42:27Z,0,I don't have any words to explain how much this video was helpful. Hope soon I will use this feature.,True
@Norogoth,2020-03-16T22:16:05Z,1,"I did container.div.div.a and got ""'NoneType' object has no attribute 'a'""",True
@arujbudhraja,2020-03-16T20:05:55Z,2,"Awestruck! It's amazingly simple to follow along! Thank you, sir, for adding to the community of self-learners!",True
@Coddammit,2020-03-16T16:40:59Z,0,"Many thanks for the clear explanation !!  I only have one question. How can you find out the prices that are only visible in the shopping cart? Can you make a video about this? :D Thanks mate",True
@FiveMCity,2020-03-12T18:40:14Z,0,after i type page_soup.h1 nothing renders.. anyone has advice?,True
@totallyvalid7607,2020-03-12T18:27:09Z,0,"python gives syntax error for ""for containers in containers:"". Anyone have a suggestion as to why this is happening? Thanks in advance.",True
@dmarsblue,2020-03-11T21:05:51Z,0,"Hi I keep getting this error please help TypeError                                 Traceback (most recent call last) <ipython-input-5-76a3a66d2039> in <module>      23       24 for container in containers: ---> 25     brand = container.div.div.a.img[""title""]      26     title_container = container.findAll(""a"", {""class"": ""item-title""})      27     product_name = title_container[0].text   TypeError: 'NoneType' object is not subscriptable",True
@Keepfaith101,2020-03-11T01:51:23Z,0,I loved this video. Thank you !,True
@anandprakashekka8582,2020-03-07T12:46:36Z,0,"In your example, how to loop second, third or fourth page of the website?",True
@ginho3848,2020-03-06T08:52:07Z,0,Oh My...how long does it take to get to this level?,True
@frantiseknovotny5499,2020-03-05T00:01:55Z,0,It's very educative. Perfect job! Thanx a lot!!!,True
@RedBricksTraffic,2020-03-04T00:38:46Z,0,I love this type of lesson. Building useful software with real world applications.,True
@hanbliraghda5140,2020-03-03T10:51:36Z,0,This is a very well done content . Thank you !,True
@ramwarutkar4364,2020-03-03T06:52:23Z,0,Fantastic !,True
@danroche8014,2020-02-29T14:11:30Z,0,Beautiful - thank you so much!!,True
@hdm_vision,2020-02-27T04:42:40Z,1,"Wow, So cool. I think this is what i need",True
@bIIIgG,2020-02-20T19:22:28Z,0,"That was very helpful in my project. Thank you!",True
@001Debjeet,2020-02-20T18:15:58Z,0,thank you sir,True
@ninjagotguns,2020-02-19T17:32:22Z,1,Tip: use SelectorGadget to find the css selectors you want,True
@elmatador2547,2020-02-17T17:25:22Z,0,"Thanks for the nice tutorial! Why I can't code that in an IDE, just in a terminal?",True
@felixgiallombardo2560,2020-02-16T01:47:55Z,0,thank you so much for this!  So helpful and useful,True
@erikaconners5633,2020-02-15T23:29:11Z,0,i did this and my computer freaked out,True
@pocketrocket6604,2020-02-15T20:34:36Z,0,"Thanks for the great video, I was able to scrape data with nearly no experience in coding. I only have one problem: I'm not able to scrape images from a website, is there anything special I have to look for for scraping images?",True
@bijudonbosco,2020-02-15T07:37:36Z,0,"Hi, For me the issue is, i can't write headers to excel file..",True
@maarefalikhan,2020-02-14T14:49:56Z,0,"Your video was really helpful, thanks a lot!",True
@mantapkrina2219,2020-02-14T14:32:04Z,0,what if i want print only 15 link that i scrapped?,True
@vaibhavdeshmukh7900,2020-02-13T20:53:33Z,0,Awesome video man... Make these kinda practical videos with live coding.... Amazing stuff.. loved it!,True
@cacarew544,2020-02-10T13:23:51Z,0,A ton of gratitude from here!!,True
@satoshinakamoto171,2020-02-10T10:56:12Z,0,why are you switching back and forth between CMD and Sublime ? you can view the output in sublime with ctrl + b,True
@vasiapupkin2014,2020-01-31T14:43:59Z,0,Cool,True
@daxj9133,2020-01-30T13:32:22Z,0,<33333333 ! thanks,True
@7iq728,2020-01-29T18:31:10Z,0,bro you are the best   Crazily good vid,True
@robertotarga,2020-01-29T08:31:16Z,0,Can you please make a video about actual code needed to iterate thru all pages of the web site?,True
@mattymallz4207,2020-01-29T01:16:28Z,1,"Happily subscribed, thanks for sharing!",True
@kelvinkk577,2020-01-23T20:45:46Z,0,Love this tutorial You explained it so well that it's really easy to follow Thanks,True
@poonasor,2020-01-18T22:37:31Z,0,you guys should really continue this series,True
@varunpandey7801,2020-01-18T08:37:13Z,1,YADAV JI LIKED IT KEEP UP THE WORK LODU,True
@paulprice5860,2020-01-18T00:40:41Z,1,Thanks. I have a basic understanding of python and html and I found this tutorial very easy to follow. You do a great job of clearly explaining things in the code which is what I need at my current skill level. Much appreciated.,True
@BloodyClash,2020-01-14T08:51:56Z,0,Very nice explained with a very relaxing voice. Thank you,True
@ashutoshanand4263,2020-01-13T13:42:41Z,0,Great video ..,True
@gordontang7837,2020-01-13T06:22:51Z,0,cool,True
@Berghiker,2020-01-12T23:52:16Z,0,How do I import BeautifulSoup into Python vers 3.8?,True
@SamuelBSR,2020-01-12T23:39:38Z,0,"Man, you are awesome! :)",True
@MasterofPlay7,2020-01-11T22:53:11Z,0,what about sas?,True
@vishwaa8227,2020-01-11T19:00:22Z,0,"I don't think this code is correct. To my understanding, you've written ""container"" instead of ""containers"" in lines 18, 21, 23. Please let me know if I'm wrong.",True
@RS-el7iu,2020-01-09T19:06:46Z,0,üëçüèªüëçüèªüëçüèª thanks a lot,True
@morteza2629,2020-01-08T05:12:47Z,0,"I've a question and I really appreciate your help  I was trying to use urlopen from urllib.request for a website but it ran into an error saying Error 403 forbidden. Is there anyway to get html of that wabpage, by automation?",True
@abuhassanabo,2020-01-06T21:23:50Z,0,Im using a website which updates as you scroll down the website. This causes me to only get 20 objects instead of the supposed ~200 that exists. How does one solve this?,True
@mixalismcgamer3188,2020-01-06T12:31:48Z,0,BESTTTTTTT VIDEO!!!New subscriber!,True
@aduck24,2020-01-05T11:41:17Z,0,This is awesome. Thank you for sharing :),True
@MrLamans07,2020-01-04T18:27:38Z,0,"Great video! How can I change the search inputs on a website when I shop for something, so it searches the website with my input and get me the data? Please help üòÅ",True
@viveksuman9600,2020-01-04T05:49:12Z,4,I saw this video and then successfully wrote the entire code without looking at the video. Not even once. This is because i understood every line of it. Thank you man. Your explanation is very beginner friendly.,True
@dereka6964,2020-01-02T02:10:45Z,0,"Really appreciate this tutorial, had to make some minor tweaks to keep it current - but the end result was the same! Thanks in 2020",True
@WTFImStreakin,2019-12-29T17:02:33Z,0,best python vid for bs4... seriously thank you.,True
@haadi6344,2019-12-29T09:38:59Z,0,Hey...... if u can read this i am getting an error while testin my urllib in cmd.... when in  call uReq on my url it gives an error saying HTTP 403 : forbidden... tell me what to do....,True
@curtishorn1267,2019-12-29T01:09:53Z,0,PyCharm?,True
@elenasyvokaite7019,2019-12-23T14:22:31Z,0,Hey how do I get my text editor to show up next to version of python when I type python?,True
@bronxed3666,2019-12-23T09:38:57Z,1,Superbly Done.,True
@elenasyvokaite7019,2019-12-22T14:50:53Z,0,This is so cool,True
@lorenzoreyes1692,2019-12-22T06:07:33Z,0,"Thank you, this was my first web scraping",True
@chikeodita6287,2019-12-21T22:11:42Z,0,How did you make this so simple?,True
@ThatGuyDownInThe,2019-12-20T21:32:51Z,4,This is actually the coolest thing I've seen in my entire life. Wow. Thank you so much I love you man.,True
@tokicorp1129,2019-12-16T15:24:14Z,0,When I do the length of the containers it says zero. Can someone help,True
@serg239,2019-12-14T22:16:14Z,1,"The format of tags in the HTML has changed. So, my current version of loop: for container in containers:     d = []     d.append(container.findAll(""div"", {""class"": ""item-branding""})[0].a.img[""title""])     # brand     d.append(container.findAll(""a"", {""class"": ""item-title""})[0].text.replace("","", ""|""))  # title     price_strong = container.findAll(""li"", {""class"": ""price-current""})[0].strong.text     price_sup = container.findAll(""li"", {""class"": ""price-current""})[0].sup.text     d.append(""$"" + price_strong.replace("","", """") + price_sup)                            # price     shipping = container.findAll(""li"", {""class"": ""price-ship""})[0].text.strip()     d.append(shipping.replace("" Shipping"", """") if not ""Free"" in shipping else ""$0.00"")   # shipping     f.write(', '.join(d) + ""\n"") The CSV output: brand, title, price, shipping GIGABYTE, GIGABYTE Radeon RX 5500 XT DirectX 12 GV-R55XTGAMING OC-8GD Video Card, $219.99, $3.99 GIGABYTE, GIGABYTE Radeon RX 5500 XT DirectX 12 GV-R55XTOC-8GD Video Card, $209.99, $3.99 EVGA, EVGA GeForce GTX 1660 BLACK GAMING Video Card| 06G-P4-1160-KR| 6GB GDDR5| Single Fan, $209.99, $0.00 . . .",True
@paulsalmon3056,2019-12-14T08:28:08Z,0,"Loved this video, clear and straight to the point, was able to make my first web scraping program",True
@kkevinluke,2019-12-14T07:12:10Z,0,Amazing !!!,True
@johntobin9404,2019-12-09T06:10:24Z,0,"That was great. Great work. I'm a beginner in Python, so it was a little bit advanced for me. If I could give one tip, it would be that as you get further and further into the details of the script, it's rather easy to lose the plot, that is, what we're doing, what we've done, and what we still have to do, so it wouldn't be any harm at certain stages to recap for the audience, saying where we are and what we did and still have to do. Just an audience thing to keep people with you. Apart from that, excellent tutorial.",True
@jefferst123,2019-12-08T20:48:28Z,0,"first big thx bro very accurate tutorial     but i have an issue i can write my csv no prob grab all datas i needed no prob   but excel (office 2016) is not doing his job i mean he only uses column A     i had to play alot around (went scrapping amazon)     so my code is kinda different :     import requests from bs4 import BeautifulSoup  # url my_url = ""https://www.amazon.fr/s?bbn=429879031&rh=n%3A340858031%2Cn%3A%21340859031%2Cn%3A429879031%2Cp_36%3A9733302031&dc&fst=as%3Aoff&qid=1575829938&rnid=9733298031&ref=lp_429879031_nr_p_36_3""  # user-agent headers = {     ""User-Agent"": 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0'}  filename = ""Pcportable4.csv"" f = open(filename, ""w"")   page = requests.get(my_url, headers=headers) soup = BeautifulSoup(page.content, 'html.parser') names = soup.findAll(     ""span"", {""class"": ""a-size-medium a-color-base a-text-normal""}) prices = soup.findAll(""span"", {""class"": ""a-price-whole""})  for price in prices:     for name in names:         print(price.text + "", "" + name.text.replace("","", ""|""))         f.write(price.text + "", "" + name.text.replace("","", ""|"") + ""\n"")  f.close()",True
@lloydkira,2019-12-08T11:33:26Z,0,Awesome tutorial man! I'm new to web scraping and I must say boy this is worth the watch!,True
@abilovestotrade,2019-12-07T15:35:43Z,1,I found this in 2019 and it's awesome!  thank you <3,True
@jyotikumari-st7yx,2019-12-06T13:10:45Z,0,very useful video....thanku,True
@odds87,2019-12-05T21:23:53Z,0,why not just use a web crawler? can do all this with some very minor configuration and one button click,True
@Tocy777isback0414,2019-12-05T13:15:00Z,205,"It's weird to think about it like that, but this video started my whole Python learning back in 2017 and I am SO SO SO much thankful for it.",True
@alexanderbarrera9906,2019-12-04T20:02:51Z,0,Hands down one of the best CS tutorials on this god forsaken website. I will subscribe and continue to watch :D,True
@LordOlegg,2019-12-04T09:32:32Z,0,Ty!,True
@sarthak7413,2019-12-01T07:41:51Z,0,How can I scrap all reviews brandwise from Amazon page?,True
@ethanmofokeng4792,2019-11-29T05:03:38Z,0,Where is the documentation for this?,True
@2580mia,2019-11-28T14:47:16Z,0,"YOU ARE THE BEST, THANK YOU SO MUCH",True
@WestSideLausanne1,2019-11-24T16:38:02Z,3,"when I try to follow, it gives me the following error message:  brand = container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'a'",True
@MrZH6,2019-11-21T23:07:36Z,0,Awesome! Thank you for this video.,True
@SquaredbyX,2019-11-19T14:25:00Z,0,"Soup, WIne and homebrew",True
@tranngocthuong1594,2019-11-17T15:40:12Z,0,"i'm newbie. please help me how to fix ' line 216, in _init_     % "","".join(features)) bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lzd.parser. Do you need to install  ' . I tried to install the parser but failed. I am using python version 3.6. so thanks you!",True
@Autombot,2019-11-16T08:39:04Z,1,You can create web scrapper with Autom https://youtu.be/ltSXCsqdfYk,True
@vikalpmehta6019,2019-11-15T18:55:31Z,0,Great MAn!!,True
@pranavkhatri9564,2019-11-15T16:35:54Z,0,I am using python3 and I am getting error urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)>,True
@rickyricardo75,2019-11-14T05:07:02Z,1,is there a way to download a channel playlist with the names of the videos in a txt or xls file (WITHOUT downloading the video itself)?. I believe 'youtube-dl' doesn't do this,True
@emadkamel1961,2019-11-13T21:04:44Z,0,"if I get stuck, would you be able to guide me please?             { as i am really stuck with my code  :)        }",True
@binyameen4514,2019-11-13T11:46:42Z,0,You are perfect.,True
@pradhumnapancholi4350,2019-11-09T20:16:25Z,0,"Never actually used python just before just learned basic syntax. Had a few obstacles, but luckily we have stack overflow. I can't believe that I am saying this but the tutorial was better than the one by Brad Traversey.   Keep it up.   Can you make a tutorial about django rest api?",True
@shashankshankar4021,2019-11-09T15:03:42Z,0,What a legend!,True
@alexzhang5816,2019-11-08T23:38:43Z,0,"Can someone please help me, my one only returns a single result, it doesn't loop through all containers?",True
@mehmetyersel449,2019-11-08T16:07:50Z,1,Great video! Thank you for sparing the time and explaining in such a straight forward delivery.,True
@frozy3155,2019-11-07T00:45:28Z,3,"wow even almost 3 years later this video helped me so much and helped me to make a program that picks a random steam game, this was so hard, but i figured it out, big props to you and this video <3",True
@Pulits,2019-11-06T05:35:17Z,2,"I did a Web Scraper not so long ago with another set of tools. This video has motivated to create one, too!",True
@shyam15287,2019-11-06T03:36:48Z,0,hello sir the video content is very useful update the video content for the updated webiste now the existing video content is  very time consuming thank you for the useful content,True
@jayph9649,2019-11-02T03:38:56Z,0,"Super tutorial, you go straight to the point ! üëç",True
@Eltopshottah,2019-11-02T02:19:52Z,0,"File ""newwebscrapper.py"", line 36, in <module>     brand = make_rating_sp[0].img[""title""].title() TypeError: 'NoneType' object is not subscriptable",True
@lawesosah9214,2019-10-31T18:43:25Z,0,Great video!!!! Really useful. It would be Awesome if you could make an updated one for 2019. A lot of people would appreciate that(trust me),True
@learnmandarinwithkaili1102,2019-10-30T07:48:09Z,14,"When I watched this tutorial, it seemed easy to scrap until I stuck a thousand times while actually scrapping a webpage. Happy Coding for dummies lollll",True
@gamestv4875,2019-10-28T00:28:15Z,0,"This is the closest I will get to hacking ...lol Nice tut , thanks",True
@trashman6024,2019-10-27T03:52:41Z,0,"anyone have finished working code willing to post for this example? Mine is only pulling back 1 graphics card =/   from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup  my_url = 'https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20cards' # opening up connection, grabbing the page uClient = uReq(my_url) # off loads the contents into a variable page_html = uClient.read() # close the file uClient.close() #html parsing page_soup = soup(page_html, ""html.parser"") #Grabs each product containers = page_soup.find_all(""div"",{""class"":""item-container""}) divWithInfo = containers[0].find(""div"",""item-info"")  filename = ""products.csv"" f = open(filename, ""w"")  headers = ""brand, product_name, shipping\n"" f.write(headers)  for container in containers:   brand = divWithInfo.div.a.img[""title""]   title_container = container.find_all(""a"",{""class"":""item-title""})  product_name = title_container[0].text   shipping_container = container.find_all(""li"",{""class"":""price-ship""})  shipping = shipping_container[0].text.strip()   print (""brand: "" + brand + ""\n"")  print (""product_name: "" + product_name + ""\n"")  print (""shipping: "" + shipping + ""\n"")   f.write(brand + "", "" + product_name.replace("","", ""|"") + "", "" + shipping + ""\n"")    f.close()",True
@JanikaHill,2019-10-26T18:18:05Z,0,Hello from Vietnam!,True
@hungnguyenngoc3069,2019-10-25T16:32:56Z,0,Thanks very much,True
@stefancondriea8195,2019-10-24T22:07:17Z,0,How can i use web scraping in Android Studio?,True
@marwahg,2019-10-23T01:02:57Z,0,"<option data-formated='&lt;span class=""price""&gt;AUD $100.08&lt;/span&gt;' data-qtyid=""qty-219"" value=""1"">                                         Unit Price                              </option>   how do i get AUD $100.08 out of this?",True
@bartekpoznan,2019-10-22T22:10:21Z,0,Can You do price of gc - because have only sheeping costs ;),True
@suwarnachoudhary8591,2019-10-20T14:48:04Z,0,Loved the tutorial. Thank you üôÇ,True
@susanwjh,2019-10-17T02:16:48Z,1,the findALL syntax is updated to find_all.   findAll -> find_all Source:Beautiful Soup Documentation https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree,True
@happycakes1946,2019-10-15T04:25:24Z,0,"In case anyone gets stuck at the container.div part, other comments have addressed this but here is a solution that is current. instead do divWithInfo = containers[0].find(""div"",""item-info""). This will create the object you need. Afterwards you can work you're way toward the goodies with the following example: divWithInfo.div.a.img['title']",True
@notHere8801,2019-10-14T02:28:18Z,0,"Doesn't work, I've typed this into cmd   pip install bs4   and  I get this error:   python: can't open file 'pip': [Errno 2] No such file or directory",True
@loy.3826,2019-10-13T11:32:29Z,0,"Hey, do you know how I can make this work on sites with Cloudflare DDoS Protection? Because when I run it, I just get the HTML code for the redirecting CloudFlare site.",True
@AniketRajAniketkno,2019-10-13T06:10:43Z,0,I love how passionate you are about this and how clear it is to understand your explanation. I WANTS IT ALL! GIMME! Lol Awesome video!,True
@quentincaldway,2019-10-13T05:00:46Z,0,This is ridiculously awesome bro! Love your energy. I def appreciate you making this video. I'm addicted to you channel now.,True
@schlongmasterlol2724,2019-10-13T02:27:23Z,7,16:04 Command for it on windows is  CTRL + SHIFT + P :),True
@chandrimaroy8919,2019-10-11T07:20:15Z,0,Great video but What to do if multiple pages are there,True
@smartaitechnologies7612,2019-10-10T20:40:07Z,1,very helpful try to do more this gon help everyone.,True
@darshandhabale143,2019-10-09T14:03:55Z,1,I didn't like it...I loved it Clear and concise ‚ù§Ô∏è,True
@ilyamaldini,2019-10-09T12:35:46Z,0,The best tutorial. Thank you. Much better than all videos in russian lang,True
@tjmarx,2019-10-07T00:09:05Z,0,This is way better made than the newer videos in R. Can you please make an new video like this expanding on the concept? Perhaps a script that scraps the data then turns it into a new html document?,True
@chriswashingtonbeats,2019-10-06T09:38:20Z,2,the first div that it showed was item badges how do i navigate to different divs?,True
@jonathangzzben,2019-10-05T22:52:04Z,0,"Wowowow awesome video, I thought learning web scraping might take me a week or so, and thanks to you I learned it in 30 minutes",True
@fuad471,2019-10-02T14:48:18Z,0,I suggest to make video with pandas and API.,True
@felipeazank3134,2019-10-01T17:31:02Z,0,I love u bro,True
@brandonklapholz3433,2019-09-29T08:39:41Z,0,This is great,True
@davidlawal24,2019-09-25T22:59:14Z,1,hey man love the video its really helping for school project but does this work in real time,True
@leen5085,2019-09-24T02:06:48Z,0,fy,True
@blackalk9420,2019-09-19T18:17:51Z,6,"def Data Science Dojo():     Data Science Dojo = (""like"", ""share"", ""sub"")     good job = (input.comment(""Thanks you very much ! "")) if good job in Data Science Dojo :      print(""love and respect from Kuwait"") else:     print(""sorry maybe next time"")    Data Science Dojo()           -------   Output :- peace out and happy basic coding :D",True
@saswatnanda3481,2019-09-19T15:51:37Z,0,He sounds like Tom Holland. Great video dude it's the coolest web spider tutorial!,True
@haxxorlord7327,2019-09-18T20:17:29Z,5,"this soup is very beautiful, goddamn",True
@IllegalAlien51,2019-09-18T18:16:57Z,0,MKAY?,True
@jakaseptiadi9845,2019-09-18T11:39:10Z,0,How to scrape data on multiple page (next page)?,True
@maaske,2019-09-17T23:49:14Z,0,"Extremely helpful video!!    Would it be possible to make this program repeat every ""x"" time interval (ex. every 5 seconds) and write over the old document? where would someone start who was looking to do this? Basically every x interval it would be looking for new products. If a new graphics card came to the list, it would then update.   Hope this makes some sense",True
@shaikhmohij206,2019-09-17T15:24:16Z,0,Thnx for the beautiful video this is help me a lot even i am b.com student but with the help of this video i did what i want.. Thnku so much bro..... ‚ù§Ô∏è,True
@chetanyat4632,2019-09-17T06:35:08Z,0,"You made it so easy to understand everything. Thank you so much, We need teachers like you really. Appreciate your quality of work. Keep Going and teaching and we will keep learning :D",True
@aaaaaaaall4243,2019-09-15T12:20:52Z,0,"Loved the video! I can‚Äôt tell you how much it helped me.  I am a total noob to all this in general, so I apologize if the question is silly.  How did you get the program to run so the data went to excel? I keep getting a syntax error",True
@walete,2019-09-12T03:56:50Z,0,"such an amazing video, thank you so much!",True
@dasubxxx,2019-09-10T07:55:55Z,1,"Good job man, very helpful",True
@raajkumar-zg2rs,2019-09-08T09:43:22Z,0,Awesome man,True
@nirmalpatel6543,2019-09-06T12:25:30Z,0,I like it ;,True
@gosinP,2019-09-05T19:36:20Z,0,"My jaw is on the floor O.O  I could not imagine that things like this are possible to do with Python or with any other program.  This is awesome. It is so fucking useful.  Gosh!  I want to learn these things.  Great skill.  Thank you for this video, for me is like eye-opening information.  I will subscribe to your channel.  :D",True
@saffronyoutubeshorts,2019-09-04T06:24:20Z,0,"i would love to do this, it‚Äôs super useful to me right now, but is this legal?",True
@kai2955,2019-08-31T20:48:39Z,0,"So when i input python in the cmd, I get a message saying that the python interpreter is in a conda environment, but it isn't active. I went to look at the environment info but found nothing. I tried to follow the tutorial without fixing the issue, but I can't even install the pip bs4 thingy so idk what to do",True
@jayexum8479,2019-08-31T01:19:58Z,1,"when I run page_soup.h1, I get no error, but I also get no answer (this is for a different page than the one in the video).  What do I do now?",True
@ianjohnston8057,2019-08-25T04:36:09Z,0,"This is awesome. Thanks, man. I really enjoy your style of teaching.",True
@AcquahEmmanuelBaiden,2019-08-21T22:24:09Z,0,Great tutorial. Right to the point,True
@miaomiao2454,2019-08-21T20:35:22Z,0,Can I use this to scrape business info from Yelp for all California restaurants? Or do you recommend using API?,True
@SinaDibaji,2019-08-21T19:34:09Z,0,Great tutorial. Wish you've used Jupyter Notebook instead of the command line. It would be easier to follow along.,True
@felixamey,2019-08-18T22:07:19Z,0,"ashop.findAll(""p"", {""class"":""adr""}) [<p class=""adr""><span class=""street-address"">9205 Skillman St Ste 134</span><span class=""locality"">Dallas,¬†</span><span>TX</span>¬†<span>75243</span></p>] >>> how do i grab the city, state and zip from this p tag , Dallas , TX 75243 to place in separate columns  i was able to grab the street with your method using   ashop.findAll(""span"", {""class"":""street-address""})   but also need city, state and zip from this p tag. thanks",True
@johanneszwilling,2019-08-18T06:01:48Z,0,üòé Straight to the point! Very nice!,True
@Lastrevio,2019-08-14T12:21:59Z,0,thank you,True
@nubiec7402,2019-08-13T15:59:19Z,1,"Excellent tutorial. Thank you so much. I profite to ask for helping: when you are on amazon of 1 products page , and you want to scrap page 2 , 3, 4, etc, is there a code to scrap page 2, 3, 4, etc?? or do you have to scrape one by one?... thank you for your help!",True
@Jack258jfisodjfjc,2019-08-13T03:05:58Z,89,you look like a god when your writing multiple lines at the same time.,True
@Jack258jfisodjfjc,2019-08-13T03:05:14Z,0,your such a great teacher! Just because you can code doesn't mean you can teach. Awesome!,True
@durat,2019-08-10T23:16:36Z,1,"it the website updates the prices, only need to run again the scrapper or not?",True
@nikolaidimitrov7251,2019-08-10T10:41:56Z,0,Please help cant exit the console with control + c,True
@robinkempes9100,2019-08-10T05:59:40Z,1,"Thank you so much for this good sir, this was excellent!",True
@TaskSwitcherify,2019-08-09T03:17:59Z,0,"Is there a better way to find a specific item than Find / Find All? Searching text this way isn't efficient, especially if it's already been processed and extracted into an array or a list. Is there a direct way to get at each element, e.g. by the [""TagType""] or another indexed way? Also, you should show how to extract the price :-) Finally, there's no word ""deliminated"". Thanks!",True
@SS-uv5kj,2019-08-08T17:05:49Z,1,"Hi, how can we do it for multiple pages? Suppose what if there are like a 10-20 pages ?",True
@BritniSpirs-tc6dq,2019-08-07T13:48:07Z,0,"Thank you very much, this was a great and useful video!",True
@Rohit-nb8nf,2019-08-06T22:51:03Z,0,"I am getting many errors at the beginning itself from uReq(url) ,urllib error ,,,",True
@denisshmelev4990,2019-08-06T19:45:50Z,0,"I thought web scraping was hard until I found your video. Huge thanks man, you saved so much time for me!",True
@Datasciencedojo,2019-08-06T19:16:55Z,224,"Table of Contents: 0:00   -   Introduction 1:28   -   Setting up Anaconda 3:00   -   Installing Beautiful Soup 3:43   -   Setting up urllib 6:07   -   Retrieving the Web Page 10:47 -   Evaluating Web Page 11:27 -   Converting Listings into Line Items  16:13 -   Using jsbeautiful  16:31 -   Reading Raw HTML for Items to Scrape 18:34 -   Building the Scraper 22:11 -   Using the ""findAll"" Function 27:26 -   Testing the Scraper 29:07 -   Creating the .csv File 32:18 -   End Result",True
@matthieuboucher143,2019-08-06T10:25:11Z,0,"Hi, this is really cool! Absolute Legend :D  My_request > I would love to see a tutorial on how to scrape hotel prices for London for example. I would also like to know how to loop through dates in order to show seasonality patterns in the data.",True
@duncantse7528,2019-08-06T07:38:47Z,0,"well presented ! [1]  if code could be downloaded, it would be wonderful.  [2]  if Chinese character be read as it is read on the screen.  If not, what codes be added.",True
@adarshgoldar,2019-08-06T06:59:53Z,0,"I need to scrape data from different websites so how I can I do that in this script. Also either any method to pass a text document to the ""my_url =  "" So I can put all the websites in that text document and then pass it to the my_url variable to scrap from those sites and store in a csv file.",True
@harikiran2963,2019-08-05T18:22:18Z,0,awesome intro @dojo,True
@jeroenstrompf5064,2019-08-05T16:07:19Z,0,"Thank you! Your video helped me greatly on my way to learn webscraping. I analysed your tutorial in depth on my wiki: http://wiki.devliegendebrigade.nl/Webscraping#Voorbeeld_NewEgg. In the end, this is the code that I used:  #! /usr/bin/python3 # # Newegg webcrawling-example - Compact ################################################################### # # Load libraries ################################################################### # from bs4 import BeautifulSoup import requests   # Fetch webpage ################################################################### # url = 'https://www.newegg.com/global/nl-en/p/pl?d=graphics+card' p_html = requests.get(url).text p_soup = BeautifulSoup(p_html, ""html.parser"")   # Process webpage ################################################################### # cs = p_soup.findAll(""div"",{""class"":""item-container""})  i=0 for c in cs:     i=i+1     print("""")     print(i)      if (c.find(class_=""item-brand"") is not None):      c_brand = c.find(class_=""item-brand"").img['alt']      print (""Brand: ""+c_brand)      if (c.find(class_=""item-title"") is not None):       c_name = c.find(class_=""item-title"").text       print(""Name: ""+c_name)      if (c.find(class_=""price-current"") is not None):       c_price = c.find(class_=""price-current"").strong.text       print (""Price: ""+c_price)",True
@themirestudios3326,2019-08-05T15:24:31Z,1,"Amazing video, very clear and easy to follow. Best intro ive come across",True
@OnceIcaughtafiash,2019-08-02T16:38:11Z,0,Great video! Is there a list of other videos done by this guy?,True
@maanavdoshi3100,2019-08-02T12:11:16Z,0,Excellent Video! Please keep making such content.,True
@priyankajindal4979,2019-08-01T14:11:25Z,0,thank you.,True
@insigpilot,2019-07-30T21:39:54Z,0,This was very good. I'm a beginner to Python and this webscraping tutorial left me with very little questions.,True
@RameshPatil-mr6rl,2019-07-29T05:19:59Z,0,How to copy certain contents(it might be image) with formats of HTML to docx?,True
@RameshPatil-mr6rl,2019-07-29T05:19:44Z,0,How to copy certain contents(it might be image) with formats of HTML to docx?,True
@alessandroqueiroz6339,2019-07-26T18:23:25Z,0,Really nice video my friend! Thanks a lot,True
@khaleddawood2282,2019-07-24T20:04:39Z,0,"thanks dude , that was a really nice tutorial  (Y)",True
@Orokusaki1986,2019-07-24T11:28:26Z,5,"Just use pycharm, man :-P",True
@shilpapatil1012,2019-07-24T06:05:18Z,0,Below is also a great article on Web Scraping using Python & Beautiful Soup and what we can do with scrapped data. Complete Source Code is also provided. https://www.opencodez.com/web-development/web-scraping-using-beautiful-soup-part-1.htm,True
@pensamentosimpuros4945,2019-07-20T14:41:44Z,0,"Since u installed anaconda, Y dont u use one of the  many IDE¬¥s avaliable there ?? I guess spyder is the most conftable in the set...",True
@pelumiobasa3104,2019-07-19T17:02:38Z,0,when I tried to do uclient.read it came out as an incomplete read . what do I do,True
@ivana1755,2019-07-18T04:46:23Z,0,"So I'm using ""containers = page_soup.find_all(""div"", class_='modals-container') and no matter which synthaxis I use, it's always giving me zero. Any solution?",True
@doramyers9723,2019-07-17T22:42:03Z,1,"I've successfully been able to set up my web scrape but my only problem is that every time I run it, it dumps the data into my excel sheet a million times. As the code runs on my command prompt it shows that the data is collected only once, so I don't see it it's collected numerous times on the excel sheet... Please help!",True
@mostafaali3769,2019-07-17T20:48:33Z,0,How can someone reach out to you? Like a DM or something??,True
@nikoaz,2019-07-17T02:49:55Z,2,"Thank you for the video. I followed along but ran into issues when attempting this on another site. I found a dataset of containers I wanted to use, but when I ran len(containers) it returned 0. This was trying to use findAll. I tried to to page_soup.div.div.div and navigate down to the containers with no such luck. I was wondering what I would need to do different on this site: https://www.twitch.tv/speedgaming/videos?filter=archives&sort=time",True
@mgmartin51,2019-07-16T16:24:06Z,1,Boy is this ever clear. Very straighforward presentation!,True
@obscuredsweetness,2019-07-16T11:59:40Z,0,Awesome tutorial,True
@hareebollu2379,2019-07-15T10:04:05Z,1,"i take its an example ,when iam doing it showing len(containers)is zero . can u pls  give me the solution",True
@davidscutar1403,2019-07-14T11:21:23Z,0,IT WAS SO FUCKING FRUSTRATING. I did it in Sublime text and anaconda just like he. And it didn't fucking work. i tried it in Spyder and it worked!,True
@bluebellhs,2019-07-14T01:27:53Z,0,"Thank you so much, you're an excellent teacher!",True
@rockieRAGE117,2019-07-13T22:48:28Z,0,Can someone post their final script code please? I got lost because the code doesn't work anymore in this video.,True
@salahaddin2009,2019-07-13T15:10:15Z,0,"Guys, its called windows power shell on newer laptops now.",True
@falconfarmscostco3436,2019-07-12T03:06:44Z,0,"Thanks, your video help me a lot.",True
@joelkaruri3069,2019-07-11T22:06:44Z,0,"Amazing stuff man. Very well explained. Do a tutorial on parsing xml and text files in python? That should be really helpful too, especially where we have to analyze log files for instance",True
@ScreamingEagle,2019-07-05T04:14:43Z,0,Hi would you recommend using urllib or requests?,True
@diongkc,2019-07-02T13:00:10Z,0,"Hi, how do I get product url and image url?",True
@expertojordigg,2019-07-02T07:25:22Z,0,Take a shot everytime he says Ok,True
@rupertrussell1,2019-06-28T22:32:12Z,2,Fantastic tutorial! gave me 95% of what I needed for my first screen scraping project.,True
@NaveenkumarYadav-sg8yh,2019-06-28T11:20:36Z,0,"Excellent Explain as compare to other. Hope through this channel , my concept on Data science will clear shortly. And only this channel where i got good concept on R .",True
@nxs_toxic6998,2019-06-27T20:08:27Z,0,Wait for what is that...Is that ilegal,True
@JB-in4dj,2019-06-25T05:11:13Z,0,"for container in containers:     brand = container.div.a.img[""title""]     title_container = container.findAll(""a"", {""class:"": ""item-title""})     product_name = title_container[0].text.strip   when I run this I get an error saying ""IndexError: list index out of range."" Do you have any idea why this is happening?",True
@harshmehta7773,2019-06-24T15:59:45Z,1,"brand = container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'a'    Can anybody help me solve this error?  Thanking you in anticipation.",True
@cosminmavrichi1142,2019-06-24T14:27:39Z,0,"Damn your tutorial was very good. Helped me make a CSV for all apartments in the area i want to move that accepts dogs. As a side note I had a problem with the open file, if it gives you error about encoding or something about char add this to the ""open()"": open(filename, ""w"", encoding='utf-8') . You need the encoding part and its all good.",True
@TeddyTheBigTeddyBear,2019-06-24T01:23:05Z,0,This is more helpful if you guys want a written code to follow when scraping https://websitescrapingtutorials.wordpress.com/2019/06/23/how-to-scrape-yelp/,True
@sharde26,2019-06-23T15:53:27Z,0,This is so awesome! I feel powerful on the internet :P,True
@XRXONE,2019-06-21T07:45:55Z,0,What if i don‚Äôt have excel ? Is their another alternative I can use? Thank you,True
@andreabtahi9519,2019-06-20T23:04:11Z,1,I am just starting web scrapping and I can honestly say that this video clearly explained everything. I watched this at 1.5 speed and it made sense. I would love more videos like this. I loved how you made it generic so it can apply to more than one website!,True
@pradhyumsharma1710,2019-06-20T21:10:18Z,0,You are a great teacher I believe I was not much not into data Science  but after watching your video it made it simple and easy. Thank you I took 100% from it. :) requesting more videos from you..,True
@bubbert,2019-06-18T12:17:20Z,0,Does the command line have to be in a particular folder? Or must it be in the same directory as the IDE and text interprators?,True
@networkfreddy2000,2019-06-18T11:23:41Z,0,Is there a reason to use BS4 instead of Scrapy?,True
@vthamilventhan,2019-06-16T15:02:31Z,0,"great tutorial, can you please help me how to get the data ( corn and oil relative strengths)from this web site https://finviz.com/futures.ashx? I greatly appreciate",True
@dubey_ji,2019-06-16T10:50:13Z,0,Thank you now I can start with with scraping,True
@chaos299909,2019-06-15T08:08:01Z,0,I wrote this script using python/ BeautifulSoup. Scrape any crypto currency historical data by simply putting in the name. :D https://github.com/gitFaisal/crypto_currency_scraper,True
@mzaphod64,2019-06-14T01:53:53Z,0,i love you man! I Loove Yoooyyy thank you so much!,True
@jahangiralam8206,2019-06-11T17:09:25Z,0,"Excellent, the video is really very amusing and most importantly the way you code is very excellent. I just fest that coding is very joyous if we can code the way u are doing. Really liked it and thank u a lot.",True
@ChrisShepperd,2019-06-09T13:12:19Z,0,This is very helpful.  You make this technical information easy to understand.  Thank you very much.,True
@alexisparedes1805,2019-06-08T05:50:09Z,0,like wtf... i dont understand shit,True
@fruitpooplooper150,2019-06-05T06:54:42Z,2,This guy sounds drunk as hell on 3/4 speed.  great stuff tho,True
@carlosmatosfanpage2856,2019-06-04T21:42:09Z,1,Hello. How do I put the results of web scraping on a website using something like Django?,True
@SofonToSafes,2019-06-04T19:29:47Z,0,Excellent work!  Where can I find the code please?  Thanks a lot!!!!,True
@andresfernandoasfg,2019-06-04T17:33:03Z,0,"Hello EveryBody why some page gives 404 Error Like this:  uClient=uReq(my_url) Traceback (most recent call last):   File ""<stdin>"", line 2, in <module>   File ""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 222, in urlopen     return opener.open(url, data, timeout)   File ""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 531, in open     response = meth(req, response)   File ""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 641, in http_response     'http', request, response, code, msg, hdrs)   File ""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 569, in error     return self._call_chain(*args)   File ""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 503, in _call_chain     result = func(*args)   File ""C:\ProgramData\Anaconda3\lib\urllib\request.py"", line 649, in http_error_default     raise HTTPError(req.full_url, code, msg, hdrs, fp) urllib.error.HTTPError: HTTP Error 404: Not Found",True
@andresfernandoasfg,2019-06-03T21:41:06Z,0,html5lib and html_parser What are the differences? Thanks in advance.,True
@night23412,2019-05-30T12:32:50Z,0,Absolutely brilliant! Thanks so much!,True
@freerapalbums025,2019-05-28T22:17:06Z,0,"I've installed Anaconda and when I open the cmd and type ""python"" it says unknown command...",True
@CapoeiraRecio,2019-05-28T16:50:50Z,0,I¬¥ve been watching your video and trying to replicate it but the problem is that the web page has a different estructure now. There is a <div> inside that first <a> so when i type container.div it does not go to the <div> below that <a> but rather shows the <div> inside that first <a> how can I tell python to show me the <div> outside that first <a> ? Thanks!,True
@konstantinvogel2362,2019-05-27T16:07:01Z,0,mr garrison mkay,True
@igor_L7,2019-05-27T03:30:07Z,0,"Awesome! Thank you! Please, more videos about scraping.)",True
@jeremycheong8036,2019-05-27T01:07:44Z,0,Thanks for making it fun!,True
@aaryan3461,2019-05-26T16:44:41Z,1,I'm at soup.,True
@jakaseptiadi1752,2019-05-25T06:58:12Z,0,I have a question. How to web Scraping with a lot of pagination? Please give me a solutions. Thanks.,True
@kenshinnanashi9469,2019-05-24T16:53:19Z,0,great vid,True
@1LOUISIS,2019-05-23T22:33:29Z,0,No need for terminal in sublime ctr+b with run it like the terminal,True
@paulbrooks5612,2019-05-22T20:17:34Z,0,This video could have been <5 minutes were it not for pointless chatter,True
@arthurhq9487,2019-05-16T19:20:30Z,0,Does it works for Dynamic web?,True
@raduulea591,2019-05-16T10:18:28Z,0,"Hello, am I the only one blocked by ""incapsula Robot"" at page_soup.h1 ?",True
@jdpbernal,2019-05-12T19:12:13Z,0,This is incredible! Thank you so much. I tweaked your code a bit because I guess newegg has changed their website from the time you created this video.,True
@annapeterson1906,2019-05-10T19:25:02Z,0,"Thank you! If someone is scraping another website that won't give permission, check here and try using the correct answer: https://stackoverflow.com/questions/44865673/access-denied-while-scraping",True
@AshishPatel-kn3kc,2019-05-09T16:24:12Z,0,"It was really great video, I am python beginner and it helped me a lot. But I would like to know how to scrap google reviews of places. It will be great if you post some video on that. Will be really appreciated.",True
@-Night-Is-Dark,2019-05-09T08:00:47Z,0,Thank you for great content!,True
@nelsongg347,2019-05-07T20:54:28Z,0,"Hi, could anyone give me a hand on this? this line: brand = container.div.div.a.img['title'], throws this error:  Traceback (most recent call last):   File ""C:/Users/nelso/PycharmProjects/Scrapping/Scrap001.py"", line 23, in <module>     brand = container.div.div.a.img['title'] AttributeError: 'NoneType' object has no attribute 'a'",True
@inadiv,2019-05-03T22:35:23Z,0,"thanks for putting together, I didn't run in anaconda, just regular python 3.7  and it worked fine",True
@rubencalzacorta879,2019-04-30T10:11:10Z,0,awesome!. had to make my way around a couple of modifications of the website but really good!.  just finished my first web scrap!!!,True
@reeyadutta4969,2019-04-29T20:02:22Z,0,"i am getting this error!! Traceback (most recent call last):   File """", line 41     print(brand:  + brand)     ^ IndentationError: unexpected indent",True
@brennanbugbee,2019-04-28T03:35:11Z,0,This is great thank you,True
@HighClasAsian,2019-04-27T17:37:53Z,0,Im very lost at 19:00 container.div.div.a does not work for me PLEASE HELP!!! container.div gives me item-badges,True
@delt19,2019-04-26T16:36:32Z,8,"Coming from an R user, this is a very well done introductory tutorial into web scraping in Python. I like the real world example with Newegg and troubleshooting along the way.",True
@miguelcastillo7465,2019-04-22T15:57:33Z,0,AWESOME TUTORIAL! Has all the information needed to start web scraping,True
@priyankabhakuni2546,2019-04-20T03:52:09Z,0,Thank you so much for this video. This really helped me to go in the right direction. :),True
@thedivyahuja,2019-04-14T08:24:24Z,0,This is amazing!,True
@jn14624,2019-04-12T22:46:01Z,1,"Having problems in this line brand = container.div.div.a.img[""title""] it comes back with the following error Traceback (most recent call last):   File ""/home/john/Documents/DATA ANALYSIS/video_webscrap.py"", line 33, in <module>     brand = container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'a'   Thank you for your input!",True
@matthewthehuman1744,2019-04-11T07:22:34Z,0,"Good video, thank you",True
@samyogsubedi7677,2019-04-11T02:28:55Z,0,Beautiful !!,True
@yourbrother9518,2019-04-10T21:31:10Z,0,Wow! Amazing trick. Kindly upload more like this.,True
@tforbes1350,2019-04-10T12:50:21Z,0,"While following along step-by-step (in python shell); I get an error when I run the line that says ""uClient = uReq(my_url).   I can't seem to get past that point.  I get a message that says ""Traceback (most recent call last):"" and then proceeds with line after line of what looks like commands from the urllib.  I'm new to this, so any help in pointing me in the right direction of what might be happening would be appreciated.",True
@devendravijay1303,2019-04-09T08:20:10Z,0,"One of the best teacher I have come across Youtube. Web Scraping explained so well that even a layman can follow and understand the basic concepts. I wish, in life I had a teacher/mentor/friend like the one teaching in this video.",True
@sameast6008,2019-04-06T17:31:01Z,0,"Hello! I'm trying to scrap a website to collect video links I've followed upto 14:30 of video. i'm having difficulties in using the findAll() or find_all() methords. Inside the html there are these          <div _ngcontent-c8 class = ""class name example"" ......>  and         <a _ngcontent-c8 class = ""class name example""......>    tags so if i try to use the    container = page_soup.findAll(""div"",{""_ngcontent-c8 class"":""class name example""})  or container = page_soup.findAll(""div"",{""class"":""class name example""})   it returns me an empty list. i've tried using page_soup.find_all() tags as well but it all returns the same empty list.   KINDLY IF ANYONE KNOWS HOW TO RESOLVE THIS PLEASE HELP ME!",True
@felicciasc,2019-04-05T11:41:40Z,0,"I looked and findAll is exactly the same as find_all, with find_all being naming convention compliant for python 3.",True
@saradakrory,2019-04-04T14:38:22Z,0,"unfortunately i found this error when calling the html.parser. the error is "" bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested:  html.parser. Do you need to install a parser library?: how to solve this error????",True
@HeaanLasai,2019-04-03T20:06:52Z,0,Thank you for making this tutorial! Really!,True
@rameshpoly,2019-04-03T07:24:20Z,0,"very nice explanation, it is very useful to me",True
@praveensingh5922,2019-04-02T18:51:12Z,0,"Hi, could u plz make one video to explain data scraping from different page of same website?  Thanks in advance.",True
@saadiyafourie,2019-04-01T04:12:52Z,33,"Absolute champion, quite possibly the best code tutorial I've ever watched. Oh the possibilities! Thank you :)",True
@rajesh.singha,2019-03-29T10:52:06Z,0,awesome video. keep doing the great work.,True
@shaiganofficial,2019-03-28T21:33:16Z,0,"You explained amazingly, keep creating more content",True
@imrulhasan195,2019-03-28T17:28:23Z,0,I am having this problem. Can anyone explain why ?  https://prnt.sc/n49eqq,True
@cosmic_voyager_,2019-03-26T19:52:19Z,2,‚ù§Ô∏èlove. This is my first ever seen video on web scraping and you did an awesome job explaining it to me..,True
@lucccar,2019-03-26T19:30:52Z,0,"really good tutorial, hummkay?",True
@sgmerrymoon9903,2019-03-26T18:41:39Z,0,"Hi, do you mind to share how should the codes loop to scrape some contents based on the main category and sub category? Meaning, instead of just a particular url¬†(specific page), I would like to scrape and arrange some data based on the main and sub category when I'm on my general url.",True
@Mario-dx5kh,2019-03-25T18:39:01Z,0,"1. when i shift right click in a folder it says open windows powershell not command window. ????? 2. when I type ""python"" in cmd prompt and windows powershell they both dont work? I have python installed",True
@LeeJiaHe,2019-03-24T14:14:31Z,0,GR8 tutorial!  Could you do a tutorial on how to extract tax and shipping cost data of a product for different shipping destinations? Thanks!!!,True
@AhmedEssam-lv5vn,2019-03-21T11:04:29Z,1,Thank you for that amazing tutorial. What if the web developers changed the classes names? Than I have to rewrite my script or there is an easier way?,True
@brubrudsi,2019-03-19T01:38:54Z,0,Awesome and to the point. Thank you so much,True
@widadbouzida1463,2019-03-18T11:11:01Z,0,thanks !,True
@serkantan2951,2019-03-12T18:17:56Z,0,How did you open the cmd as admin.,True
@cedericbosmans1964,2019-03-11T21:37:40Z,2,"Very useful and to the point, thank you!",True
@TriWaZe,2019-03-10T05:47:23Z,0,"When I try to do a variable.div.a it tells me ""ResultSet object has no attribute 'div'""",True
@harsh3305,2019-03-10T03:47:38Z,726,"MINOR SUGGESTION As of 10/03/2019, If you are following along this tutorial. ""container.div"" won't give you the div with the ""item-info"" class. Instead it will give you the div with the ""item-badges"" class. This is because the latter occurs before the former. When you access any tag with the dot(.) operator, it will just return the first instance of that tag. I had a problem following this along until i figured this out. To solve this just use the ""find()"" method to find exactly the div which contains the information that you want. For e.g.   divWithInfo = containers[0].find(""div"",""item-info"")",True
@KingZero69,2019-03-09T20:14:10Z,1,this guy is super amateur...,True
@KDuubi,2019-03-07T08:41:15Z,0,"Insanely easy to understand, comfortable to listen. Thank you a lot! :)",True
@TriWaZe,2019-03-06T21:26:12Z,0,"What if you had a series of urls you wanted to scrape in a loop? I'm assuming the first step would be putting them all in a list like urls = ['url', 'url', 'url']. But then I'm not really sure what I would do after that.",True
@eggfry9020,2019-03-06T05:25:21Z,0,Fucking Messy tutorial....,True
@mahmoudadel7600,2019-03-05T08:30:34Z,0,Thank You So much  u are awesome <3,True
@evanzhao3887,2019-03-03T23:20:03Z,17,"If you had some prior experiences with web crawling, this video can makes your crawling skills into a whole new level. Allows you to crawl website containing complicated info about multiple items into a very organized dataset. The various tools introduced in the video are also fantastically helpful as well. A BIG THANK YOU",True
@Stormdaklak,2019-03-03T05:54:27Z,0,great,True
@dokken63,2019-03-02T15:43:25Z,0,"thank you my guy, you rock!",True
@gazingstar95,2019-02-28T06:10:32Z,0,"Running container.div returns me weird result:   >>> container.div <div class=""item-badges""> </div>",True
@RAGHAVENDRASINGH17,2019-02-27T10:41:18Z,0,Nice tutorial,True
@ScremoSam1,2019-02-26T22:46:49Z,3,"This has been so useful. Thanks so much. What I need to know now, is how I can get the scraper to continue working when there's a 'Load More' button, which doesn't take you to another page. If anyone knows anything about this please let me know.",True
@muhammadisrarulhaq9052,2019-02-26T20:24:11Z,509,"I was able to make a program for my client i never thought was possible. I got paid real money for this. Blessings so much learned, this is like magic",True
@Crusaders782,2019-02-26T05:54:27Z,0,"great video, learnt it so fast",True
@ykartd,2019-02-25T23:58:54Z,1,"doesn't work with amazon though, error msg says HTTP Error 503: Service Unavailable",True
@vineet5678,2019-02-25T22:19:40Z,2,"this is gold , very informative video and easy to understand",True
@launo8210,2019-02-23T09:45:07Z,0,Great and clear tutorial for beginners. Cheers!,True
@elpintor2091,2019-02-23T04:05:05Z,0,Wow you such a way of teaching that just made something intimidating look so easy. Thank you sir XOXO.,True
@workthentravel,2019-02-23T01:06:55Z,0,This is so much fun!,True
@seeker7689,2019-02-22T12:48:53Z,0,"SIr, I followed everything you said but everytime I use from urllib.request import urlopen as uReq  - I get a traceback stating ""*No module named request*"". Please help if anyone know how to solve this.",True
@leftrightupdown8134,2019-02-21T18:22:27Z,1,THANK YOU!!,True
@abdullahraihanbhuiyan2346,2019-02-21T15:32:18Z,0,Thanks for this awesome tutorial.,True
@abyssreal1878,2019-02-18T11:27:43Z,0,"hello, found this video very helpful to the most part. Im a complete bot with python and im just learning it. the uClient keeps coming up wit herrors can anyone help?",True
@DannBerg,2019-02-17T01:36:52Z,1,Fan-freaking-tastic tutorial. Finally got this working the way I want it to. Thank you!,True
@feridkymet7488,2019-02-16T23:22:09Z,1,"in 21:51, there is no container variable, how its running?",True
@PunitSoni00,2019-02-16T22:20:10Z,0,Great introduction to web-scraping. Thanks for posting this.,True
@feridkymet7488,2019-02-16T13:08:04Z,0,"its really perfect tutorial, thank you",True
@nomansarwar2927,2019-02-14T18:26:11Z,1,great video we want more,True
@fouratthamri2330,2019-02-14T15:58:21Z,1,Thanks bro! very useful and well explained !,True
@jasperzanjani,2019-02-14T04:20:42Z,0,I ain't watchin any tutorial by a guy named Phuc,True
@bokabosiljcic8694,2019-02-13T23:58:04Z,4,"This was fast, precise and beautiful! By saying beautiful I didn't mean to state the obvious :) Thanks",True
@ryotaichikawa2624,2019-02-13T22:22:15Z,1,Thanks man it helped me a lot!!,True
@natekolker3563,2019-02-13T11:21:48Z,1,"I'm having trouble. I've installed anaconda, but when I access the command prompt and I type 'python' it returns an error. I also cannot find the ""webscrape"" folder you are using, and if we have to make it, why did you place it in downloads? Also, when I shift-right-click it asks me to open the powershell which also returns an error.",True
@monirrad4,2019-02-11T23:51:48Z,0,Great video,True
@arturjancikkran2308,2019-02-09T17:45:19Z,0,23:02 that sound when you do the coma :D Very good tutorial! Thanks a lot!,True
@DincerHoca,2019-02-08T18:37:42Z,1,Thanks for the video. This was the best web scraping tutorial I have seen on youtube.,True
@amroel-sheikh2712,2019-02-07T12:19:41Z,0,"Urgent question! is it possible to click on a ""show text"" button before scrapping data from a given url? how do i do that?! please help =(",True
@esteban578,2019-02-05T22:45:34Z,2,Amazing tutorial just discovered you but take a breath man lol Looking forward to more of your videos,True
@anonyme103,2019-02-05T20:58:19Z,4,This is very well explained and I enjoyed every second of it ! please do more ^^,True
@greypilgrim9967,2019-02-03T17:08:09Z,0,This is probably a foolish question but do anaconda and sublime have to be in the same folder? If anyone could let me know I'd really appreciate.  Also thanks so much for this video Phuc!!,True
@agungsukariman,2019-02-03T16:10:48Z,0,I LOVE YOU MAN ~ ..,True
@rajath1964,2019-02-02T09:19:22Z,0,"I have a webscrapping use case which I need help with -  So, I have a list of blog posts that  I have written on a particular website, I was to scrape them all and put it on my portfolio website (my personal webpage), I want to automate this I mean embed the webscrapping script in my portfolio website's html so that everytime I write a new blog on that site my portfolio webpage gets appended for the list of blogs I have written. How do I go about? I don't think the CSV way works",True
@911Salvage,2019-02-02T06:07:09Z,0,Okay.,True
@Kenny-ly6kk,2019-01-31T05:16:47Z,0,My nigga,True
@andrewnightingale8156,2019-01-30T16:41:33Z,0,Very good stuff! Thank you!,True
@unreachablelevel,2019-01-29T22:56:19Z,0,"Excellent presentation, well done!",True
@Aicorners_info,2019-01-26T19:48:33Z,0,"Hi tanks for perfect tut about bs4 , i just a question about how i can login to web site and crawl the page some web sites need to login to access data . can you tell me how can i do it???",True
@andrebrasil9327,2019-01-25T18:15:37Z,2,Amazing tutorial!  Definitely worth watching!,True
@JakeTheMDog,2019-01-25T10:18:47Z,1,"I already come across a problem: when opening CMD and type in the 'pip...' part, the output says that pip is not recognized... Anyone?",True
@out2u1969,2019-01-22T21:44:17Z,0,Top drawer instruction. Excellent,True
@desiganpang5324,2019-01-22T06:47:33Z,0,"BeautifulSoup Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER. is anyone facing this problem ?",True
@howidrawhd,2019-01-21T01:43:41Z,0,"Amazing, i was trying to do exactly that!!",True
@67Keldar,2019-01-20T13:33:41Z,0,Awesome... Great video... Learned loads that other videos and written tutorials don't cover... \o/,True
@PanamaSoftwash,2019-01-20T07:05:18Z,0,I dont know much about coding but the way you explained this made perfect sense. I hope to learn a lot from your channel.,True
@charlene3027,2019-01-19T19:06:48Z,0,VERY CLEAR!!!thank you,True
@fuad471,2019-01-17T15:47:17Z,0,very useful really thanks,True
@brunowebart,2019-01-13T23:11:31Z,0,"Amazing content, very very helpful. Thank a lot for taking the time in doing this.",True
@iacobvlad-ionut7037,2019-01-13T17:28:56Z,0,"Great tutorial, thank you so much !",True
@keithlim5543,2019-01-13T11:05:45Z,0,When i type in python it just says python is not regonized as an internal or external command. Same with the 2+2,True
@animals42life8,2019-01-13T08:00:27Z,0,"I have a question. I followed you all along. But there are other divs which have a class of item-container also that are located at the top. They are within a div that has an id of recommendations. And when I do containers[0] what is shown is not the first product that has bigger image, instead what is shown is the first product at the very top that has smaller image--the products that are recommended? Please help. Thanks.",True
@adammarsono8908,2019-01-12T20:35:26Z,6,"Hello, at 20:14 , the <div> tag (in my case) jumps to <div> tag inside <a> tag. How to choose which tag we want to grab if there is more than 1 tag with same name",True
@samlocascio5800,2019-01-12T02:39:03Z,1,"Very good video!  great explanation of all the steps involved, I'll agree that this is best scraping video i've found .  Thanks!",True
@paulchaundy7202,2019-01-11T14:52:37Z,0,"Really great video. did manage to scrape a site well from how you explained it. good approach you took. one question i have about scraping the PRICE. in my situation they don't seem to have consistent html class for price sometimes its ""price"" then in some products it might show ""rrp-price"" then "" now-price"". how can i do this without having to load it into 3 different variables. so i can pass the actual sale price no matter if its in a now-price class or not. do i need to use IF STATEMENTS here? if so i can work out how",True
@YashPatel-vw9nz,2019-01-11T13:23:10Z,0,how to install bs4 in mac terminal,True
@sebastiansosa3072,2019-01-10T23:38:02Z,0,I KISS YOU!!!,True
@dasarathiratha8374,2019-01-09T18:09:46Z,0,After scraping how I can implement in flask,True
@df6148,2019-01-08T21:40:32Z,0,"Senior Data Scientist, Senior Database Engineer... I know a fellow gamer when I see one!  Thx for the the Tutorial.  All this time...all I ever wanted from most of the internet was the ability to ""scrape"" (new term for me) what I wanted so that I can do something with that data.  I like to organize things and categorize them.  I always thought rss was okay...twitter okay...reddit okay...but I just want specific feeds from those sites and this is exactly what I was looking for!  Better than paying a monthly fee to somebody who won't even teach you how to do it.  Maybe its from collecting cards as a kid or playing video games that had really in depth inventory systems (rpgs).  But it is enjoyable when you can get the exact bit of information you want and then do something cool with it.  This is helpful!  Where were you when I needed to organize my bank in world of warcraft!!!",True
@user-gx9hk8gt3k,2019-01-07T17:11:46Z,1,"It has been more than two years since this uploading. ""class"" : ""item-title""  is gone forever!  Any suggestion?",True
@NukiZuhriSaefudin,2019-01-07T15:42:45Z,0,Can it apply to https page?,True
@Kidskingdom8240,2019-01-05T14:12:09Z,0,"""from urllib.request import urlopen as uReq""     this lib not running why ? plzz tell me  i use ubuntu opreting system and run this code terminal and error ""ImportError: No module named request""    i install      'pip install request '     no solution  plzz u tell me solution",True
@grantmelson,2019-01-04T00:10:06Z,0,Fantastic Video,True
@tobiashelbing1233,2019-01-03T14:24:26Z,0,I am a beginner in Python. This was awesome.,True
@deejaykaye,2019-01-02T10:44:50Z,0,Brilliant - thanks,True
@pablogrant4485,2018-12-30T04:42:02Z,0,32:12 -rape.py. Wake up sheeple.,True
@bernardtumanjong4856,2018-12-29T15:44:58Z,1,"Truly enjoyed your simple step by step explanation on why each command or function is needed, and what it does.  Your Python knowledge and skills are evident, as you  are able to provide immediate solutions to errors and or challenges to the problem you are attempting to solve.  Followed along with the tools and enjoyed the session. Thank you.",True
@squeezeb58,2018-12-28T05:44:06Z,1,"brand = container.div.div.a.img[""title""] does not work anymore. I tried alt as well and it doesn't work. Anyone know how to fix this?",True
@tmnatm510,2018-12-27T07:32:24Z,0,"when I type container.div                       it  actually print <div ""class=badges""> in the terminal.  Could you teach me how to fix it?",True
@YasarHabib,2018-12-27T05:35:22Z,25,"This was by far the best introduction to web scraping I've found online. Clear, concise, and easy to digest. Thank YOU!",True
@pratikshah9219,2018-12-25T18:17:23Z,0,Man ! what a video ! Amazing.,True
@navidmohammadzadeh2141,2018-12-17T09:28:36Z,0,"thank you for your video, I have learned so much from your video! Good luck! regards",True
@muthukumara,2018-12-17T05:23:05Z,0,good video but rushing through and not much explanation.,True
@liamyo7821,2018-12-14T05:12:08Z,0,great video but why so zoomed in on text? i legit thought this was at a conference or something for the first half,True
@kevitralph4265,2018-12-12T23:25:10Z,0,legend!!!!!!,True
@diggyvolatin9735,2018-12-11T10:54:26Z,0,"Thanks for your video, can you make one doing scraping using proxycrawl service?",True
@ayokhan5616,2018-12-09T20:28:30Z,0,"At 19:42 when he did 'container.div', all it showed for me was this:   <div class=""item-badges""> </div>",True
@vijaykumar-zc7gu,2018-12-09T14:58:15Z,0,Nice Tutorial thank you,True
@MS-nh7zj,2018-12-08T22:33:34Z,0,"awesome tutorial , upload more Data Science videos .",True
@luqmanahmad3153,2018-12-08T17:50:43Z,2,"container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'a' can you help me why i am getting this error? i am using python 2.7",True
@freem4nn129,2018-12-07T15:25:41Z,0,good stuff sir,True
@narmadhaanu1539,2018-12-06T10:43:25Z,0,"Any one can tell,  how to find number of occurrence of a particular word in a given website using python beautiful soup...",True
@adrianramos2989,2018-12-04T11:32:44Z,4,This material is just amazing. Thank you! Have you considered making an intro to Web Scraping using R?,True
@Prinjal_Boruah,2018-12-03T18:41:03Z,0,14:14,True
@onee,2018-11-28T20:47:39Z,2,"Could you do an updated version? They changed the website. And I haven't been able to grab the brand. So, I deleted that part and updated the URL. And it still works.",True
@djtoddles8750,2018-11-28T16:52:11Z,0,3:30 install finished,True
@YogeshSinghPatel,2018-11-28T12:49:51Z,1,the first element of your container did not match with the first element of my container. It is also counting in the adds that are shown! what should i do?,True
@djtoddles8750,2018-11-26T17:44:18Z,0,27:00 multi line editing,True
@ThePsychoSugar,2018-11-26T02:24:30Z,0,You helped me a lot <3,True
@syomantakchaudhuri9935,2018-11-25T18:07:22Z,6,"Looks like they added another div at the very beginning of each item-container. The brand name can now be extracted with a little more effort- brand_container = x.findAll(""div"",{""class"":""item-info""}) print(brand_container[0].div.a.img[""title""])",True
@emmanuelldx7788,2018-11-19T12:50:09Z,0,"Awesome video: right pace, great content, very useful tricks...!",True
@valkon_,2018-11-17T19:19:59Z,0,I love your energy,True
@Radi_flows,2018-11-16T07:55:08Z,0,"Great tutorial! I'm having trouble exporting to the csv though! When I print, my loop is working correctly and scraping all of the data that I want. But, when I export, I only get the data of one product... none of the other 50. When I enter my web scrape file name into the command prompt and I execute the code, I see all of the product data that I'm attempting to pull flash on my screen, yet only the last product appears in my csv. Any Ideas what's going on?",True
@theheadorzz,2018-11-13T10:46:37Z,0,"Nice tutorial, thank you very much. Do you or someone else in the comments know how to put in browser headers, so that the website (Amazon) isn't blocking my requests?",True
@jdsr4c,2018-11-12T01:01:50Z,5,"I'm getting this error when I try to run it:   File ""<stdin>"", line 2, in <module>  NameError: name 'page' is not defined",True
@RevolvingMr,2018-11-11T12:29:00Z,0,Nice! Thanks for this!,True
@lupinedreamexpress,2018-11-09T00:19:24Z,0,"product_name = (container.findAll(""a"", {""class"":""item-title""}))[0].text",True
@avinashthakur2485,2018-11-06T06:01:20Z,0,awsome web scrapping tutorial,True
@pratikpatil4880,2018-10-30T21:53:34Z,0,Unable to get shipping? what should I do?,True
@jaromtollefson3127,2018-10-30T03:42:45Z,4,I keep getting 0 when I call len(containers),True
@satputeable,2018-10-29T02:45:03Z,0,"Very helpful. Thank you. I followed step by step, and used another webpage for the scrapping but somehow the structure of that webpage did not allow me to find the correct html tags to use for extraction. Will continue to try.",True
@alexic_yt,2018-10-27T08:17:53Z,0,Awesome tutorial!!! I don't even know any python and I was able to follow just fine. Really excited to dive into my first web scraping project :) Thanks!,True
@fawadjavid3220,2018-10-26T21:00:00Z,0,the best and detailed  web scraping video i have ever seen. Enjoyed a lot !,True
@Solo-lb1lo,2018-10-26T18:15:47Z,3,"hi I have a question when i type in the command prompt :  uClient = ureq(my_url) it responds like this:  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""C:\Users\Houman\Anaconda3\lib\urllib\request.py"", line 222, in urlopen     return opener.open(url, data, timeout)   File ""C:\Users\Houman\Anaconda3\lib\urllib\request.py"", line 531, in open     response = meth(req, response)   File ""C:\Users\Houman\Anaconda3\lib\urllib\request.py"", line 641, in http_response     'http', request, response, code, msg, hdrs)   File ""C:\Users\Houman\Anaconda3\lib\urllib\request.py"", line 569, in error     return self._call_chain(*args)   File ""C:\Users\Houman\Anaconda3\lib\urllib\request.py"", line 503, in _call_chain     result = func(*args)   File ""C:\Users\Houman\Anaconda3\lib\urllib\request.py"", line 649, in http_error_default     raise HTTPError(req.full_url, code, msg, hdrs, fp) urllib.error.HTTPError: HTTP Error 403: Forbidden  what does it mean? does it mean the website doesnt reapond the request?  Thanks for the video",True
@sajedayeasmin9003,2018-10-26T12:06:03Z,1,We want more data scrapping video! This was awesome!,True
@EustaceKirstein,2018-10-24T07:51:50Z,7,"32:30, I started cheesing at how awesome the end result of this whole project was.  Definitely inspiring - thank you for the excellent guide!",True
@bushwhack12,2018-10-23T12:24:31Z,0,"Whew,  made it through the tutorial. Thanks a lot.",True
@mycssubs1769,2018-10-21T08:44:36Z,0,seriously copy past back n forth btw cmd line and sublime? man if u like to check if ur line of code works just use jupyter notebooks bcs what u do is a brain fckg :3,True
@KundoKun,2018-10-20T02:39:55Z,0,thanks!,True
@BenManThe,2018-10-19T10:35:41Z,0,"hey, great video! I got a bit confused on the part with the ""for loop"". in the video, you wrote: for container in containers:     brand = container.div.div.a.img[""title""]  and further on the video you added similiar statements to the one above. my question is, how does python knows to search and activate on the ""container""? I mean, it's inside the variable (for example, brand). And also, ""container"" on the sublime text haven't been configured.",True
@haowu1460,2018-10-19T09:15:09Z,0,more p‚ÄÜl‚ÄÜz,True
@erkamtokgoz6130,2018-10-17T19:51:02Z,0,"please help me. Save as csv but only 1 column !!! How can I make 3 columns?     [32:44] [minute,second]",True
@vitodanelli,2018-10-17T11:06:40Z,0,Thank you! Very informative.,True
@alexrobert4614,2018-10-15T23:52:48Z,0,This is one of the only clear|fun python tutorials out there. Congrats,True
@ayanabarnes6255,2018-10-14T21:37:10Z,0,Excellent keep ‚Äòem coming! üëèüèæüëåüèæ,True
@Billsethtoalson,2018-10-14T05:22:33Z,1,DUDE! High Quality Content!! You are very good at walking through the logical steps for breaking down a page! Other tutorials are great but are always geared toward the specific task at hand. With this it felt like I also learned how to tackle a page!   This helped a bunch!,True
@sfinxbb1158,2018-10-12T07:17:31Z,0,I need the proxy to scrape some stuff. Do you have some experience with one of these? https://medium.com/@raimondofanucci/top-5-residential-proxy-providers-2018-dc69d9503155,True
@harrif23,2018-10-10T08:27:13Z,0,"python: can't open file 'filename.py': [Errno 2] No such file or directory for some reason, im getting this error while running it",True
@zeeshanqureshi9252,2018-10-09T23:11:54Z,0,"Sir, you got me have a real taste of python.. <3  Thank you so much..",True
@darnaud3679,2018-10-09T11:23:25Z,0,Automate Everything with web bots! This actually made my life easier... https://simplestipsandtricks.blogspot.com/2018/10/the-power-of-headless-chrome-and.html,True
@toundembodj5819,2018-10-08T06:00:11Z,0,Tips: add the headers paramter for fake web client to the Request call function to avoid http 403 or 503 error when requesting some websites with security against spider/bot. https://stackoverflow.com/questions/44280807/data-scraping-using-python,True
@toundembodj5819,2018-10-08T05:01:52Z,0,Great Job !!,True
@antisocialfox7268,2018-10-05T18:38:29Z,0,"i love this so much, i was just checking out what webscraping is but HOLY MOLY THIS IS SOME BROKEN ASS ABILITY",True
@M4s84,2018-10-03T20:00:53Z,0,I love you,True
@themepress9057,2018-10-03T10:05:32Z,0,Loop is not working for me ... it only writing single data into file.,True
@ChrisOHalloran,2018-10-02T08:52:59Z,0,"Hey great tutorial, my script only ran 1 item? What did I do wrong??  from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup  my_url = 'https://www.newegg.com/Product/ProductList.aspx?Submit=ENE&DEPA=0&Order=BESTMATCH&Description=graphics+card'  # opening up connection, grabbing the page uClient = uReq(my_url) page_html = uClient.read() uClient.close()  # html parsing page_soup = soup(page_html, ""html.parser"")  #grabs each product containers = page_soup.findAll(""div"", {""class"": ""item-container""})  filename = ""products.csv"" f = open(filename, ""w"")  headers = ""brand, product_name, shipping\n""  f.write(headers)  for container in containers:     brand = container.div.div.a.img[""title""]      title_container = container.findAll(""a"", {""class"":""item-title""})     product_name = title_container[0].text      shipping_container = container.findAll(""li"", {""class"":""price-ship""})     shipping = shipping_container[0].text.strip()   print(""brand: "" + brand) print(""product_name: "" + product_name) print(""shipping: "" + shipping)  f.write(brand + "","" + product_name.replace("","", ""|"") + "","" + shipping + ""\n"")  f.close()",True
@idrissberchil25,2018-09-30T16:53:31Z,0,"UClient = urlopen(LINK)  ERROR :raise HTTPError(req.full_url, code, msg, hdrs, fp) urllib.error.HTTPError: HTTP Error 403: Forbidden",True
@ensabajo4201,2018-09-29T16:45:26Z,0,Amazing tutorial thanks for your help,True
@christiansacks9198,2018-09-28T13:14:54Z,0,This is a really good lesson into web scraping. Thank you for demonstrating it.,True
@samchacko007,2018-09-26T18:16:46Z,0,excellent tutor and very clear description...please do more,True
@KeepRunningOz,2018-09-26T13:18:03Z,0,Does anyone know why I only get the first index when i run the my_first_webscrape.py file? It doesn't loop through all the products. It only gives me one at a time,True
@805bluebell,2018-09-26T07:04:10Z,0,enjoyed. thanks,True
@Rich65501,2018-09-25T19:33:39Z,0,Awesome  video. Can you do a Python program to scrape current stock prices from yahoo finance for a list of ticker symbols in a csv or xlsx  file?,True
@awndlr,2018-09-21T14:52:27Z,0,thank you so much. super helpful!,True
@s1ack3r07,2018-09-20T18:39:38Z,0,This was well done. Thanks for the demonstration along with the explanation!,True
@alegaultcesta,2018-09-19T18:16:00Z,0,Funny how you want to make a academical search and the 3rd thing that pops up is Nikki Menage... lol Hurray for our society....,True
@thesoundunit,2018-09-19T13:10:31Z,0,pointless,True
@kaihu86,2018-09-19T09:56:45Z,0,Awesome!!,True
@johnthevessel,2018-09-18T17:43:53Z,0,You get down Phuc! Appreicate you sharing. More Web Scaping videos please where you also post your code - GitHub / Jupyter Notebook status.,True
@dd7demo508,2018-09-15T22:46:37Z,0,"If I wanted to print all of the say results for the item titles, how would I go about doing that in the for loop? Since obviously, I can't increment a string, and if I do 'containers[0].text' it will just print the same thing 12 times.",True
@harrybanda,2018-09-15T10:42:23Z,0,Thanks for the tutorial really helpful,True
@JuanHernandez-om6wr,2018-09-14T00:17:51Z,0,Really awesome tutorial 1+ suscriber,True
@snpranay,2018-09-13T06:20:00Z,1,BY FAR the best tutorial I've watched for web scraping.  But could someone just help me out with scraping through multiple pages? I know the guy mentioned something about it in the very end but still,True
@besllu8116,2018-09-12T18:15:12Z,0,"As a newbie I can tell this helped a lot, thanks mate. Excellent job.",True
@benzavaleta92,2018-09-12T03:35:14Z,0,two days finding answers and you give me all that i need in 30 minutes !!! thanks so much!!!,True
@srinivaspithani7645,2018-09-11T16:33:36Z,0,"Thanks , liked it.",True
@commentor93,2018-09-06T12:37:00Z,0,"This tutorial is everything I searched for. Thank you for covering it that broadly and including several ways to get differenty kinds of information, both the constant and the varying ones. That's really something that sets this video apart from all the other tutorials.  Now that I've come quite far with my code, I've stumbled upon one exception and maybe you can help me here:  The code snippets that I try to collect from my file with ""find_all"" (in your case the containers) sometimes contain line breaks. This leads to the function making one entry per line and thus splitting a text where it should stay together and creating incomplete entries. Do you have an idea how I can prevent this from happening? Since the function isn't a string, I can't just append "".replace"" or something so what would be the best way to solve this?",True
@DY-gd4iu,2018-09-05T17:49:27Z,0,Great video! Please do more video about web scraping with python,True
@Mrfunkyerik,2018-09-02T20:45:12Z,0,"I get an syntax error when I put the ""pip install bs4"" in the cmd",True
@Mrfunkyerik,2018-09-01T21:44:27Z,0,Awesome video!,True
@thatosrsdude4831,2018-08-30T20:34:46Z,1,THIS WAS AMAZING,True
@ilyavarchenya,2018-08-30T07:11:35Z,1,"You shouldn't use names of variables in ""camel case"". Use ""snake case"" for it. Programmers use ""camel case"" for classes naming.",True
@davidbristoll195,2018-08-28T19:03:02Z,0,This is so helpful! The video has been really well put together and was answering my questions as they came to me. I'll probably be watching this a few more times as I practice! Thank you!!!,True
@darkandgreen3212,2018-08-27T16:15:48Z,0,THANKS A LOT. How does this compare to using  miyakogi/pyppeteer¬†? Can we use BeautifulSoup for Amazon? I have read we must use pyppeteer¬†for that. Thanks,True
@neila8483,2018-08-25T17:36:16Z,0,"Divinfosys.com Web development company in india, Best Web Design and Development Company, Web Scrapping a leading web scraping services provider in India. one of the top web-scraping companies in India. if you are looking for a fully managed web scraping service with most affordable web scraping solutions compare to other service provider. We can deliver the data in various popular document formats like XML, excel and CSV and also the websites which are login or PDF based too. It is located in India. Perhaps it is based on India. About my knowledge company in my mind which has been done 2000+ projects done in web scraping. We also have developers you can hire to do the job for you. contact  aneil19shop@gmail.com or benjamin@divinfosys.com",True
@divyabhuvanaramesh6514,2018-08-25T17:30:47Z,0,"Divinfosys.com Web development company in india, Best Web Design and Development Company, Web Scrapping a leading web scraping services provider in India. one of the top web-scraping companies in India. if you are looking for a fully managed web scraping service with most affordable web scraping solutions compare to other service provider. We can deliver the data in various popular document formats like XML, excel and CSV and also the websites which are login or PDF based too. It is located in India. Perhaps it is based on India. About my knowledge company in my mind which has been done 2000+ projects done in web scraping. We also have developers you can hire to do the job for you. contact  aneil19shop@gmail.com or benjamin@divinfosys.com",True
@saliths9069,2018-08-24T20:18:04Z,0,"I am trying to data scrap on yahoo finance.  I am facing an error ""Index out of range"" while trying to pull the data from the right end side of the site which gives the company details and description. period = soup.find_all(class_=""D(ib) W(47.727%) Pend(40px)"")[0].get_text()  Please help me to resolve this issue.",True
@jonpotter5776,2018-08-23T18:28:54Z,0,"I successfully got my version of this to run. I have a couple of questions.   1) Do you need to specify the [0] index for each variable in the for loop? I just tested with and without on a sample script and it made no difference.  2) I don't have Excel, so I imported the CSV into Google Sheets. I am getting a ""-"" showing up from what I believe is the item price varaible. It is breaking the line in my CSV and moving the price to cell A3 instead of cell C2. I have tried price.replace(""-"", "" "") (price is my chosen variable) and it is not fixing the issue, nor removing the ""-"".",True
@ravikadam4u,2018-08-23T05:56:34Z,0,Very good and easy to understand. Thanks :),True
@jonpotter5776,2018-08-22T03:30:35Z,0,"As someone self learning Python (my first programming language) with a web scraping script in mind, this was great!",True
@stanislavalekseev3933,2018-08-19T21:30:45Z,0,"Traceback (most recent call last):   File ""D:\web scraper.py"", line 7, in <module>     uClient = uReq(myUrl)   File ""C:\Python36\lib\urllib\request.py"", line 223, in urlopen     return opener.open(url, data, timeout)   File ""C:\Python36\lib\urllib\request.py"", line 532, in open     response = meth(req, response)   File ""C:\Python36\lib\urllib\request.py"", line 642, in http_response     'http', request, response, code, msg, hdrs)   File ""C:\Python36\lib\urllib\request.py"", line 570, in error     return self._call_chain(*args)   File ""C:\Python36\lib\urllib\request.py"", line 504, in _call_chain     result = func(*args)   File ""C:\Python36\lib\urllib\request.py"", line 650, in http_error_default     raise HTTPError(req.full_url, code, msg, hdrs, fp) urllib.error.HTTPError: HTTP Error 400: Bad Request [Finished in 0.5s with exit code 1]  What I am doing wrong ?",True
@lauren-althead6445,2018-08-19T19:14:15Z,0,"Sorry in advance, I am not an expert, I am looking for help. I have followed this code to the ""t"".   Instead of graphics cards I am pulling from AMD Ryzen processors on Newegg of which there are 20 in the search. When I input containers and findAll then check the length I do get back ""20"".   However, I am stuck at trying to loop through and output all 20 items. Each time I go to print (brand, name, shipping) I only receive the output for the first item. I have watched the video over numerous times and have matched the 'for container in containers:"" loop and outputs exactly. I have tried removing the '[0]' from product name which then produces an error and instructs me to use find instead of findAll, I have done this as well and it still only outputs the first item.   I have looked through a ton of these comments and not found and answer and I have looked at StackOverflow and not found an answer either.",True
@dmanmeds7374,2018-08-17T22:36:47Z,0,"here is a code to get the item title, company name, price, and shipping cost looping through multiple webpages and extract to excel file. I'm not very experienced in python so there is probably a more efficient way. code below. Thanks Data Science Dojo for the help!  from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup import re  TAG_RE = re.compile(r'<[^>]+>') def remove_tags(text):     return TAG_RE.sub('', text)  counter = 0    filename = ""products2.csv"" f = open(filename, ""w"") headers = ""item_title, company_name, item_price, item_shipping\n"" f.write(headers)   base_url = ""https://www.newegg.com/Xbox-360-Games/SubCategory/ID-516/Page-{}?Order=BESTSELLING&PageSize=96"" new_url = """" for i in range(4):   new_url = base_url.format(str(i + 1))   uClient = uReq(new_url)  page_html = uClient.read()  uClient.close()  page_soup = soup(page_html, ""html.parser"")  containers = page_soup.find_all(""div"", {""class"":""item-container""})    for container in containers:   # item title   title_container = container.find_all(""a"", {""class"":""item-title""})   product_name = title_container[0].text    # company name   company_container = container.find_all(""a"", {""class"":""item-brand""})   if not company_container:    company_name = ""NA""   else:    company_name = company_container[0].img[""title""]    # price   price_container = container.find_all(""li"", {""class"":""price-current""})   price_name = price_container[0]   dollar = str(price_name.strong)   cents = str(price_name.sup)   price_char = dollar + cents   price_float = remove_tags(price_char)    # shipping   shipping_container = container.find_all(""li"", {""class"":""price-ship""})   if shipping_container[0].text.strip() == ""Free Shipping"":    shipping_price = ""0""   elif shipping_container[0].text.strip() == ""Special Shipping"":    shipping_price = ""Special Shipping""   else:    shipping_price = shipping_container[0].text.strip().split("" "", 1)[0].replace(""$"", """")    f.write(product_name.replace("","", ""|"") + "","" + company_name.replace("","", ""|"") + "","" + price_float + "","" + shipping_price + ""\n"")    print(""product name: "" + product_name)   print(""brand: "" + company_name)   print(""price: "" + price_float)   print(""shipping price: "" + shipping_price)   print(counter)   print(new_url)   counter += 1  f.close()",True
@dmanmeds7374,2018-08-17T16:50:52Z,0,"for those wondering about getting the price, this worked for me but there is prob a more efficient way of doing it  used the following URL: https://www.newegg.com/Xbox-360-Games/SubCategory/ID-516?Order=BESTSELLING&PageSize=36  price_container = container.find_all(""li"", {""class"":""price-current""}) price_name = price_container[0] dollar = str(price_name.strong) cents = str(price_name.sup) price_char = dollar + cents # import re TAG_RE = re.compile(r'<[^>]+>') def remove_tags(text):     return TAG_RE.sub(' ', text)  price_float = float(remove_tags(price_char))",True
@TheHollowayPower,2018-08-16T00:51:45Z,0,: can't find '__main__' module in '' I don't undertstand this error when building. In my command line on windows it gives me an error just importing the request. HELP,True
@naveen_devinda,2018-08-15T03:22:59Z,0,"nice, this is awesome.",True
@Jasmohan,2018-08-12T03:44:51Z,0,I really enjoyed it. Thank you.,True
@sjrmartines,2018-08-12T03:29:42Z,0,"Man, great great video! Pretty simple, fast and clarifying one.  Thx a lot.",True
@NerdyCoder,2018-08-10T12:31:53Z,0,You should publish on pluralsight,True
@karthik-ex4dm,2018-08-10T12:28:32Z,0,"At 26:48, free_shipping[0].text takes the ""TEXT"" out of tag. How do the same in case of numbers??",True
@aj_s_unboxing,2018-08-09T14:49:42Z,0,"Hi There, I am new to python and i tried to scrape a site looking your video. However i failed miserably. I am working as data analyst and i wanted to learn web-scraping but its not working at all . Could you please help me? Here is my email ID (abhjagtap@gmail.com) waiting for you response eagerly.",True
@lagz89,2018-08-09T14:26:09Z,0,urllib.error.HTTPError: HTTP Error 403: Forbidden,True
@TheStrikerHD,2018-08-08T20:37:56Z,1,I don‚Äôt usually comment on videos but this was phenomenal. Thank you.,True
@gvsharan28,2018-08-08T12:10:03Z,0,Amazing Intro video for scraping from the web!,True
@souravmahanty7025,2018-08-07T16:30:17Z,0,This is the first tutorial on this that actually makes sense. THANK YOU. You earned a subscriber.,True
@abellara9169,2018-08-06T13:16:24Z,0,Basic question: Is that legal? Can you do that without owners permission?,True
@petermanmanman4550,2018-08-05T22:47:51Z,0,"This was great, very easy to follow and fun to learn. THank you!",True
@jshen4037,2018-08-03T12:46:20Z,0,PermissionError: [Errno 13] Permission denied: 'NeweggWebscrape.csv'?????,True
@nenobarca2822,2018-08-03T01:02:22Z,0,GREAT TUTORIAL!!,True
@TheSagitube,2018-08-02T16:39:19Z,0,how to select a date in a datepicker on a webpage using python?,True
@jshen4037,2018-08-01T20:45:54Z,0,do you have your own Youtube channel? :),True
@codingwithjoyk,2018-08-01T11:09:18Z,0,"Great tutorial!  How about setting up a script to run automatically, changing just one parameter with each pass?  Of course, I think I have to set up one of my laptops as a server first?",True
@MrKiraBR,2018-07-31T15:12:01Z,0,Is it possible to scrap facebook with this Soup thing? Having in mind that facebook doesn't load all the information at once on it's timeline.,True
@MrKiraBR,2018-07-31T13:23:48Z,0,Amazing tutorial my friend. Thank you so much!,True
@alancoates,2018-07-30T13:42:39Z,1,Your presentation and explanation are awesome!  You have opened my eyes to the uses of Python and Beautiful Soup.,True
@n0mex567,2018-07-29T19:24:43Z,0,Great!,True
@alphadecay3116,2018-07-27T17:45:45Z,0,"Thanks for the video, really enjoyed it! I have one remark though.  Don't you think that you could have used find() instead of find_all() inside the loop.  I think that find_all() might be redundant in this case, because you are extracting only one tag.  I might be wrong though, please correct me if I am!",True
@NELUMINDRALAL,2018-07-25T17:29:06Z,0,"This is a great video, thank you so much, I followed that step by step.",True
@sharukhan3118,2018-07-24T20:44:44Z,0,Great tutorial ..,True
@WilliamCase,2018-07-23T20:23:29Z,0,That was a great tutorial. Thanks so much!!!,True
@akashtyagi1269,2018-07-22T13:25:34Z,0,Awesome tutorial,True
@ooarioo5138,2018-07-20T15:44:15Z,0,Thank you so much !,True
@Jeroeny,2018-07-18T17:17:17Z,2,Thank you for this video. Very interesting! I have 1 question though. When I open my created CSV file. All the data is shown in 1 column. While it is seperated in 3 column in yours. I have used the exact same code as you have. How do I divide the headers with the according data in different columns? I use Python 3.6.5. Thank you! I am looking forward to your answer.,True
@SnehilSinghsl,2018-07-18T14:47:39Z,0,"I cant believe I actually sat through 33 minutes learning web scrapping, something completely new to me. I was looking for a shortcut but your tutorial was just perfect! :D Thanks for this.!",True
@touchesoftwares9604,2018-07-18T13:39:52Z,0,could you upload the script? What about this scrip yellowpagesspider.com,True
@filobrosgolf,2018-07-18T03:53:13Z,0,Can I hire you?,True
@inkwhy9276,2018-07-17T13:32:57Z,0,dude ur awesome,True
@user-gv4vx2tt7k,2018-07-15T05:28:56Z,0,HTTPError: HTTP Error 416: Requested Range Not Satisfiable how can i solve this problem  plzzz guys help me,True
@pdubocho,2018-07-15T02:52:58Z,6,"The man, the myth, the legend.  You have no idea how much stress and lost time you have prevented. THANK YOU!",True
@GonzaloherreraTuc,2018-07-11T19:25:02Z,1,Great tutorial!! thanks,True
@taniadutta1934,2018-07-10T08:31:02Z,0,is it complete tutorial?,True
@pjamshidian8,2018-07-09T21:07:29Z,1,This was AWESOME,True
@paulbell337,2018-07-09T15:10:26Z,0,"Thank you very much for this, very informative, would be very interested in learning combining this with the loop to go through the pages, or maybe aggregating data from more than one site.",True
@tommyshaw2420,2018-07-08T13:16:57Z,0,"this is badass, seems easier than  cheerio and request",True
@prayerbabies,2018-07-08T05:16:23Z,2,The first video of this type that really made sense to me ...   thank you very much.,True
@coxixx,2018-07-07T15:09:17Z,0,Awesome,True
@joseantoniogarciarivas8042,2018-07-07T07:56:29Z,1,"Wonderful, saved us a lot of time. Amazing!",True
@sarvagyaan1097,2018-07-05T16:37:42Z,14,"enjoyed, data science ! Need more like this one",True
@brendan3966,2018-07-04T21:50:04Z,0,This is a good tutorial but for me it would be better if you prototype and script on the same screen or just put he script in the description,True
@Rohitsingh2410,2018-07-02T09:55:10Z,0,trying to grab angellist url example: https://angel.co/mygola-com Not working help anyone,True
@vaibhav10E,2018-07-02T08:05:41Z,0,Thank you! Great for beginners,True
@isaach3099,2018-06-29T19:40:47Z,0,"hi i am not using urllib because i don't feel comfortable using it and i am using requests and bs4 here is my code  import requests from bs4 import BeautifulSoup  #REQUESTING PAGES STORED IN LETTER VARIABLES  #PAGE 1 WILL BE KNOWN AS A  a = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/33000236_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 2 WILL BE KNOWN AS B  b = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2109484460_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 3 WILL BE KNOWN AS C  c = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2090398550_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 4 WILL BE KNOWN AS D  d = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2098308901_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 5 WILL BE KNOWN AS E  e = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2089101245_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 6 WILL BE KNOWN AS F  f = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2125487175_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 7 WILL BE KNOWN AS G  g = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2146464112_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 8 WILL BE KNOWN AS H  h = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2131705720_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 9 WILL BE KNOWN AS I  i = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/2105202779_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #PAGE 10 WILL BE KNOWN AS J  j = requests.get(""https://www.zillow.com/homes/for_sale/Fleetwood-Yonkers-NY/pmf,pf_pt/32999946_zpid/776504_rid/globalrelevanceex_sort/40.933179,-73.832331,40.914955,-73.861642_rect/14_zm/"")  #CONTENT SECTION  # a content  a_1 = a.content  # b content  b_1 = b.content  # c content  c_1 = c.content  # d content  d_1 = d.content  # e content  e_1 = e.content  # f content  f_1 = f.content  # g content  g_1 = g.content  # h content  h_1 = h.content  # i content  i_1 = i.content  # j content  j_1 = j.content  #PARSED VERSION OF CONTENT  soup_a_1 = BeautifulSoup(a_1, ""html.parser"") soup_b_1 = BeautifulSoup(b_1, ""html.parser"") soup_c_1 = BeautifulSoup(c_1, ""html.parser"") soup_d_1 = BeautifulSoup(d_1, ""html.parser"") soup_e_1 = BeautifulSoup(e_1, ""html.parser"") soup_f_1 = BeautifulSoup(f_1, ""html.parser"") soup_g_1 = BeautifulSoup(g_1, ""html.parser"") soup_h_1 = BeautifulSoup(h_1, ""html.parser"") soup_i_1 = BeautifulSoup(i_1, ""html.parser"") soup_j_1 = BeautifulSoup(j_1, ""html.parser"") house_details = soup_a_1.find_all('header', {""id"":""yui_3_18_1_2_1530295575578_1986""})  #PRINT VALUES  print(house_details)  #PAGE 1 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_a_1.prettify()) file.close()  #PAGE 2 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_b_1.prettify()) file.close()  #PAGE 3 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_c_1.prettify()) file.close()  #PAGE 4 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_d_1.prettify()) file.close()  #PAGE 5 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_e_1.prettify()) file.close()  #PAGE 6 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_f_1.prettify()) file.close()  #PAGE 7 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_g_1.prettify()) file.close()  #PAGE 8 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_h_1.prettify()) file.close()  #PAGE 9 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_i_1.prettify()) file.close()  #PAGE 10 UPDATING FILE ZILL.CSV file = open(""zill.csv"", 'w') file.write(soup_j_1.prettify()) file.close()",True
@MyLifeWithAI,2018-06-29T15:45:17Z,0,"Hi, I have the following issues: 1) The output in the command line has all results , but in the excel I have 1 row only. 2) The shipping data is going to incorrect column - it is shown on the second row in the column ""brand"". The row for output format that I have is the same like in the video:   f.write(brand  + "","" + product_name.replace("","", ""|"") + "","" + shipping  + ""\n"") f.close()",True
@ShahidNihal,2018-06-28T17:31:52Z,0,Loved it! A good tutorial finally!,True
@GregoryKaleka,2018-06-28T16:19:09Z,0,You should really use the csv module. Saves lots of boilerplate you had to enter when writing your CSV.,True
@tonybengue,2018-06-28T15:46:14Z,0,It is a good tutorial yes :),True
@joshambush8574,2018-06-28T03:12:24Z,0,i wonder what this guy looks like? very interesting voice,True
@vigneshsundaram2977,2018-06-25T03:30:37Z,1,Excellent tutorial. Thank you!,True
@schwazroda7882,2018-06-24T23:06:29Z,2,Really really helpful! Thank you so much!,True
@nhanon67as,2018-06-23T03:54:48Z,0,"Before using BeautifulSoup, this is the first strategy you should look to use to scrape a website.  https://medium.com/@HallyouGot/a-web-scraping-technique-i-should-have-learned-a-long-time-ago-python-b39262a8247d",True
@feerayaar7330,2018-06-22T08:03:15Z,0,Sir I need learn web scraping in php please make some video,True
@sabanaydn8952,2018-06-21T13:43:41Z,0,Thank you,True
@fullmetaltranshumanist8511,2018-06-19T14:38:55Z,0,Didnt work on windows 10.,True
@AlanWagoner,2018-06-17T17:03:54Z,0,This was a very cool (and practical) video.  Thanks for publishing!,True
@Miionu,2018-06-17T09:43:12Z,0,Great video üòä,True
@daitavan297,2018-06-16T05:45:13Z,0,Thank you very much for your comprehensive tutorial video. Good job from Vietnam.,True
@jonlorenzini5552,2018-06-12T18:38:54Z,0,When looking to explore what is going on in the containers I had issues with the unicode encode error (I am a n00b). Using     print(container.encode('utf-8'))   would help you see what is in your elements as you work through prototyping!,True
@stuckinbook8965,2018-06-12T18:16:34Z,0,"File ""<stdin>"", line 1     imoprt urllib.request                  ^ SyntaxError: invalid syntax  how can I solve this issue help me this is'nt woking for me i'm on ubuntu this is n ot working but on windows this is working",True
@E_Crypto,2018-06-11T23:22:17Z,2,Excellent video!  Thank you for your help!,True
@hugueskir6203,2018-06-11T20:44:12Z,0,"I'm trying to web scrap weather station, but I don't have numbers in my HTML code...? Continuously messing the value of the actual temperature. Any tricks?",True
@patrickjane276,2018-06-10T23:23:08Z,1,that was awesome man - so much appreciation for things like this! you could throw in adding the csv into a database - and then throw in a query on the best card!,True
@tawilk,2018-06-06T17:58:23Z,0,how do you open a command prompt for Mac OSX users?,True
@bishwasadhikari9249,2018-06-05T14:02:19Z,0,+1 for the short and straightforward tutorial.,True
@NewWaveDesigns,2018-06-04T22:49:46Z,0,Thank you so much for this! ... I actually managed to create a scrapper to scrape all of the containers and output them to a csv based upon your tutorial... I would however like to know if there is an easy way to continue onward to the next page. I currently am changing the number in the reques url but i know there's a way that it can be automated ... is this something you can provide the code for? thank you once again...,True
@AyeAyeMon,2018-06-04T06:21:20Z,0,Amazing!! Thanks so much for the tutorial!,True
@marcd4144,2018-06-04T03:52:15Z,0,Great video bro... Finally an explanation I can understand.,True
@IxlosiddinUrmanov,2018-06-03T21:45:57Z,0,Omg üòÆ You are the best! I have finally started to understand everything! Millions of thanks üôè,True
@boyu722,2018-06-03T04:57:22Z,0,Why is the CVS file an excel file?,True
@vpop464,2018-06-02T20:51:00Z,0,Thank you so much this was great! Sooo sooo great,True
@vpop464,2018-06-02T18:52:09Z,0,"When I try to do the control c as shoe at the very beginning of the video I get a ‚Äúkeyinterrupt‚Äù message, any help on this?",True
@yuriipidlisnyi2248,2018-06-02T10:42:14Z,0,"Here is my version of code. I used requests instead of urlib.requests since they are much easier to use.  I scraped newegg's laptops.  from bs4 import BeautifulSoup as soup import requests  f = open(""data.csv"", ""w"") f.write(""Brand, Title, Shipping\n"") url = requests.get(     'https://www.newegg.com/Laptops-Notebooks/Category/ID-223') html = url.text page_soup = soup(html, ""html.parser"") containers = page_soup.findAll(""div"", {""class"": ""item-container""}) for container in containers:     brand = container.div.div.a.img[""title""]      title = container.find(""a"", {""class"": ""item-title""}).text      shipping = container.find(""li"", {""class"": ""price-ship""}).text.strip()     f.write(brand + "","" + title.replace("","" , "";"") + "","" + shipping + ""\n"")  f.close()",True
@yuriipidlisnyi2248,2018-06-02T09:57:04Z,6,"Maybe it's better to use find() instead of findAll() to get product's name? So code will be less complex, like this :  title = container.find(""a"",{""class"" : ""item-title""}).text",True
@dhawalsanghvi5327,2018-06-01T18:50:14Z,0,"Great Video.Thank you :D However,i have one doubt. Are comma seperated values automatically stored in different columns in a csv file? Please help.",True
@fimnowtvonair9311,2018-05-31T05:51:57Z,0,How to get data with pagination? Not only in a page but in page 1 2 3 4 5.....,True
@yogikumar221,2018-05-30T16:37:26Z,0,Awesome video. Very Informative!!,True
@MatthewHuberty,2018-05-29T18:29:57Z,0,"Great content! I've watched dozens (hundreds?) of coding vids on youtube and I normally up the speed and skip around because they tend to go really slowly. You did a really good job of moving quickly while also covering most of the little details that beginners would get stuck wondering, like pressing ""up"" in the console to repeat your last command.  My only constructive feedback is that the font was a little bit too big. While I would much rather it be too big than too small, there's a happy medium somewhere in the middle a few sized down from what you were displaying, mostly just when displaying the html.  Again, great vid and I'll definitely check out some more of your stuff!",True
@yogajangkungs,2018-05-26T07:42:56Z,0,"hi, i have a question, you wrote container = containers[0] on python console but not in the code script, and it works like magic. How was that happen? is python doesn't need to know that variable information? sorry i'm a noob",True
@AbdulwahedMansour,2018-05-26T00:29:54Z,0,Thanks you are TOP,True
@Locane256,2018-05-25T20:13:50Z,0,That was great!  You're a great teacher.,True
@papakazoo1488,2018-05-25T01:18:44Z,0,how do you know if its an html.parser file? Where can i find that information if i was to be on another site of a different file type?,True
@BenStuu,2018-05-24T17:28:06Z,3,"29:01 ""rape.py""",True
@germannewsupdate2032,2018-05-24T12:06:35Z,0,"Hi, this is a great video tutorial. Can you explain if i want to scrape inside those products? And if there's a pagination, how can i work with that? Thank you",True
@sameerk12982,2018-05-24T05:56:07Z,0,Thank you very much Sir for creating such an informative video tutorail.,True
@Locane256,2018-05-23T20:48:33Z,0,"9:00 Could have just called ""page_html[:100]""",True
@lamay8889,2018-05-23T11:04:12Z,0,"Subscribed, Thank you very much you are awesome",True
@stevechan9498,2018-05-21T08:36:10Z,0,Really cool stuff!,True
@neosudheer,2018-05-17T18:56:24Z,0,"Just one word...., Awesome !",True
@maksetemadi6061,2018-05-15T20:23:29Z,0,"great video, I am not sure how you loop multiple pages, you said something about it.  How do we loop over multiple pages, any one can give me a hint that would be great. thanks",True
@williamkidwell1504,2018-05-15T01:55:13Z,0,Dude - Good job!  Now - go take your meds...  :),True
@hrududu1690,2018-05-14T21:08:48Z,0,"Excellent, very clear",True
@huifortrack,2018-05-12T22:12:43Z,0,Thanks   How do you loop though all pages ?  Do you have to find the url patern?,True
@jeshand3159,2018-05-12T21:50:56Z,0,wow thanks man! that's SO awesome!!!!!,True
@kathir9042530419,2018-05-10T21:33:13Z,0,"I'm using Ubuntu, for loop is not working only one container things are being exported to csv... any error would be?",True
@nivaech,2018-05-09T22:35:32Z,0,Great tutorial indeed. Thanks,True
@inowhy1930,2018-05-09T17:56:41Z,0,"If you're using Mac and you've installed Python3, make sure to double click the Install Certificates.command file before you use the uReq func or you may have SSL Certificate verification errors.   File Path: Applications > Python 3 > Install Certificates.command",True
@ksumeet,2018-05-09T00:05:02Z,0,Very helpful!! Thanks :),True
@mohammedraza100,2018-05-07T23:28:01Z,0,"Very concise and useful video thank you.   For my specific application, when I click on my product it takes my to a page that contains additional information that I would also like to scrap. Is there any way of incorperating this into the code somehow?  Thanks.",True
@alexlorenlee,2018-05-07T22:17:59Z,0,YOU EXPLAIN EVERYTHING SO WELL!!!!!,True
@joohokim5257,2018-05-07T05:10:27Z,0,"So, if I type in containers[0], does it give me every single codes for this class (item-container)?",True
@stalin2789,2018-05-06T00:10:10Z,0,Htanks bro... Beatiful,True
@ebicer,2018-05-04T20:13:48Z,0,Perfect...,True
@TheMasterMindAcademy,2018-05-03T21:22:17Z,1,"Thank you for the great video! I followed your instructions, but by checking the functionality of the script I get an error: ""ImportError: No module named bs4"" I don¬¥t know why because the installation was successful. The import in the terminal was also successful. Does anybody have an idea?",True
@congamike1,2018-05-03T20:57:52Z,0,"OK but... at 24:40 this came very close to what I need for this project! What I need now is something like: container = page_soup.findAll('a', {""class"":""set""}.[""Change""] (or something)",True
@gaowendy5432,2018-05-03T06:56:00Z,0,Quite Good for Beginners. Thx~~~,True
@vivekkumar-cj2gg,2018-05-02T19:01:49Z,0,when i do len(containers) it outputs 0. Help..!!!,True
@bazareta6936,2018-05-02T12:33:10Z,0,"i follow step by step but it not works with me i have this error   File ""webscrap.py"", line 22     product_name = title_container[0].text                                          ^ IndentationError: unindent does not match any outer indentation level",True
@tgroh007,2018-05-02T08:26:37Z,0,"very informative, short and sweet! Thank you.",True
@seibervideo,2018-04-29T03:12:50Z,0,"I'm bummed, this looks so simple but I keep getting an error on this part:  from urllib3.request import urlopen as uReq Traceback (most recent call last):   File ""<stdin>"", line 1, in <module> ImportError: cannot import name urlopen  Any idea how to fix?",True
@TheCreativeMale,2018-04-28T16:15:59Z,0,"That was great and based on that fact, Ive sub'd and ""notified"". Thanks  Im just starting out in data science (very later starter but really fascinates me) and all this is good knowledge. Id like to see the same concept but with a API call to a url and how to break down the return items and display them.   Will go through your other videos.   Thanks again",True
@rubenseoane7621,2018-04-25T14:16:48Z,0,"Great tutorial! I just made a Jupyter notebook while following it, feel free to improve upon (for example more explanations as the ones given in the video): https://github.com/thepartisan101/NLP/blob/master/Web%20Scraping%20with%20bs4.ipynb",True
@ProductGrower,2018-04-24T19:17:08Z,0,"Fantastic Tutorial! ***If your findAll is returning empty in some cases, use a try, except case: Example....  try: containers.findAll  except Exception as e: container = None",True
@KawsarJami,2018-04-24T04:00:23Z,0,"web scraping is great! i can do any scaping for you, see my profile here https://goo.gl/iTHgqT",True
@theacademy7670,2018-04-23T18:32:51Z,1,Thank you for a good video,True
@lydialim2964,2018-04-23T03:42:40Z,7,"THIS IS AMAZING!!! Everything was very well-explained and instructed, I managed to get my first webscrape off an E-commerce site! Thanks so much, you have a loyal subscriber in me!  Perhaps you could cover using time sleeps to avoid getting blacklisted by the websites we are scraping? And also how to scrape multiple pages in one go?",True
@aishmunaz9939,2018-04-21T12:17:59Z,0,"Sir, how to read this div tag <div class=_3215  row> I am unable to read this using soup.findAll(""div"",{""class"":""_3215 row""})",True
@CodeKiLLa26,2018-04-20T23:27:07Z,0,You could also use Visual Studio and add the Python module from the Visual Studio Installer.  This gives you the benefit of intellisense and a ton of other features of Visual Studio live having an interactive python window below your editor window.  It also makes it easy to search for packages and to install or remove them.,True
@djhemag,2018-04-20T21:14:16Z,0,"This is great! Would love to see how to handle edge cases. Also, how would you scrape the data from each product page, i.e. follow the link and store the description, tech specs,",True
@fsix3451,2018-04-20T07:45:35Z,0,AMAZING,True
@fun2badult,2018-04-19T17:47:45Z,0,Ok Now I'm stuck. I'm trying to pull data but a specific data has several sentences in it. Now I'm getting result where the sentence breaks up and is spilling into the next csv cell instead of keeping all the text in one cell. How can I make sure the stuff stays together? I tried .split() and few others but can't get it to work.,True
@sandiegoman,2018-04-18T18:04:49Z,1,Very good explanation. Thanks! You have good teaching skills!,True
@AtenKadet,2018-04-17T19:17:53Z,0,mmmkay..,True
@vitorcarlessi3022,2018-04-16T15:01:52Z,0,Amazing tutorial!!!!  Thanks :),True
@12345yug,2018-04-15T22:17:10Z,0,Thank you so Much.. :) Followed you and Mission Accomplished.. Firstly I was getting issue of SSL: not working... on macOS then this worked for me... import ssl ssl._create_default_https_context = ssl._create_unverified_context  rest was working like butter...,True
@shiqidai5548,2018-04-15T02:21:33Z,0,It's very helpful. Thank you!,True
@qal192,2018-04-14T17:52:10Z,0,Does anyone know how to get the price text?? I did use the findAll() function but I still can't extract the price text data,True
@fancylad2006,2018-04-11T22:45:41Z,0,Nicely Done!,True
@mundraa,2018-04-10T10:39:57Z,0,Nice,True
@fun2badult,2018-04-07T06:57:06Z,0,Wow. Can't believe I just learned webscraping with beautiful soup in 30 mins. Great video. Fast pace and talk but that's what pause and rewind is for.,True
@ZenergyLIVE,2018-04-06T15:42:11Z,1,This is the best web scraping tutorial I've ever seen! Thank you so much!,True
@MrPeppino1991,2018-04-06T13:27:52Z,0,HONESTLY YOUR EXPLANATION WAS PERFECT! THANK YOU,True
@caramba43,2018-04-05T21:56:33Z,0,you may want to change the parser type 'html_parser' to 'html.parser',True
@AnalystRising,2018-04-04T18:47:33Z,0,"Hi, i love your YouTube channel. I also love using Python. Can we subscribe and support each other?  :-)",True
@benjaminmiao3630,2018-04-04T13:29:17Z,0,"OK well 10:50, why did you simply type page_soup.h1 instead of print(page_soup.h1) and it still work??? why it didn't work on mine??? I must type 'print()' to print the h1 tags. I wasted 30mins trying to solve that problem...",True
@tjgaba9748,2018-04-03T09:31:54Z,0,Thank You <3<3<3,True
@sean_goral,2018-04-03T07:09:35Z,0,Radical! thank you,True
@ilhamefendy9868,2018-03-31T09:15:11Z,0,this is really helpful very much for a free tutorial. thank you so much,True
@AaronCrittendenForgedCriTT,2018-03-30T17:39:20Z,0,If i'm pulling from a webpage using pagination do I have to change anything?,True
@congamike1,2018-03-28T01:27:55Z,2,"This is great but mine blew up. Do you have the source available somewhere? Update: I was scaping a URL that didn't like my ""browser."" it gave a 'forbidden' error and I thought it was a package error. Now I will watch the video again. Another update: I followed your steps on a different URL and created a script that will be very helpful to me. THANKS!",True
@AshikAdnanMark,2018-03-27T14:50:21Z,0,"Hello, thanks or sharing this great tutorial. I tried scraping following this tutorial and it was a great success. However I am stuck in another script where i need to grab similar posts and every post contained inside in each single list or <li>. The problem is the class name inside this <li> contains a post number which varies from <li> to <li>. The list looks like this::  <li class="" post post-81993 type-post status-publish format-standard has-post-thumbnail hentry category-crime""> You can see the number there and each list that contains the post i need to grab contains a different number. So apparently if i write that class to grab that post it will only grab one single post, not all of them. Any kind of help would be greatly appreciated. Thanks!",True
@tejasarlimatti8420,2018-03-24T10:19:29Z,0,awesome,True
@senthilcaesar,2018-03-22T22:24:16Z,0,Easy to follow and understand . Thank you,True
@maladict8891,2018-03-20T10:37:04Z,0,"Thank you for a great video. For someone a newbie, this was a bit confusing, since you need to add paths for both python and pip separately in order to run them from the command line. :)",True
@anonimoose6171,2018-03-19T23:02:10Z,0,"This was genuinely an amazing video. Thanks very much. Very, very useful.  Any chance you could do another, this time on web scraping with cookies? (I.e. with login details, age checks, locations, etc.)",True
@grupomexico4679,2018-03-18T15:04:37Z,0,"how can i read a .txt file with a list like: juan pedro maria lola luis  i need reed this file .txt from another file but read first the line1,then the line2,then the line 3 etc and when the last line come brake automatically",True
@dylandejonge6511,2018-03-17T13:53:37Z,0,Holy shit. This is awesome you saved me a lot of time by learning me how to scrape. Thank you also for having a pleasent voice to listen to for 30+ minutes :)  Cheers!,True
@10aDowningStreet,2018-03-14T08:58:28Z,0,"Pretty much new to code other than html & css, I've dabbled and failed, but I'm going to give this a try, this is the best scraping tut I've found. For a long time I've wanted to build a very basic comparison site for my hobby because people spend silly amounts of time searching for the best prices and so far nothing exists to help us, most retailers don't have API's so scraping is the way to go. Wish me luck!",True
@benb5743,2018-03-11T00:44:19Z,0,Awesome video! I can't wait to try this out and get it into a SQL DB,True
@EWOKakaDOOM,2018-03-10T05:07:11Z,0,really great video !,True
@MrVanhovey,2018-03-09T14:32:44Z,0,"Can't get past uClient = uReq(my_url), returns errors. Traceback (most recent call last):   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1318, in do_open     encode_chunked=req.has_header('Transfer-encoding'))   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1239, in request     self._send_request(method, url, body, headers, encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1285, in _send_request     self.endheaders(body, encode_chunked=encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1234, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1026, in _send_output     self.send(msg)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 964, in send     self.connect()   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1400, in connect     server_hostname=server_hostname)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 407, in wrap_socket     _context=self, _session=session)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 814, in __init__     self.do_handshake()   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 1068, in do_handshake     self._sslobj.do_handshake()   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 689, in do_handshake     self._sslobj.do_handshake() ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen     return opener.open(url, data, timeout)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 526, in open     response = self._open(req, data)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 544, in _open     '_open', req)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain     result = func(*args)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1361, in https_open     context=self._context, check_hostname=self._check_hostname)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1320, in do_open     raise URLError(err) urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)>",True
@sudipta_samanta,2018-03-09T10:24:28Z,0,Please make more of this! This is the first tutorial I saw about web scraping and I understand totally. Thanks.,True
@ismailshah6984,2018-03-08T16:26:24Z,0,thank you so much for this tutorial. I enjoyed the tutorial a lot until the last part where the task was to open saved python text file... It's showing that there is no such file on directory... could you please help me out with the error that I'm dealing with???,True
@shubhamshukla6680,2018-03-07T16:18:32Z,0,"sir pls make video on how to update,insert,delete data from sqlite through  fetching it from treeview it would be very  helpfull for many of us pls sir",True
@minaakbarabadi8201,2018-03-07T14:49:39Z,0,It was really helpful to me. I was confused by some other videos but this one was great. Thank you,True
@muqitzoarder9878,2018-03-07T08:55:48Z,0,It's really beautiful and amazing. I have learnt many things in a single video. It's like different fruits in a single banch of a tree. Keep it up. Big clap for you.,True
@casperj7036,2018-03-07T03:39:59Z,0,AWESOME VID,True
@etanleibovitz6884,2018-03-04T03:09:13Z,0,Excellent video,True
@isaiah8867,2018-03-03T21:38:08Z,1,"I scrapped a wiki and put it on kaggle, gave shot outs to this tutorial. Thank you! https://www.kaggle.com/isaiahspearman/first-webscrape-of-marvel-super-heroes-wiki",True
@vijaybhargav3525,2018-03-03T06:41:39Z,0,Great Video man!!,True
@T.Vercetti1951,2018-03-02T16:51:44Z,0,what does he press that he gets from?  >>>  ^C C:\Users ~~~~ I always get SyntaxError: invalid syntax or SyntaxError: unexpected character after line continuation character,True
@dayma100,2018-03-01T23:37:55Z,0,good stuff. Thanks,True
@ahmedkrichene3386,2018-02-28T11:18:44Z,0,"Thank you for the video, very clear. Someone can tell me how to avoid being banned from some websites because of daily scraping ?",True
@I_love_mbs,2018-02-28T06:23:53Z,0,Thank you for this video. You are very good at what you do.,True
@tjordans,2018-02-27T15:16:28Z,0,This was perfect for me--exactly what I needed. Thanks so much!,True
@wrillywonka1320,2018-02-27T04:58:57Z,0,dude your a bad ass. i am barely familiar with html and css and want to get more into bots. i am however very new to this area of coding. i think your video was super good and i was able to understand you pretty good for me not knowing shit but i was a little left in the dust a couple times and wish you had a video for super noobies like myself to become familiar with the lingo and programs. just thought i could use notepad for writing code and like what if i want to say for example scrape information from several different pages. example if i wanted to scrape emails or phone numbers from craigslist would i have to literally go into every ad link  that url..im sure theres a way to have the scraper do that work for me correct? either way great job on your video dude your gonna be my go to sensai for tech stuff,True
@Faax244,2018-02-25T07:15:12Z,0,"Best Webscrape Tutorial on youtube! Thx! One cool guy from Germany ChaosComputerClub is scraping the complete news website called ""Spiegel-online.de"" for 3-4 years, every day, 24/7 and saved all of that. Very cool and useful meta-datas to get very personal Information about the intern Company structure. Its online on youtube search for ""ccc spiegel online""",True
@wardrich,2018-02-23T20:59:28Z,0,"Let's say we're working with a really sloppy site where the developer can't handle their own naming conventions properly.  I've got two separate arrays set up - one for table headers, one for dynamic content.  But, due to the genius coding on the site, some of the headers show up as dynamic content.  The bogus data does have a unique ID, but the problem is it meets the wildcard findAll for both array creation lines that I've made.  ie:   Good data1:  fooBar_[random characters for each item]_lblThisRow bad Data1: fooBar_[even more random characters]_lblSillyProgrammerUsedWrongPrefix Good data2: fooBar_[more random characters]_lblThatRow  Is there any way I can create a few lines of code to pluck out the few instances of bad data?  The (bad) naming convention is at least consistent with the template, so any search results come back have the same mistake.",True
@gerardoruiz7109,2018-02-22T23:52:44Z,0,U RE AWESOME! <3,True
@chevonnwosu5843,2018-02-20T21:53:14Z,0,What are some common algorithms used in web scraping?,True
@amgmrahmanmizan5207,2018-02-20T01:07:51Z,0,"hello sir,, i have learned Scraping from "" watched this video"" & successfully, its working.. but today.. i have faced a problem!!! sir, how can i scrap these kind of site ""http://ketchumidaho.org/BusinessDirectoryii.aspx?ysnShowAll=1&lngNewPage="". & rubi,php, etc"""" looking for your help!!!",True
@vladalx27,2018-02-17T16:59:52Z,0,"I keep getting 'return codecs.charmap_encode(input,self.errors,encoding_map)[0] UnicodeEncodeError: 'charmap' codec can't encode character '\xae' in position 104808: character maps to <undefined>' when I try to run the script in the cmd.  Do you know what the souce of this problem might be?",True
@alyyaizzati3622,2018-02-17T14:01:40Z,0,"Help me :(  from urllib.request import urlopen as uReq Traceback (most recent call last):   File ""<stdin>"", line 1, in <module> ImportError: No module named request",True
@adityakaushal3955,2018-02-17T07:06:33Z,0,"Very easy to understand and to follow. Good explanation, with good content. Thank you for sharing your knowledge.",True
@matthyland2965,2018-02-17T06:53:46Z,0,"im getting  python my_first_webscrape.py   File ""my_first_webscrape.py"", line 27 print(""brand: "" + brand) IndentationError: unindent does not match any outer indentation level",True
@allahalkareem8055,2018-02-14T20:22:00Z,0,"Got stuck at 21 minutes... I've noticed that your example is *very* convenient for scraping and that, in reality, most websites probably are not. For example, for the website you used, every item (graphics card) has the same container, literally called ""item-container"", so ther are 12 of those. The site I want to scrape from though, there are 20 items (car lease deals) and 20 different containers, example deal_3273683264, deal_456736420, etc.) Great video either way, I have learned alot despite being unsuccessful.",True
@MrHelium273,2018-02-14T13:28:35Z,0,"If you get a ""403 forbidden"", add a header to your variable like so :   url_1 = 'Your url here'  url_2 = Request(url_1,headers={'user-agent': 'Mozilla/5.0'})",True
@idcashflow,2018-02-14T12:02:48Z,0,"u are very good explaining this content, thank you so much. it so easy :D",True
@theredflagisgreen,2018-02-11T09:54:44Z,2,"Ok, yeah, I'm a rookie --- getting an error when I type ""pip install bs4"" --- what to do? Thanks",True
@daviidon,2018-02-10T17:30:16Z,0,Thanks a lot for this tutorial... Having a bit of difficulty with going through pages but I will get it eventually üôå,True
@flyffreak93,2018-02-09T07:44:29Z,0,How do you resolve a incapsula issue it block my bots,True
@jordymunozaravena3286,2018-02-08T20:38:35Z,0,"Some people don't get it, but being able of doing this is fucking awesome.",True
@MaxDavidsonArgentina,2018-02-07T20:07:16Z,0,Really useful. Thank you very much!Please carry on,True
@migueltavares9108,2018-02-07T18:36:49Z,0,How do I update the information that is inside of excel file automaticaly?,True
@joaoostrowski,2018-02-07T16:16:25Z,0,"Thank you so much for the videos Data Science Dojo! I'm a data analyst working in Hungary, and I received a small scraping task to do in the last few days. I had a previous notion of how requests and BeautifulSoup worked, however, your video brought me from zero to hero to finish the task. Keep the good work!",True
@akhilkarkera9178,2018-02-07T10:48:18Z,0,"Hey dude great video... wanted to scrape keyword so that when we search any keyword (example : Company name) it will go on google and extract basic info such as company's -URL,  contact details, latest news, social media page etc... can you make a project of such kind? and is there anyone that can help?",True
@sahan_desilva,2018-02-07T10:47:23Z,0,"Thanks heaps, your explanation was perfect and the tutorial was very informative! :)",True
@yauul,2018-02-07T07:00:40Z,0,Great video! Really enjoyed it.  Do you have any suggestions on how to web scrape data from Facebook? Is that even a thing?,True
@mattshort181,2018-02-04T15:47:16Z,1,"Awesome video. Ran into one problem, all my prices are numbers, none of them show up as Free Shipping even though when I look at the page source and can see that they say Free Shipping.",True
@maryammemon3795,2018-01-31T14:22:40Z,0,"when i write  page_soup = soup(page_html, ""html.parser"") its giving me error",True
@viky789,2018-01-31T09:50:05Z,0,I think i got blocked by new egg T.T,True
@daniellynes1714,2018-01-30T20:45:48Z,0,"Had the Nonetype error and I got around it using the try, except, pass method. Her is my code for anyone who is also struggling with this.  from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup  #webpage url that we are goint to scrape information from my_url = ""https://www.newegg.ca/Video-Cards-Video-Devices/Category/ID-38?_ga=2.114534094.850576897.1517253140-181998792.1517253140""  uClient = uReq(my_url)  page_html = uClient.read()   uClient.close()  #html soup parsing page_soup = soup(page_html, ""html.parser"")  containers = page_soup.findAll(""div"",{""class"":""item-container""}) filename = ""products.Video-Cards-Video-Devices""  f = open(filename, ""w"")  headers = ""brand, product_name, shipping\n""  f.write(headers)  for container in containers:  try:   brand = container.div.div.a.img[""title""]  except:   pass  title_container = container.findAll(""a"", {""class"":""item-title""})  product_name = title_container[0].text   shipping_container = container.findAll(""li"",{""class"":""price-ship""})  shipping = shipping_container[0].text.strip()      print(""product name: "" + product_name)  print(""shipping: "" + shipping)  print(""brand: "" + brand)    f.write(brand + "","" + product_name.replace("","", ""|"") + "","" + shipping + ""\n"")  f.close()",True
@miller1520,2018-01-30T19:44:04Z,0,Excellent! Thanks a lot!,True
@sassafras2336,2018-01-29T23:22:29Z,0,"tip for windows 10 (or others) I had to put .\python instead of just python in the cmd, hope it helps!",True
@fongchekee7698,2018-01-29T17:06:33Z,0,"I've encountered problem with the user agent but I was able to solve that by specifying a fake agent. When i reach the page_html step where it sets the html.parser, then I couldn't read anything from any elements like h1. Nothing gets return.Then  I change my url to the one this video shows, the h1 was returned. What is possibly the problem? Could it be the https page that I am trying to request from? Please help :(",True
@fongchekee7698,2018-01-29T14:26:17Z,0,"I've encountered problem with the user agent but I was able to solve that by specifying a fake agent. When i reach the page_html step where it sets the html.parser, then I couldn't read anything from any elements like h1. Nothing gets return.Then  I change my url to the one this video shows, the h1 was returned. What is possibly the problem? Could it be the https page that I am trying to request from? Please help :(",True
@DezziBeats,2018-01-28T18:35:49Z,0,you are a legend <3,True
@maryammemon3795,2018-01-28T18:21:26Z,0,Great video u explain very well plz upload more python related videos,True
@evildevilinme,2018-01-26T03:59:05Z,0,"if you are getting Traceback (most recent call last):   File ""scrape.py"", line 21, in <module>     brand = container.div.div.a.img[""title""] TypeError: 'NoneType' object is not subscriptable  its is because the line  brand = container.div.div.a.img[""title""] is looking for an brand image with the value of title embeded in it. some days/some products won't have a brand image and will cause an error, you can fix this by adding an if/else statement or you can reanalyze the cntrl+u like i did and use  brand = container.a.img[""title""]  brands = brand[0:5] to take first 5 characters of the large pictures id which seems to be consistent!  thanks for the tutorial!!!!",True
@bobn8r53,2018-01-25T22:56:30Z,0,"Great video, I think I'm going to like web-scraping. You seem to have covered all the bases, except how would you include the product image?",True
@Lockey007,2018-01-25T16:22:43Z,1,Fantastic! Thank you very much! Very clear and detailed tutorial!,True
@ipomopmno,2018-01-25T14:36:27Z,2,"I have problem. every data write in only one column (A1,A2....) And I dont know why.",True
@sidharthhemani6068,2018-01-25T13:53:01Z,0,Really Superb Video,True
@joanemmanuelleamato3963,2018-01-25T12:08:17Z,0,"Great video! I'm a beginner and I really understood it all! Just subscribed to your channel.  I just have a little doubt. After running the code, this message appears: Traceback (most recent call last):   File ""C:\Users\amato\Downloads\webcrawler.py"", line 19, in <module>     brand = container.div.div.a.img[""title""] TypeError: 'NoneType' object is not subscriptable  I wonder if someone may help me!",True
@dylanduregger4906,2018-01-25T03:36:51Z,0,"What do you do if you want to get data from a tag, but there are multiple of the same exact tag?  for example: <div id=""sortable"">      <td align=""right"">27</td>      <td align=""right"">30</td>      <td align=""right"">19</td> If i just want the second one how would I do that? If i use soup.find(""div"", id=""sortable"") it only comes up with the first one? What would i do if i just want the middle or last one?",True
@aishwaryaprakash7935,2018-01-24T08:17:03Z,0,"m just using  it on ubuntu 16.04 command terminal it does not recognising the command ""from urllib.request import urlopen as uReq."" it shows error in this no module named request",True
@user-rl4cw1uv4s,2018-01-24T04:56:52Z,0,"Great video, thank you.",True
@fackmedude,2018-01-23T03:04:11Z,0,"ye nice one broski, i understood everything",True
@SeanCotton,2018-01-20T21:26:31Z,0,"Great video,  I am getting the following error:  ValueError: I/O operation on closed file.  any idea why?",True
@pastorofmuppets7654,2018-01-19T17:35:58Z,0,2+2 is 4 quick math,True
@akashkesani4629,2018-01-18T20:58:55Z,0,This was mindblowing.,True
@kaostyl,2018-01-16T16:55:38Z,0,Ho yes Thank you!! I'm a really beginner in python and I almost undertood everything!! I already have many idea how to use that ;) Do you know how to scrape pages on a website where you have to login in? I mean is it possible to login in on a website with python? Again Thank you so much :),True
@binaykushwaha4887,2018-01-15T14:32:34Z,0,"best web scraping tutorial, i have been ever seen. thanks bro !",True
@exili,2018-01-15T04:44:31Z,0,"Excellent video. I am just starting with python. Taking a stab at this and I am stuck already. I get stuck right at the:¬† from urlib.request import url open as uReq raceback (most recent call last): ¬† File ""<stdin>"", line 1, in <module> ImportError: cannot import name urlopen  I am on python 2.7 is urllib something that I have to download and install? I already added Anaconda so I am not sure what to do here",True
@arunm619,2018-01-13T09:56:33Z,0,how to programatically enter text in a text field and submit using python?,True
@manishjain143,2018-01-12T04:42:14Z,0,"Thank you #datasciencedojo so much for this great tutorial. I have got a question - I want to click a button ""load more products"" and then scrape more. How do we do that? I would really appreciate any pointers.",True
@josephwright8140,2018-01-11T02:27:48Z,0,Excellent tutorial!,True
@nikksengaming933,2018-01-10T21:12:08Z,0,Easy to follow and understand. Very nice tutorial. Thank you very much!,True
@cd-ux9ot,2018-01-10T18:38:47Z,0,Awesome video! I think you can use join() to concatenation with delineator,True
@amazon628,2018-01-10T14:09:28Z,0,excellent,True
@stank666ocean,2018-01-10T03:37:42Z,1,getting an error around 8:23:  urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)>  Maybe this has something to do with using OS X instead of Windows?,True
@christophedamour6919,2018-01-08T15:59:33Z,6,A BIG BIG THANK YOU: the most understable tutorial I've ever seen on how to scrape a web page (and I have visionned like 100 of them),True
@kloudmuka,2018-01-07T16:28:50Z,0,this video perfectly showed how bad can a tutorial be,True
@jessewalker1880,2018-01-07T01:20:19Z,0,Fantastic video! More data science videos! Im an aspiring data engineer,True
@ankitjoshi691,2018-01-04T10:48:05Z,0,"You, sir, have beautifully explained the workflow of web scrapping. Thank you so much.",True
@xavierbays8392,2018-01-04T09:09:07Z,0,Really good content! thank you,True
@tonyd6853,2018-01-03T21:32:01Z,0,Great Intro to this stuff! Nice Video!,True
@mathiasrandryyt,2018-01-02T19:32:36Z,0,THANK YOU!,True
@deepakg8645,2018-01-02T07:20:44Z,0,Really excellent way of teaching...understood every single word..just one doubt...what if i want to extract historical data ??say i want to scrape through  last 1 month airline price details  ??Appreciate if you could point me in the right direction in this case.,True
@sunilgupta3717,2018-01-02T03:01:49Z,0,Very nice video,True
@technohubm6014,2017-12-31T10:08:53Z,0,can we scrap number of web pages and collect only text to collect keywords,True
@mahimsd7645,2017-12-31T08:29:26Z,0,it is more than 100% percent......,True
@pycoderguy1835,2017-12-30T21:14:11Z,0,Can you also grab the image of a product and store it?,True
@pycoderguy1835,2017-12-30T18:18:46Z,0,"Great video! But when I tried running the script at at 29:01, the Anaconda Prompt said: 'python: can't open file 'WebScraping_NewEgg': [Errno 2] No such file or directory'",True
@PratimaShri,2017-12-30T13:45:20Z,0,"Thank you for sharing this video.. Awesome content. I like the way you explained the flow and hands-on. Here is a question, when I tried scrapping other site , I am getting 403 forbidden error , how do I fix that? Is it possible to scrap a secure site?The website has https.Another question I have is,  in a real world situation, do data scientists go through each individual websites and web scrap like you did or there is some other ways? Lets say I want to get a list of all top sales online in women clothes, then do I web scrap all the websites selling women clothes?",True
@yashi340,2017-12-29T17:19:49Z,0,"This is the best tutoring for webscraping... In just one vedio I learned so much and understood very nicely. It was my first time and I didn't know even a inch but I just stuck to this one ,not even for once opened any other video. It's damn amazing. Thank you for making it and making it so well.",True
@fishface6247,2017-12-28T22:08:05Z,0,This is a great video. Thanks so much. How about how to scrape and download images from a website?,True
@fishface6247,2017-12-28T21:52:26Z,0,Is there a tutorial to scrape YouTube? To Make a backup of everything?,True
@appoljuce,2017-12-28T15:58:32Z,0,"one of the best tutorials ever, tbh",True
@aroneinhorn8570,2017-12-28T04:06:15Z,0,Loved this very clear tutorial. You should look into using Jupyter Notebook instead of having to jump between the script and the command line. Thanks,True
@edentan3891,2017-12-27T15:08:20Z,0,"Sir, what does container[0] mean?",True
@87Mrdoki,2017-12-27T12:31:21Z,0,"very good video. 22:36 you get the a tag's ""item-brand"" because the container itself is the div  ""item-containter"" so instead off findAll you can do container.div.a.text and get the same result faster (explicit tagging is faster than findAll).",True
@shairfair,2017-12-25T07:03:41Z,0,"Hey man, love the video! I'm getting stuck on one thing and can't get past it.  change = page_soup.findAll(""tr"")  for contain in change:     coin_container = contain.findAll(""a"", {""class"": ""currency-name-container""})     coin = coin_container[0].text      percent_container = contain.findAll(""td"", {""class"": ""percent-24h""})     percent = percent_container[0].text      cap_container = contain.findAll(""td"", {""class"": ""market-cap""})     cap = cap_container[0].text.strip()      price_container = contain.findAll(""a"", {""class"": ""price""})     price = price_container[0].text  print(coin) print(percent + ""%"") print(cap) print(price)  With that, I'm getting this error: Traceback (most recent call last):   File ""webcrawl.py"", line 31, in <module>     coin = coin_container[0].text IndexError: list index out of range  Any suggestions would be amazing.  Thank you!",True
@jaikishanchudasama8579,2017-12-21T13:52:45Z,0,i am having an issue trying to open the webpage because now it uses https instead of http as in the demonstration,True
@SANJIVRAI6693,2017-12-20T19:43:09Z,0,"Great Video Sir, Down below is the code (plz delete if not allowed)  #urllib is a package, .request is a module, urlopen is a function from urllib.request import urlopen as uReq #if i call soup as a function gonna call a beautiful soup function within the bs4 package from bs4 import BeautifulSoup as soup   # use the command line to check the validity of your code my_url = 'https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card'  #Grab the webpage and download it, make it a connection uClient = uReq(my_url) #off loads this client into a variable page_html = uClient.read() #Since it's an open web connection u wanna close it uClient.close()  # giving it a var so doesnt get lost.. we need to parse it --> WHAT DOES THAT MEAN? page_soup = soup(page_html, ""html.parser"")  #grabs each product containers = page_soup.findAll(""div"",{""class"":""item-container""})  #saving file in CSV filename = ""productsGC.csv"" f = open(filename, ""w"")  #Header for the file headers = ""brand, product_name, shipping\n"" f.write(headers)  for container in containers:  brand = container.div.div.a.img[""title""] #Grabs the thing that makes the graphic card    title_container = container.findAll(""a"", {""class"":""item-title""}) #finds the class  product_name = title_container[0].text   shipping_container = container.findAll(""li"", {""class"":""price-ship""})  shipping = shipping_container[0].text.strip()   print(""brand: "" + brand)  print(""product_name: "" + product_name)  print(""shipping: "" + shipping)    f.write(brand + "","" + product_name.replace("","", ""|"") + "","" + shipping + ""\n"")  f.close()",True
@MamunAgni,2017-12-20T16:13:02Z,0,"I was interested to learn python scraping. Only from this video in YouTube how things working :D Now, I can build small scraper. Would you please make a video about python function with live example like this video.",True
@mkmkm586,2017-12-19T19:57:23Z,0,24 mins,True
@vedsengupta8956,2017-12-19T04:57:58Z,0,extremely helpful thanks a lot,True
@arthurperceval9005,2017-12-17T18:13:31Z,0,Nice tutorial. I have a question about web like (google) that know ban your ip if you send a lot a request for scrapping. How can we avoid to be banned by such web sites?,True
@reezalaq6522,2017-12-15T17:52:13Z,0,"How do you return only the text from span like below ?  >>> container.findAll(""span"",{""class"":""value""}) [<span class=""value"" itemprop=""price"">US $15.49 - 26.49</span>] >>>",True
@reezalaq6522,2017-12-15T16:11:52Z,1,I love this tutorial!  Just what I've been googling for months.,True
@SmartKnowl3dg3,2017-12-14T04:25:36Z,0,inspect element by pressing f12 key,True
@meitoli,2017-12-11T02:05:22Z,0,is the easiest tutorial i have ever seen thanks!,True
@kellyleaveck9923,2017-12-10T19:49:16Z,0,Awesome! Thank you.,True
@mahimsd7645,2017-12-10T12:48:45Z,0,"gr8 work ,........excellent.... add some more real world  examples .......",True
@bryanmckeon1515,2017-12-08T20:06:38Z,0,"I'm very new to python. I did this 100%, great tutorial. I am trying to do this on another page now but I keep getting something like ""urllib.error.HTTPError: HTTP Error 403: Forbidden"" after entering the line ""uClient = uReq(my_url)"". From what I've read it detects me as a bot. So my question is how do I go about  changing the user agent for urllib using this?",True
@netdevgamer,2017-12-08T19:28:54Z,0,Fantastic tutorial.. thats so clear!!,True
@mohsin-ashraf,2017-12-08T16:11:05Z,0,Its a very great video for beginners to learn web scraping.,True
@ehsan3102,2017-12-08T12:20:17Z,0,"Thank you, this video was really informative and explained in a clear way",True
@draganoiugeorge6010,2017-12-06T18:29:41Z,0,first day learning python. i hate it. wtf are all these packages. why use urllib when it fucks up on headers. who cares about headers anyways...  urllib.error.HTTPError: HTTP Error 403: Forbidden  because python :D  literally pasting the url in the browser and it works. in urllib fuck no. gg,True
@assujabbari,2017-12-06T18:20:46Z,0,This is the superb video I came across and it helped me learn too many things. I made a small scraper also. But now when I am trying to scrape news form following website I cannot do so https://techcrunch.com/search/healthcare#stq=healthcare&stp=1 can anyone help,True
@simerpreetsingh6765,2017-12-06T02:44:18Z,0,Thank you!,True
@mmarchand1,2017-12-05T01:15:31Z,0,"AMAZING TUTORIAL --- *QUICK Q*  Did anyone else have issues with the loop? I've been trying to figure it out but it seems to continue only printing (brand/product) for container[0].   If I define each container (i.e. 1 or 2 or 3) before running the loop - it changes the outcome accordingly, but still only prints for the one container.  Any advice?",True
@bulbouscorm,2017-12-05T00:44:09Z,0,God I've been so stupid!! Inspecting elements!!! AAAGH,True
@sweetemosean,2017-12-02T21:37:08Z,0,Thank you very much! You taught this exactly the way I learn the best.,True
@KSals,2017-12-02T04:02:56Z,0,"I didn't pick up on the part where you say ""product_name = title_container[0].text "" Why is the ""[0]"" there for title_container in our for loop?",True
@cdh04c,2017-11-30T03:55:51Z,0,"Thanks man! I tried to use the same framework on another site and I can't get it to loop through all containers. What am I missing?  containers = page_soup.findAll(""div"",{""class"":""thumb-block""})  for container in containers:  image_url = container.div.div.a.img[""data-src""]    title_container = container.p.a[""title""]  title = title_container    vid_num_container = container[""id""]  vid_num = vid_num_container  print(""image_url:  "" + image_url) print(""title:  "" + title) print(""vid_num:  "" + vid_num)",True
@theawesomeskux,2017-11-28T13:00:59Z,0,Can anyone type me some quick code on how you would loop this to grab more the next page of data as well. Cheers,True
@gamesmaster5322,2017-11-25T20:36:11Z,0,I can call the page_html = uClient.read() code on the newegg website but when i try other websites i get this error: ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host . And yes i have http:// not https://,True
@jrq3rq,2017-11-23T22:06:37Z,0,solid video!,True
@alexklein278,2017-11-22T21:55:57Z,0,"GREAT introduction to webscrapping, thank you",True
@kartiksareen,2017-11-22T12:28:34Z,0,"I have a simple question .......when v make an object page_hmtl it reads the page and saves in it...but what is it doing..when we r writing page_soup = soup(page_hmtl, 'html.parser), is this also reading the page and saving it...kindly explain if anyone knows..I am a bit new....so have more quesitons",True
@shabanasheikh6058,2017-11-21T18:29:37Z,0,SOooo nicely explained thank u soooooooooooo much,True
@simonlink5060,2017-11-21T14:26:24Z,0,2+2 is 4 - 1 thats 3 quick maths,True
@mchojo4646,2017-11-21T05:12:52Z,0,28:00 now dis bish just showin off lol,True
@benjaminbock5589,2017-11-20T20:35:37Z,0,Incredibly helpful and easy to follow. Now I just have to find a way to run that thing on a server.,True
@whatisahandleeee,2017-11-18T03:13:21Z,0,Why don't you use scrapy?,True
@shahrukhcress,2017-11-15T17:11:23Z,0,"import urllib2 card = ""https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card"" page = urllib2.urlopen(card) from bs4 import BeautifulSoup soup = BeautifulSoup(card, ""html.parser"") print soup.prettify() soup.findAll(""div"",{""class"":""item-container""}) after this only 4 results are loaded (len(soup)) but in video there are 12, what am I doing wrong here?",True
@freephone9312,2017-11-14T10:01:16Z,0,GREAT TUTORIAL!!,True
@mayurdangar9946,2017-11-14T08:10:33Z,0,It is great to learn data science with your favorite site(steam) :D  hardcore gamer,True
@hugoharada5301,2017-11-13T17:01:54Z,0,Great video!!!,True
@amirahmadi1981,2017-11-13T15:51:23Z,0,THANK YOU SO MUCH FOR THE GREAT VIDEO!!,True
@davidrodriguezgimeno4863,2017-11-12T20:17:32Z,0,Thanks a million for this video !,True
@vineetravi3658,2017-11-12T07:15:08Z,0,EXTRACT ALL THE EVENT DETAILS FROM THE FOLLOWING START URL start_url : http://allevents.in/new delhi/all How to do this using scrapy? Is there any experts in scrapy who can help me out?,True
@LampOfMagic,2017-11-11T17:37:00Z,0,wait did they bring python to windows recently?,True
@timmorphine,2017-11-10T11:04:43Z,0,"I really enjoy your teaching style, keep it up and more on scraping, python, coding. Thank you!",True
@JoenneGee,2017-11-10T06:08:48Z,0,"its not seperating the data into seperate columns in the csv file, do anyone know why?",True
@zeneto2157,2017-11-09T17:19:45Z,0,"So ... i spent 2 days figuring out your mistake to make my code work. 4:50 ---- from urllib.request import Request, urlopen ---",True
@getdilip,2017-11-08T12:18:44Z,0,This is the best I found on this topic. I also tried this to implement for Amazon but didn't get the products container although I was able to get the other informations. Probably I need to pass the header and post data for the same. Can you please help me out.,True
@prashant5611,2017-11-08T08:34:17Z,0,Thanks a lot for the great video.. best video on web scraping. this is what exactly I was looking for :),True
@LaurinSchaller,2017-11-07T13:11:03Z,0,Cool but how would i add a request header   in this example? The page i want to scrape just keeps throwing 403 responses back at me. It probably thinks im a crawler or bot and blocks me.,True
@drashyakushwah5452,2017-11-07T07:16:37Z,0,Thank you soooo much...... one of the best tutorial I have seen on YouTube... this helps me a lot ‚ò∫Ô∏è,True
@braanddon2628,2017-11-06T00:49:03Z,0,"It dosen't work for me, when I install Anaconda and open the prompt and type ""python"", it says there is an error! Can someone help?",True
@CyberAbyss007,2017-11-05T21:14:47Z,0,Thank you! Spent several hours with Derek Bana's Python primer then your video was the second one. I can't believe it worked first try on almost every step. Only had to reboot after installing Anaconda before getting the bs4 import to not fail. Just the OS needing to update the path.,True
@durneacosmin1227,2017-11-05T19:55:39Z,0,Hello everyone! Is this solution applicable for websites which are built with AngularJS?,True
@smilingcrocodile,2017-11-05T15:44:55Z,0,"Thanks a bunch, helped a lot !",True
@thorstentorento2036,2017-11-05T15:22:52Z,0,"hi,   Does anyone know how i can access a second div rather then the first one.   example:   My container looks like this  <div> 1<img> </img> </div>  <div>2.1<div>2.2 </div><div>   By adding container.div.div i am told there is no such container. I guess because i am accessing div 1 and in there there is no further div.   In the tutorial the the path was kind of straight and i have no clue how to get to my second div, so i hope someone can hep with a solution.",True
@kishorsubedi4215,2017-11-04T18:00:11Z,0,"It shows "" Urllib.error  certificate verify failed. ""Why is that? anybody help me please.",True
@chrischandler3041,2017-10-30T02:59:46Z,0,Just wondering your thoughts on whether or not web scraping is  copyright infringement?,True
@samarkishor4607,2017-10-28T09:52:27Z,0,"Hi, I am getting this syntex error. WHat's the reason please ?  >>> my_url='https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card' >>> my_url'https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card'   File ""<stdin>"", line 1     my_url'https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card'                                                                                               ^ SyntaxError: invalid syntax",True
@astonsong5616,2017-10-28T00:40:58Z,0,"Thank you, this is very helpful and easy to follow.",True
@delvecchiogiuseppe3454,2017-10-27T21:06:06Z,0,AMAZING!!!!! you just helped me so much with my school project! please make more videos like that about popular python packages! love this programming intros and tutorials!!,True
@peshonetev3458,2017-10-26T20:15:07Z,1,Really nice and clear tutorial. Thanks :),True
@seherkhan1951,2017-10-26T19:21:30Z,1,"Thanks a ton! You've made this really, really easy!",True
@aaryanbatra9265,2017-10-25T12:42:54Z,0,"Nothing returns? Here is my code.. I just searched u graphics card on the site.. maybe the code changed? I installed anaconda and i have python 3.5.2 I'm on a mac version 10.11.6 OSX El Capitan  from urllib.request import urlopen as uReq #urllib is a package, .request is a module, urlopen is a function from bs4 import BeautifulSoup as soup #if i call soup as a function gonna call a beautiful soup function within the bs4 package my_url = 'https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card' # use the command line to check the validity of your code  uClient = uReq(my_url) #Grab the webpage and download it, make it a connection uClient.read() #dumps it out  page_html = uClient.read() #off loads this client into a variable uClient.close() #Since it's an open web connection u wanna close it  page_soup = soup(page_html, ""html.parser"") # giving it a var so doesnt get lost.. we need to parse it --> WHAT DOES THAT MEAN?  containers = page_soup.findAll(""div"",{""class"":""item-container""}) #grabs each product  for container in containers:  brand = container.div.div.a.img[""title""] #Grabs the thing that makes the graphic card    title_container = container.findAll(""a"", {""class"":""item-title""}) #finds the class  product_name = title_container[0].text   shipping_container = container.findAll(""li"", {""class"":""price-ship""})  shipping = shipping_container[0].text.strip()   print(""brand: "" + brand)  print(""product_name: "" + product_name)  print(""shipping: "" + shipping)  print(""hello"")",True
@hongren99,2017-10-25T12:20:50Z,0,"Everything is great just the voice, bearing it",True
@ManuelIzturriaga,2017-10-24T06:12:20Z,1,"This was great. At first i thought I'd just watch the first 10 minutes to get an idea since I am just starting with python, but it was so engaging and simple to follow that I was surprised when i realized I got to the end of the video and wanted more. Thank you!",True
@StuMorris,2017-10-23T11:56:26Z,0,"This is great thank you, really clear and easy to follow. Just one question, how do I scrape a site which requires a password to access it. At work we use a site which requires a password to see our timetable each week, I would like to be able to pull this data into a csv file without needing to access the site in order use the data elsewhere. Is there a video on this?",True
@jgoep2310,2017-10-22T19:59:23Z,0,Great! This has been really straight forward. :) I think I won't go on with my crappy web scraping book. And the source code editor is amazing! Thanks a lot!,True
@sk8rhippie,2017-10-21T17:14:49Z,0,"I'm surprised the final excel doc didn't have a column for prices..? I only skimmed the video, but I should be able to add a ""price"" column for my own purposes. If you went over this in the video, let me know!",True
@olimpiatobiasz1407,2017-10-20T20:19:46Z,1,Big thanks for the tutorial! You made it easy to understand.,True
@DDay_8,2017-10-20T00:22:32Z,0,When I put uClient = uReq(my_url) I get a bunch of errors. Please help.,True
@NikosKatsikanis,2017-10-19T13:49:52Z,0,What if you just want to use JS?,True
@davidskarbrevik,2017-10-17T18:18:08Z,0,Is the code posted somewhere? Looked in the description but didn't see it.,True
@michaelsalam1475,2017-10-17T14:43:06Z,1,THANKS A LOT!!!,True
@cowshrptrn,2017-10-17T00:37:31Z,0,"Great video, but you should start using jupyter notebook for interactive prototyping. Much easier than copying and pasting from command line.",True
@ParkerWilliams002,2017-10-17T00:03:38Z,0,"I have tried this multiple times but it doesn't seem to work with https secured websites. Any ideas around this or how to fix that so that it works?  Example:  >>> import bs4 >>> from urllib.request import urlopen as uReq >>> from bs4 import BeautifulSoup as soup >>> my_url = 'https://www.lds.org/general-conference/conferences?lang=eng' >>> uReq(my_url) <http.client.HTTPResponse object at 0x10f733160> >>> uClient = uReq(my_url) >>> page_html = uClient.read() >>> uClient.close() >>> page_soup = soup(page_html, ""html.parser"") >>> page_soup.h1 <h1><b>Not Available</b></h1>",True
@MsFlorinT,2017-10-16T18:14:20Z,1,Great video!,True
@freshcube94,2017-10-15T16:56:30Z,0,"Hey guys, my Sublime is not automatically gathering the commands. Does anybody has a clue why? Thanks so much in advance :)",True
@luisaburto3821,2017-10-11T18:09:47Z,1,amasing work mate thaks a lot,True
@donaldducksdaughter,2017-10-10T20:15:38Z,1,THIS WAS SO AWESOME AND SO MUCH FUN! THANK YOU!!!!,True
@bugsarecool8483,2017-10-09T19:33:18Z,0,Wow!! You are great!! Please make more videoooooooosss Pleeeeeeeeaaase!!!,True
@blackdedo93,2017-10-09T13:09:54Z,1,"DUDE, this is the best video I`ve seen, Very detailed, not boring, even python noobie can follow up, I can't believe I watched 33 min without being bored, gj looking for more amazing vids",True
@vinaybhardwaj100,2017-10-07T20:06:29Z,0,"when writing data into .csv file, first data set comes in first row in different columns, no problems there but, 2nd data set comes in 4th row i.e. leaving a gap of 2 rows.can anybody help there",True
@vinaybhardwaj100,2017-10-07T15:31:20Z,0,it was life saviour.,True
@moealmaw,2017-10-07T13:18:40Z,0,Alright,True
@rahulchandra759,2017-10-07T09:04:11Z,0,Couldn't find a tree builder with the features you requested: html_parser. Do you need to install a parser library?,True
@rahulchandra759,2017-10-07T07:52:33Z,0,"I ran into error on this line of code  page_soup = soup(page_html,""html_parser"")  I have python 3.5 and I was able to install bs4 without error Anyone, any ideas ?",True
@arjoon,2017-10-06T04:01:35Z,68,"This was really good content, definitely the best intro to web scraping I've seen. You don't go through it as though you're reading from the documentation, there's more of a flow.",True
@OrcaChess,2017-10-05T23:54:31Z,0,"Hello everybody, in my excel file I did not get a separation of columns. somehow is all information in one column. Did somone else had this issue? Thanks in advance!",True
@OrcaChess,2017-10-05T21:44:24Z,0,Can I use Pycharm instead of sublimetext?,True
@gigibostan,2017-10-04T12:56:14Z,0,"hey, one question though, what if you need a piece of information that is available only if you open one specific video card page? For example you need the comments, but you can see them only if you open each video card page? Can you go that deep with it?",True
@recipesforallful,2017-10-04T10:17:03Z,0,"Great video, the first one that I was able to follow until the end.  I have an issue with my output file as it only prints the headers and the first line. How can I create a container loop so it goes through all available containers. When we defined container = containers[0] I dont see where in the python code that line was added. Thanks for the help and keep it up.",True
@JesusMRamosPerez,2017-10-02T14:03:31Z,1,Loved the video! This is my first python script and crawler. Thanks a lot brah! Keep them coming.,True
@josephsmith6924,2017-10-02T12:55:27Z,0,"my urllib says that urlopen is non existant wtf, only works if i do it the long way not ""from x import x as y"" way",True
@alibee6232,2017-10-02T06:09:36Z,6,"when i type uclient = ureq(my_url) it gives me a 403 error forbidden and a bunch of timeout, does this mean that it works but it crashed or will crash if it runs?",True
@ohadedri5737,2017-10-01T19:51:19Z,0,amazing!!!,True
@laylow9000,2017-10-01T11:49:59Z,0,Great video!! I learned a lot.   One question; is it possible to loop through drop down menus? I think that would make a great addition to this video.   Thanks again!,True
@xablau0344,2017-09-30T01:39:00Z,0,"Hi, I'm Brazilian, I need someone who can help me urgently.",True
@matthewanderson2959,2017-09-29T15:11:04Z,0,Great introduction for web scraping in python using beautifulsoup. Do you also happen to have a tutorial on using for example csv.dictreader to write your csv?,True
@ataidemachado1409,2017-09-25T16:39:17Z,0,"Thank you very much for the knowledge class, I found it very didactic and now I can progress in my projects.",True
@Lulu008esim,2017-09-25T14:06:30Z,0,"Thank you ! One question : ""internet becomes an entire database"", is it sometimes illegal to retrieve data from a website ? How many percentage of websites really allow data sharing ?",True
@moneyharry,2017-09-23T06:59:13Z,0,"it was fun, really enjoyed it!",True
@jacksmollen9023,2017-09-22T16:59:46Z,0,AWSOME VIDEO!,True
@KamilTM1,2017-09-22T10:00:53Z,0,"There is a shortcut for findAll provided ;) You can just type soup('div', 'class':True}) or sth ;)",True
@tayebsaadi,2017-09-22T00:49:37Z,0,Thank you,True
@benjaminmiao3630,2017-09-21T14:05:51Z,0,"Geez, I can't believe that I learned so lot in this simple video.",True
@juancruzalric5771,2017-09-20T19:32:37Z,0,"Hello, when i open my csv file i don't get the information divided in columns, i got it divided with "","" and i dont know why. I follow step by step of your guide. Please someone help me. Thanks!",True
@iloveanime9226,2017-09-20T16:34:09Z,0,"I love excel, now it's time to add python into it :) as a programmer I am used to semicolon :( sad I hated it at one point XD",True
@SZ-ZS-,2017-09-19T17:27:00Z,2,"hopefully you read this soon, when i try to do page_soup.h1, or anything, it says:  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module> AttributeError: 'tuple' object has no attribute 'h1'   if you know how to fix, please lemme know",True
@Moccar,2017-09-18T21:34:15Z,3,"I get an error when trying to call uClient & page_html. It says: File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1318, in do_open"" and then like 40 more lines like that but from different files like client.py and ss1.py and so forth. I don't really know how to fix it. Anyone here has the same issue?",True
@hamids4550,2017-09-18T20:27:17Z,0,That was great. This is the first coding lesson that I saw useful. Please post more of different projects. Gave you a BIG LIKE!,True
@Patrick-on6mv,2017-09-17T00:17:37Z,0,Had to dislike as this video is stolen directly from another Youtuber with less views. EVERYTHING YOU DID YOU JUST COPIED. scriptkiddie.,True
@kandouci,2017-09-16T22:04:55Z,0,"I like it so much, thank you :)",True
@sejecore,2017-09-15T11:03:14Z,0,"Awesome video dude. Really, really nice!",True
@johnhechtlinger9465,2017-09-14T15:01:50Z,0,That was quick but very good...,True
@memoriasIT,2017-09-14T13:58:34Z,0,BEST Tutorial I have ever seen <3,True
@resistpen6582,2017-09-14T05:11:20Z,0,"man...!!! this was and is the best programming video I have ever watched. I watched tons of videos. Your speed to me was reasonable, voice clear, skill level super and bug free! thanks millions man :) liked, subscribed !",True
@christianenriquezolguin6172,2017-09-13T00:15:48Z,0,"Beautiful, really nice. thank you. Do you know how to do it in a continuous way?  let say each 30 minutes, just extracting the new items added  in those time.",True
@popofabulous,2017-09-12T10:18:27Z,0,As a beginner I have no idea whats going on lol,True
@Demindh,2017-09-10T15:29:33Z,0,How do you scrape data which is post loaded via JavaScript?,True
@bharath132,2017-09-10T10:46:51Z,0,"Hi Dojo, Nice video but am unable to get the reactjs element.Kindly help me, <div class=""_1vC4OE"" data-reactid=""326"">                     <!-- react-text: 327 -->‚Çπ                     <!-- /react-text -->                     <!-- react-text: 328 -->769                     <!-- /react-text -->       </div>",True
@NazarMalyy,2017-09-10T02:53:36Z,0,"Dude! You're AWESOME!!!! Fast, easy to understand, clear ...and right in point!!!! +1",True
@wadephz,2017-09-09T18:21:31Z,4,"Hi, thanks for the video! How do you get to the second div tag in ""container""?",True
@dcbfung,2017-09-09T11:27:45Z,0,what a legend,True
@Prakash1234ization,2017-09-09T09:16:20Z,0,"Yeah, I enjoyed the video so much. Thanks",True
@frankconte467,2017-09-08T17:45:46Z,0,"Great video. However, I can't get python to loop as called for here. I produce a csv file,  ""products.csv"" but it only has 1 record.  I did replace some code but I am sure that has nothin to do with it  as it worked up to that point.   That is to say I used what commenter Pyhna suggsted. In other words I replaced the section with "" container.div.div.a.img[""title""]  "" with  title_container = container.findAll(""a"", {""class"":""item-title""})  brand = title_container[0].text.split(' ', 1)[0]  product_name = title_container[0].text Please advise.",True
@osherezra131,2017-09-08T08:33:02Z,0,Great video thanks you. But what do you do next? how do you apply this to your web? or plugin,True
@ahanjilec,2017-09-06T12:10:41Z,0,"Could you pass maybe code in .csv from video? Tnx a lot, bro =)",True
@minibun04,2017-09-05T12:46:20Z,0,"what if I need to get the second div? because the html consist of multiple ""div"" in the ""form"", each with different ""id"" or ""class""",True
@justinsumner4964,2017-09-04T17:58:20Z,0,"Anybody know if you can use wildcards when searching through the HTML tags for particular words? Like say he only wanted to look for 10 series GPUs can you state only grab ""10**""? Or is there a more elegant solution to that?",True
@cosimocuriale8871,2017-09-04T13:28:05Z,0,Very nice video! Simple to learn!!! Thanks!,True
@nikolayvasilev9660,2017-09-03T23:06:34Z,0,thanks!,True
@marcod3252,2017-09-03T17:43:22Z,0,Very helpful Tutorial. Great Job. Just one quick question: What if I wanted to scrape only the first 6 products and not all of them? How would I do that?,True
@locorocozhong3941,2017-09-03T15:14:55Z,0,Thank you for your tutorial. It helps a lot!,True
@tehfunnybunny,2017-09-03T12:38:29Z,0,Thanks! this is awesome,True
@jmp0207,2017-09-03T05:48:20Z,0,This tutorial was awesome! Thank you very much! This allowed me to write my first web scrapper without much fuss.,True
@oliverinsulander,2017-09-02T05:18:38Z,0,"Hello  I get an error, bellow, how would I be able to solve this? Thanks you  >>> uClient = uReq(my_url) Traceback (most recent call last):   File ""C:\Python36\lib\urllib\request.py"", line 1318, in do_open     encode_chunked=req.has_header('Transfer-encoding'))   File ""C:\Python36\lib\http\client.py"", line 1239, in request     self._send_request(method, url, body, headers, encode_chunked)   File ""C:\Python36\lib\http\client.py"", line 1285, in _send_request     self.endheaders(body, encode_chunked=encode_chunked)   File ""C:\Python36\lib\http\client.py"", line 1234, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File ""C:\Python36\lib\http\client.py"", line 1026, in _send_output     self.send(msg)   File ""C:\Python36\lib\http\client.py"", line 964, in send     self.connect()   File ""C:\Python36\lib\http\client.py"", line 1400, in connect     server_hostname=server_hostname)   File ""C:\Python36\lib\ssl.py"", line 401, in wrap_socket     _context=self, _session=session)   File ""C:\Python36\lib\ssl.py"", line 808, in __init__     self.do_handshake()   File ""C:\Python36\lib\ssl.py"", line 1061, in do_handshake     self._sslobj.do_handshake()   File ""C:\Python36\lib\ssl.py"", line 683, in do_handshake     self._sslobj.do_handshake() ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""C:\Python36\lib\urllib\request.py"", line 223, in urlopen     return opener.open(url, data, timeout)   File ""C:\Python36\lib\urllib\request.py"", line 526, in open     response = self._open(req, data)   File ""C:\Python36\lib\urllib\request.py"", line 544, in _open     '_open', req)   File ""C:\Python36\lib\urllib\request.py"", line 504, in _call_chain     result = func(*args)   File ""C:\Python36\lib\urllib\request.py"", line 1361, in https_open     context=self._context, check_hostname=self._check_hostname)   File ""C:\Python36\lib\urllib\request.py"", line 1320, in do_open     raise URLError(err) urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)>",True
@e.b.3396,2017-08-31T18:47:28Z,0,"When I am starting the code I get this: File ""C:\Python\lib\urllib\request.py"", line 570, in error     return self._call_chain(*args) What does it mean and how can I fix it?",True
@lukamatuzic4662,2017-08-31T13:56:42Z,0,Why is he using commas for the concatenation in the end? couldn't it be done without them?,True
@haroldpepete,2017-08-31T13:11:55Z,0,"you deserve my subscription by this video, take your thumb up and my subscription",True
@ibrahimisrafilov1248,2017-08-31T06:25:17Z,0,"I have that kind ofa problem with opening the file    File ""my_first_web.py"", line 23     shipping_container = container.findAll(""li"", {""class"":""price-ship  IndentationError: unindent does not match any outer indentation level",True
@malayshah8421,2017-08-30T15:55:04Z,0,That is something great !! I loved it.. I learned something :),True
@jasonbq,2017-08-30T05:05:02Z,0,"Well done, Keep on going :D",True
@reginaldgalan9678,2017-08-30T03:38:59Z,0,"it returns only one row of results  pls check my code    import requests import bs4  from urllib import urlopen as uReq from bs4 import BeautifulSoup as soup  my_url = 'https://www.newegg.com/global/ph/Product/ProductList.aspx?Submit=ENE&DEPA=0&Order=BESTMATCH&Description=graphics+card&N=-1&isNodeId=1' uClient = uReq(my_url) page_html = uClient.read() uClient.close() page_soup = soup(page_html, ""html.parser"")  containers = page_soup.findAll(""div"", {""class"": ""item-container""}) container = containers[0] #container.a  filename = ""products.csv"" f = open(filename, ""w"")  headers = ""brand, product_name, shipping\n""  f.write(headers)  for container in containers:  brand = container.div.div.a.img[""title""]  title_container = container.findAll(""a"", {""class"":""item-title""}) #title_container product_name = title_container[0].text  shipping_container = container.findAll(""li"", {""class"":""price-ship""}) shipping = shipping_container[0].text.strip()  print(""brand: "" + brand) print(""product_name: "" + product_name) #print(""shipping: "" + shipping)  f.write(brand.replace("","", ""|"") + "","" + product_name.replace("","", ""|"") + "","" + shipping + ""\n"")  f.close()",True
@silasalberti3524,2017-08-29T11:42:14Z,0,Use conda instead of pip when using anaconda! (It's better anyway),True
@sunitadhenge2268,2017-08-29T11:01:20Z,0,awesome!!,True
@evangelosbatsalis9551,2017-08-28T23:42:04Z,0,"exactly what i needed, Thank you....... one of the best explanation tutorial ever...",True
@pedropau288,2017-08-28T09:25:37Z,0,Good day I can't seem to do the python code you did at the very beginning of your video.,True
@mathildacornrow5906,2017-08-27T09:46:40Z,0,"Thank you so much, that was an easy to understand Tutorial. I'll definitely rewatch it as I am progressing in my Topic Modeling-Project. I hope this kind of Script'll work for my purpose as well. What I didn't understand was, if I had to download bs4 or is it already implemented in Python/Anaconda? And is there a way to write every output in a single file?",True
@YemiDa,2017-08-27T09:08:28Z,0,This was very well done. Many thanks,True
@Rbn49,2017-08-26T17:21:06Z,0,"Nice tutorial, but could be shorter.",True
@jrmartinss,2017-08-26T12:33:03Z,0,"It's a great video, thank you for this.",True
@ankitaaarya,2017-08-26T09:59:04Z,0,when you said anaconda i thought you were kidding......then i get serious.,True
@bharath9190,2017-08-25T16:43:17Z,0,"very good one liked and helped me a lot, help me with how to navigate pages on the url??",True
@chimsgraphic,2017-08-25T11:35:02Z,0,Wow! Just completed the tutorial hands on. Already falling in love with Python. Nice work!,True
@jasonrobinette4486,2017-08-23T14:20:12Z,0,"Traceback (most recent call last):   File ""my_firstwebscrape.py"", line 25, in <module>     brand = container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'img'   everything worked but i got the above att error...anyone know what that means?",True
@ConanRider,2017-08-22T21:34:39Z,0,The tutorial is great but you sucking your teeth makes me want to kill myself.,True
@jovantap2144,2017-08-22T13:13:50Z,0,Nice,True
@NaniAdupa,2017-08-22T10:13:14Z,0,Really Very Nice,True
@Jayant1947,2017-08-21T22:09:28Z,0,Superb! Very useful. Short yet detailed.  Would love to see more of such videos.,True
@GibsnRage,2017-08-21T08:34:37Z,0,"31:40 - ""Deliminindated"" RIP  Thanks for the vid mango",True
@terencemakkahei,2017-08-21T07:02:22Z,0,The best web scraping tutorial I have watched. Would it be possible that you can also demonstrate how to web scrap on the entire site products?,True
@vinsavi,2017-08-21T02:15:40Z,0,Love the passion n directly coding in front of me to solve the problem. Plz show say handling streaming  video or automatically alerting a price drop or how to write a bid sniper,True
@AfroFPV,2017-08-19T12:11:16Z,0,Awesome tutorial buddy!,True
@abudawudsideeq9299,2017-08-19T04:43:14Z,0,Master piece,True
@letrat7021,2017-08-18T17:23:25Z,0,yes please,True
@rubenfernandez9603,2017-08-18T14:58:34Z,1,Great tutorial!!!!  Are you going to record the part two about fetching more pages o categories? :),True
@feikyid,2017-08-18T12:16:02Z,0,damn thankyou mister !!!,True
@akshayyalpi4581,2017-08-17T17:17:19Z,1,EXCELLENT VIDEO!!,True
@akshayyalpi4581,2017-08-17T17:13:50Z,1,Thank you so much! This video helped me a lot. The explanation is pretty good.,True
@kckong3,2017-08-17T06:50:20Z,0,freaking awesome!!!!,True
@ronstubed,2017-08-15T20:31:31Z,0,"Will keep this knowledge handy, if not for e-commerce sites. Thanks.",True
@gmshadowtraders,2017-08-13T19:59:54Z,0,Do one in R now please,True
@davidlanday6102,2017-08-13T14:40:17Z,0,You are a badass sir! Thank you for the tutorial.,True
@knotratulshorts,2017-08-12T19:02:50Z,0,"LOVED IT MATE! I'VE BEEN TRYING WEB SCRAPING FOR A LONG TIME, GOT THE SYNTAXES AND EVERYTHING FROM OTHER TUTORIALS, BUT THE WAY YOU SHOWED HOW ITS DONE, LIKE THE HANDS ON PROJECT BASED REAL LIFE IMPLEMENTATION AND THE WAY YOU MADE IT WORK CLEARED SEVERAL PROBLEMS I HAD IN MY CONCEPT! FEELING REALLY HYPED THAT I CLICKED ON YOUR VIDEO FROM MY SUGGESTIONS AT 3 45 AM! SUBSCRIBED! PLEASE MAKE MORE DATA SCIENCE VIDEOS! <3 Thank you :p",True
@catanainc,2017-08-11T11:46:49Z,0,what do you do in the case of websites with dynamically generated content? what changes?,True
@CreativJennseN,2017-08-10T19:27:37Z,0,I really appreciate this kind of video. Thanks a lot :),True
@sgteamx,2017-08-10T14:40:37Z,0,"For those on python 2.7, there might be some syntax error. try python -m pip install bs4, it worked for me",True
@AlqGo,2017-08-10T01:52:32Z,29,"You don't want Annaconda. There's so much junk that comes with it. Just install regular Python and open your terminal or powershell and install packages you need. For example, pip install beautifulsoup4, if you want to install Beautiful Soup. It's MUCH more cleaner than installing annaconda rubbish.",True
@sgteamx,2017-08-09T16:54:14Z,0,"Watched this halfway but i was so impressed had to write a comment before finishing it. Thanks for the video! Unlike others where they just code away and make passing statements without really explaining, you did well by at least explaining what you were doing for those familiar with programming but not necessarily with python and its syntax. :)",True
@linuxit5869,2017-08-09T07:40:20Z,14,"Awesome tutorial, Please add how to scrap multiple pages :)",True
@zohabali7130,2017-08-07T07:53:04Z,0,Awesome!,True
@princeagbonze5440,2017-08-06T06:09:38Z,0,"*Note:* if your connection is not strong to download web content, you will get a None output when you do page_soup.h1 as shown at 11:08",True
@syhVincent,2017-08-05T14:15:11Z,1,best teacher ever,True
@WisdomSeller,2017-08-03T08:57:01Z,12,could you upload the script?,True
@whatisahandleeee,2017-07-31T02:44:02Z,0,I really enjoyed watching this. Thank you.   I was wondering why is the way you scrape so different from the way they teach in ATBS?  https://automatetheboringstuff.com/chapter11/,True
@MoizesAraujo,2017-07-31T00:55:46Z,0,So nice video. Thanks!!!,True
@sahan.shyamal,2017-07-30T18:28:42Z,0,Nice video.. but we need also part 2 :),True
@ritheshbcanvas4963,2017-07-30T05:46:37Z,0,Superb,True
@MrKing3485,2017-07-30T01:35:08Z,0,"help i type python in the command window and it said""python is not recognized as a internal or external command,operable program or a batch file"" what should i do????",True
@grupomexico4679,2017-07-29T18:34:01Z,0,"i i need your help im triyng do this for obtain only the ip proxi with port number like this  187.189.60.141:3130  scraping for this page https://www.sslproxies.org/ with this code but nothing hapend  from bs4 import BeautifulSoup as soup import urllib  my_url = urllib.urlopen('https://www.sslproxies.org/').read() # https://hidemy.name/es/proxy-list/ page_soup = soup(my_url,'html.parser')  containers = page_soup.findAll('tbody',{'tr[]':'td[]'}) # //*[@id=""proxylisttable""]/tbody/tr[1]/td[1] print (containers)  can you pass me the way correct for doit thanks and congratulations for your channel",True
@gunshek,2017-07-29T17:29:24Z,0,"Really nice Video. I am getting the following error while scraping a website.""Unicode encode error, charmap cannot encode character '\u309'. Anyone seen this",True
@bishalbiswas1953,2017-07-29T14:48:49Z,0,Really nice video I learned a lot.,True
@jaytheist,2017-07-29T06:05:54Z,0,I got blocked from the site I was scraping! :P,True
@vancexavier8339,2017-07-28T09:46:39Z,0,500MB kind of seems excessive only for web scraping. Are there no lighter alternatives?,True
@GAV32,2017-07-28T00:14:51Z,0,"I get an error when running my code:  Traceback (most recent call last):   File ""webscrape.py"", line 20, in <module>     brand_name = container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'img'  I double-checked the html file and img was present. what do I do?",True
@Superjake3ds,2017-07-27T19:59:02Z,0,"Nice video dude Just a suggestion,  to really get introduced with web scraping¬†use the entire thing not just importing it as X.¬†Just a suggestion... but yeah great video really opened my eyes to webscraping.",True
@monkeyrater,2017-07-27T03:22:28Z,0,"You know that there are beautify/prettify html/css/js plugins for Sublime, right? Why would you paste it into an online beautifier when you can just use a hotkey in Sublime??",True
@technicalilm8999,2017-07-25T19:20:22Z,0,Great video,True
@RojinaPanta1,2017-07-23T15:17:03Z,0,Great Video,True
@NitinSaseendran,2017-07-19T06:00:31Z,0,More coding videos please! Keep up the great work.,True
@1973tmpk,2017-07-18T12:15:17Z,0,Fast paced and perfect help for those trying this out!,True
@vedprakash-bw2ms,2017-07-18T04:38:56Z,0,"Yes , Loved it. Please make more videos on Web scrapping. It was fun learning.",True
@michaelhilliker3347,2017-07-17T12:04:43Z,0,Great tutorial!,True
@taojunfengsu2876,2017-07-17T04:59:21Z,0,Fantastic Video! Very practical!,True
@protorialsbysaif8156,2017-07-16T11:36:03Z,0,Very Informative and interesting. Looking for more tutorials specially scraping data from html tables. thanks :),True
@saiarva6126,2017-07-15T03:08:42Z,0,I cannot pass through www.funda.nl please help me,True
@octoparse7774,2017-07-15T02:35:02Z,0,I cannot pass through www.funda.nl/koop/heel-nederland Could you please help me,True
@octoparse7774,2017-07-15T02:34:35Z,0,"Superb Tutorial ,  No one can cross you.  Excellent and I have one question for you  I cannot pass through www.funda.nl/koop/heel-nede...  Could you please help me",True
@saiarva6126,2017-07-14T03:12:42Z,0,"Could you please me with the PAGINATION (looping through multiple pages, clicking NEXT button until it disappears)",True
@saiarva6126,2017-07-14T03:11:22Z,0,really awesome.,True
@MidiHippi,2017-07-13T22:41:26Z,0,Hell yes I want to see more of this. This was awesome man thanks for putting this up ! All I have to figure out now is how to download the images associated with a real estate listing and my project is Done!,True
@luckxn,2017-07-09T22:20:25Z,0,"Thanks Phuc Duong,  When I run the code I have the following erros:  --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call last) <ipython-input-117-2de5f3cdedda> in <module>()       1 for container in containers: ----> 2     brand = container.div.div.a.img[""title""]       3        4     container.findAll(""a"", {""class"":""title""})       5   AttributeError: 'NoneType' object has no attribute 'img'  Do you know why?  Here is my code:  for container in containers:     brand = container.div.div.a.img[""title""]          container.findAll(""a"", {""class"":""title""})          title_container = container.findAll(""a"", {""class"":""item-title""})     product_name = title_container[0].text          shipping_container = container.findAll(""li"", {""class"": ""price-ship""})     shipping = shipping_container[0].text.strip()          print(""brand: "" + brand)     print(""product_name: "" + product_name)     print(""shipping: "" + shipping)",True
@arshdeepsinghahuja,2017-07-08T09:08:47Z,17,"shipping_container = container.findAll(""li"",{""class"":""price-ship""})  GETTING THIS ERROR  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module> TypeError: 'tuple' object is not callable",True
@arshdeepsinghahuja,2017-07-08T08:15:35Z,0,can u tell what uClient.close() doing??,True
@riviera443,2017-07-06T02:27:31Z,0,Very helpful,True
@bcotter89,2017-07-06T00:46:21Z,0,"So I have downloaded Anaconda, but whenever I go to type ""Python"" into my Command Prompt to show version, it only shows: ""Python 2.7.13"" it doesn't show the Anaconda version I am running. Anyone else running into this problem? I am on Windows 10...",True
@achin4140,2017-06-30T16:58:33Z,0,i beg you please tell about how to make web crawler using python in simple and easy method/steps,True
@steve02a,2017-06-29T07:06:53Z,0,"I asked one question on a forum on how to extract data from a website.  One person replied saying web scrape with Beautiful Soup.  Google search found me this video.  I just did my first scrape thanks to this video and man, I just opened myself up to a world of data gathering now!  Thank you for this great beginner video.",True
@BubblegumFetish,2017-06-28T19:34:34Z,0,THANK YOU SO MUCH! It was so good!,True
@andresuhartanto3385,2017-06-26T12:24:55Z,0,you rock!!!,True
@maxiewawa,2017-06-26T01:29:14Z,0,Enjoyed it! What about if it DOES have an API?,True
@alisina1472,2017-06-25T22:09:59Z,0,"When I open the command line and type python, I get, ""'python' is not recognized as an internal or external command, operable program or batch file.""",True
@makedredd299,2017-06-25T20:54:37Z,4,"Hi, I'm getting stuck at 28:50 when running the script.  How do I solve this problem?  $ python Dojo.py Traceback (most recent call last) : File ""Dojo.py"", line 18, in <module>      brand = container.div.img[""title""] TypeError: 'NoneType' object is not SUBSCRIPTABLE   Best Regards",True
@alberto6888,2017-06-24T11:43:34Z,0,Best tutorial ever,True
@angemcauslan2551,2017-06-22T13:29:30Z,0,Thank you so much. This was an excellent introduction at a level that I could easily understand.,True
@readyyfireaim3310,2017-06-22T04:03:16Z,0,"This is a great video! Even as someone relatively unfamiliar with coding and python I was able to follow along a lot of the big picture, I need to learn more basics before becoming totally fluent but this was great. Tyvm for producing this content",True
@myworld123321,2017-06-20T16:24:28Z,0,Wow!! Thank you a lot. Been trying to learning web scarping and this video took me a long way. Thank you.,True
@RaviKumar-eg6ww,2017-06-20T10:27:44Z,2,"Can you resolve the problem...?? the first line in the for loop is not working...it says  brand = container.div.div.a.img[""title""] AttributeError: 'NoneType' object has no attribute 'img'",True
@parthisaade,2017-06-20T03:40:05Z,0,"so useful! please, more of this exact same web scraping thing>>>",True
@wangchangkevin,2017-06-18T07:26:10Z,0,"Really enjoyed this. One of the most clear, concise and entertaining web scraping videos. Would really like to see more web scraping videos and how to incorporate them into a program that will systematically retrieve and update the information and present in a user friendly format :)!",True
@sharper4221,2017-06-17T23:16:14Z,0,Very good video! Really quick and straightforward tutorial!,True
@alexandrajohnsson3703,2017-06-17T17:58:27Z,1,Amazing Phuc Duong! Could you do a tutorial on handling data updated by JavaScript and AJAX? I keep coming across different values.,True
@freediugh416,2017-06-16T03:22:38Z,13,"wow this was great! I am completely new to this and still could follow perfectly fine and loved the explanations of everything. Would love to know how to run this script every day automatically and send results to phone or create alerts for changes and send those to a phone. Again, awesome job!",True
@vastauine6398,2017-06-14T06:11:03Z,0,y not write to sublime Directly its a bit annoying copy paste.,True
@rikschoonbeek,2017-06-10T20:37:24Z,0,Great video!,True
@dylanjohnson7168,2017-06-10T02:09:58Z,0,This was amazing.  I went from not knowing a single line of python to extracting 5000 data objects from a local website today thanks to this video.,True
@rishabhrajshrm00,2017-06-08T14:01:30Z,0,"Great Video really helped me, it would be good if u could do more videos on web scrapping and using the data for analytics with python",True
@Lorddarklord666,2017-06-05T21:00:43Z,0,"""'my_first_webscrape.py' is not recognized as an internal or external command, operable program or batch file. "" This keeps happening, and I do not know what to do..............",True
@adityabansal4033,2017-06-01T17:19:30Z,0,U taught me scrapping ...thanx a lot and keep posting such stuff..,True
@JordanShackelford,2017-06-01T06:48:05Z,0,Don't know why you have to use command prompt when you can just type it into IDLE,True
@jappievanleuven123,2017-05-31T21:03:00Z,0,"When I juse this url: http://www.funda.nl/koop/amsterdam/. It raises this error: Traceback (most recent call last):   File ""funda.py"", line 5, in <module>     uClient = uReq(my_url)   File ""C:\Python27\lib\urllib2.py"", line 154, in urlopen     return opener.open(url, data, timeout)   File ""C:\Python27\lib\urllib2.py"", line 435, in open     response = meth(req, response)   File ""C:\Python27\lib\urllib2.py"", line 548, in http_response     'http', request, response, code, msg, hdrs)   File ""C:\Python27\lib\urllib2.py"", line 473, in error     return self._call_chain(*args)   File ""C:\Python27\lib\urllib2.py"", line 407, in _call_chain     result = func(*args)   File ""C:\Python27\lib\urllib2.py"", line 556, in http_error_default     raise HTTPError(req.get_full_url(), code, msg, hdrs, fp) urllib2.HTTPError: HTTP Error 405: Not Allowed  whats does that mean?",True
@akpofureenughwure6177,2017-05-31T18:23:24Z,0,Thank you for this video... How can I insert a loop statement into  this code (for url pages) so that each page of the website is scraped and how do I include the try statement on this code.. so that fields that doesn't have the keys will show?,True
@benrawner5218,2017-05-30T01:18:59Z,0,Do you need Anaconda if you are using a MAC?,True
@Ibiassu,2017-05-26T23:33:24Z,0,this is awesome!,True
@yaacha,2017-05-24T06:05:52Z,0,"That was extremely helpful, thank you!",True
@poortisahni4261,2017-05-23T06:21:51Z,0,i am getting the result of len(container) as 0 ?,True
@dharmasakthi2303,2017-05-20T17:50:03Z,0,where did u learn this stuff,True
@sachi097,2017-05-20T07:48:37Z,0,i have pure python 3.6 so is it fine if i refer your tutorial as you have anaconda do the commands you write in anaconda work in python 3.6?  please help me soon.,True
@felixmartivalverde388,2017-05-19T21:01:28Z,0,Very good video. Bravo!,True
@revolution77N,2017-05-19T05:50:36Z,0,"Thanks man! great tutorial, I learnt quite a lot!",True
@Jeswingify,2017-05-16T17:50:03Z,0,"Very helpful, thank you very much!",True
@jereziah,2017-05-16T09:14:27Z,0,"Awesome video, awesome content, extremely useful.  Perhaps you could put your source-code in a github repository or something like that for those of us have trouble following along at your speed?  All the best & thank you.",True
@tetracilin,2017-05-14T07:53:14Z,0,Nice video ! You explained so clearly which is rare for programmer video. Keep it up,True
@ctqpro,2017-05-13T16:51:58Z,0,"I tried to get the quantity on a website and I applied most of your code. It then returned with the script. Unfortunately, I can't separate elements in the text (below) into different parts such as ""pn_sku"", ""part_available"" and ""part_search_term"". It works with creating and printing out the data into the csv file but it came with the whole text. Any ideas to separate those? <script> var utag_data = {             page_site: 'US',             page_language: 'en',             wt_use_udo: 'True',             page_content_group: 'Part Search',             page_content_sub_group: 'Part Detail',             page_title: 'Part Detail',             page_type: 'PS',             page_sub_type: 'PD',             page_id: 'PD',             pn_sku: '1740-1017-ND',             part_id: '1154763',             part_available: '4324',             transaction_type: 'v',             transaction_quantity: '1',             supplier_id: '1740'                 , part_search_filter: 'No Filter'                 , part_search_term: '568-3651-5-ND'                 , part_search_term_ext: '568-3651-5-ND'                     , part_search_results_count: 1             , video_source: 'Part Detail'         } </script>",True
@gigavols8800,2017-05-12T22:00:58Z,1,Does python understand plural or something? He never sets a variable for container.,True
@lnasution76,2017-05-10T14:14:37Z,0,very educative mate - Thanks,True
@johnvasquez1084,2017-05-08T20:38:34Z,0,When I put in the website I want to scrape and use the findall function it doesn't find anything,True
@johnvasquez1084,2017-05-07T19:48:14Z,1,Amazing tutorial! Which brings me to this question. You see how you pasted all the info into excel?¬†Is it possible to¬†paste it into another website? Let's say website 1 is where I gather all info(I want to¬†find 200 listings)¬†to search then Website 2 gives me Photos + more info(of 200 listings) and now I want to copy and paste 200 of those specific text or photos onto website 3. Can I use the same coding in this tutorial to accomplish this or would I need some additional module or something? Thank you for your wisdom.,True
@BabyBalla3score,2017-05-06T05:05:24Z,0,At the 28 minute mark how did you manage to copy each of those terms. I'm in pycharm and shift didn't work for me. Nice video by the way as well. I'm looking to make a bot for certain products,True
@20dikshit,2017-05-05T19:17:59Z,0,"Great video, you should do more videos like this",True
@ariramkilowan8051,2017-05-04T12:32:56Z,0,would it not have been easier to do this in ipython/jupyter so you don't have to type stuff twice?,True
@katieleung815,2017-05-03T18:14:48Z,0,"Thanks for the video!! I have no prior coding background but I'm doing a similar project as coursework. The video saves me! I wonder instead of scraping info directly from the graphic cards, can I actually include ""urllib2.urlopen"" in the loop so I can open that graphic card and scrap details for individual web page?",True
@Petrarch1603,2017-05-02T03:05:40Z,0,"Great video, I hope to see more Python stuff!",True
@xorencryption,2017-05-01T23:01:50Z,0,"This was just great man. however , there are sites with dynamic stuff loaded by javascript which is what bs4 wont see but selenium will. I have heard of guys using Driver.Page_source() to push the full content including javascript loaded contents to bs4 and then parse it. seems to be faster that selenium alone. if you can make a video using this method , and instead of guessing the URL , interacting with the page , clicking buttons to go to next 60 items found for example . than parse in bs4 , it will be extremely appreciated.",True
@debajyotisg,2017-05-01T19:31:52Z,0,How do i add things to my csv which don't repeat for every item on the website?  Please help anyone?,True
@AdityaFingerstyle,2017-04-28T06:12:57Z,0,Fantastic Tutorial ! Thank you,True
@sto3359,2017-04-23T08:35:19Z,0,"Thank you very much, this was an excellent refresher video!!",True
@rahulshroff7626,2017-04-22T23:24:17Z,0,Excellent Video! Easy to follow & works well!,True
@sanquan2339,2017-04-19T18:14:18Z,2,"hi nice video and scraping works good! But today i found a website that give me this 403 error:  >>> uClient = uReq(my_url) Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""C:\Users\sanqu\Anaconda3\lib\urllib\request.py"", line 223, in urlopen     return opener.open(url, data, timeout)   File ""C:\Users\sanqu\Anaconda3\lib\urllib\request.py"", line 532, in open     response = meth(req, response)   File ""C:\Users\sanqu\Anaconda3\lib\urllib\request.py"", line 642, in http_response     'http', request, response, code, msg, hdrs)   File ""C:\Users\sanqu\Anaconda3\lib\urllib\request.py"", line 570, in error     return self._call_chain(*args)   File ""C:\Users\sanqu\Anaconda3\lib\urllib\request.py"", line 504, in _call_chain     result = func(*args)   File ""C:\Users\sanqu\Anaconda3\lib\urllib\request.py"", line 650, in http_error_default     raise HTTPError(req.full_url, code, msg, hdrs, fp) urllib.error.HTTPError: HTTP Error 403: Forbidden  Any idea how to fix this ?",True
@myexpfactory3189,2017-04-16T08:18:06Z,0,"Hi DataScienceDojo, great tutorial! i've been following your tutorial and testing out some URLs; some of them are throwing a 403 - Fastly Internal error... any idea why?",True
@sosobrby5309,2017-04-15T17:33:36Z,0,That was awesome. You are a great teacher and a pro!,True
@martyyu,2017-04-13T17:25:52Z,0,Thank you for explaining the syntax as you went along.,True
@ItAintNecessarilySo,2017-04-13T12:10:28Z,0,"Thank you so much Phuc!  This was incredibly helpful.  I've never used html, sublime or Beautifulsoup before but you made it really easy to follow.",True
@silviucodreanu8924,2017-04-11T19:44:52Z,0,Very interesting and very instructive !   Do you think you could scrape on url: https://www.skyscanner.nl/transport/flights/lond/vie/170608/airfares-from-london-to-vienna-in-june-2017.html?adults=1&children=0&adultsv2=1&childrenv2=&infants=0&cabinclass=economy&rtn=0&preferdirects=false&outboundaltsenabled=false&inboundaltsenabled=false&ref=home#results.   Or another simillar site (flights).   Thank you & Best regards.,True
@ruahden,2017-04-10T06:36:27Z,0,Nice! This is so comprehensive. Thank you!,True
@rana31ify,2017-04-08T23:13:18Z,0,Fantastic work keep doing it  man,True
@nitishmittal9819,2017-04-08T12:23:17Z,0,hey the tutorial was cool really learned some stuff keep doing such videos,True
@eborne66,2017-04-08T01:29:57Z,0,"This helped me scrape craigslist jobs, pull out specific jobs and open the webpage to that job...Your work in action! (Used excel and vba to do the latter, but working on a python variant)",True
@samt3036,2017-04-07T22:05:01Z,0,"Great video and thanks for walking through the process of figuring out how to extract the specific values, most tutorials and video just skip over that important step.  What are your thoughts about using XPath vs. beautifilsoup?  It seems to me that XPath would be easier because you can copy the XPath to the field directly from Chrom (right-click).",True
@RussianCosmonaut1,2017-04-06T06:08:20Z,0,"liked, subscribed, added to playlist.   Thank you Data Science Dojo.  After months of searching, i found the one reliable source i can count on. Not only you did not waste time, but you were able to explain in a way , someone without a CS degree could undertand..  Thanks alot",True
@eborne66,2017-04-06T00:33:45Z,0,"Great teaching method! First I've seen to actually demonstrate ""visually"" parsing html code and python coding the result. Your testing method is also very helpful. Thanks!",True
@aminejadid2702,2017-04-05T09:49:09Z,0,Great! Thank you,True
@MilanTheAngel,2017-04-05T02:02:47Z,10,"I'm on ubuntu, is that ok?",True
@raviravito9276,2017-04-04T02:27:39Z,0,"Although I am not completely through watching this video, you are very helpful and fun to watch Thanks!",True
@hychan13,2017-04-02T10:29:18Z,0,thank you,True
@arsalan2780,2017-04-02T09:04:44Z,1,"Excellent, God Bless You.",True
@georgewang7770,2017-03-31T10:03:01Z,0,nice tute. Thanks,True
@RealSoundNow,2017-03-30T00:25:21Z,0,Best tutorial on the web.¬† Perfect pacing and great teaching.¬† THANKS!!,True
@wallacelee8034,2017-03-29T23:02:09Z,1,"Really awesome video!!!  Hope you do more Data Science and Web Scraping videos.  One question I do have, is there way to automatically run that script daily without me having to call the program?  So take the example you show, I want to check what they have daily and save it in a folder with different date.",True
@jimmyboyd817,2017-03-29T22:19:53Z,0,"Any Ideas on why my script only writes 1 line to csv and causes the ValueError: I/O operation on closed file?   On console  brand: EVGA product_name: EVGA GeForce GTX 1080 SC GAMING ACX 3.0, 08G-P4-6183-KR, 8GB GDDR5X, LED, DX12 OSD Support (PXOC) shipping: $4.99 Shipping brand: GIGABYTE product_name: GIGABYTE Radeon RX 460 WINDFORCE OC 2GB GV-RX460WF2OC-2GD shipping: $3.99 Shipping Traceback (most recent call last):   File ""graphics_cards_to_csv.py"", line 41, in <module>     f.write(brand + "","" + product_name.replace("","", ""|"") + "","" + shipping + ""\n"") ValueError: I/O operation on closed file.   My code  from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup  my_url = 'http://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card'  #Opening up connection,grabbing the page uClient =uReq(my_url) page_html =uClient.read() uClient.close()  #HTML Parsing page_soup = soup(page_html, ""html.parser"")  #Grabs Each Product  containers = page_soup.findAll(""div"",{""class"":""item-container""})  #Write to csv and format the headers of the csv file always remember the new line on header  filename = ""products.csv"" f = open(filename, ""w"")  headers = ""brand, product_name, shipping\n""  f.write(""headers"")  #Loop for going through each container  for container in containers:       brand = container.div.div.a.img[""title""]    title_container = container.findAll(""a"", {""class"":""item-title""})  product_name = title_container[0].text    shipping_container = container.findAll(""li"", {""class"":""price-ship""})  shipping = shipping_container[0].text.strip()    print(""brand: "" + brand)  print(""product_name: "" + product_name)  print(""shipping: "" + shipping)   f.write(brand + "","" + product_name.replace("","", ""|"") + "","" + shipping + ""\n"")   f.close()",True
@shenglichen2483,2017-03-26T16:28:21Z,0,"Did anyone get the error as below after typing page_soup=soup(page_html,""html.parser"")  ?  TypeError: __init__() got an unexpected keyword argument 'strict'   try searching on web but can't find the solution to this.  Please help!",True
@sibivarghese1365,2017-03-26T15:49:28Z,0,Very Nice Video!!!!!!!!!!!! Thanks...Request to upload more Python videos..,True
@manmoncang,2017-03-26T06:18:26Z,0,How to write multiple print at [27:50]?,True
@manmoncang,2017-03-26T00:36:55Z,0,"TQ with good tutorial.   But for me, still the best is preg_match_file using PHP because you can grab between code  preg_match_file(""@front code.?*end code.si"",source,output)",True
@louisdelillo4256,2017-03-23T00:48:09Z,0,That was one of the best web scraping tutorials I have ever seen.  Went step by step through the process of how to determine tags and classes.,True
@seanodonnell3683,2017-03-22T15:03:30Z,0,I understood very little of this but I watched it all the way through; very interesting.,True
@user-ik9th1nk9n,2017-03-20T20:37:37Z,3,"Can anyone say what is the syntax when you are using two objects inside findAll.  If the HTML is    <div class=""xxxxxx"" id=""yyyyyyy"">",True
@rajatsingh9289,2017-03-20T18:40:02Z,0,awesome!! learned a lot,True
@js-xz9kv,2017-03-16T23:58:16Z,0,This was so helpful! Thank you,True
@hangchen6131,2017-03-15T10:50:37Z,0,"Hey man thanks for the video now you make the whole internet as my database! I learned this to grab some word-meaning-detail such as example sentence, synonym, antonym and sh*t from a dictionary website for my GRE word list. Solved my tedious task and make me learn new super power!!!",True
@irishdaisy1628,2017-03-15T00:16:54Z,0,"Very good information, and easy to follow. One question, what I'm looking for is to scrape all pages within a domain. Pages are all formatted the same, so basically wondering if there is a way to create a loop that will scrape all subdomains, rather than specifying exact URLs. Using the example of New Egg, rather than scraping a single page (or set of pages, /2, /3 etc) it would search all pages within the newegg.com domain. Hopefully that makes sense.",True
@user47263,2017-03-13T05:27:03Z,0,"HI Dojo! I followed your code, how come I came up with an error ""AttributeError: 'NoneType' object has no attribute 'img' Here's  my code:      from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup  url = 'http://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=video%20cards'  uClient = uReq(url) page_html = uClient.read()  uClient.close()  page_soup = soup(page_html, ""html.parser"")  containers = page_soup.findAll(""div"", {""class"": ""item-container""})    for container in containers:      brand = container.div.div.a.img[""title""]        title_container = container.findAll(""a"", {""class"":""item-title""})      product_name = title_container[0].text       shipping_container = container.findAll(""li"", {""class"" :""price-ship""})      shipping = shipping_container[0].text.strip()       print(brand)      print(product_name)      print(shipping)",True
@Nick-us1wt,2017-03-12T23:13:59Z,1,"For those who are wondering what you could do to get the price and other things that some containers don't have, you would you the try: and except: commands.",True
@kennethadelsten1059,2017-03-11T19:21:15Z,0,"Awesome stuff! The big take away for me was how you wrote it into a CSV-file.  I have a prayer for a video: Can you please make a video on how you can run a script automatically every day? For example how you would make the script you made in this video, run every day at a certain time to collect the data?  I have tried cron, but i haven't got it to work yet.",True
@gauravsharma-mi2er,2017-03-10T03:47:06Z,46,Wow great video.Can you make a video on srapping data from multiple pages,True
@4ntblack,2017-03-08T17:39:42Z,0,"Hey Dojo, great work and very nice explaining pace.  Your script runs smoothly, but i get an error when trying to write my results. (TypeError: coercing to Unicode: need string or buffer, ResultSet found). No solution found in stackOverflow unfortunately. Any idea how to resolve this issue?",True
@muhammadshahmi9994,2017-03-08T09:36:34Z,0,how do i update python 3.4 to 3.5 (windows)?,True
@LoveMajani,2017-03-07T15:31:46Z,0,"Thanks so much for this tutorial, you really have an awesome and understandable way of teaching!!",True
@todayu,2017-03-06T02:51:44Z,0,Wow!  I'm a total programming novice and this tutorial totally made sense.  Great job and thank you for posting!,True
@ben132333,2017-03-04T13:43:17Z,0,"Thank! Learning this structure in bs was really useful: soup.findAll(""p"",{""class"":""num-results first-focus""})",True
@Bone__Zone,2017-03-01T03:44:56Z,0,more webscraping please,True
@Thimiosath13,2017-02-28T17:48:02Z,0,Really awesome!!!!!,True
@dragoxofield,2017-02-28T05:53:59Z,20,Nice! I was wondering if you could do a page monitor where it tells you exactly where the website has changed?,True
@laurenvanwinkle9529,2017-02-28T00:10:28Z,0,Is there another¬†environment to enter the Python code?¬† I'm using the command prompt on Windows7 and cannot delete once I find a mistake on a line.¬†¬†It also does not like me copying¬† from Sublime and pasting into the command prompt.¬†¬† Is there¬† a python shell that is a little more flexible?,True
@rohitpandey666,2017-02-26T10:30:06Z,0,"Great Tute, I want to make a scraper for eBay and get the search results and then follow each item which got scraped and then grab the details from those details pages and then save everything in a CSV and then search each item on other websites for example amazon and if found scrap data from there and again save it to the CSV and then perform a calculation to find the difference is prices.  How can I go ahead with such a problem. till now I am able to scrape the data from eBay¬†but don't know how to save them into CSV which I got to know from your video and going to try it now but don't know how to iterate over hundreds of links one by one and grab details and save them in a csv.¬†Any help would be appreciated and if possible to make this into an app then i am willing to pay for it as well.",True
@shankargs7685,2017-02-18T14:10:17Z,8,"Hi Dojo, Really nice video. I have one doubt. The recent eCommerce sites done have class items constant, they have alpha numeric values like   class=""_3Hjcsab"" how do you scrape when the site keeps on changing?",True
@rockspider7061,2017-02-15T10:33:17Z,0,"I am using Python 2.7.. The command    containers = page_soup.find_all('div',{'class': 'item-container'}) is returning a length of 0. why is that??",True
@millerovv,2017-02-12T20:05:12Z,0,You are the best my man!,True
@LittleRhubarb,2017-02-09T15:56:35Z,0,Best tutorial I've found so far. A further video to scrape several pages would be great!,True
@phil6758,2017-02-07T00:12:30Z,0,This for me has been without a doubt the best tutorial for Web scraping. Extremely well presented and clear. Thanks.,True
@khiemnguyen7449,2017-01-27T10:31:37Z,0,love it !!,True
@ahmedalthagafi4492,2017-01-11T23:08:42Z,6,Great video. ..very easy to follow. hope you do more of that kind. Thanks.,True
@LePnen,2017-01-07T12:08:28Z,7,Thank you very much for this video!  I hope you do a second one on this subject. I'd like to know how to scrape several pages as you mentioned in the end of video. This was just what I was hoping for. Thanks!,True
