author,updated_at,like_count,text,public
@user-ef2rs1bf5w,2024-05-25T22:31:10Z,0,"Excellent explanation, Thanks!",True
@Halterofilic,2024-04-18T09:05:10Z,0,"2024, still a great reference to Transformers. Million thanks for the amazing work!",True
@user-yr5vj7xp8d,2024-04-08T01:51:57Z,0,"Jay, recentemente estive em um curso de I.A, Mas voce apresentou muito bem, de forma did√°tica a PNL.... eu aprendi muito com voce.  Obrigado. Continue sendo este cara maravilhoso.",True
@pypypy4228,2024-03-21T19:07:09Z,0,"If anybody sees this comment, what's the video on hiddens states could you recommend on par with this one by explanation power? Thank you!",True
@pypypy4228,2024-03-21T19:03:34Z,0,A huge thank you for this explanation!,True
@josephsueke,2024-02-18T13:12:39Z,0,Really clear. amazing job!,True
@user-xt9yo8eg6y,2024-01-30T00:37:44Z,0,Background music too loud,True
@KumR,2024-01-22T19:52:37Z,0,Half,True
@nileshkikle8112,2024-01-13T07:19:57Z,0,Outstanding job demystifying the inner working details of the Transformer model architecture! All the illustrations and animations for the inference working are awesome. Thank you for taking all the time and sharing your understanding with all of us. Kudos! üëç,True
@JimBob-lq1db,2024-01-05T09:00:30Z,0,"Thank you for this great explanation. Visualize , visualize, visualize, the best way to undestand how it works.",True
@tiborsaas,2023-12-29T00:30:19Z,0,This video really aged well. It came out just after GPT3 and before ChatGPT. I love it how it gives massive insights to how current generative AI works behind the scenes (but obviously in a simplified way).,True
@exxzxxe,2023-12-23T19:04:11Z,0,Maybe the best video on this subject.,True
@ishandindorkar2846,2023-12-12T02:59:20Z,0,"Jay, many thanks for your work. These videos help me a lot to understand key concepts in NLP domain through visualization.",True
@maruthiprasad8184,2023-12-08T00:43:04Z,0,"Amazing explanation, my search to understand the transformers ended here, you done the wonderful job, thank you so much for the simplest explanation I ever seen.",True
@NarkeEmpire,2023-11-17T23:27:05Z,0,You are a great teacher!!! If you chek the EQ settings and lower the music at the beginning the video is perfect!!! Thanks a lot for sharing your knowledge in this very understandable way,True
@yoonyamm,2023-10-22T23:59:10Z,0,Thank you for sharing wonderful insight!,True
@Udayanverma,2023-10-15T18:57:25Z,0,towards the end at time 27:30 how are you printing the words with probability? pl share code I couldnt find any,True
@Udayanverma,2023-10-15T03:58:20Z,0,loved it. thanks. got some new neurons in my head created by this video.,True
@gaydemaupassant6263,2023-10-06T14:28:30Z,0,Would have been better without the music,True
@tusharkhustule3316,2023-09-26T15:55:57Z,0,1 minute into the video and I already subscribed.,True
@RussAbbott1,2023-09-15T20:36:55Z,0,"At approx 23 minutes you show tokens being passed through a transformer. It appears that each token is converted to an output element. But that seems inconsistent with the rest of the video. As I understand what you have said (and how LLMs work), a sequence of tokens is converted into a single output (not one output token for each input token). The original sequence with the new token added, is then used to produce the next token. Etc. Would you say something about how the image at minute 23 is intended to convey that process. Thanks.",True
@tsadigov1,2023-09-08T20:13:25Z,0,"I am trying to understand working of transformer, you explain it much accessible way. One small thing I wish the video had less of transitions between two cameras.",True
@leeatkinson6675,2023-08-28T08:16:40Z,0,"""The Shawshank redemption, starring Ryan Reynolds and Ben Affleck, will be a first person action comedy..."".hmm, that isn't how I remember it...",True
@goelnikhils,2023-08-05T15:28:25Z,0,"I haven't see such a clear explanation of Transformers and Decoder LM Models, Amazing Work Jay",True
@muhammadjaved9169,2023-06-21T04:33:14Z,0,great dear,True
@zongmianli9072,2023-06-20T08:55:13Z,0,"Thanks for the very clear and concise explanation, Jay!",True
@sapnilpatel1645,2023-06-07T03:27:47Z,0,Great video!,True
@NZExperience,2023-05-30T15:06:47Z,0,Nice copy of 'OK Computer' in the background there :),True
@user-oe8lh2gw1u,2023-05-27T17:37:13Z,0,Great work üëçüëçüëç,True
@sudzam,2023-05-27T05:18:56Z,0,Wow! One of THE best explanation of Transformers.. Thanks @Jay!!,True
@mrityunjayupadhyay7332,2023-05-20T06:33:56Z,0,Great explanation,True
@Alex-oo5rt,2023-05-18T14:59:48Z,0,"6:13 actually, GPT-2 and GPT-3 models are both composed of an encoder-decoder architecture. The encoder-decoder architecture is a common framework used in natural language processing (NLP) tasks, particularly in sequence-to-sequence models. while GPT-2 and GPT-3 have an encoder component, it is not as prominently utilized as the decoder for generating text outputs.",True
@thecutestcat897,2023-05-18T06:11:04Z,0,"Thanks, your Blog is so clear!",True
@marnesh2008,2023-05-07T20:06:45Z,0,"your backing and forth between you and the ppt is really annoying, just focus on ppt with your little profile please",True
@user-fz5vr7yo2l,2023-04-30T15:20:48Z,0,Amazinnnng illustration of language model transformers,True
@awesometaiwan6514,2023-04-27T16:29:49Z,0,"Basically, it's the result of decades of work by thousands of individual contributions.",True
@kazimafzal,2023-04-25T03:09:04Z,1,"You sir are an amazing teacher! I'm absolutely flabbergasted by how well you've explained, to think its all mathematics at the end of the day! Thank you for taking the time to put together such a concise yet complete guide to transformers!",True
@studmatze958,2023-04-17T07:21:16Z,0,Thank you so much for you work on attention and transformers. Your posts and videos are the best i have encountered so far in terms of visualization and explanation. And you did it way better than my Professor.  Again thank you :),True
@RupertBruce,2023-04-14T21:03:19Z,0,Why not Shawshank prison?,True
@RupertBruce,2023-04-14T00:22:09Z,0,"""We have ways to calculate the error..."" - there's a lot of 'ways'-the chosen way would be interesting!",True
@xXMaDGaMeR,2023-04-13T18:54:28Z,0,"Amazing explanation! one thing is not clear, why is 'Shawshank' three tokens and not one? isnt it just one work?",True
@peterkahenya,2023-03-21T05:21:16Z,0,Wow! üéâ Awesome into.,True
@andreysguitarmusic2661,2023-03-14T09:23:58Z,0,Great explanations!,True
@Buddhawithin,2023-03-13T15:57:01Z,0,You tube  Dr Alan D. Thompson @DrAlanDThompson,True
@tshepisosoetsane4857,2023-02-27T12:43:10Z,0,Amazing work indeed thanks for simplifying things for everyone to understand this AI great work,True
@MarkCicero,2023-02-24T12:48:41Z,0,"Anyone else notice the blatant hallucination of GPT2 hallucination about the Shawshank Redemption ""starring (Ryan Reynolds and Ben Affleck), will be a first-person action comedy which is set in the South by Southwest, the largest most popular country in the United States.""",True
@vincentyang8393,2023-02-22T04:52:37Z,0,"Thanks, I learn a lot!",True
@hunorszegi4007,2023-02-19T16:21:39Z,0,"Thank you for your videos and blog posts. These were my inspiration to create a Java GPT-2 implementation for learning purposes. I can't use a link here, but as huplay I uploaded it to the biggest hosting site, and it is called gpt2-demo.",True
@curiouspie1264,2023-02-16T03:48:39Z,0,One of the most comprehensive video and blog overviews of Transformers I've seen. Thank you. üôè,True
@jamolbahromov4440,2023-02-03T10:36:58Z,0,"Your illustrations in blog post helped me a lot to understand the structure. I still have a question though. In your explanation for multiheaded attention you draw direct projection of input matrix X into 8 different Q,K,V matrices. However in the paper they draw a picture where there is a linear projection layer between QKV and each head of the multihead attention. Namely, initial projection of X into QKV is distributed into heads by additional linear layer of each head. Or do I get smth wrong, pls help, thank you in advance!",True
@RK-fr4qf,2023-01-31T00:16:57Z,0,Impressive.  Thank you.,True
@pw7225,2023-01-18T20:33:27Z,0,"Love the video. Please more. On all topics!  Pre-training was described as unsupervised (https://youtu.be/-QH8fRhqFHM?t=602). This is true but also misleading for beginners. We do have a supervision signal (somewhat required for Deep Learning) but it is directly obtained from the data, i.e. self-supervised learning.",True
@ahmedb2559,2023-01-07T13:09:08Z,0,Thank you !,True
@curtisnewton895,2022-12-25T14:04:46Z,0,your embeddings explanation is quiet vague,True
@jesuslopez3306,2022-12-10T11:16:54Z,0,Definitely it is easier to understand in a vertical way. Thanks for everything!,True
@akshikaakalanka,2022-12-08T23:16:23Z,0,Thank you very much! this is awesome and easy to understand.,True
@kobic8,2022-12-05T09:26:38Z,0,"hey Jay! love the blog on illustrated transformer, do you also have a reference to your blog on vision transformers?",True
@nikolaikrot8516,2022-12-02T13:51:49Z,0,it would be nice to remove music from the background,True
@IyadKhuder,2022-11-27T01:06:02Z,0,"I've ended up here to familiarize myself with NLP transformers. Your video was the optimal choice for me, as it' explains the concept in an understandable scientific manner. Thanks.",True
@HelenTueni,2022-11-25T10:47:00Z,0,Amazing video. Thank you very much for making this topic accessible.,True
@smoochie3331,2022-10-28T09:39:45Z,0,I m not self aware. only self aware can say that.,True
@omkarpanhalkar6837,2022-10-18T20:33:29Z,0,"Hi Jay, do you know how BERT generates CLS token for classification? Internally does it too max/avg pooling or some other approach. thank you",True
@cicik57,2022-10-11T01:57:02Z,0,"sorry, it is horrible explanation because you did not tell anything clear about the structure and processing in self attention block and overall in the transformer, the real presentation must explain each layer and operations between them - wtf are those arrows from you and you do not tell what happens between self attention and ""feed forward"" Also it is hard to understand your terms because you use confusing language with no standartized terms, for example you use word block instead of layer also almost ANY NN inclusive decoder IS feedforward and you use it in some special context here  what is not clear, because you have not explained its structure. And the last half of the video you tell completely unrelated preprocessing stuff.",True
@niundisponible,2022-09-02T08:39:31Z,0,"I see Miles Davis vinyl, kind of blue. Awesome album, and thanks for the video!",True
@ewnetuabebe5059,2022-08-15T12:56:55Z,0,Jay thank you for your Explanation !! But is Transformer used to predict Multivirite time series regression problem Example How to predict The magnitude of Earthquake??????????????????????/,True
@gorgolyt,2022-08-14T02:35:52Z,0,Over-simplified and incomplete. This description of a transformer doesn't even describe multi-head attention.,True
@stephenngumbikiilu3988,2022-06-15T21:42:00Z,0,"Your blog was referred to me by my lecture Julia Kreutzer of Google Translate, it's just amazing piece of work. It has really helped me in my understanding of these concepts. Thanks.",True
@datascileakos1936,2022-05-08T23:55:20Z,0,Thank you.,True
@danf8172,2022-05-07T18:39:04Z,0,I like your music taste :),True
@petesantago5977,2022-04-06T13:31:40Z,0,"This is very helpful, thank you. Question: Is there an easy explanation why Shawshank is broken into: Shaw - sh - ank?",True
@ankitmaheshwari7310,2022-04-02T05:28:23Z,1,Helpful.. you missed to import torch in your GitHub code.,True
@junlinguo77,2022-03-21T22:02:01Z,0,I like the way you are teaching! !!,True
@jimmorrisshen,2022-03-18T02:34:58Z,0,I see Billie holiday and OK computer. That is my vision attention which can be used for vision transformers.,True
@josephharvey1762,2022-03-15T14:34:46Z,0,"You, sir, have some good music taste!",True
@MsFearco,2022-03-03T11:24:40Z,0,I just found this now. it's super. thanks,True
@airepublic9864,2022-02-23T20:39:21Z,0,"Really thx for your time and effort but sad, that our country entenet speed is so low!  I reed yr github blog, that was awsome sir",True
@mahdiamrollahi8456,2022-02-22T16:43:20Z,0,"A question: let‚Äôs suppose that we are going to translate these two sentences into another language: 1- i go to university by bike. 2-i go to work by bike. ( the difference is just for work, university) we know that each token has different embedding vector according to input sequence( the encoder results). So, according to different embedding vectors , how transformer block can translate ‚Äúthe‚Äù to corresponding word in another language? I mean, when we have different representations of any token based on input, how we can get the proper results in output( decoder output)?",True
@mahdiamrollahi8456,2022-02-22T15:41:25Z,0,"Thanks, very intuitive‚Ä¶",True
@sharkeyryan,2022-02-19T19:17:37Z,0,"Thanks for creating this content. Your explanation is quite easy to follow, especially for someone like me who is just beginning to explore these areas of AI/ML.",True
@Opinionman2,2022-02-09T04:00:24Z,0,Awesome stuff. your blog really helped clarify my deep learning class.,True
@ygorgallina2691,2022-01-13T09:16:20Z,0,Thank you so much for your work ! The illustration help to clearly understand these models !!,True
@tachyon7777,2022-01-09T22:33:53Z,7,It would nice to have a step by step walkthrough of the training process. And why each of those steps makes sense intuitively.,True
@matthewavaylon196,2022-01-08T03:44:07Z,0,"Im confused by where do we actually get Q,K,V. You say from training but what does that mean?",True
@claudiorocchi9910,2021-12-26T11:21:00Z,0,Someone could say me which tool was used to draw vectors and matrix diagram in the video?,True
@andresjvazquez,2021-12-25T21:39:48Z,29,"Dear Teacher Alammar , thanks to this video I was able to accepted into BYU lab as an external researcher (even though I didn‚Äôt finish college) and have been invited by my professor to participate with the lab in CASP15 .  You really changed the course of my life by demystifying such complex topics for non traditional learners like me .  I‚Äôm eternally in your debt",True
@Nereus22,2021-12-17T01:51:24Z,0,"Great video, thank you!",True
@omarsultan827,2021-12-16T11:45:49Z,0,Thank you for this awesome introduction!,True
@Roshan-xd5tl,2021-12-09T09:28:19Z,17,Your ability to explain and breakdown complex topics into simpler and intuitive sections is legendary. Thank you for your contribution!,True
@ultraviolenc3,2021-12-07T10:07:54Z,0,I‚Äôve just read your ‚ÄúThe illustrated transformer‚Äù article and I wanted to say that you made very smart and simple visual representations. It seems you put a lot of thought into that.,True
@sampsuns,2021-11-19T18:21:05Z,1,"The explanation around two components in the decoder blocker is extremely unclear. Then I doubt about if Jay really understand it. By checking Linkedin, I now see he basically has little CS/ML background. Most his activities is in MBA and B school",True
@hongkyulee9724,2021-11-14T04:02:12Z,0,You are my hero. You give me reason of my life :D,True
@spacewaves94,2021-10-29T22:13:27Z,0,"Haha the chicken was a man,  thanks for all the work breaking this down!",True
@davefar2964,2021-10-21T12:00:47Z,0,"Thanks for this great high level overview. Just a small question at https://youtu.be/-QH8fRhqFHM?t=1499: if softmax turns logit -2 into probability 0.01, shoudn't softmax turn logit -1 into a probability >= 0.01?",True
@peekaachuuu,2021-09-24T07:06:00Z,0,Q) Where will computers completing our sentences be useful in the real world?,True
@atheeralbarqi3540,2021-09-10T05:05:42Z,2,"I appreciate your detailed explanation, Mr. Jay. My first reaction was to read your article (The Illustrated Transformer)  after watching the video. My question relates to the Transformer architecture, which consists of six encoders and six decoders layers, all of which seem to be very similar. What is the purpose of the six layers? since a sentence will be checked for relevant information in every word from the first layer using Self-Attention. In addition, Attention is used to boost training speed, so will these six layers slow it down?",True
@ayush612,2021-08-13T18:05:29Z,4,I remember Seeing your Transformer's Blog Jay.. It was legendary!! Was referred to by other youtubers as well... And thanks a lot for the wonderful explanation as well!,True
@evertonlimaaleixo1084,2021-07-14T01:28:23Z,0,Amazing!  Thank you for share!,True
@amirhosseinfereidooni1798,2021-06-23T07:04:11Z,0,Thanks for the great explanation. MLP (at 11:35) stands for  multilayer perceptron  :),True
@thanhphapdinh9163,2021-06-23T03:06:31Z,0,Your blog is cool but I think video is better without background music,True
@suneelkumarp489,2021-06-14T05:16:00Z,0,Why did the word ‚Äúshawshank‚Äù got split into three tokens?,True
@TusharKale9,2021-06-12T19:45:40Z,0,Great master piece explanation of NLP in real life scenario. Thank you,True
@akshaya9577,2021-05-25T09:45:23Z,0,absolutely amazing video,True
@sarmadys,2021-05-17T19:35:35Z,0,Also you didn't describe self attention...,True
@hailongle,2021-05-17T11:03:31Z,0,Fantastic teacher. Thanks Jay!,True
@armingh9283,2021-05-17T09:36:22Z,0,Thanks for the explanation. Good music taste at the background by the wayüëç,True
@sarmadys,2021-05-12T15:38:41Z,0,You didn't say why and how you put away the decoder section and only use decoder section?,True
@CristiVladZ,2021-05-12T05:57:42Z,0,Post more often!,True
@EvanZamir,2021-05-11T14:56:56Z,0,Do we even call these neural networks anymore? The architecture of something like the Transformer seems so far away from the original idea.,True
@wu_kenny,2021-05-02T17:20:09Z,1,Anyone knows what is the lm.trace_tokens in 26:30 in the final un-shared Notebook? Is it sth open-sourced for visualization? Looks pretty cool!,True
@ramnathaniel578,2021-04-22T10:33:24Z,1,Editor - stop jumping from screen to screen so quickly - you're giving me a headache!,True
@XiaosChannel,2021-04-18T11:38:29Z,0,"this is probably very good for a beginner who has never touched nlp... although a bit uninformative for me who has done some deep learning already. i guess it's hard to get both, not blaming you...",True
@jackdavidweber,2021-04-16T19:54:55Z,0,This is really great! Highly recommend!,True
@jpmarinhomartins,2021-04-07T23:47:50Z,0,"Dude I freakin love your blog, keep up with the good work! Thanks for everything!",True
@starsuper2740,2021-04-03T12:33:42Z,0,Thank youÔºÅË∞¢Ë∞¢‰Ω†ÔºÅ,True
@giofou711,2021-03-21T16:21:29Z,0,Great video!,True
@nisalbandara,2021-03-17T01:43:01Z,0,Im doing a Twitter sentiment analysis and i couldn't wrap my head around BERT and i came across this video. Perfectly explained. Thanks alot,True
@AskGeek,2021-03-16T22:50:09Z,0,Please pleaae explain self attention layer,True
@1Kapachow1,2021-03-16T20:32:08Z,0,"Really enjoyed your blog post and video, super clear - thank you very much for this amazing resource :)",True
@raminbakhtiyari5429,2021-03-13T18:28:51Z,0,"i don't khnow how must say thank you, I just can say please continue uploading your amazing videos. I live in a constrained country and this video is my only hope for learning like other peoples. yours sincerely. Ramin Bakhtiyari.",True
@prakashkafle454,2021-03-09T18:16:14Z,0,It may be simple question. I am trying to implement bert in nepali language .it  need data preprocessing before passing data to bert model or not ?,True
@damonandrews1887,2021-03-07T09:55:16Z,0,"I found this very helpful visual explainer, thanks so much for your time, and thanks for chopping it up into sections for easy revision ü§ì!",True
@rajeshnalla4497,2021-03-06T04:06:15Z,0,Thank you,True
,2021-03-06T03:36:12Z,10,"Just a personal comment on the format of the videos: I, personally, find that constant change of scene (like in ""The architecture of the transformer"" section) where the camera changes constantly showing you and then showing the computer screen and then back to you, is extremely annoying.   The content of the video itself was informative.",True
@rupakgoyal1611,2021-03-01T17:00:35Z,0,loved the music behind ..,True
@yudiguzman8926,2021-02-28T12:21:34Z,0,"I really appreciate your explanation about this topic. One more time, I check that DL is my new passion. Thanks a lot.",True
@ravitanwar9537,2021-02-27T13:22:04Z,0,paranoid android :),True
@kl6544,2021-02-26T21:09:32Z,2,‚ÄúA robot must troll‚Äù Dnw bout you but the model sounds trained to me,True
@romulodrumond3526,2021-02-15T19:26:38Z,0,One of the best videos of the subject,True
@sachinr3823,2021-02-15T18:01:03Z,0,"Omg, thanks lot for these amazing videos. Your lectures and blogs are so easy to understand.",True
@utsavshukla7516,2021-02-15T12:27:53Z,0,great explanation! also love all the pop culture references in your room :p,True
@bighit7596,2021-02-07T18:07:34Z,2,you have a gift for explaining complex materials... many other technical talks assumes the audience is very knowledgeable and are attending the session just for networking,True
@hasanb2312,2021-01-10T04:39:05Z,0,"Great video Jay, thank you so much!",True
@drtariqahmadphd3372,2021-01-02T22:48:16Z,1,Never been more excited by a YouTuber channel than when I saw this guy had a channel.,True
@javierechevarria1548,2021-01-02T04:05:51Z,0,Your are really good (excellent) at explaining a complex topic in a simple way. Congratulations !!!!,True
@KlimovArtem1,2021-01-01T10:37:54Z,0,"Now, if this super smart GPT-3 model is just a predictor of the text based on probabilities that it saw in tons of real texts, how can it solve some math problems then? ‚Äòcause it does, according to the chats with the model people post online.",True
@KlimovArtem1,2021-01-01T10:34:43Z,2,"27:56 - this explains a lot, thank you so much!",True
@KlimovArtem1,2021-01-01T10:12:14Z,2,"23:20 - so, the whole sentence is processed as one thing? How long can such sentence be? Is every Transformer Block unique from others? Is it just a single layer of perceptrons there or a few?",True
@KlimovArtem1,2021-01-01T09:54:17Z,0,"The tokenization is just a way to compress the language, right? It doesn‚Äôt add or remove any meanings or benefits? Like, the whole model could be re-trained, using every single Latin letter as a token instead, it would just take more time/resources, right?",True
@KlimovArtem1,2021-01-01T09:28:20Z,3,"15:30 - when it was trained on the huge texts, how did they decide how to tokenize is? Is it based on some linguistic objects? Syllables?",True
@KlimovArtem1,2021-01-01T09:23:33Z,5,"14:15 - so, the Self-Attention layer is actually the thing that‚Äôs trying to understand the meaning of the whole sequence? How does it work and how can it be trained? How long sequenced can it analyze?",True
@StellaKatsarou,2020-12-29T15:40:40Z,0,<3,True
@DanFrederiksen,2020-12-27T20:57:00Z,0,I'm not well versed with transformers. Is the attention aspect the part that allows verbatim absorption of rare sequences? In my experience with NNs they do common generalities first and only reluctantly adapt to particulars. 1 in a billion instances would vanish in the jitter of training. Of course this is sequential data so different but perhaps they singled out strong failures and repeated them more frequently to focus the training. I've certainly done that.,True
@snehansughosh2111,2020-12-26T16:10:59Z,0,Simply great Jay .. all it matters is keeping simple while spearheading the objective and  you are bang on it,True
@jacakopl,2020-12-21T12:34:07Z,1,This is the best video I have seen by far in this domain.  You strike a perfect balance in assuming the level of understanding of audience :),True
@tehseenzia3135,2020-12-21T06:50:34Z,0,Amazing illustration. Keep going Jay.,True
@SerJon5,2020-12-14T16:43:34Z,2,"omg, man. I've found several posters of really good music in your room! I defenetely need your TOP-10 albums list!!))",True
@AltafRehmani,2020-12-12T05:24:08Z,0,awesome video jay! noob question - given enough large pre-training data which trains the language model - the predictions that the model produces is only based on what it has seen before (and the task it was trained on). isnt machine learning supposed to let machines generalise on unseen data? your views?,True
@yuchenyang4394,2020-12-05T17:06:43Z,0,Great content! can't wait for more.,True
@kalinda619,2020-12-02T21:32:04Z,2,A phenomenal extension of your blog post. Commenting for that bump in the recommendation algorithm!,True
@AdityPai,2020-11-24T17:20:48Z,1,Thank you for writing the blog. It has helped me .,True
@ans1975,2020-11-20T16:01:17Z,27,The Illustrated Transformer blog is a masterpiece!,True
@bharath5666,2020-11-13T17:13:52Z,0,Amazing.,True
@Arocksum,2020-11-11T05:09:34Z,0,"Thanks for the great video ! Question though ; During tokenization, you show  that the model vocabulary is 'only' ~50.000 tokens (exactly, WordPiece ; so it could be word, or something smaller). That's why the input, even if it's a word, is tokenize into smaller part, which each of them become an input embedding into the first layer. Question now : knowing that, what are the chances that the word the model want to predict is actually written ""as-is"" inside the vocabulary ? Does the model, at the softmax step, only predict the best wordpieces, and then there is a step to ""build"" the word to predict from selected wordpieces ? Tell me if it make sense ! Thanks again.",True
@maxbeber,2020-11-09T16:05:19Z,0,Thank you so much for the clear and concise explanation. Keep it up the great work.,True
@mikemihay,2020-11-05T23:25:35Z,0,Awesome content! thank you!,True
@mertcokelek4595,2020-11-05T16:58:46Z,3,"Thank you for the great explaination. I am new to this topic, and I wonder why the ""shawshank"" word is tokenized into 3 pieces, the ""sh"" and ""ank"" are meaningless, is it a result of a learned model? Or the tokenization is done hand-crafted? Thanks in advance.",True
@pavelbaidurov228,2020-11-05T14:23:54Z,0,"Can you build a ""GPT-Scientist""? feed all of the sci-hub to it(if its legal) Basically a neuralnet capable to write its own math/biology/physics papers, just from a simple question and its own new math equations? imagine a ""scientist"" who can operate millions of papers in his ""head"" and create a new one. It will cut R&D costs by x10 000, we will successfully invade into a cellular biology and genetics with a machine with capable to mimic human intuition while keeping more than 70 million+(sci-hub) papers in its head! it will be able to create even more complex genes/proteins than mother nature has ever created.",True
@diogo.magalhaes,2020-11-04T22:30:02Z,9,"Jay, as a PhD student, I'm a fan of your ability to explain complex topics, in a very simple, illustrated and didactic way! I always recommend your ' illustrated' posts to my colleagues. Thanks again for this great video, keep up the good work!",True
@itall9025,2020-11-03T05:41:50Z,0,Great explanation! Please keep doing this format.,True
@vslobody,2020-11-01T23:16:01Z,1,"Jay - i think this question was asked somewhere else, but i cannot find good answer -  From the article: > In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.  In other words, the output logits (i.e. word translations) of the decoder are fed back into that first position, with future words at each time-step masked.  I'm not quite sure how it all flows, b/c with several rows representing words all going through at once (a matrix), it seems like you would need to run the whole thing forward several times per sentence, each time moving the decoded focal point to the next output word...  where is this loop in the Decoder layer, i am struggling to figure it out n my own.  Thanks much in advance,  Volodimir",True
@a.e.5054,2020-10-30T02:35:50Z,0,The best explanation of the Transformer and GPT  model !!,True
@parthchokhra948,2020-10-27T02:39:21Z,226,Your blog on Illustrated Transformer was my intro to Deep Learning with NLP. Thanks for the amazing contributions for the community.,True
@o_felipe_reis,2020-10-26T20:09:29Z,1,Great video! Best regards from Brazil!,True
@quietkael7349,2020-10-26T18:25:34Z,9,Thank you so much for all the tireless work you do for us visual learners out there! I‚Äôm looking forward to videos where you get into your excellent visualizations of the underlying matrix operations. Your visual abstractions both at the flow chart level and matrix/vector level have really shaped my mental model for what I think about when I‚Äôm engineering models. I‚Äôm so grateful and so excited to see what you come out with next (this library you hint at looks wonderful!),True
@user-or7ji5hv8y,2020-10-26T18:02:05Z,0,Great video!,True
@pastrop2003,2020-10-26T17:24:54Z,0,"Hey Jay, first of all, thank you so much for your blog post and the video, those are awesome.  I still have a question regarding going from the last step of the transformer to the logits.  As a matter of fact, am I am mostly interested in the same step during the BERT pertaining in the case of the masked language model.  Concretely,  the case you are discussing is clear. You take an embedding of the sentence (however it is produced out of embeddings of the words) feed it into a linear layer that blows it into 50,000 softmax.  In the case of the BERT masked language model, you have something like The Robot must MASK hurt the human.  You hope that it will predict ""not""  Is it also interpreted as a sentence (using CLS token embedding or whatever...) before it gets fed into the linear layer?  I couldn't find a detailed description of this step anywhere Thanks!",True
@ruaismail3426,2020-10-26T16:00:39Z,0,üëè üëè üëè,True
@nmstoker,2020-10-26T12:50:28Z,2,"Watching it now, thanks so much! It's really helpful to go through these kinds of things with clear examples and explanations. My only preference would've been to reduce the volume of the background music in the intro. So many podcasts do this and it's an annoying trend!",True
@rsilveira79,2020-10-26T12:20:55Z,4,"Nice collection of albuns man! Miles Davis, Radiohead, John Coltrane, very classy! üëèüëèüëè",True
@shaz-z506,2020-10-26T11:15:52Z,1,"Wow, that's really a good video on the transformers, how did you get that cool output display in jupyter notebook.",True
@Murphyalex,2020-10-26T09:44:59Z,1,"Amazing video. Have to admit that every time I heard the wrong pronunciation of ""Shawshank"" it did feel a bit like nails on a blackboard but easily forgivable. Jay, your resources and videos are phenomenal :) Thank you for putting in the work to help us all out.",True
@haswanthaekula7656,2020-10-26T09:35:11Z,3,"This is a noob question, I was just curious when I was watching the video. How is it Unsupervised pre-training when you are actually providing the correct output (label) at the end?",True
@MohamedSayed-et7lf,2020-10-26T09:29:40Z,0,Waiting for the illustrated transformers to be updated with the new lovely visualizations.,True
@anishjain3663,2020-10-26T08:47:10Z,0,Finally you come,True
@parmarsuraj99,2020-10-26T08:26:39Z,5,‚ù§Ô∏è That library!!!!,True
@FahadKB,2020-10-26T07:53:33Z,0,üî•‚ù§Ô∏è,True
