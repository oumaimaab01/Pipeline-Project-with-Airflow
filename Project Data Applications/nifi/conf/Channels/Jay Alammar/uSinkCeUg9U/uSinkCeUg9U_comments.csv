author,updated_at,like_count,text,public
@KumR,2024-02-04T19:50:52Z,0,"Hi Jay - Great video.. Wondering if this is similar to computers doing everything in 0s and 1s although from the OS level the abstraction is different . At least conceptually. Coming to the book , I am not able to find it anywhere... Is there a link ?",True
@prabaj84,2023-08-22T19:31:25Z,0,"Hi Jay, thanks again for explaining a complex topic in simple way, if I may ask, what tool do you use to generate graphics for your blogs? Thanks in advance",True
@khaledsrrr,2023-08-16T21:34:20Z,0,‚ù§ very nice,True
@123arskas,2023-08-11T11:40:00Z,0,Good one,True
@goelnikhils,2023-08-06T16:28:53Z,0,Great Work,True
@ranjancse26,2023-08-03T00:54:58Z,0,This is incredible. Great work! Keep it up :),True
@jwilber92,2023-08-02T00:00:51Z,0,"Great content as always, Jay!",True
@amittripathi6664,2023-07-31T21:31:54Z,0,"Hi Jay, thanks for the video. Could you also please share the code?",True
@gama3181,2023-07-30T03:32:19Z,0,And how the people know wich tokenizer is the best way to split the vocab? This follow a math rule or statistical pattern? or it depend on the computing budget?,True
@kidsfungaming6756,2023-07-29T19:57:45Z,0,"Hi Jay,  I love your presentation, it is so inspiring and you make the hard concepts simple and clearer. Regarding the tokenizer, if every word is one token and the same is mapped over a single vector (embeddings) then how do LLMs clearly understand the meaning of the same word in different contexts? I will appreciate your answer and I am sorry if my question is too naive.  Thank you",True
@Patapom3,2023-07-28T16:17:22Z,0,Great! But how does the tokenizer works now? üòÖ,True
@shabiehsaeed8633,2023-07-27T15:58:45Z,4,"Hi Jay,  I love the work you have done! Ever since I read the Illustrated Transformer, I was blown away by your explanations and illustrations. You really explain advance concepts with such clarity and simplicity. I am very grateful to you for that! I really look forward to reading and learning from your book! Thank you so much!!",True
@TheAero,2023-07-27T05:54:47Z,0,I would love if you could go into the following: RLHF. PPO. PEFT. LORA etc. Adapters. soft-prompting. scaling transformers.,True
@ashisranjanlahiri,2023-07-26T16:42:23Z,1,"Hi Sir, your video always amazed me. Need more videos for sure. Can you please share the notebook link.",True
@mohamadbebah8416,2023-07-26T14:18:31Z,0,Great!! Thank you very much,True
@WhatsAI,2023-07-26T14:13:30Z,1,Great video as always Jay! :),True
@tanmoy.mazumder,2023-07-26T14:06:25Z,1,could you perhaps do an even deeper dive about how these models exactly produce the output vectors and then how those get turned into these tokens and stuff?,True
@boonkiathan,2023-07-26T13:26:29Z,5,Neither have our neurones,True
