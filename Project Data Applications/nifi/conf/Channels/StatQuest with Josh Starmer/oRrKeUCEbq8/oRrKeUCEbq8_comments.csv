author,updated_at,like_count,text,public
@statquest,2022-05-08T18:56:47Z,1,Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@lilianaaa98,2024-05-03T02:57:57Z,1,what an amazing song in the beginning of this videoÔºÅÔºÅÔºÅ,True
@weii321,2024-04-22T03:36:19Z,2,"Hi Josh! Your XGBoost videos are great! By the way, do you have a tutorial about LightGBM?",True
@myelinsheathxd,2024-04-14T03:08:26Z,1,Tripple BAM!,True
@doyelmukherjee2769,2024-02-11T18:30:42Z,0,Hi Josh...does one on one variable correlation and multicollinearity affect  ML models???,True
@nitinsiwach1989,2024-01-27T01:22:30Z,1,"Hello Wonderful Josh,  Do you think it is time for xgboost video 5? Explaining the additions brought in by xgboost 2",True
@user-gq3uo8dl1l,2024-01-20T15:47:16Z,0,,True
@deana00,2023-11-04T18:54:36Z,0,"Hi Josh, Thank you for your great series.   Btw, do you happen to know how LightGBM find its candidate splits in Histogram-based split finding?",True
@jtruler,2023-06-05T16:58:43Z,2,I just wanted to say thanks. I really struggled in college because I always felt like the professors explained things like we already knew everything they were teaching. The way that you break everything down is super helpful. I hope that you continue to make these sorts of videos even when you because a world famous musician. Keep it up :),True
@xiaotianxiaotian9974,2023-05-31T18:49:14Z,1,"So great, cant praise your guys much!üëç",True
@nitinsiwach1989,2023-04-08T19:34:34Z,0,"I have yet another question on the histogram-building process:   Let's say I had 1M rows and 1 feature 1. Build the histogram. Now there are approx 33 split-points 2. Splitpoint 10 gives max gain 3. Data from the first 10 bins go to the left child and the data from the 10th bin onwards go to the right child *4. To further split the left and right child on the same feature, (a) Are histograms with ~33 split-points built again for data that landed in the left and the right child. Or, (b) is it that now only 10 already computed split-points would be considered to compute gain for the left child and 23 already computed split-points for the right child?* I think it is (b) since I think that is the only one that can result in a speedup IMO",True
@nitinsiwach1989,2023-04-07T21:40:52Z,0,"Finally.. graduated in tree based algos from the statquest academy. What a feeling :) :)... One absolute last hiccup-- What does ""xgboost splits the data so that both drives get a unique set of data"" mean? What is a unique set of data there? And why does it ensure that parallel reads can happen? Why can't parallel reads happen if the ""unique set of data"" isn't there on different drives?",True
@nitinsiwach1989,2023-04-07T19:25:13Z,2,"This is the most lucid, most comprehensive single stop shop for the tree based algos that you are going to need in any job at all. Lightgbm missing definitely leaves something to be desired. Shall we expect a Lightgbm video anytime soon, Josh?",True
@lima073,2023-02-22T15:33:30Z,1,Thanks!,True
@rahul-qo3fi,2023-02-20T18:58:45Z,1,‚ù§‚ù§‚ù§‚ù§,True
@vigneshht2917,2023-01-31T09:37:15Z,2,Great work. You deserve more than a million subs for your effort and dedicationüòÄ,True
@hoatrinh401,2022-12-15T06:01:27Z,1,BAM BAM,True
@thedatascientistchannel7176,2022-12-14T08:53:19Z,1,Simple but powerful!!!,True
@dandyyu0220,2022-10-09T13:47:06Z,1,"Thank you thank you for such a great series of XGBoost videos, so clear explained while in depth!",True
@mohammadelghandour1614,2022-08-26T11:56:15Z,0,"in 14:25  what if we have 5 samples at 0.1 probability instead of 2 samples as well as another group of 5 samples at 0.9 probability instead of 2 samples in addition to the last two samples with low confidence ? will the first and second group end up in two separate quantiles with total weight of 0.45, each? if so, then then third quantile will contain the last two samples with opposite residual since their sum of weights is almost equal to that of the first two quantiles, i.e., 0.48",True
@erfanmoosavi9428,2022-08-10T11:08:04Z,1,TRIPLE BAM!!,True
@HasanAYousef,2022-07-27T20:14:14Z,0,Can we use XGBoost for time series forecasts?,True
@dihancheng952,2022-07-13T00:22:03Z,1,"your videos never disappoint me, I feel I can click on ""like this"" even before watching your videos",True
@tytaifeng,2022-06-28T13:06:40Z,1,"thank you so much josh, I felt so lucky to find your videos on youtube, ultimately clearly explained and I just love it.",True
@jadore801120,2022-05-07T07:40:38Z,1,This video is just too awesome,True
@KnowNothingJohnSnow,2022-03-20T15:38:00Z,0,i need to study operation system ...,True
@purandixit2384,2022-03-10T17:31:09Z,0,"best explanation in the world, I think there is typo at 19:12, instead of Dosage<7.5, it should be Dosage<15.5. I may be wrong. Thanks in advance.",True
@rezaliswara4086,2022-02-24T02:50:30Z,0,"Nice explanation! I want to ask. When we use large dataset for sparsity aware split finding, are we must do parallel learning and weighted quantile sketch for find the threshold?",True
@zeusserch98,2022-02-23T10:25:22Z,1,Thanks for the great explanation! I have two questions: 1. How many dataset that we need so we can use parallel learning? 2. In parallel learning is we just make one tree so we can find weigth quantile sketch?,True
@hrishikeshpotdar5889,2022-02-20T01:22:31Z,0,Machine Learning is more than just applied statistics - Josh Starmer,True
@shashankkapoor2828,2022-01-25T23:13:26Z,2,"I read the paper thousand times, it was never this clear. not even close. Thank you so much.",True
@rajeshnimma155,2022-01-20T07:41:05Z,0,"Can you please help us understanding Light Gbm , cat-boosting  algorithms",True
@ericzhang5987,2021-12-24T05:21:55Z,1,Amazing!!!,True
@matthewbetty5237,2021-10-02T04:11:12Z,1,"Hi Josh. I am not sure if you still check these comments, but I wanted to thank you for making these really amazing and informative videos. I am not sure I could pursue machine learning in my own time if I did not have these great resources to clearly explain the content to me.   Thanks for making the maths fun and showing all the cool details, its great :)",True
@zhihaoxu756,2021-10-02T03:55:56Z,1,"Wow! How is it possible there are some downvotes??? This video is incredible. I vote for StatQuest, thank you so much!",True
@baochung1751,2021-09-20T07:37:56Z,3,"Great series on XGBoost! Thank you very much for making them. Now I got more clear understanding about XGBoost, especially its boosting trees and computational advantages that make it fast. The animations are wonderful, making things more easily to follow. I love them so much. Hope that you will make something on LightGBM and CatBoost.",True
@anweshbiswal5180,2021-08-31T05:30:15Z,0,Hi Josh!! Thanks for this ‚ù§Ô∏è. But can you explain if how did you find the residuals value of missing dosage using initial predictions in Sparsity Aware Split Finding? Or if anyone else knows can you please help on me this . Thanks in advanced.,True
@PasinKunamart,2021-08-25T17:22:03Z,1,"Thank you for the detailed but easy-to-understand video. I'm also interested in LightGBM algorithm as well (seems like it was compared with xgboost a lot), so I would be happy if you made one for lgbm as well.",True
@programmer8064,2021-08-07T19:10:53Z,1,Thank you so much,True
@rohitrajora9832,2021-07-25T11:57:14Z,1,BAAAAAAAAAAAAAAAAM !!!!!!!!!!!!!!,True
@soujanyapm480,2021-07-10T16:19:07Z,9,OMG !!!! what an explanation ! extremely detailed explanation of extreme gradient boosting. I think no one on this planet can explain this topic like you Josh!  You have literally done the autopsy of this algorithm to get into that details:) Thanks a ton for this amazing video !!!!,True
@abrahamgk9707,2021-07-10T14:39:11Z,1,Can you please do a video on expectation maximization algorithm üôèüôè?,True
@tonywang5203,2021-07-01T03:51:42Z,1,This is dope.,True
@xixi1796,2021-06-20T22:12:31Z,0,Great video! But I don't understand why *Hessian* is used to serve as *weights* for quantile histogram. What is the underlying mathematical reason that the 2nd order derivative plays a role of weight?,True
@travelwithadatascientist,2021-04-04T17:34:31Z,1,Now  I can read the XGBoost paper much easily‚ù§ Thanks a lot for these stat questsüôå,True
@sabrinahung5584,2021-03-30T06:25:42Z,0,Thank you so much for your videos! I have learnt so much from them. Could you do a video on LightGBM and catboost as well? :),True
@beckswu7355,2021-03-15T03:58:54Z,0,"At 19:56 when choose leaf for missing value, you select left branch as default path. It make senses because missing value residuals (-3.5 and -2.5) are negative, which is similar as nonmissing value residuals (-5.5. and -7.5). I wonder if I can select Right branch as default path for missing value if my residual is large positive e.g. 10.5 in stead of -3.5 and -2.5.",True
@tejashshah5202,2021-03-04T07:55:23Z,5,Please accept my virtual standing ovation :) I finished the entire XGBoost tutorial and you made it sound super simple. Please also add LightGBM to the list. Thanks again for all your work.,True
@alessandroalbano5891,2021-03-03T11:04:50Z,1,You are a king,True
@TheShadyStudios,2021-02-18T01:23:16Z,1,you are the GOAT!!,True
@pragatiparhad6309,2021-02-12T14:40:55Z,1,Please make video on time series model also,True
@RajeshSharma-bd5zo,2021-02-12T13:42:07Z,2,"Simply an awesome playlist on Boosting, so AWESOME BAM!!",True
@virgildjogbessi,2021-02-12T12:38:51Z,0,"Thank you for your really helping serie about XGBoost  I've got a question. When you talk about huge data base, what do you mean? Also, is a 67,000 rows X 9 columns can be considered as a huge data base?  Thank you in advance for your answer. BAM!",True
@pannawitathipatcharawat1585,2021-02-07T05:54:33Z,0,"Thank you for the amazing video !! but I have some questions Does it mean that all the missing value for that feature (let's say feature A) need to be in the same side (all in left or all in right)? so if it is, it means that XGBoost will treat all the missing value in the feature A as the same? Thank you Josh : D",True
@kennethleung4487,2021-02-01T03:37:13Z,1,Fantastic!,True
@lkhhoe,2020-12-21T11:10:13Z,0,"Hi Josh, thanks for making these awesome video to learn the XGBoost in depth. But I want to ask the *weight* you mentioned in, is it same with the *cover* you mentioned in previous videos? Since both of them have the same formula.",True
@samerrkhann,2020-12-12T10:37:11Z,8,I couldn't thank you enough for the efforts that you have put into this series of lectures. Thank You JOSH!,True
@jjj78ean,2020-12-05T19:44:56Z,0,"Hi,Josh. Thank you for exciting explanation! What about to make the same amazing series about LightGBM and Catboost?",True
@parijatkumar6866,2020-11-18T08:35:51Z,1,Your videos make Machine Learning - Human learnable,True
@Raven-bi3xn,2020-10-12T18:22:21Z,1,"Thanks, Josh. One question about the greedy part (4th minute); in random forest (say, regression applications), even though we use a subset of features for each tree and for a bootsrapped subset of the data, we could still end up with many thresholds to examine similar to the problem with xgboost. Why not using quantiles for building random trees that deal with a high number of thresholds?",True
@ashwathshettyr5363,2020-09-28T12:14:38Z,0,"but, what if one missing values residual fit into the left side of the tree and one to the right side of the tree? then how will you predict?",True
@thamus90,2020-09-15T21:31:41Z,1,Loved it! Thanks!,True
@shivamkaushik6637,2020-09-15T15:57:27Z,1,The best explanation to XGBoost so far.,True
@huishiyang3561,2020-08-28T10:28:09Z,0,"Hello Josh. Many thanks for another super BAM video!  I have a question about the missing value part. You explained how xgboost incorporates missing values in training and makes predictions for missing values in future data. What happens when there are no missing values in training, but there are in testing/future data?",True
@aaryannakhat1842,2020-08-16T10:45:40Z,13,"All I can say is:  BAM!! Double BAM!! Triple BAM!! Mega BAM!! And finally, EXTREME BAM!!!!!!  Thank You Josh for these wonderful lectures!",True
@ronaldgiliolucana7210,2020-08-11T22:51:04Z,3,"Super cool!!!, It's the best explanation in the world.",True
@rishabhahuja2506,2020-08-05T07:24:22Z,0,"Thanks Josh for this amazing video. Your explainations are really great and helpful. It would great if you could share your approach for understanding the complex algorithms like xgboost. Currently I am digging into catboost, just curious to know what resources or plan you follow when you want to understand a new algorithm like reading research papers, understanding maths behind it, etc.",True
@Exiled1517,2020-07-24T09:12:10Z,0,"If i may ask politely, can someone explain it to me in general yet simple explanation of how did xgboost fill the missing value in datasets? It's for my thesis as my lecturer ask me to explain it more specific about it.  Thank you:)",True
@vidhyapc,2020-07-19T16:18:27Z,3,"So grateful for these videos.  Has made understandable of xgboost so simple though the concepts are little complicated.  Closing my eye i can know say how the tress are built, how are they optimised, how are they faster etc",True
@mohammadshahadathossain1544,2020-07-12T07:47:40Z,1,Informative video with detailed explanation.,True
@nehamanpreet1044,2020-06-15T20:16:59Z,0,"At15:06 minutes, we have sum of weights in one quantile as 0.18, in another we have 0.18, then the last two we have 0.24. But as far as  I understood you explained that in weighted quantiles, the sum of weights in all the quantiles are equal but here its not equal in all 4??",True
@HemangJoshi,2020-05-29T10:32:47Z,0,Your videos are the excellent quality but please stop the bad music at every video start.... It is a humble request... Because it distracts the mind before starting the learning process...,True
@yukinakamura5231,2020-05-17T11:46:01Z,2,BAMMMMMMM I appreciate you,True
@sureshparit2988,2020-05-17T05:04:54Z,5,"Josh aka Heisenberg, You're Awesome. Thanks for the series.",True
@pandikapinata62,2020-05-12T13:49:21Z,0,"Thanks for XGBoost series, if the feature like Dosage is a string format how XGBoost calculate that to build a tree? Like time series data  ex. Sunday, Monday, Tuesday etc. Thanks",True
@SasidharKhambhampati,2020-04-19T11:53:39Z,4,"Hi Josh, Thank you very much for the videos on XGBoost. You have successfully explained the topic clearly though it is complicated..Thanks a ton",True
@the-brick-train,2020-04-05T20:11:02Z,1,This is pure class,True
@VBHVSAXENA82,2020-04-04T09:16:28Z,1,"Hi Josh, thanks for another great video. Would request you to make one on hyper parameter tuning as well.",True
@teetanrobotics5363,2020-04-03T00:44:07Z,2,Could you please make a course on Probability as well ?,True
@marcoscosta829,2020-03-31T20:07:57Z,0,"Hey Josh, great job, looking forward for your next video!   Just out of curiosity, dou you plan to make a video teaching catboost?",True
@radocisar3420,2020-03-25T10:45:38Z,1,"Great, as always.",True
@yanbowang4020,2020-03-06T19:51:57Z,35,"Hi Josh, would you please do a series to clearly explain Bayesian & Genetic hyperparameter tuning algorithms?",True
@diedrichschmidt5869,2020-03-06T16:31:46Z,1,"Thanks for the highly accessible explanations for how XGBoost performs its calculations. What about doing a few videos on tuning the hyperparameters for improved model fits? For boosted tree, and random forest, this is not so complicated, but XGBoost has many, many parameters that can be tuned, making the optimization of the model quite challenging.",True
@manuelagranda2932,2020-03-06T04:45:45Z,4,I meaannnn! maybe you can see my thesis project and watch your name in the appreciations !!! jaja thanks a lot!! Grettings from Medell√≠n-Colombia,True
@user-nv6tt4gf3f,2020-03-05T06:56:41Z,120,I appreciate your endeavor making this video. You read and summarized the paper of XGBoost! I think you are the one who best explains XGBoost in the world!!!,True
@arjunpukale3310,2020-03-04T07:37:42Z,0,Please make video on Generative Adversarial network (Gans),True
@arpitshrivastav4822,2020-03-04T06:08:35Z,1,Best explanation for XGB available!,True
@xinnywillwin,2020-03-03T01:35:17Z,1,‰∏âËÅî,True
@brunomaciel4135,2020-03-02T21:09:16Z,2,"Thanks you very much for the videos. You give the best explanation.  I'm looking forward to the lightGBM video (hope there will be one, someday). That would be a HUGE BAAM!",True
@KarthikNagadevara7791,2020-03-02T07:01:36Z,3,Thank you so much for doing this ‚ô•Ô∏èideo. Would it be possible to do videos on Ensemble Machines.,True
@alanpress3019,2020-03-02T06:04:47Z,0,"Th great ML channel. Josh, are you planning to give lectures on Convolutional neural network and Capsule network for Deep Learning?  I'm expecting those Bam!",True
@aditya4974,2020-03-02T05:31:24Z,16,"There is only one channel whose videos can be liked even before watching for one second,  and that's StatQuest! Bam!",True
@rrrprogram8667,2020-03-02T05:30:29Z,0,What do u use for making the diagrams??... Power point or any other software??,True
@rrrprogram8667,2020-03-02T05:27:57Z,2,MEGAAA BAMMMMM is back with  BAMMMMMMM,True
@mikhaeldito,2020-02-23T19:14:45Z,1,You are amazing!,True
