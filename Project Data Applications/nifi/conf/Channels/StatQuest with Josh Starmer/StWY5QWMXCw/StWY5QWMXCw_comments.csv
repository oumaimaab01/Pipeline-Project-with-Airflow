author,updated_at,like_count,text,public
@statquest,2020-03-09T01:08:28Z,37,"NOTE: Gradient Boost uses Regression Trees. I explain them in this StatQuest: https://youtu.be/g9c66TUylZ4   Corrections: 6:58 log(p) - log(1-p) is not equal to log(p)/log(1-p) but equal to log(p/(1-p)). In other words, the result at 7:07,  log(p) - log(1-p) = log(odds), is correct, and thus, the error does not propagate beyond it's short, but embarrassing moment.  26:53, my indexing of the variables gets off. This is unfortunate, but you should still be able to follow the concepts.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@jayseagal3681,2024-05-17T10:12:36Z,1,Josh Starmer the super statistician,True
@rochitranjan,2024-04-18T12:15:42Z,0,I could not understand how you removed the summation sign at 6:05 in the video. Can you please explain ?,True
@user-lw1jz3ez4i,2024-04-05T16:30:56Z,0,"hi josh! Sorry for bothering you, i just have a question about why we don't just calculate the derivative of 'loss function' with respect to 'gamma' but to use Taylor formula, i calculated it by hands and find the derivative seems to be [log(yi/(1-yi)) - F_m-1(x)] if there is only 1 sample in leaf i. it seems not that messy(if im doing right). So could you explain it more explicitly? Thanks a lot!",True
@FelipeArayaable,2024-03-26T13:53:55Z,0,"Hi josh! I think you made a little mistake in when building the first tree, shouldn't the residuals of each leaf be [0.33] and [0.33, -0.67], instead I see [0.33] and [0.33,-0.7]. Also, I think for part 3 of the series. the initial log(loss) value was 0.7 (ln(4/2)), but then we use that value to get the residuals instead of p (4/6 = 0.66), so we do ""observed - log(loss)"" = 0.3 (positive cases) or =-0.7 (negative cases), whereas here we do ""observed - p"" = 0.33 (positive cases) or = -0.67 (negative cases). I am so confused now about which one is it, it makes more sense to me to do ""Observed - p"" since observed is also a probability so we are doing apples to apples, no?",True
@JakeCarl80,2024-02-08T03:21:13Z,0,"Hi, these are really great videos, and I really appreciate the detail they go into. However, at 9:40 the differentiation of log(x) is said to be 1/x. But isn't this just for natural log ln(x)? Shouldn't this be 1/(x*ln(10)) ? Or is log() in this context not referring to base 10, or is there something else that I'm missing/not aware of? Thanks!",True
@fatihboyar9697,2024-01-26T18:31:08Z,1,this is epic man,True
@user-yz5ho2jf3q,2024-01-26T15:27:09Z,0,"Thanks for such deep explanation! Can you tell me, if there is a loss function for multiclassification problem, and how it is to do with it beyond your algorithm?",True
@user-gq3uo8dl1l,2024-01-18T00:57:56Z,1,,True
@pranavreddy9218,2024-01-16T18:37:32Z,0,"at 34.38, please check the 3rd data point residual is -0.52..",True
@pranavreddy9218,2024-01-16T17:58:46Z,1,"what a great lecture sir, i never seen this kind of explanation with derivations for such very difficult Boosting algorithms. Hats off to you sir.",True
@jingzhouzhao8609,2023-12-10T18:04:48Z,0,"Excellent explanation, the best I've ever seen. There's just one missing piece in my mindset. At 19:04, I've been wondering how the first equation is converted to the second one using the second-order Taylor polynomial. I tried deriving it using the common Taylor polynomial extension formula, but all attempts failed to yield the second equation. Puzzled for a long time.",True
@jingzhouzhao8609,2023-12-03T21:50:46Z,0,"I am an idiot and have a question. At 25:44, I think the output value for R(1,1) should be positive infinity because that will make the loss function for R(1,1) close to zero. I know I am wrong but I can't find an answer.",True
@sachinsavale2007,2023-12-01T16:19:07Z,0,"Josh, your explanation is incredibly clear and helpful. After watching XGboost video, a question arose in my mind, which I'd like to discuss here. In XGBoost, the similarity score is calculated to determine gain, with the formula being the sum of residuals, squared divided by the sum of the previous probability multiplied by (1 - previous probability) plus lambda. I'm curious to know which formula is employed for calculating the similarity score in gradient boosting for classification. Thank you.",True
@nitinsiwach1989,2023-11-11T16:03:59Z,0,"I come to your videos once every 6 months. It helps me a lot.  You have mentioned that in both gradient boosting for classification and regression we fit a regression tree on the residuals. That means using the split that minimizes the MSE, right? I think the similarity scores split in the xgboost make so much more sense. They are completely aligned with the objective of minimizing whatever the loss function is given. Am I right in thinking that the only difference between a regression-tree and the xgboost-tree is the splitting criteria?",True
@deana00,2023-11-03T15:51:15Z,0,"Does it mean in 3:39, you were using binary cross-entropy as a Loss Function?  Or, is log(likelihood) different from binary cross-entropy? I'm sorry, I found it little confusing",True
@luvxxb,2023-10-17T07:20:32Z,1,thank you so much!! bam bam bam,True
@user-wp5gw3jl4s,2023-10-17T01:31:26Z,0,where is that 0.7 coming from when creating a tree,True
@user-fi2vi9lo2c,2023-09-24T08:15:58Z,1,"Thanks a lot, Josh! I took my pen and I wrote down everything I saw in this video (formulas, examples, etc.) to keep in somewhere in my notes. That was really helpful!",True
@user-fi2vi9lo2c,2023-09-23T12:27:03Z,1,I would like to put TRIPLE LIKE to this video! But that's only one allowed :(,True
@goodtoyable,2023-09-17T12:08:05Z,0,"Great Work! Thank you for the excellent videos. One thing I am still confused is how to do step 2. b: Fit a regression tree. Such as at 34:45, how do we get the second tree with  Age > 65.6? Appreciate it if you could direct me to any previous videos that have the explanation.",True
@user-ew8el1ic9h,2023-09-15T05:02:34Z,0,Thanks for the video. Is this part in your book (illustration lol) as well? I need a book on this so I can learn this completely at my own pace (not that the video isn't good) : ),True
@hopelesssuprem1867,2023-08-30T11:11:39Z,0,this would be better if u made axamples with multiclass GB using softmax because this thing is used in sklearn,True
@CrazyProgrammer16,2023-07-21T17:06:20Z,0,Isn't that binary cross entropy?,True
@robertofirmino5283,2023-06-11T03:39:59Z,1,holy freaking smokes üî•,True
@diegoarend3281,2023-03-07T05:26:13Z,0,"Okay, I dont know if you're still answering questions but I've been confused at one part. So, at 9:26 when checking the derivative of the Loss Function, why you derive respect to log(odds)? And also why dont you derive the Loss Function by log(p) while it's still in p form (if you even can derive from p)? If you cant, would you be mind telling the reason? (Sorry, it's just been in my mind ever since I'm writing my paper).",True
@Gautam1108,2023-02-20T11:35:17Z,1,"Mind = (Blown)^100. Thanks for making this so much easier to understand, Josh!",True
@ShadabAlam-jz4vl,2023-02-19T06:35:12Z,1,"great explanation of such a complex topic in easy way, loved it. Thank you Josh.",True
@susmitsingh9611,2023-02-08T10:03:44Z,0,How do we calculate log(odds) in case of multiclass classfication problem,True
@knightedpanther,2023-02-04T22:53:00Z,0,"Hi Josh,  Thank you so much for the video. Appreciate the efforts!! I have a question:  If we put the calculated value of gamma back in the loss function equation, we will get something like sum of squared residuals for all observations divided by sum of p(1-p)  for all observations. Why don't we use this as the split criteria like we do in XGBoost? They do the similar thing to derive the similarity score, right?",True
@knightedpanther,2023-02-04T02:58:17Z,0,"Hi StatQuest, is it true that we use weak learners in boosting and strong learners in random forests or both use weak learners?",True
@lyndonhe3886,2022-12-16T16:55:56Z,0,"Hi Josh, really appreciate this amazing video! I have a question that confuses me for a long time. At step 2 (B) (check out 16:00), when we try to build a regression tree to fit the residual, are we still using minimized SSR to determine which variable and threshold to use for splitting? How does the loss function help in this process? I don't know how the loss function works.  I'd be appreciated it if you can answer my questions at your convenience.",True
@himanshusingh-bh5kh,2022-11-15T15:10:33Z,0,"Can You Make Videos on  Cat Boosting, It will be very helpful  Josh Both Regression & Classification",True
@raminekhteiarisalmas4766,2022-11-06T22:51:18Z,0,That's very useful. I'm wondering if text versions of your videos are available?,True
@anunaysanganal,2022-10-28T18:02:50Z,1,"Awesome video! Just one question, for the decision tree we fit on the residuals, what is the splitting criteria, is it MSE, as you said its a regression tree?",True
@himanshusingh-bh5kh,2022-10-26T15:02:53Z,0,At 13:43 Basically in Last step  of Step 1  F0(x)=Log(2/1) supposed  value should  be  =0.30 with  taking common base^10   How your  are getting  Log(2/1)  = 0.69 ??,True
@LakshyaRawat,2022-10-10T18:00:20Z,1,"Maths teachers around the world should teach this way. ""Now back to where we were, after we did a whole bunch of calculus"" Hats off!!!üòÇüòÇü§£üëçüí™",True
@hongweiwu9269,2022-09-16T13:34:46Z,1,You are the best. Very impressive ...,True
@abhayjoshi2121,2022-09-09T05:48:40Z,4,"Thanks for such an explanation, It becomes really easy .. I Have purchased the illustrated guide and hope to see more such content and books from your side as you have a lot of knowledge and keeping it simple is your moto... Wish to have lectures on Time series in future .. Thank you so much",True
@thepresistence5935,2022-09-06T07:38:50Z,0,Why It's difficult to understand for me,True
@mohammadelghandour1614,2022-08-22T19:11:25Z,0,"in 32:08 , How can a single sample end up in multiple leaves?",True
@barryfeng6602,2022-08-19T22:32:25Z,0,"Terrific explanation and animes! For the loss function, why not just use the log(likelihood) instead of - log(likelihood) since we can get the largest number of log(likelihood)?",True
@simaranvohra,2022-08-11T10:01:59Z,0,How can a sample end up in more than one leaf? (step D - need for a summation during prediction updation),True
@beshosamir8978,2022-08-09T12:54:17Z,1,i hope u didn't feel bored from my questions but i have a quick question  why do we convert the loss function dealing with probability to deal with log(odds) ? is it because it is easier to solve ? can we even still using it with probability and solve it ? or we have to convert it first ?,True
@swethanandyala,2022-07-12T07:36:48Z,1,Triple BAM!!! i completed this video. A lot to digest.,True
@simonedigregorio5352,2022-07-11T16:50:45Z,25,"I have studied boosting for regression at university, but we didn't get in depth with the method and its generalisation, we just looked at it superficially as a progressive combination of weak learners fitted on residuals, with the mean of the residuals as output of the leaves; your videos were enlightening and super interesting to watch! I was worried most videos online would not get technical enough, but your 4-part series about gradient boosting has proven exceptional! Thank you so much üëç",True
@nalidbass,2022-06-23T17:49:13Z,0,Hi Josh. What is the 'x' dependence implied by F_0(x)?. What exactly does 'x' represent?,True
@nalidbass,2022-06-22T23:11:41Z,0,How did you set the initial predicted probability to be 0.67?.,True
@gabrielpadilha8638,2022-06-20T11:46:37Z,1,Could you do the video about taylor polynomials?,True
@sushmitgoyal9760,2022-06-01T14:31:35Z,0,I really love your content Josh but the number of ads make it kinda hard to focus. Please take care of it.,True
@thoang188,2022-05-15T08:42:09Z,0,Can someone help to put a link here of the video on the explanation of Learning Rates by Stat Quest if there is one? I can't seem to find it. Thank you in advance.,True
@user-ps8sd8zb5g,2022-05-08T07:14:56Z,0,"I get gi = -(p-y) / (p (p-1)) if I derivative the loss function with respect to the p, but we get gi = y-p(residual) in 21:12, however, you answered me ""yes"" when I ask ""So can I say whatever we derivative the loss function with respect to the log odds or to the p, we can get the same result?"".  I'm sorry about asking you the same question again. It's might sound silly. Could you explain more about this? Thanks a lot.",True
@user-ps8sd8zb5g,2022-04-27T15:14:30Z,0,"Hello Josh. Thank you for the great series of Machine Learning videos. They help me a lot. After I watched this video, I still have a question about how the equation d/d F() (in 21:00) can plug the equation d/d log(odds) (in 10:09). Don't they cause any problem?",True
@elvinaghammadzada5382,2022-04-23T19:10:51Z,1,One of the greatest of all time statistics behind ML explanations. Much appreciated. U r next level üíØ,True
@buh357,2022-02-22T19:21:04Z,1,"Thank you, your video helped me a lot to learn machine learning. Mega Bammmmmmm!",True
@NFShammer,2022-02-07T11:34:33Z,0,Am I right that we have to use the softmax function as the loss function if we face a multiclass problem?,True
@hyejinlee913,2022-02-01T15:02:36Z,0,"I have one question. Is ""pseudo response"" different from ""pseudo residual"" ?  I mean some materials seem to use pseudo response instead of pseudo residual.  What is ""pseudo response"" ??",True
@joseanealmeida5781,2022-01-26T12:44:49Z,1,Awesomeeee üòÉüòÉüòÉ,True
@sandipansarkar9211,2022-01-17T17:46:19Z,1,finished watching,True
@rajeshnimma155,2022-01-17T06:59:09Z,1,MEGA BAM :) Very good explanation. Thank you,True
@marjaw6913,2021-11-20T17:22:24Z,0,"How could we adapt this scenario for multiclass classification problems? For example, how would the odds be defined? Right now it is number of class one divided by number of class zero, but this will not work for more than two classes.",True
@marjaw6913,2021-11-13T20:19:37Z,0,Awesome video! Just wondering which log is used here? Log to the 10th base? Sorry if this is should be obvious,True
@sipintarpatrick,2021-11-12T03:42:59Z,0,"how to making the decision tree, sir?",True
@75hilmar,2021-11-05T12:32:40Z,1,I am very much weirded out that a person of age 90 who loves popcorn and the color green loves Troll 2... the triple bam at the end completes the divergence of my reality...  Thank you for the math though.,True
@zachary692,2021-10-14T12:51:08Z,0,"The step B that generates a new tree suggests to fit a REGRESSION tree targetting residuals, even though this algorithm is for CLASSIFICATION. Does it mean that the loss function of step B is still aiming at minimizing MSE as it is in any other regular regression tree model (i.e. regular CART model)?",True
@zachary692,2021-10-13T15:56:43Z,0,Wonderful video! Just be wondering further: how would it be like when there are more than two categories (i.e. softmax instead of logistic)? Thanks!,True
@vidazamani102,2021-10-02T13:39:14Z,1,That was awesome! I'm really grateful üôè,True
@MadhurDevkota,2021-09-27T04:44:03Z,11,"These playlist are goldmine for a knowledge seeker and are in true sense equivalent to a University semester's course....but I have yet to find out Prof that could cogently explain it with such brilliance....truly awesome work. And after this WFH is over, I am proudly & joyfully gonna go to the office wearing StatQuest Double Bam hoodie! :)",True
@nenthop2628,2021-09-15T11:17:56Z,1,Love from India .Thanks a lot,True
@ElderScrolls7,2021-08-19T19:28:08Z,0,"Hi Josh, thank you very much for the wonderful lecture. In the Friedman book and in your equations the initial prediction and then the minimizations of the regression trees for the residuals are both done with respect to the same parameter (gamma). But in the first case gamma is the predicted log odds, whereas in the latter it is the outcome of the terminal node, analogous to the mean residual from the gbm for regression. But almost everywhere including your lecture people use the same parameter for both. Yet it is the predicted value in one case, and the overall residual in the other. Why not denote them differently? (them being the gamma from step 1 and the gamma from step 2-C) Why do they call them the same in both cases? Is it just convenience, and implicit that they represent different things? Thanks again!",True
@aubdurrobanik4036,2021-08-09T09:52:35Z,0,log(2) = 0.30 not 0.69 plz correction this sir,True
@GirishKumar-xs8on,2021-08-02T12:53:13Z,0,"At 18:02, the generic form is written as  -y_i*(F_m-1(x_i) + gamma)+ log( 1 + exp(F_m-1(x_i) + gamma)), I have a doubt in it there should be log in first expression also i.e. -y_i*log((F_m-1(x_i) + gamma))+ log( 1 + exp(F_m-1(x_i) + gamma))",True
@ebenezerscrounge8398,2021-08-01T18:13:33Z,0,Note: You can AVOID the PRODUCT RULE at 21:57 by making this simple adjustment. exp(log(odds))/[(1+exp(log(odds))] = 1 - [1+exp(log(odds))]^(-1) and then differentiate 1 - [1+exp(log(odds))]^(-1) with respect to log(odds). This is similar to differentiating 1 - [1+ e^(x)]^(-1) with respect to x.,True
@abir95571,2021-07-24T05:49:59Z,0,"8:07 Just a doubt , odds = p/1-p that means 1-p = 1/(1+odds) so why can't we write log(1-p) as  - log(1+odds) ?? I mean I get that e^log(odds) is nothing but ""odds"" but why bear the hassle ?",True
@0807tanguy,2021-07-22T21:01:06Z,1,"I am missing something. If I am correct, working with the binomial loss for classification ends up being super close to regression with the MSE. Indeed, the weak learners are fitted to the gradient of the binomial loss (the pseudo residuals). As it happens, the pseudo residuals are equal to (p_i‚àíy_i) which look just the like residuals of a regression! So predicting (p_i‚àíy_i) sounds like the final prediction to me because the more boosting steps we add, the more p_i will get closer to y_i! Besides, doesn‚Äôt predicting (p_i‚àíyi_) results in minimizing the binomial loss (in which case wouldn't the next step of the boosting algorithm - minimizing the loss function from the predicted pseudo residuals - be redundant)? Why are we not stopping the m-th boost here ?",True
@caglarsubasi2433,2021-07-20T15:26:51Z,0,"Do you have an intuitive explanation for ""why we reach the variance of bernouilli likelihood, by taking the second derivative of the negative log of the loss function with respect to logodds""? (at 24:28). In short, what is the intuitive relation between second derivatives and variances?",True
@capt7663,2021-06-25T00:55:44Z,0,"hi, I want to ask. when you made the regression tree, why you used >65.5? not <65.5?",True
@shikamaru7889,2021-06-24T14:13:08Z,0,"at 34:46 how you get the new gamma values?? can you write it down in details?? I tried to calculate like this, since the new predicted log(odds) is 1.89 then p=0.87  gamma1,2= 0.48/(0.87*(1-0.87))=4.2  or we calculate p= 1.89/1+1.89=0,65  gamma1,2=0.48/(0.65*(1-0.65))=2,1  I still couldn't get gamma1,2= 1.9  please help me, mr starmerüôèüèº",True
@capt7663,2021-06-23T13:11:23Z,0,"I'm sorry but can you explain more at 34:38 how you end up with ri,2= 0.13 ; 0.48 ; 0.52 ???",True
@aravindreddy3972,2021-06-16T11:00:22Z,0,"hey, why can't we just use probability(p) in derivations instead of log(odds)?",True
@leiwang7213,2021-06-12T04:12:22Z,0,"Thanks for your video.But in 19:03,isn't the Taylor's expansion a function of gamma?In other words,isn't d/d gamma and d^2/d gamma^2 in the Taylor's expansion? f(gamma)=f(0)+f'(0)gamma+(f''(0)/2)gamma^2",True
@annawilson3824,2021-06-11T01:32:53Z,0,30:16 equation for gamma,True
@arda8206,2021-06-06T15:51:03Z,0,Instead of dealing with log(odds) if we directly take derivative wrt to p we do not even need taylor approximation. Same formula can be computed super simply.,True
@thomaspellegrom9555,2021-05-17T12:51:56Z,0,At 17:30 why is it Rij and not Rjm?,True
@assafv1,2021-05-10T14:19:54Z,1,Great Video!. Is it True to say that since  this method uses the second derivative of the loss function than it uses some kind of newton's method for optimization? or is it still considered gradient descent? or is it neither?,True
@gutsa3389,2021-04-25T21:09:01Z,0,Great video on gradient boosting. What kind of tools do you use to make your slides so cool ?,True
@sjwang3892,2021-04-19T00:43:17Z,0,"I think there is a mistake on 7:03. Log(p) - log(1-p) is equal to log(p/(1-p)), but not log(p)/log(1-p). Is that right?",True
@DevilErnest,2021-04-18T10:18:26Z,2,I remember commenting that your video wasn‚Äôt diving deep enough into the maths.  You really did improved and you earned my sub today. Thank you for the videos.,True
@maxdinckelban3206,2021-04-16T08:41:47Z,0,"I've heard another version of statement that a normal GBDT didn't use the infomation of the 2nd order Taylor like XGBoost, which is different from this video.It confuse me a lot.I wonder which one is right?",True
@techgeekal,2021-04-15T08:29:09Z,1,BAMMMMMMM! i love your explanations!!! Thank you :D,True
@clairebi631,2021-03-24T10:13:26Z,1,"Thanks for this awesome video! Really appreciate your clear and simple explanation. One small question, why there's a possibility that one sample may end up in several leaves?  Thank you again!",True
@karannchew2534,2021-03-21T16:39:22Z,0,"Two questions please. 1) It's called Gradient Boost because it uses the gradient to adjust the amount of correction. But why is it called ""Boast""? Boast implies increase, additional, more. But the correction can be either add or minus, right?  2) It creates many trees (so does AdaBoost, Forest Tree). Each tree evaluate the parameters/variables, and combined to give a result. Is it possible to find out how much each parameter/variable affect the prediction overall? Or possible to rank the parameters by the amount of influence? What would be the logic/method to do it?",True
@karannchew2534,2021-03-21T09:53:27Z,1,"Notes for my future reference (detailed).   First, derive Loss Function L.   L is given by log(likelihood of the observed value vs the predicted outcome), ‚àë over all samples. Result of derivation:  L = -observed*ln(odd)+ln(1+e^ln(odd)) = Loss Function  Note: L can be in terms of ln(Predicted Prob) or in terms of ln(Odd). ..ln(p/(1-p)) = ln(odds) ..p = e^ln(odd)/(1+e^ln(odd))    Now, find the odd value that minimise L.   Solve dL/dln(odd)=0 to get the Odd (and hence Prob) that minimise L.      dL/dln(odd)=0 -> min L  F = ln(odd) value that minimise Loss = the Predicted ln(odd)  (which can be converted to Predicted Prob)    For each sample, get residuals based on the Predicted Prob. Residuals = r im i = sample index, m = tree index    Build decision tree using the residuals values.   For each leaf of the tree, get an Output Value OV aka 'boost'.  Steps: For all relevant samples (i.e. samples ended here) in the leaf, OV = Gamma that minimise Loss function, L (Observed values - previously Predicted Prob for the relevant samples + Gamma)‚àëover all relevant samples those ended in the leaf. OV is the incremental value of the new predicted ln(odd).  Solve d(Loss Function)/d (Gamma )=0 The resulting Gamma (and Prob) gives minimum Loss. The resulting Gamma = incremental ln(odd) that give the minimal Loss The resulting Gamma = OV of each leaf  Once repeated for each leaf, each leaf has a OV    Using the tree and OV from tree, predict the new prob for each sample. F = New Predicted ln(odd) = Old Predict ln(odd) + rate*‚àëOV of tree F.m = F.m-1 + v*‚àëtree  F is in terms of ln(odd), which can give the New Predicted Probabilities.    With the new predicted Prob , calculate the new residuals, build a new tree, get new OVs and get the newer predicted prob  F = New Predicted ln(odd) = Old Predict ln(odd) + rate*‚àëOV of tree1 + ...  rate*‚àëOV of treeM F.m = F.m-1 + v*‚àëtree1 + v*‚àëtree2 +... v*‚àëtreeM Repeat until no improvement (or max no. of tree reached).  Finally, several mini trees would have been created. Each created using residuals from the previous round. Each has leaves with Gamma value, which will be used to generate an incremental values for each data sample.    To predict (classify) a data, pass it through all the mini trees, add up all the values generated by each mini tree OV, lead to a new F.  Turn F, the Predicted ln(odd) to Predicted probability.  P = e^ln(odd)/(1+e^ln(odd))",True
@karannchew2534,2021-03-20T22:52:11Z,0,"Notes for my future reference (simplified overview)  Initiate tree root with the first Predict Probability (e.g. with the average prob).  Residual R = Observed Prob of samplle - Predicated Prob  Build new decision tree.   Root node contains the Residual R values of all samples.  Each new tree leave contains Residual R values (the decisions) of data samples that ended at the leaf.  For each leaf, transform the R values into Output Value, aka the boost value.   R value transformed = ‚àë R / ‚àë (P x (1 - P) ) = Output Value Each leaf will have an OV  For each data sample, calculate new predicted probability.  (But got to first get the log(odd) value) x = New log(odd) = log(old odd) + rate*Transformed Residual (aka Output Value aka Boost Value) in relevant tree leaf. New Predicted Prob = e^x/(1+e^x)  Newer Residual R = Actual Prob - New Predicted Prob  Build a new decision tree for the Newer Residual of each sample.  Get residual, output value, log odd, predicted probabilities and another set of newerer residual value.   Repeat. Until certain number of tree, or the residual can't contribute more.  To classify, pass the data through all the trees.",True
@tymothylim6550,2021-03-16T15:36:27Z,2,Thank you Josh for another exciting video! I had a few BAM moments seeing the algebraic and Taylor Series stuff! Really fun! I love popcorn too!,True
@beckswu7355,2021-03-12T19:23:20Z,0,"Hi StatQuest, I want to confirm one formula if I understand correctly. To F_1(x) , we need F_0(x) . Is F_0(0) = log(p/(1-p)) and p is the observed probability to be 1?",True
@manjushang,2021-02-19T17:51:00Z,1,Thanks a ton for all your videos. Simple and the best !,True
@vianadnanferman9752,2021-02-16T11:34:42Z,0,"THX, please, after residuals are founded  how to solve tree? as I understood from you solve like regression tree am I right?",True
@kswill4514,2021-02-06T14:24:12Z,0,"14:26 why multiply the derivative of loss function by -1, when calculating the pseudo residual? Thanks",True
@rolandheinze7182,2021-01-21T16:15:39Z,0,"Why do we even need to fit trees and use terminal regions? Could we not just assign each sample to its own individual terminal region and update the guess for that sample independently? Does using trees to group samples together into terminal regions help in some other way, perhaps time to convergence or algorithm stability?",True
@jaketh3d0g64,2021-01-18T13:13:37Z,0,Great video but can u explain to me why F0(x)  = log(2/1) = 0.69,True
@jakubp6301,2021-01-17T14:48:24Z,3,Really like this in-depth analysis. Reading original papers with all the different symbols could be quite intimidating but your video made it quite easy.,True
@zkhandwala,2021-01-17T04:13:01Z,0,"Great video, as always! I'd love to know if Josh, or anyone else here, might be able to provide some intuition around the formula at 30:35. I do understand how we arrived here via loss function minimization. I just feel there must be a way to see some intuition in the formula, but can't find it. (And while I await an answer, I'm off to watch the XGB vids...)",True
@KonstantinosKonstantinidis1993,2021-01-13T21:29:27Z,1,"The previous parts of this series on GB classification were very good! However, this video tries to delve further into the technical details. The errors made here (especially, with respect to log) carry over as mistakes to the rest of the analysis. I think you should fix them and re-upload the video.",True
@rolandheinze7182,2021-01-11T20:29:02Z,0,"I believe one of the most subtle things in this video that goes somewhat unexplained is the notation for the term ""F_m-1(x_i) + gamma"" in the ""L(y_i, F_m-1(x_i) + gamma)"". What does this expression actually capture? F_m-1(x_i) is the previous guess for the log(odds), but why do we ""add"" gamma? Is this just short-hand for ""an updated guess at the log(odds)""? The problem for me is that the ""+ gamma"" actually matters for the 2nd order Taylor approximation (essentially dx)... just trying to bridge the gap between notation and concept.",True
@zhenyuzhao1626,2020-12-30T05:55:19Z,0,"Is it necessary to always say ""x sub one"" rather than just ""x one""? The sheer number of times I'm hearing ""sub"" is really distracting me from the actual content of what you are trying to convey in a sentence.",True
@8952ification,2020-12-29T11:05:54Z,0,7:07 log(p) - log(1-p) is  not equal to log(p)/log(1-p) but equal to log(p/(1-p)),True
@bxp151,2020-12-25T00:44:49Z,0,"Thanks for all your amazing videos Josh.  I had a question about computing output values.  I tried playing around with different values of gamma to see how it affected the loss function for the first output value.  It seems like when there are two residuals in the leaf, everything works as intended (output value minimizes the loss function at -0.77).  For the single residual, I can keep increasing the value of gamma and the loss function has no minimum.  I made a google sheet to demonstrate:  https://docs.google.com/spreadsheets/d/1wTcaWBavHPqkVjR7t54RqxYVvhvF3pnwy00p472VmzY/edit?usp=sharing",True
@kswill4514,2020-12-14T12:59:41Z,0,Any idea why to convert loss function with regard to log(odds) instead of p?,True
@mateokee7524,2020-12-13T21:40:35Z,0,"Hi Josh, thanks for a great video. I'm a bit confused regarding log(likelihood) of data given predicted probability shown here. I always had the impression that log(likelihood) was regarding likelihood of parameter given data?",True
@boxiangwang,2020-12-08T16:40:03Z,1,Mega BAMMM!!,True
@gokulnath4509,2020-11-07T13:15:21Z,0,There is a mistake when we find gamma for the second leaf of the first tree insteam of y2 and x2 in the denominator we would have used  y3 and x3 two times,True
@shaunma2901,2020-10-28T02:35:02Z,0,The only thing I wanna know after watching this video is what the hell is my prof talking about,True
@farbodsafe,2020-10-23T23:59:48Z,0,"Quadruple BAM!! In 32:07 you mention ""Note: This summation is there just in case a single sample ends up in multiple leaves"". How can a sample end up in multiple leaves in a decision tree when at each node a sample is either passed to the left or right child? In other words for each tree doesn't each sample go through a unique decision path leading the sample to a single leaf?",True
@sherryqixuan,2020-10-20T07:58:13Z,2,"Very cool. I feel I still don't quite understand why we want to use log(odd) instead of the probability as the prediction, f(x)? Thanks a lot.",True
@soumyadev100,2020-09-28T16:34:38Z,1,The best video on GBM,True
@VishalChauhan-yy6ql,2020-09-26T12:15:34Z,1,"best video for gradient boosting,i watched all gradient boosting twice to understand math and logic,its really awesome",True
@nikunj2554,2020-09-23T15:52:33Z,0,"Hello Josh. Thank you for an amazing video about the math of GBM for Classification. I however, had one question. In the 2nd order Taylor approximation which you mentioned, shouldn't the subscript be i instead of 1 for the entire equation. Shouldn't it be L(yi, Fm-i(xi) + gamma) = L(yi, Fm-i(xi)) + d/dF()*(yi, Fm-i(xi))gamma + d^2/dF()^2*(yi, Fm-i(xi))gamma^2. ?? Why is it that the subscript is just one, is it because we are solving for only the first sample?",True
@guoshenli4193,2020-09-22T03:58:26Z,1,super great!!! Thank you for explaining!!!!!,True
@mohajeramir,2020-09-20T19:13:38Z,0,"I wish the video explained how the splits are done. Do we try all different possible splits and find the one that minimizes the loss function? Or, instead of minimizing loss function, we aim at maximizing purity or gain?",True
@yujiezhao9825,2020-09-15T16:27:05Z,0,"I think there is a typo in 19:26 , where the 4th line should be L(y_1, F_{m-1}(x_1)) instead of L(y_1, F_{m-i}(x_1)).",True
@phungtruong6698,2020-09-13T01:48:47Z,1,The chain Ruleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee :3 : 3 :3,True
@setyoajipriambodo6700,2020-09-04T07:25:20Z,1,"Hi Josh, why we should approximate the function using Taylor polynomial order 2, I can't wait to know it",True
@SagarKar10,2020-08-10T08:58:53Z,0,Can you please do a video on the basics of Lambdamart and LeTor,True
@TuNguyen-ox5lt,2020-08-02T15:02:16Z,1,extremely useful. you made boosting more understandable . thank you,True
@subterrain5293,2020-07-19T09:33:38Z,1,"Thanks for explaining this with a simple example. I'll have to go through it more than once to grasp it, but loved the way you have explained every step in the algo.",True
@meenashankar772,2020-07-18T20:53:53Z,0,How do we get new residuals when m=2? Towards end. Can you please tell the calculation alone to get new residuals,True
@nitinsiwach1989,2020-07-14T09:51:28Z,0,"When you have just one sample in a leaf, why is the output of the leaf computed via Taylor series approximation and then minimizing the loss? If the observed == 1 the gamma should be some very high number (why? this maximizes the log-likelihood) and if the observed ==0 the gamma should be some very small number (why? this maximizes the log-likelihood). using Taylor series approximation should be done only within radius of convergence. Also, using the derivative you have to check the boundaries as well",True
@Patrick881199,2020-07-13T13:31:22Z,0,"Hi, Josh, this is a giant and fantastic walk-through. I just wonder in order to become an applied data scientist, do I have to reach to such a high level of recognition, or part 3 of this series is enough? :)",True
@wuwanjun2489,2020-07-13T07:11:35Z,1,It is the best video I have ever seen in machine learning. Thank you sooooo much for explaining those tough material in such an easy and fascinating way!!! I enjoy it so much :),True
@adityak913,2020-07-07T18:39:05Z,0,Why do you want to multiply the derivative with -1 again?  because you have multiplied loglikelihood with -1,True
@henokmekonnen4241,2020-06-27T20:44:51Z,1,Thank you very much!This complicated stuff is made easy thanks to you.,True
@daviddang1044,2020-06-22T00:54:06Z,0,Isn't exp(log(odds)) = odds? Ah I see why you leave it that way. You're taking the derivative with respect to the log odds instead of odds.,True
@setyoajipriambodo6700,2020-06-07T14:41:36Z,0,Is gradient boost in this explanation same with gradient boosting machine?,True
@zkhandwala,2020-05-29T22:56:53Z,6,"This was super rewarding... Following my comment on video #2, at some point I began wondering why we weren't using the same criteria (i.e., same loss function) to create splits as we were to create the gammas, but subsequently realized a fascinating thing going on here: while the ensemble is modeling a categorical variable, each tree is actually modeling a continuous one. This came as an epiphany, and in my mind this explains the two loss functions, though I suspect the real reason is more nuanced...  Anyway, I've learned a *ton*. Thank you!",True
@jiwoosong6055,2020-05-16T21:53:25Z,2,This video needs more views. Thank you so much for explaining this so clearly!!,True
@lihaolihao1059,2020-05-14T14:19:20Z,1,"I like your videos and the songs, especially the song ""Last Night"". You are creating good videos and good songs!",True
@andrewhuang7913,2020-05-09T10:17:03Z,0,"There is one error in 28:28, the gamma, the left part should be (y2, F(y2)) not (y3, F(y3))",True
@kumarsuraj2872,2020-05-05T18:26:41Z,0,"Hi Josh, this might sound silly, but why do we use log(odds) as F(x) for classification? Could it be any other function of x (x = predicted probability), e.g. mean of predicted probabilities, like we use F(x) = mean(x) in regression trees?",True
@muzamilshah8028,2020-04-25T18:20:14Z,0,is there any other possible loss function we can use for same problem?,True
@feilu6978,2020-04-23T10:24:03Z,0,"Hi Josh, I'm your fans. I'm not sure Is your purpose of converting the p function into the odds function to make the derivation easier?",True
@rajonkar1515,2020-04-16T19:26:58Z,0,.i was trying to avoid using the taylor series....I dont get the reason as to why the derivative of the loss function gets messy..not sure if i am doing it wrong.,True
@shuyiwang3421,2020-04-10T19:45:49Z,0,"So to summarize, we are maximizing log(likelihood) tree by tree. 1. Final predictions is a summation of weighted output of all the trees.  2. Output of each tree, is the ""additional"" log(odds) that maximize the log(likelihood) of all the observations on the leaf, considering existing predictions of all the previous trees.  3. Trees are built on residuals ( predicted probability - observed). 4. Maximizing log(likelihood) is equal to minimizing loss function defined here.",True
@sushilchauhan2586,2020-03-31T11:00:58Z,2,"def   r u going to cover ""deep-learning"": If yes:     return infinite bam else:     return bam with chainnnn reactionnnn   print(""thankyou-Josh"")'",True
@kshitijpemmaraju4177,2020-03-29T14:11:39Z,0,"Time stamp 2.54: log(2/1) = 0.3010 ,as per predictability of log its log(no. of yes/number of nos) here we have 2 yes 1 no  so how come log predictability 0.67 coming?",True
@radocisar3420,2020-03-24T16:55:36Z,1,Brilliant walk through. Thank you.,True
@rajarajeshwaripremkumar3078,2020-03-21T00:26:14Z,0,Is the video on Taylor Polynomial available?,True
@nitanurmala5300,2020-03-15T16:41:29Z,1,thank you very much !! could u do a video explaining gradient boosting survival tree please :) ?,True
@statquest,2020-03-09T01:08:28Z,37,"NOTE: Gradient Boost uses Regression Trees. I explain them in this StatQuest: https://youtu.be/g9c66TUylZ4   Corrections: 6:58 log(p) - log(1-p) is not equal to log(p)/log(1-p) but equal to log(p/(1-p)). In other words, the result at 7:07,  log(p) - log(1-p) = log(odds), is correct, and thus, the error does not propagate beyond it's short, but embarrassing moment.  26:53, my indexing of the variables gets off. This is unfortunate, but you should still be able to follow the concepts.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@shengchaohou4901,2020-03-06T23:52:11Z,26,i think i cried a little when the second derivative of the loss function became p*(1-p).. amazing,True
@supermonkey965,2020-02-18T09:38:28Z,0,"At 32:11, it is said that one sample can belong to multiple leaves in the decision tree. How is this possible ? Is there probabilistic decision trees that don't work with binary decisions but probabilistic decision ? This is a new concept for me.",True
@pratikdeoolwadikar5124,2020-02-15T15:12:49Z,1,"holy freaking smokes, Thanks a lot !! , now I can code it from scratch",True
@mohitbansal9073,2020-01-27T18:57:29Z,0,"Hello Josh, Can we use gradient for multi-label classification?",True
@VVV-wx3ui,2020-01-18T13:55:43Z,1,"Holy smoke Gradient Descent/Boost series has very lengthy video duration. It may take years to memorize it as I am new to this level of Maths . So, i am making notes with the high level points to cover the flow, since all the formulas and calculations are taken care by the respective Libraries (viz., Python, R ). I love the way you simplify lengthy formulas to easily understandable equations, like second Derivative of the Loss Function (@24mins Product Rule + Chain Rule) to simple two terms and then replacing second derivative to find Gamma. Kudos to you Josh.    Respects and Admiration for your plain simple explanations of complex concepts.  You are a Guru Josh Ji /\. Triple BAM.",True
@jianbingding4087,2019-12-21T13:44:13Z,1,"how do i get the ppt ,it's perfectÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ",True
@mrsamvs,2019-11-29T14:40:22Z,0,"Hi You are super awesome, I need a video on XgBoost, but I cant see the same over here.",True
@mamk687,2019-11-26T15:40:50Z,0,"This is awesome!! Thank you so much!  This is the most amazing and impressive video I have seen before. I have just a question about the paper written by Friedman (2001)...he explains that we need to find the best gradient descent step-size œÅt...  you just simply used gamma, so what can I understand this difference? Thanks in advance!",True
@intoeleven,2019-11-14T22:15:39Z,1,"FYI, 8:04 is referring to https://youtu.be/BfKanl1aSG0?t=279",True
@mayankjauhari878,2019-11-02T20:03:38Z,0,at 7:18 log a- log b is not equal to log a/ log b but equal to log (a/b),True
@hellochii1675,2019-10-27T00:45:08Z,3,"Lol, I'm 20, my favorite color is pink and 50% like Popcorn. But I don't love Troll2~~",True
@justalge8129,2019-10-23T12:45:32Z,2,"awesome video! like all other.. you are great, thank you!!!!",True
@giuseppefasanella5446,2019-09-15T11:07:28Z,1,Thanks for this simple and complete explanation!,True
@benjaminassel8151,2019-08-19T21:25:10Z,31,"Thanks for the nice video. A few comments:  - Around 7:00 there is a small mistake in the manipulation of logs.  log(p) - log(1-p) = log(p/1-p) is right, but it is not equal to  log(p)/log(1-p).   - From around 19:10, in the Taylor expansions, I find a little strange that the derivatives of the loss function are not written with L. Maybe just a notation I am not familiar with. - The gammas that are computed are not so small in this example, so it is unclear why using only the first terms in the Taylor expansions is a good thing to do.",True
@yingzhan2589,2019-08-04T14:22:47Z,2,this is awesome!,True
@okioking2,2019-07-30T01:06:59Z,34,"The most difficult thing is to explain complex things into something easy, and yes you did it Mr. BAM! thank you very much, i'm your fan now!   ps: could you explain XGB in the next video, please? thank you before",True
@rodriguechidiac8648,2019-07-25T16:12:26Z,0,"I would like to thank you for the efforts in creating the series. You made the GBM concept crystal clear immediately, esp. that a lot of literature can be found yet they can rarely express the concept as clear as in here. Furthermore, i would like to ask about the CART base learner while solving for the additive model: In one of the articles i read, and in case of the GBM for regression, the CART learner is solved at each iteration by minimizing either the MAE or MSE functions (this was done using stump trees). In case of binary classification as in case of your videos (3 and 4), what is the loss function? i.e. how the tree is solved for? is it through using minimizing gini? I am asking because i am a bit confused about that, especially if there is a link between the overall Loss function that the gradient boosting is trying to minimize, and the loss function within the base learer (i.e. CART in our case). Thank you",True
@martindepellegrini4228,2019-07-13T14:12:07Z,0,"I didn't why you are using the log(odd), is it just for simplicity?",True
@daemon3322,2019-07-02T03:16:48Z,2,"Thank you, Mr. BAM! The most impressive video I have watched.",True
@telmo.rodrigues,2019-06-30T23:30:44Z,0,"Your videos are awesome! I can't stop watching them! On 33:11, 33:26 and 33:43 it is said that the result is respectively ""better"", ""worse"" and ""better"" based, I think, on more/less positive values of log(odds). Shouldn't it be converted to probabilities (using exp(.)/(1+exp(.)) to take conclusions (far/closer to 0.5)? Since evaluating a log function is not so automatic or logic...",True
@joery8290,2019-06-23T13:50:12Z,1,3blue1brown also has an excellent video on taylor polynomials,True
@13statistician13,2019-06-12T20:48:48Z,0,"From a pedagogical perspective, it was probably a poor idea to use ""Troll 2"" as the example.  Having the word ""two"" thrown around when talking about other numbers like one, two, etc. can be quite confusing for new students.  In the future, I'd recommend using examples without numbers in the name/title.  I also find your speech too slow.  Other than that, I think you've done a nice job here.",True
@thedeliverguy879,2019-06-09T01:42:21Z,1,Love ur vids. Best in the world. Thank you for the awesome videosÔºÅ,True
@deepakkumarsisodia7092,2019-06-03T10:11:23Z,3,"This is awesome!! Thanks a ton Josh. No other video or tutorial explains it better than yours. From quite some time I was looking for a detailed explanation of gradient boosting algo for classification but found none which made it so clear. Some minor typos/errors : 1. In Step 2 (C) at 17:08  Instead of summation over xi in Rij, it should be summation over xi in Rjm i.e. all data points present in the jth leaf of the mth tree. 2. In step 2 (D) at 32:09 A single sample can NOT end up in multiple leaves, it can only go to any one of the leaves. Indicator variable value is zero for all regions except the one where the sample ends up. Thus, summation used with indicator variable is a nice way to mathematically represent the single region where sample ends up.",True
@erv993,2019-06-01T18:34:06Z,1,That's was awesome! Thanks a lot!,True
@deepm5047,2019-05-31T12:17:06Z,0,14:29 why do we multiply with -1?,True
@jack-cb2je,2019-05-29T12:04:39Z,1,"BAM!!! I have to say, this is one of the best XGBOOST video on the youtube. GOOD JOB! Thankyou!",True
@husthu5667,2019-05-12T07:08:08Z,0,xgboost please,True
@sandralydiametsaha9261,2019-05-11T13:53:10Z,1,thank you very much !! can u do a video explaining the XGBoost please :) ?,True
@srudeeppa9650,2019-05-09T06:47:24Z,0,Can you do a video on Naive Bayes?,True
@PBrrtrn,2019-05-03T06:07:59Z,2,"Thank you SO MUCH for your videos. You explain topics that are fairly advanced in a very easy to understand manner that doesn't leave any information out and that's very hard to find here on YouTube. Bam, gratitude!",True
@rrrprogram8667,2019-04-30T05:38:19Z,1,Just a matter of time... This channel will have MEGA (million) subs... MEGA BAMMM,True
@rrrprogram8667,2019-04-30T05:37:49Z,1,What the next videos that are in pipeline???... Before u release it.. I would like to do little research... That will help to prepare in advance....,True
@rrrprogram8667,2019-04-29T05:26:36Z,1,Back again for a revision,True
@jotablanco,2019-04-28T16:02:06Z,0,"Thank you Josh, really appreaciate your work!  As other comments, I'd love if your could make a video on XGB. After understanding Gradient Boost so well with your videos I'm now intrigued of what makes XGB special.",True
@ccuuttww,2019-04-27T08:58:40Z,0,well part 4 needs sometimes to learn since this part is very complicated u should organize it better  for example if u wan to show y as predicted value keep using y for the formula and add a small bracket(predicted value),True
@ninguhosaptot,2019-04-24T00:18:53Z,0,"Please. be ""m"" a ""j"" and ""M"" a ""m"", common sense notation.",True
@MrPranaysawant,2019-04-23T12:15:27Z,0,"Great Work.....Josh loved your content ... I am waiting for SVM's Support Vector Machine Video. When you are doing svm. Please forward me link, if you hv already created one...",True
@yulinliu850,2019-04-23T10:57:49Z,1,"Thank you so much, Josh!",True
@danielenrique7184,2019-04-23T09:32:05Z,3,"Hooray!, thank you and keep up with the amazing work!",True
@GauravSharma-ui4yd,2019-04-23T05:00:25Z,0,Please make videos on time series.,True
@victorwang5953,2019-04-22T22:25:33Z,3,Josh your exaplantion is god damn clear. Far better than all the books and classes I've done about machine learning. Hope you will continue making more and more videos. I'd like to see some XGB explanation and deep learning sections in the future.,True
@321MrMateus,2019-04-22T18:18:17Z,59,Hooray! Could you do a video explaining the XGB and LightBoosting? Anyways thank you for the series!!,True
@smdave97,2019-04-22T17:00:31Z,2,You are like BILL NYE for data science so simple yet effective,True
@rrrprogram8667,2019-04-22T16:46:20Z,1,Actually... This is the way i really love to learn and listen.... But very rarely you find this type of teaching in books or videos... Josh... Can u please tell us... How did u learn the way u learn.... I dont find this type of teaching in any machine learning books,True
@rrrprogram8667,2019-04-22T16:43:50Z,1,The is by far the most awaited video for me.... Josh you are the BEST at what u do...,True
@CC-um5mh,2019-04-22T15:38:37Z,2,Hooray! Part 4 is here!,True
@GauravSharma-ui4yd,2019-04-22T15:35:11Z,8,"Waiting for this video deadly. Please make videos on quantum clustering, guassian mixture models, restricted Boltzmann machine and deep belief network  Sir please concern my request.",True
