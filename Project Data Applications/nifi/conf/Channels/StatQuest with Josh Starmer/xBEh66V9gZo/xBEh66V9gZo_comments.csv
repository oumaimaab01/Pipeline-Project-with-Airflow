author,updated_at,like_count,text,public
@statquest,2021-09-11T13:55:03Z,9,"The full Neural Networks playlist, from the basics to deep learning, is here: https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@legendary_gameron6760,2024-05-28T05:10:48Z,0,"Can you or any one help me, how can I use this method to adjust all perameters simontanously of a 2 hidden layer containing  nural network  and also do I need to calculate manually and then plug in the values in my network. You may need a video to explainüòÖüòÖ......",True
@RahulVerma-Jordan,2024-05-28T02:19:18Z,0,This should also reach to the AI/ML scientists behind these algorithms.,True
@johannesweber9410,2024-05-22T13:00:59Z,1,"Nice Video! First I was a little confused (like always) but than I pluged your values and the exact structure of your Neural Network into my own small framework and compared the results. After I did this, i followed your instructions and implemented the backpropagation step-by-step. Thanks for the nice video!",True
@irisfreesiri,2024-04-20T23:12:34Z,0,"In my case, my MLP didn't use the bias nodes, but I need to update weights in backpropagation process. So I wonder why this video didn't introduce that :(",True
@farrukhzamir,2024-04-17T13:00:10Z,2,Brilliantly explained. You explain the concept in such a manner that it becomes very easy to understand. God bless you. I don't know how to thank you really. Nobody explains like you.‚ù§,True
@r0cketRacoon,2024-03-15T04:16:06Z,0,Thank you very much for the video Backpropagation with multiple outputs to me is not that hard but it's really a mess when do the computations,True
@dianaayt,2024-03-08T13:44:59Z,0,20:14 if we have a lot more training data we would just add all the training data we have in this to make the backpropagation?,True
@user-bk4gi4ge9v,2024-03-06T09:45:45Z,0,please create one such Series for single layer Perceptron as well  and show the derivative as well,True
@noualhaguetful,2024-02-27T18:14:37Z,0,No demonstrations of derivative in part 5 !!!,True
@hangchen,2024-02-07T01:37:13Z,0,Awesome explanation! Now I understand neural networks in more depth! Just one question - shouldn't the output of the softmax values sum to 1? @18:57,True
@YLprime,2024-02-04T20:50:06Z,3,"This channel is awesome, my deep learning knowledge is sky rocketing everyday.",True
@pranavreddy9218,2024-01-28T17:26:03Z,2,"why are you fixing weights and just playing with b3, complete ANN you never talked about weights even in back propagation.",True
@GLORYWAVE.,2024-01-23T22:03:12Z,0,"Thanks Josh for an incredibly well put together video.  I have two quick questions:   1) When you initially get that new b3 value of -1.23, and then say to repeat the process, I am assuming the process is repeated with a new 'batch' of 3 training samples, correct? i.e. you wouldn't use the same 3 that were just used?  2) Are these multi-classification models always structured in such a way that each 'batch' or 'iteration' includes 1 actual observed sample from each class like in this example? It appears that the Total Cross Entropy calculation and derivatives would not make sense otherwise.   Thanks again!",True
@madankhatri7727,2024-01-20T13:08:24Z,0,Your explaination of hard concepts are pretty amazing. I have been stuck in a very difficult concept called adam optimizer. Please explain it. You are my last hope.,True
@sergeyryabov2200,2024-01-19T02:03:54Z,1,Thanks!,True
@ecotrix132,2024-01-18T06:22:40Z,0,Thanks so much for posting these videos! I am curious about this :  while using gradient descent for SSR one could get stuck at local minimum. One shouldnt face this problem with cross entropy right?,True
@harkatiyoussef9994,2024-01-08T11:23:40Z,1,"what's the difference between Softplus and Softmax ? Is it only about the softness of the toilet paper ? ü§£ü§£ü§£ just kidding, you do an awesome job, your videos are way above everybody else in ML / DL",True
@donfeto7636,2023-12-02T20:24:04Z,1,You are a national treasure BAAAM. Keep doing  those video they are great.,True
@jaheimwoo866,2023-11-27T13:50:38Z,1,Save my university life!,True
@user-fe8pl5fu4w,2023-11-21T10:17:06Z,0,I didn't get that why did you take the derivative of virginica wrt raw setosa,True
@ngocanle3037,2023-11-09T08:40:06Z,0,what about another weight/bias ex: b1 w1 ??,True
@minerodo,2023-11-02T05:44:10Z,0,"Thank you!! I understood everything but just a question: here you explain how to modify a single bias, and know I understand how to do it for each one of the biases. My question is how do you back propagate to the biases that are in the hidden layer ? In what moment ? After yo finish with b3, b4 and b5? Thanks!!",True
@Lucas-Camargos,2023-09-18T19:04:03Z,1,This is the best Neural Networks example video I've ever seen.,True
@user-fi2vi9lo2c,2023-09-12T00:23:52Z,0,"Dear Josh, I adore your lessons! They make everything so clear! I have a small question regarding this video. Why do you say that the predicted species is setosa when the predicted probability for setosa is only 0.15 (17:13 - 17:20)? There is larger value (0.46) for virginica in this case (17:14). Why don't we say it's virginica?",True
@simhasankar3311,2023-09-05T04:59:47Z,2,Imagine the leaps and bounds we could achieve in global education if this teaching method was implemented universally. We would have a plethora of students equipped with the analytical skills to tackle complex issues. Your contributions are invaluable. Thank you!,True
@samore11,2023-09-02T05:45:05Z,1,"These videos are so good - the explanations and quality of production are elite. My only nitpick was it is hard for me to see ""x"" and not think the letter ""x"" as opposed to a multiplication sign - but that's a small nitpick.",True
@AbdulWahab-mp4vn,2023-08-28T13:22:26Z,2,WOW ! I have never seen anyone explaining topics in such minute level detail. You are an angel to us Data Science Students ! Love from Pakistan,True
@user-tc5lz1ju9i,2023-08-22T22:42:11Z,0,"thank you very much, video is awesome  i have a question mark in one point,   when you take derivative of ce(setosta, virginica, verticolor) to b3, you used Raw for setosta for all of them (for setosta, virginica, verticolor),   * b3 is for Row setosta   * b4 is for Row virtigina  * b5 is for Row verticolor            but what happens for w1 or w5,   [should i sum all of them and apply to chain rule? (i guess its not the true way)] because  * b3 is for Row virtigina or verticolor is equal to 0  only Row setosta  is equal to 1 but * w1 is for Row virtigina or verticolor or setosta is equal to =  1 if (pedal width) > 0 else 0 (from derivative of relu [1 if x>0 else 0]) thanks for reading my very long question, BAM :)",True
@saurabhdeshmane8714,2023-08-14T13:25:21Z,1,Incredibly done....it doesn't even feel like we are learning such complex topics...keeps me engaged for going via entire playlist..thank you for such content!!,True
@rajpulapakura001,2023-08-04T07:53:49Z,2,"Clearly and concisely explained! Thanks Josh! P.S. If you know your calculus, I would highly recommend trying to compute the derivatives yourself before seeing the solution - it helps a lot!",True
@KayYesYouTuber,2023-08-02T00:24:17Z,1,So beautiful.  Never seen anything like this!!!,True
@ligezhang4735,2023-07-23T02:33:15Z,1,This is so impressive! Especially for the visualization of the whole process. It really makes things very easy and clear!,True
@anisrabahbekhoukhe3652,2023-07-05T21:40:48Z,2,"i literally cant stop from watching those vids, help me",True
@wuzecorporation6441,2023-06-17T09:03:13Z,0,18:04 Why are we taking sum of gradient of cross entropy across different data points? Won't it be better if we take gradient for one data point and do back propagation and then take gradient of another data point to do backpropagation?,True
@yourfutureself4327,2023-06-02T22:37:39Z,1,üíô,True
@epiccabbage6530,2023-05-28T16:02:59Z,1,"This has been extremely helpful, this series is great. I am a little confused though as too why we repeat the calculations for p.setosa, i.e. why we cant simply run through the calculations once, and use the same p.setosa value 3 times (So like, x-1 + x + x) and use that for the bias recalculation. But either way this has cleared up a lot for me",True
@arielcohen2280,2023-05-25T14:13:49Z,0,"hate all the songs and the meaningless sound affects, but damn I have been trying to understand this concept for hell long of a time and you made it clear",True
@giacomorotta6356,2023-05-10T08:59:48Z,0,"great video, but still I cannot understand why in this cross-entropy function you are considering only the true output class as the only variable in the function and not also the other class as variables(that are multiplied by 0 in the cross-entropy since they are not the true class but are still variables in the function), is it because the other class derivatives(the one that you did not consider) are going to be zero and so they are not relevant for backpropagation?",True
@Xayuap,2023-04-23T00:01:25Z,0,"hi, serious question, ¬øcan I do the same with the final w weights? something is not converging in the tests.",True
@Xayuap,2023-04-21T22:06:38Z,0,"yo, Josh,  in my example, with two output if I adjust repeatedly one b, then the other b doesn't need almost any adjust.  ¬øshould I adjust both in paralell?",True
@Xayuap,2023-04-18T05:19:31Z,0,"Double Bam,  ¬øcan we use 2 instead of e for base? I mean it would be more arquitecture wise.",True
@yourfavouritebubbletea5683,2023-03-23T03:39:58Z,3,Incredibly well done. I'm astonished and thank you for letting me not have a traumatic start with ML,True
@naf7540,2023-01-22T10:54:33Z,15,"Dear Josh, how is it at all possible to deconstruct so clearly all these concepts, just incredible, thank you very much, your videos are addictive!!",True
@sonoVR,2023-01-13T11:46:44Z,0,"This is really helpful!  So am I right to assume that in the end, when using one hot encoding we can simplify it to d/dBn = Pn - Tn and d/dWni = (Pn - Tn)Xi ?  Given n is the number of outputs, P is the prediction, T is the one hot encoded target, i is the number of inputs, Wni is the weight associated from that input to the respective output and X is the input.  Then when backpropagating, we can transpose the weights, multiply the weights by the respective error of Pn -Tn in the output layer and sum them to get an error for each hidden node if I'm correct",True
@GamTinjintJiang,2022-11-12T09:33:44Z,1,wow~ your videos are so intuitive to me. What a precious deposits!,True
@borishjha5700,2022-10-31T16:26:23Z,0,"The word ""probability"" is spelled wrong at the timestamp 18:55",True
@rahulkumarjha2404,2022-10-08T08:34:22Z,2,"Thank you for such an awesome video!!! I just have one doubt. At 18:12 of the video. The summation has 3 values because there are 3 items in the dataset.  Let's say if we have 4 items in the dataset i.e 2 items of setosa, 1 for virginica and 1 for versicolor. So our summation will look like {(psetosa - 1) + (psetosa - 1) + psetosa + psetosa}  i.e the summation is for the data setosadata_row1 , setosadata_row2, versicolordata_row3, virginicadata_row4  Am I right?",True
@stan-15,2022-10-02T03:53:16Z,1,"since you used 3 sample data to get the value of the three cross-entropy derivitives, does this mean we must use multiple inputs for one gradient descent step when using cross-entropy? (more precisely, does this mean we have to use n input samples, that each light up all n features of the outputs, in order to be able to compute the appropriate derivative of the bias, and thus in order to perform one single gradient descent step?)",True
@salahaldeen1751,2022-09-25T01:23:27Z,1,"I don't know where else I could understand that like this. Thanks, you're talented!!!",True
@RC4boumboum,2022-09-24T09:03:43Z,2,Your courses ara so good! Thanks a lot for your time :),True
@Waffano,2022-09-23T15:12:16Z,0,"Thanks for all these great videos Josh. They are a great resource for my thesis writing!  I have a question about the intuition behind all this:  Intutively it really doesnt make sense to me, why we need to include the error of the virginica and versicolor, when we are trying to optimize a value that only affects setosa? Would a correct intuition be: It is because they ""indirectly"" indicate how well the Setosa predictions are? In other words, because of Soft Plus, we will always get a probability of Setosa no matter what input we use? And then we might aswell use all the data, since more data = better models?Hope I didnt miss anything in the video that explains this!",True
@Waffano,2022-09-23T14:51:19Z,0,"Watching these videos makes me wonder how in the world someone came up with this in the first place. I guess it slowly evolved from something more simple, but still, would be cool to learn more about the history of neural networks :O If anyone knows of any documentaries or books please do share ;)",True
@user-qe6dn1rg2v,2022-09-16T07:14:48Z,0,Awesome video. I really appreciate how you explain all these concepts in a fun way. I have a question in the previous video for softmax you said the value for predicted probabilities for classes is not reliable even though they correctly classify input data because of our random initial value for weights and biases. now by using cross entropy we basically multiply observed probability in the data set by log p and then optimize it. so  Is the value of predicted probabilities for different classes of an input reliable. ?,True
@zahari_s_stoyanov,2022-08-26T12:58:57Z,0,I wonder what would be dCE for the corresponding weights though,True
@beshosamir8978,2022-08-15T18:08:54Z,0,"Hi Josh,  I have a quick question ,i saw a video on YouTube the man who was explained the concept said they use segmoid function in output layer for a binary classification and RelU for hiddens layers , So,i think we fall in the same problem here which is the gradient of the Segmoid Function is too small which is make us ends with take a small step , so i thought about it which we can use Croos entropy also in this situation Right ?",True
@yjchoi1561,2022-07-25T03:26:04Z,11,"Your videos are truly astounding. I've gone through so many youtube playlists looking to understand Neural Networks, and none of them can come close to yours in terms of simplicity & content! Please keep up this amazing work for beginners like me :)",True
@muntedme203,2022-07-11T19:10:18Z,1,Awesome vid.,True
@evilone1351,2022-06-27T20:03:30Z,0,"Excellent series! Enjoyed every one of them so far, but that's the one where I lost it :) Too many subscripts and quotes in formulas.. Math has been abstracted too much here I guess, sometimes just a formula makes it easier to comprehend :D",True
@TheTehnigga,2022-06-14T22:41:07Z,0,Is cross entropy backpropagation done on a test set?,True
@pedrojulianmirsky1153,2022-05-02T23:48:13Z,1,"Thank you for all your videos, you are the best! I have one question though. Lets suppose you have the worst possible fit for your model, where it predicts pSetosa = 0 for instances labeled Setosa, and pSetosa = 1 for those labeled either Virginica or Versicolor.  Then, for each Setosa labeled instance,  you would get dCESetosa/db3 = pSetosa - 1 = -1, and for each nonSetosa labeled instance dCEVersiOrVirg/db3 = pSetosa = +1. In summary, the total dCE/db3 would be accumulating either +1 for each Setosa instance and -1 for each non Setosa. So, if you have for example a dataset with 5 Setosa, 2 Versicolor and 3 Virginca:  dCE(total)/db3  = (1+1+1+1+1) + (-1 -1) +(-1 -1 -1) = 5-2-3 = 0. The total dCE/db3 would be 0, as if the model had the best fit for b3. Because of this compensation between the opposite signs (+) and (-), the weight (b3) wouldn¬¥t be adjusted by gradient descent, even though the model classifies badly. Or maybe I missunderstood something haha. Anyways, I got into ML and DL mainly because of your videos, can't thank you enough!!!!!!!",True
@aritahalder9397,2022-04-24T18:05:14Z,0,"hi, do we have to consider the inputs as batches of setosa,versicolor and verginica?? what if while calculating the derivative of total CE we had 1st row setosa as well as the 2nd row setosa?? what will be the value for dCE(pred2)/db3??",True
@saibalaji99,2022-04-16T11:09:05Z,0,Do we use the same training data until all the biases are optimised?,True
@vishnukumar4531,2022-04-11T19:08:51Z,1,"0 comments left unreplied! Josh, you are truly one of a kind! ‚ù£‚ù£‚ù£",True
@susmitvengurlekar,2022-04-03T15:57:37Z,2,"There is nothing wrong in self promotion and frankly, you don't need promotion. Anyone who watches any one video of yours, will prefer your videos over any other videos henceforth.",True
@susmitvengurlekar,2022-04-03T15:47:22Z,2,"""I want to remind you"" helped me understand why in the world is P(setosa) involved in output of versicolor and virginica.  Great explanation!",True
@abhishekjadia1703,2022-03-14T13:35:16Z,1,"Incredible !! ...You are not teaching,  You are revealing !!",True
@ferdinandwehle2165,2022-03-11T10:02:47Z,0,"Hello Josh, your videos inspired me so much that I am trying to replicate the classification of the iris dataset. For my understanding, are the following statements true:  1) The weights between the blue/orange nodes and the three categorization outputs are calculated in the same fashion as the biases (B3, B4, B5) in the video, as there is only one chain rule ‚Äúpath‚Äù.  2) For weights and biases before the nodes there are multiple chain rule differentiation ‚Äúpaths‚Äù to the output: e.g. W1 can be linked to the output Setosa via the blue node, but could also be linked to the output Versicolour via the orange node; the path is irrelevant as long as the correct derivatives are used (especially concerning the SoftMax function). 3) Hence, this chain rule path is correct given a Setosa input: dCEsetosa/dW1 = (dCEsetosa/d‚ÄùPsetosa‚Äù) x (d‚ÄùPsetosa‚Äù/dRAWsetosa) x (dRAWsetosa/dY1) x (dY1/dX1) x (dX1/dW1) Thank you very much for your assistance and the more than helpful video.  Ferdinand",True
@Freethinker33,2022-02-19T05:07:44Z,17,"Right now I am reading the ML book ""An Introduction to Statistical Learning"" by James, Witten, Hastie and Tibshirani.   Many a times, I stuck at the mathematical details and could not comprehend and stopped reading.  Although I love that book a lot but felt frustrated.  But now I use your videos and read the book side by side and now everything start making sense in the book.  You are such a great story teller.  They way you explains in the video with examples, it seems like I am listening to a story ""There was a king ...""  It is so soothing and complex topics become easy.  I feel you are my friend and teacher in my ML journey who understand my pain, and explains me the hard things with ease.  BTW, I have done Master in Data Science from Northwestern University and got good ML foundation from that course.  But I can tell you now I feel complete after going through most of your videos.  Mr. Starmer,  we are lucky to have you as such a great teacher and mentor.  You are gifted to teach people.  I will pledge to support your channel from my heart. Thank you.",True
@harshchoudhary2817,2022-02-13T10:15:50Z,0,What I see from here is that the gradient descent optimizes on the basis of total cross entropy and tries to minimize it Suppose for some data the actual o/p is setosa but the neural net predicts versicolor with a very high probability say close to 1 so the loss would still be minimized and the gradient desent won't optimize it. So we will get a wrong o/p with very high probability. Is it so or am i missing something here?,True
@andredahlinger6943,2022-01-30T22:07:24Z,1,"Hey Josh, awesome videos <3 What I don't get here is why we optimize for the predicted ""probabilities"". My understanding of the predicted probabilities was that they are mainly a transformation of the raw output values because the predicted probabilities are more handy to interpret. But if it's ""just"" a transformation of the raw output, why would we optimize it's ""prediction"" error (1-predicted ""probability"") after we applied this principle already in the back propagation process to get the raw values, where we also optimized the ""prediction"" error (i.e. the SSR)? This seems like optimizing after we already optimized. Is this necessary, cause we have to adjust the weights and biases to the new output ""scale""?",True
@tejaspatil3978,2022-01-23T12:41:02Z,1,your way of learniing is on next level. thanks for having us this best sessions..,True
@ariq_baze4725,2022-01-22T09:43:59Z,1,"Thank you, you are the best",True
@saraaltamirano,2022-01-12T22:23:26Z,1,,True
@grankoczsk,2021-12-23T00:34:24Z,1,Thank you so much,True
@hisyamzayd,2021-10-22T14:26:52Z,0,"Thank you so much Mr. Josh,  I wish I had this back time when I first learn neural networks.  Let me ask question.. so the Cross Entropy must use batch processing a.k.a. multiple row/data for each training? Thank you",True
@nabeelhasan6593,2021-10-16T06:19:48Z,1,At last I am really thankful for all your hard effort you put in these videos immensely helped me in making a strong foundation in deeplearning,True
@Tapsthequant,2021-10-11T11:18:50Z,0,"So much gold in this one video, how did you select the learning rate of 1. In general how do you select learning rates? Do you have ways to dynamically alter the learning rate in gradient descent? Taking recommendations.",True
@nbndanzo3685,2021-09-11T21:21:50Z,0,"Can you help me, dear Josh ?I didn't understand the meaning here in notes 15.04 and 18.09(Why did we decide to use one prediction for each observation, or to use three observations with two inputs for finding the derivative?)Thank you very much for everything)",True
@bonadio60,2021-08-31T14:37:10Z,1,Your explanation is fantastic!! Thanks,True
@Pedritox0953,2021-08-22T02:08:06Z,1,Great explanation,True
@Recordingization,2021-08-07T18:43:51Z,1,Thanks for nice lecture!I finally understand the derivative of cross Entropy and optimization of bias.,True
@rohitrajora9832,2021-07-23T08:21:54Z,1,BAAAAM!,True
@user-rt6wc9vt1p,2021-07-07T20:12:34Z,0,"Are we calculating the derivative of the total cost function (ex - log(a) - log(b) - log(c)), or just the loss for that respective weight's output?",True
@user-rt6wc9vt1p,2021-07-05T01:10:25Z,0,Is the process for calculating derivatives in respect to weights and biases the same for each layer we backpropagate through? Or would the derivative chain be made up of more parts for certain layers?,True
@lancelofjohn6995,2021-06-17T15:27:22Z,1,"Bam, this is a nice video.",True
@omkarghadge8432,2021-06-09T16:08:38Z,2,INFINITE BAMs! wont be enough to thank you!,True
@user-ik8my9kb5h,2021-06-07T19:39:02Z,0,"Nice video! I only have one question How i do it when there is more than 3 data (for example there is, n for setosa ,m for virginica , k for versicolor)",True
@user-bz8nm6eb6g,2021-06-03T02:00:21Z,1,Appreciated it so much!,True
@praveerparmar8157,2021-05-27T05:34:24Z,2,"Waiting for ""Neural Networks in Python: from Start to Finish""  :)",True
@jamasica5839,2021-05-26T09:58:13Z,1,This is even more bonkers than Backpropagation Details Pt. 2 :O,True
@_BlitzKrieg,2021-05-21T22:12:04Z,1,BAAMMM i got it sir :D,True
@samerrkhann,2021-04-26T21:30:25Z,3,A huge appreciation for all the efforts you put. Thank you josh!,True
@kamshwuchin6907,2021-04-18T03:02:02Z,0,Thank you for the efforts in making these amazing videos!! It helps me alot in visualising the concepts. Can you make a video about information gain too? Thank you!!,True
@chethanjjj,2021-04-09T07:13:37Z,0,@18:20 what i've been looking for awhile. thank you!,True
@wennie2939,2021-04-02T16:00:45Z,15,Josh Starmer is THE BEST! I really appreciate your patience in explaining the concepts step-by-step!,True
@_epe2590,2021-04-01T16:56:29Z,1,Please could you do videos on classification specificly gradient descent for classification.,True
@tulikashrivastava2905,2021-03-31T13:01:47Z,0,Thanks for posting the NN video series. It was just in time when I needed it üòä You have the knack to split complex topics into logical parts explain them like a breezeüòÄüòÄ Can I request you to share some videos on Gradient Descent Optimisation and Regularization ?,True
@erniec.2088,2021-03-15T09:24:07Z,1,Bam!,True
@iZapz98,2021-03-13T16:07:30Z,13,"all your videos have helped me tremendously studying for my ML - exam, thank you",True
@RubenMartinezCuella,2021-03-09T09:16:21Z,24,"Even though there are many other youtube channels that also explain NN, your videos are unique in the sense that you break down every single process into small operations easy to understand by anyone. Keep up the great work Josh, everyone here appreciates so much your effort!! :D",True
@dr.osamahabdullah1390,2021-03-08T00:49:41Z,0,Is there any chance to talk about deep leaning or compressive sensing plz; your videos are so awesome,True
@bigbangdata,2021-03-07T16:49:12Z,105,"Your talent for explaining these difficult concepts and organizing the topics in didactic, bite-sized, and visually compelling videos is astounding. Your channel is a great resource for beginners and advanced practitioners who need a refresher on a particular concept. Thank you for all that you do!",True
@osamahabdullah3715,2021-03-07T11:37:39Z,1,"I really can't give enough from your videos, what an amazing way of explanation , thanks for sharing your knowlege with us, when is gonna be your next videos plz ?",True
@pietrucc1,2021-03-05T11:01:30Z,1,"I started using the techniques of the machine learning from a little less than a month, I found this site and it helped me a lot, thank you very much !!",True
@rhn122,2021-03-05T03:08:48Z,0,"Hey cool video, though I actually haven't fully watched your neural network playlists, just want to keep things simple with traditional statistics for now hehe! But I want to ask you about all these steps and formulas, do you actually always have in mind all of these methods and calculations, or only keep the essential parts and their ups & downs when actually solving practical problems?  Because I love statistics, but can never fully commit myself to be in one with the calculation steps. I watched your videos to understand the under the hood process, but only keep the essential parts like why it works and its pitfalls, and leaving behind all the calculation tricks.",True
@l.prasathpresss8288,2021-03-04T14:51:44Z,0,Bro I had no idea how to code this softmax backpropagation . Please put video bro üôè,True
@environmentalchemist1812,2021-03-04T03:10:27Z,0,"Some topic suggestions: Could you go over the distinction between PCA and Factor Analysis, and describe the different factor rotations (orthogonal vs oblique, varimax, quartimax, equimax, oblimin, etc)?",True
@charliemcgowan598,2021-03-04T01:54:47Z,2,"Thank you so much for all your videos, they're actually amazing!",True
@justinwhite2725,2021-03-03T16:50:39Z,0,I think I need a derivatives 101.  I've followed every gradient descent/neural net video and you jumped straight to derivitaves like we already knew what they were. It's a huge 'black box' to me and when I try to do exactly what you say I can't get a handle on it because my exact scenario is different and I don't know how to figure out the derivatives (or even what a derivitive actually is),True
@lokeshbansal2726,2021-03-03T10:26:32Z,0,Thank you so much! You are making some amazing content. Can you please suggest some good book for Neural Networks in which mathematics of algorithms is explained or can you please tell from where you are learning about machine learning and neural networks. Again thankyou for these precious videos.,True
@nonalcoho,2021-03-03T09:02:33Z,1,"It is really easy to understand even I am not good at calculus.  And I got the answer that I asked you what's the meaning of the derivative of softmax in the last video. I am really so happy! Btw, will you make more programming lessons like you made before~? Thank you very much!",True
@shubhamtalks9718,2021-03-02T02:22:10Z,1,BAM! Clearly explained.,True
@MADaniel717,2021-03-02T01:30:10Z,1,"If I want to find biases of other nodes, I just do the derivative with respect to them? What about the weights? Just became a member, you convinced me with these videos lol, congrats and thanks",True
@Anujkumar-my1wi,2021-03-01T17:44:29Z,0,Can you tell me why we don't use polynomials as activation function?,True
@Meditator80,2021-03-01T08:30:40Z,1,Thank you so much! It is so clear for explaining the calculation of Cross Entropy Derivative and how to use it in BP,True
@yuewang3962,2021-03-01T07:10:52Z,2,Caught a fresh one,True
@mrglootie101,2021-03-01T06:35:22Z,1,Finally üéâ,True
@shreeshdhavle25,2021-03-01T05:16:41Z,1,Finally was waiting for new video so long...,True
@JainmiahSk,2021-03-01T05:15:29Z,1,Love ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è it,True
