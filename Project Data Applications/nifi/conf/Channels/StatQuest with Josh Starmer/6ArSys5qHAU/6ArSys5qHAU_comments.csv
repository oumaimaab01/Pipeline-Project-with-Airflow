author,updated_at,like_count,text,public
@statquest,2021-09-11T13:54:49Z,10,"The full Neural Networks playlist, from the basics to deep learning, is here: https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@studentgaming3107,2024-05-26T18:37:04Z,0,dislike for the shitty intro's but good video though,True
@TasteVeng3ance,2024-05-19T20:41:31Z,1,"I come for the great content, but stay for the ""beep boop beep boop"" calculation noises. Please make a coffee mug with the calculation nosies and I will buy it.",True
@BalajiChegu,2024-05-07T15:45:33Z,0,Thanks a lot for simple and elegant explanation. Can you please provide download link for slides of this video ?,True
@Noah_R98292,2024-04-18T02:57:57Z,1,"I am a student at Merton College, Oxford University. Please consider visiting our university sometime. Thank you for your absolutely brilliant content.",True
@r0cketRacoon,2024-03-14T15:18:03Z,0,have u had a video to prove the formula of cross-entropy? I noticed that the formula of cross-entropy kind of different from that of entropy (which u had a video of demonstrating it very well),True
@MLLearner-sb1ds,2024-02-11T00:59:47Z,1,"Thank you sooo much I have a masters degree in CS and this is substantially better than anything I learnt in college, I understand it at an intuitive level. Thank you sooo much!!",True
@scarlaticious,2024-02-02T14:40:18Z,2,"What an awesome channel. I've been learning and using ML for years and still these videos help me build intuition around basic concepts that I realize I never had. Also, love the songs and the BAMs. Thank you!",True
@InsiderMiner,2024-02-02T01:15:17Z,0,"Hi. Can I ask where the formula for Cross Entropy is defined. It appears around minute 2:22. Is that the definition of it? It looks like the definition of Entropy although at minute 2:22 it doesn't have the sigma sign at the beginning. In Wikipedia, I see a definition but it is not exactly this one. It is -E sub p of log q. I didn't see a definition of Cross Entropy in this video though. Is there another video where Josh defines cross entropy? I saw his supremely wonderful video on Entropy but I don't see any more. I more or less understand the argument that the observed probability that the data comes from virginica and versicolor are zero. Any help would be greatly appreciated! BAM",True
@user-se8id1xv6x,2024-02-01T15:06:30Z,1,love your songs!,True
@pauledam2174,2024-01-28T03:50:24Z,1,as always a truly wonderful presentation. It could be good to do the KL Divergence first and then explain that minimizing the KL divergence results in minimizing the cross entropy.,True
@AlexandrSarioglo,2024-01-14T17:35:32Z,8,"I‚Äôve inquired about the reasons behind using the logarithm to calculate the loss for so long, no one could explain well enough to develop intuition about it. This did it. Thank You!",True
@ferminbereciartua6432,2023-12-01T13:52:05Z,1,you rock josh!!,True
@jacobcrowley8207,2023-11-28T15:57:24Z,1,"Thank you, this makes sense now.",True
@xxxiu13,2023-11-20T18:47:05Z,1,Great explanation in an entertaining way. Bam!,True
@Ram-oj4gn,2023-11-12T08:35:56Z,0,"As you said in the current video, if the cross-entropy function helps us more in the gradient descent process than what Sum of squared function does, why don't we use the same cross-entropy for the optimization of linear models such as Linear regression also.. why we use SS there and not entropy ? Thank you for the wonderful videos to understand the math and functions ..",True
@shishi1826,2023-11-01T16:48:05Z,2,"I'm in a stats phd program and we had a guest speaker last week. During the lunch time the speaker asked us which course we liked most in our school, one of my classmates said actually no, he likes statquest ""course"" the most. And I was like nodding my head 100 times per minute. We discussed like why US universities hire professors good at research but not hire professors good at teaching, why there is no tenure-tracked teaching position......US education system really needs to change",True
@pabloxdark,2023-10-11T14:08:14Z,1,I am currently starting my bachelor's thesis on particle physics and I was told that a big part of it consists in running a neural network with pytorch. Your videos are really really useful and thanks to you I have at least have a vague idea on how a NN works. Looking forward to watch the rest of your Neural Networks videos!! TRIPLE BAM!!,True
@pauldevereaux5537,2023-09-19T14:34:56Z,0,"Hi Josh, it is me again. Thank you for all these amazing videos. Currently upgrading my own programmed NN to support classification. However, my own softMax function results in slightly different values around 1:30 namely .68, .11 and .21. So, is this just due to rounding or is there something wrong with my function. Many thanks in advance!",True
@supersnowva6717,2023-09-18T15:50:38Z,5,I would not be able to get how neural networks fundamentally work without this series.  Thank you so much Josh! Amazing and clear explainations!,True
@satyamgupta4808,2023-09-10T00:32:51Z,1,very very intuitive and very great explanation,True
@meeseeks1489,2023-09-03T14:49:17Z,21,What is this guy made of??? what does he eat??? Are you a God?? An alien?? You are so smart and dope man!!! How do you do all this? He should be a lecturer at MIT! SO underrated contentüíûüíûüíûüíûüíûüíû,True
@iandanforth,2023-09-01T19:14:40Z,0,"Please note that the image for -log(""p"") at 7:07 is incorrect. Both -log(x) and -ln(x) are 0 at x=1. The image also makes it looks like the function asymptotically approaches 0, but that is not the case. It is actually a much steeper decent than pictured.",True
@miki77YT,2023-08-29T22:35:49Z,1,bam,True
@onebyzero-4608,2023-08-23T22:37:04Z,0,"why observed probability is 1 in case of setosa , kind of confusing still ?",True
@oldcowbb,2023-08-12T01:35:02Z,0,"still trying to wrap my head around how is this related to entropy if entropy is the expected surprise, it's like we are using a different distribution for the surprise and the expected value",True
@josyulaprashanth2976,2023-08-10T17:53:43Z,6,I just can‚Äôt believe how you opened my eyes. How can you be so awesome üëåüëå. Sharing this knowledge for free is amazing.,True
@kaanzt,2023-08-07T20:30:13Z,0,"Josh, thanks for such an explanatory video. But i couldn't understand why residual^2 graph is linear in 7:31.",True
@Piccadilly_,2023-08-02T21:34:01Z,1,Thank you for this video! It and others helped me pass my exam! :D,True
@phattailam9814,2023-06-16T05:23:59Z,1,thank you so much for your explanation!,True
@amnont8724,2023-06-09T20:55:27Z,0,"Hey Josh, I saw one of your videos about entropy in general - which is a way to measure uncertainty or surprise. Regarding Cross Entropy, the idea is the same - but now it's for the SoftMax outputs for the predicted Neural Network values?",True
@yourfutureself4327,2023-06-02T22:26:42Z,1,üíö,True
@AhmedKhaliet,2023-05-19T11:12:48Z,2,"Wow , I feel when I say thank you it's nothing in compare with what you do ! Very impressive‚ù§‚ù§",True
@hashbrwn1339,2023-05-19T06:17:20Z,1,really good explanation .Difference of squared error vs cross entropy is very well explained .,True
@Noor.kareem6,2023-05-14T23:46:15Z,0,Argmax and Softmax are using only with classification? Or could use them also with regression?  And same question for cross entropy is used for classification only?  Thank you,True
@user-sd4fi5em1g,2023-05-14T08:52:40Z,1,"Just wow, thumbs up, great explanation sir",True
@Xayuap,2023-04-18T01:50:49Z,2,üéâ,True
@skillato9000,2023-04-09T17:06:33Z,0,Ok but why do we need to calculate cross-entropy?,True
@user-xn7ot7ij7d,2023-03-20T03:31:04Z,1,Your video makes my mind triple BAM!!,True
@Snipknot57500,2023-03-17T17:48:34Z,0,Why not saying that SSR is more adapted for regression while cross entropy fits better for classification? Awesome video though!,True
@sanchibharti858,2023-02-03T12:46:28Z,2,What an amazing video. Never found any content or video better than this one anywhere on this topic. Thank you so much.,True
@muhammadumarsotvoldiev8768,2023-01-09T05:59:50Z,3,It's amazing !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!,True
@muhammedyavas5765,2022-12-14T11:31:06Z,0,"Thank you for the videos. They are really helpful. I have question about the softmax process in minutes 5:13. All outputs for Versicolor species, were already smaller than 1. Even all outputs smaller than 1, do we have to continue to process of softmax ? And Can't we use raw outputs for cross entropy? Thank you again.",True
@jiaqint961,2022-11-25T12:53:38Z,1,Gold!,True
@dsazz801,2022-11-20T13:05:03Z,0,2:30 wtf just happened üòÇ I enjoy watching your videos. Thank you for the great explanations!!,True
@elielberra2867,2022-10-15T01:20:41Z,1,Thank you so much! Your videos are always very clear and easy to follow :),True
@gwinnifer4609,2022-10-09T17:50:55Z,2,Hey Josh!  the way you teach is incredible. THANKS A LOT!‚ù§,True
@mukulbarai1441,2022-10-02T09:58:58Z,4,I was predetermined that I would need to watch several videos to grasp this concept. OMG!! You have explained it so intuitively. Thanks a lot for saving my time and energy.,True
@amiralikhatib4843,2022-09-03T14:34:54Z,1,üòçüòçüòçüòçüòçüòç,True
@nathanzorndorf8214,2022-08-26T15:22:35Z,1,Thank you. Another complicated topic made simple. !!!!,True
@PrasadHonavar,2022-08-23T17:28:45Z,2,This is a amazing video,True
@beshosamir8978,2022-08-14T02:46:31Z,0,but in a Regression problem we still using SSR right ? so what will happen if we still using SSR in a Classification problem and after the backpropagation ends his work Check for the maximum output ? is that because if we have an output = 1.64 and the observed = 1 it also tends to decrease the distance so we needed to invent a function to control what is the min and maximum value ? in our case 0 and 1,True
@petercourt,2022-08-09T09:13:25Z,1,Really well explained! Thanks Josh :),True
@tiago9617,2022-06-21T10:38:20Z,1,"The hero we wanted, and the hero we needed, StatQuest...",True
@franciscoruiz6269,2022-06-15T15:07:00Z,1,MY Friend! I'm on part 6. How can I learn the differences between LSTM and Bi-LTSM and Recursive Neural Network?,True
@lindaaa3299,2022-06-06T20:01:37Z,1,I love this this way of learning!,True
@gbchrs,2022-06-05T08:21:14Z,1,"amazing work, can't wait to start on the book once I finish all your videos",True
@neoklismimidis6403,2022-06-03T20:16:24Z,1,"Hello Josh!   I have to say WOW!!  I love every single of your videos!! They are so educational. I recently started studying ML for my master's degree and from the moment I found your channel ALL my questions that I wonder get answered! Also, I noticed that u reply to every post in the comment section. I am astonished.. no words. A true professor!  Thanks for everything! Thank you for being a wonderful teacher.",True
@shivangisingh1190,2022-05-30T09:56:02Z,2,"Love each and every video by StatQuest. Thank you Josh and team for providing such clear, easy-to-digest concepts with a bonus of fun and entertainment. Quadruple BAM!!!",True
@satya8411,2022-05-17T10:20:44Z,1,Kudos!!!! üôåüèª BAM!!!!!,True
@skippy1234459,2022-05-04T17:06:24Z,1,This is great! Thank you!!!,True
@vusalaalakbarova7378,2022-04-26T20:06:52Z,1,"Another fantastic video. You make these topics such straightforward to understand, that most lecturers overcomplicate by writing down unnecessarily long formulas and just showing off with their knowledge. Thanks a lot!",True
@c3realpt,2022-03-31T19:58:58Z,12,So refreshing and so different from the mathematical riddles that are used in university to teach us this stuff. Thank you!,True
@adilahnafmugdho8132,2022-03-26T08:04:07Z,1,Thank you sir,True
@_The_Sage_,2022-03-25T16:03:34Z,0,"Hey Josh. The -log(""p"") outputs used in the video are wrong. Eg. -log(.52) = .28 not .65  Intuitively, the -log(""p"") should decrease as the value of ""p"" increases..",True
@LH-wc5vz,2022-03-12T15:38:15Z,3,This video helps a lot! The explanation is brief and clear.,True
@RaynerGS,2022-03-07T19:42:59Z,5,"I admire this professor a lot. I hope one day be a good teacher like you. Salute from Brazil. In my classes, I try to do it also, take a subject and make it easy as possible.",True
@mahdimohammadalipour3077,2022-03-06T14:57:05Z,1,Hello josh. could you explain me how you used entropy function here? I've saw the entropy StatQuest for data science and now I'm wondering why you used observed probability outside of the log and predicted inside it not the opposite (observed inside and predicted outside) I know that results in log(0) which is undefined but I'm seeking the exact intuition. Thank you in advance :),True
@user-sx1cd2ot5q,2022-02-19T11:26:27Z,1,i can always find answers here when I have trouble attending MIT DL course,True
@zingg7203,2022-02-16T18:39:27Z,0,Why the voice sounds so soothing?,True
@fndpires,2022-02-15T00:46:46Z,1,"He just cant stop getting better, THANK YOU MA MAN!",True
@sarbajitg,2022-01-31T10:26:19Z,0,"@4:49 is it because we know data is from Virginica, we are putting its probability [0.58] there, or 0.58 is the maximum thus we are taking the observation as Virginica? What if the prob were [Setosa, Virginica, Versicolor] = [0.5, 0.2, 0.3], in that case, will we still take 0.2, or take it as Setosa?",True
@palsshin,2022-01-30T16:01:02Z,1,you nailed it!,True
@kevinayers7144,2022-01-26T22:17:23Z,0,Why is the error not averaged across the three predictions?,True
@grabbenskarve3518,2021-12-26T19:17:56Z,1,I found my treasure. So great!,True
@yoonchaena671,2021-11-27T18:09:19Z,1,Baam!!!! Thank you~!!!!!!!! It is so clear~!,True
@az120121,2021-11-24T21:18:14Z,0,can you do label smoothing! Plz~~,True
@p-niddy,2021-11-19T09:42:19Z,1,"My friend, the news is good.",True
@tamtran2274,2021-11-06T15:18:49Z,1,love your song :)),True
@andreasapostolatos6624,2021-10-31T21:42:07Z,3,"That is an outstanding teaching video, thank you tons!",True
@knowledgedistiller,2021-10-24T23:20:51Z,0,What happens if we want to use soft labels? Would the cross entropy loss still be a good loss function that would help the neural net converge to a good predictor of the soft labels?,True
@jiesun31,2021-10-14T15:37:08Z,2,"Love your videos, they are so intuitive!",True
@bastinjerry2843,2021-10-08T23:13:15Z,0,please do a video on RNN network,True
@SushilKumar-dr9rj,2021-10-08T06:42:22Z,1,"Josh, you are a savior man. I cannot emphasize this enough. I would have given up on understanding these concepts long ago if you had you not made these videos.",True
@leozheng370,2021-10-07T17:05:17Z,0,I can imagine it takes a lot of time to make the videos. Thanks for the amount of efforts!   cross entropy function is convex while squared error is not due to the logit function in softmax.,True
@CaptainBravo87,2021-09-08T07:51:14Z,1,Pure Brilliance,True
@bobsmithy3103,2021-09-06T14:28:27Z,0,"3:28 Please correct me if I'm wrong, but the negative sign is before the summation sign, so shouldn't it be - ObservedSetosa*log(PredictedSetosa) + ObservedVersicolor*log(PredictedVersicolor) + ObservedVirginica*log(PredictedVirginica)? Like shouldn't the negative signs be positive instead?",True
@sattanathasiva8080,2021-09-05T04:54:22Z,4,"Happy teacher's day, from India. It's teacher's day today  in India.  Thanks for all your teaching",True
@Pedritox0953,2021-08-22T01:59:54Z,1,Wonderful explanation,True
@sandeepganage9717,2021-08-21T21:13:09Z,1,This is the shortest and the easiest explanation. Excellent job Josh!,True
@Noah-zp2fn,2021-07-14T17:11:16Z,4,this really clarified so much of the concepts of this topic! i always wondered what is the purpose of having cross entropy when we can use other loss functions like mean squared error! thank you so much!,True
@amitanand7534,2021-07-08T12:30:44Z,8,That wasn't shameless self-promotion. That was selfless giving.,True
@godfather5557,2021-07-04T05:23:21Z,0,Why do we use squared error for regression problems then?,True
@vijaykumarlokhande1607,2021-06-24T16:29:21Z,9,"Whenever I 'wonder' while watching statquest, josh tells me the solution just after:)",True
@user-bz8nm6eb6g,2021-06-01T09:20:31Z,1,Thank you so much~!,True
@mathrisk,2021-04-29T06:46:23Z,1,Thanks \m/,True
@iliasaarab7922,2021-04-27T22:55:01Z,3,Amazing explanation as always! BAM!,True
@nguyenkhoituan9291,2021-04-21T23:46:51Z,1,"Hi Josh, just want to let you know that the link for ""Neural networks with multiple inputs and outputs"" in the description is broken (though I was able to find the video in your Neural Network playlist).",True
@shandou5276,2021-04-10T15:02:21Z,4,The softmax fuzzy bear cracks me up so much :D Fantastic video Josh!,True
@ericdarcy,2021-04-09T00:50:40Z,1,Thanks a lot,True
@derbaur2330,2021-04-08T09:05:23Z,2,this is just the best channel in youtube!,True
@technojos,2021-04-04T06:34:15Z,1,Hi Josh. You have prepared a amazing video again. Firstly thanks  a lot for this. I have  a question. You said that sum of predicted probabilities must be equal to 1. But sum of the probabilities in the video differs 1. are they  random datas to explain for video making or I am wrong?Please clarify this issue. Thanksss a lot in advance :) Bam !!,True
@hadizand3753,2021-04-01T21:15:10Z,3,Excellent teaching skill,True
@WIFI-nf4tg,2021-03-31T09:37:54Z,2,"Thank you for saving so much of my time. There are so many blogs on NN that I have wasted so many hours and days on across various topics, then I found your channel. Thank God for that.",True
@thiccboi1221,2021-03-29T11:21:17Z,1,You can't just make a neural network on the surface of Mars...,True
@dengzhonghan5125,2021-03-25T02:28:04Z,2,great video as always.,True
@somanshbudhwar,2021-03-21T12:19:24Z,17,This is a life saver! Thank you so much again and again. Love your simple and elegant explainations.,True
@alberteinstein4547,2021-03-11T15:53:27Z,1,This video is godlike. Thank you.,True
@codekomali1760,2021-03-02T07:59:58Z,3,I want a jetski for christmas.....I am convinced that you are either god or santa!...either way I would like mine in red...Thanks!,True
@tonirbaena1,2021-03-01T15:11:50Z,0,"Thanks!  One question, in 5:40 you measure the total cross entropy as the sum of the 'train' set cross entropy. Could it be bias in unbalanced datasets? Do you recommend this method in these datasets? Thanks again",True
@datasciencefreeedu4066,2021-03-01T06:45:44Z,2,Josh u r vedios  are amazing  great  work üëè ‚ù§,True
@salutoitoi,2021-03-01T05:35:27Z,3,"It is on time. I am actually using tensorflow for an image classifier, thank you for your video :)",True
@NStewF,2021-02-21T19:15:57Z,122,Josh once again demonstrates his amazing ability to simplify complicated topics into elemental concepts that can be easily understood.  BAM!,True
