author,updated_at,like_count,text,public
@statquest,2020-05-29T11:58:13Z,98,"NOTE: This StatQuest is sponsored by JADBIO. Just Add Data, and their automatic machine learning algorithms will do all of the work for you. For more details, see: https://bit.ly/3bxtheb BAM!   Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@Nndsrdn,2024-05-24T03:47:59Z,1,Perfect!,True
@PeterBednarik-kj5tj,2024-05-23T20:49:29Z,0,"3:01 this is unclear to me, i understand that we count all the words for normal messages, but I do not understand the count for spam messages why did we also include ‚Äúlunch‚Äù when the word is not between spam messages? Did we count all the words for spam messages and then compare this list of data to normal messages and if something is not included we add this to the list?  Because later in video if we want to prevent getting 0 we add number 1 to all messagesüëÄ  btw your videos are awesome",True
@sofiasaoficial,2024-05-12T14:52:47Z,1,you saved me from losing 1 hour attempting to understand my imperceptible class slides. thank you!,True
@jingzhouzhao8609,2024-05-07T08:02:00Z,1,"Awesome video!!! Just wondering at 1: 53, why the denominator is 17 in p(Dear | Normal) = 8/17, I thought you've got 8 normal messages rather than 17.",True
@ichael5844,2024-05-06T11:03:02Z,1,Thank you for existing‚ù§,True
@trevmatlo4363,2024-05-03T08:38:32Z,0,"5:50 What is the reason that the probability 0.09 is proportional to the message is normal given that it says dear friend? Also we didn't explicitly calculate the probability for when a message contains ""Dear Friend"" ( we did it for ""dear"" and ""friend"" but not both), so I think that's why I'm confused.  Great video by the way. Hello from Australia.",True
@azizbekibnhamid642,2024-04-28T03:21:37Z,1,Amazing,True
@guilhermehx7159,2024-04-18T04:11:40Z,1,the link to the study guide doesnt work Josh,True
@proggenius2024,2024-04-14T20:36:14Z,1,so awesome,True
@ameliasundman2388,2024-04-09T15:04:49Z,1,"you're an INSPIRATION to me. If you can make stats fun, i think i can make anything fun in my life for me and other people. You rock! ",True
@fs-lg4sk,2024-04-09T03:12:52Z,1,thank you Josh.,True
@shakithiyankv2128,2024-04-03T16:21:22Z,1,BAMMMM,True
@ZhengTaing,2024-04-02T17:53:51Z,2,i think i lost it when he said SPAMmm,True
@maxmejia789,2024-03-31T18:37:46Z,1,I gave it a thumb up right after the intro song,True
@thedarkknight9032,2024-03-31T14:45:35Z,0,"This explanation was excellent! I have a question though, I have come across a couple of solutions to spam filtering questions and when they apply Laplace smoothing to it they increase the numerator by 1 and the denominator by the total no. of distinct words in the training set. So, for example if the SPAM category had 5 distinct words and the NON-SPAM had 3 distinct words, they increase denominator for both the cases by 8.",True
@kalakannan4247,2024-03-31T01:15:40Z,1,"Hi Josh, You are the best teacher I have ever seen. I always enjoyed your stat quest videos and I enjoy your book as well. What a pleasure to learn from you",True
@stanlymark1495,2024-03-29T11:32:55Z,1,this video helped me understand 100% what i was crunching the textbook for half an hour (i didnt understand a thing when doing that),True
@benstiller1297,2024-03-27T19:08:14Z,1,Great explanation üî•üî•üî•üî•,True
@VishwajeetVKale,2024-03-20T10:32:28Z,1,I feel Dr. Sheldon Cooper is teaching me Naive Bayes,True
@znikerizm,2024-03-19T11:01:32Z,1,"3 years on, and this video is still super helpful, thank you Josh! :)",True
@VinodKumar-ic6qd,2024-03-16T01:30:38Z,1,This is simply AWESOME!!!!!!!!!!,True
@amartyanayak7533,2024-03-12T14:35:28Z,0,4:48,True
@sayanbhowmick9203,2024-03-10T15:13:58Z,1,Amazing Video üòÑ,True
@leonardvermeer7908,2024-03-04T17:43:32Z,1,"We're not children, don't speak to us like we're children.",True
@user-ip3kd6io6c,2024-02-28T17:15:55Z,0,"Hi Sir, Very simplest clear explanation of Naive Bayes' .  But how the formula is applied here -- P(N | words) = P(words | N) P(N) / P(words)...  Thank You.",True
@rmcgraw7943,2024-02-28T09:41:26Z,0,"Josh, when you adjusted for your alpha in the calculations, you didn‚Äôt recalculate your probabilities for each data entry.  That is, you added one to the number of words found in messages, but you didn‚Äôt recalculate your probability for each word, hence your results are incorrect.  I assume that you add the alpha to the word count as well as as the total words for that Normal or Spam group, and if so, your numbers are wrong.  For instance, when you add alpha to NORMAL and DEAR, your 8:17 becomes 9:18, which becomes .5(50%), not .47.  üòÇ  No worries. I wouldn‚Äôt have caught it, except I‚Äôm writing this algorithm into a C# class so I can just plug in data and get anything of the info I want based upon parameters I pass into it.  Also, I‚Äôm using a decimal type which has a LOT more precision, so you‚Äôre off a bit there too.  U wouldn‚Äôt want to present this data to an executive committee if one of them had a calculator on them.  Love the vids by the way.  I‚Äôm putting all these ML algorithms in a library, so I only have to remember why to use which one, and not the formulas.  LOL",True
@AnasHawasli,2024-02-25T21:31:48Z,0,The video is great  thank you  please consider talking faster and not using all these flash words really annoying,True
@user-ee9pq9gm3j,2024-02-24T16:35:45Z,1,Thanks for this great explanation!!!,True
@Mangosrllysuck,2024-02-23T02:39:00Z,1,Amazing and intuitive explanations!,True
@udaraabeyrathne7167,2024-02-22T10:01:49Z,1,"Josh, you are awesome. Wish I could buy your guide, but at the moment I'm broke AF. I'll definitely buy it after having some money. üòÅ Bless you!",True
@flavienrobert2516,2024-02-18T14:17:56Z,1,"I ve been watching many videos about naive bayes but...as usual, Statquest is by far the best. Congratulations !",True
@arenashawn772,2024-02-16T23:59:57Z,1,"This may be a question with an obvious answer, but since p(N) and p(S) is decided using our initial training data, and they are used to decided the comparison of probabilities of p(N | Dear Friend) and p(S|Dear Friend) we are assuming the composition of any real world data (proportion of spam vs. normal messages) are similar enough to our training data (which is essentially the assumption behind all classifiers and I guess why data drift is such a headache for trained then deployed ML models‚Ä¶), is my understanding correct? In other words if I do not stratify my sample well in initial my train/test split or if some of the events are really rare therefore not well represented in my sample, then it could lead to unstable performance of the model?‚Ä¶  Thanks as always and hope you had a great talk and pleasant Valentine‚Äôs Day in New England üòä",True
@LearnAvecAmeen,2024-02-14T21:45:00Z,2,Beautiful. All the best insh'Allah,True
@jamesimmordino4855,2024-02-09T13:17:29Z,0,Content may be fine but attempts at being funny makes it virtually unwatchable.,True
@serwus3869,2024-02-08T17:26:22Z,1,"Hi, I wanted to ask why you are dividing by the total count of words in normal messages for p(Dear | Normal)? I dont really get this point, why you are looking at the count of words instead of the count of normal messages like in the formula: p(dear|Normal) = p(Dear AND normal) / P(normal)  it would be a huge help if you/ somebody could help me there! :)",True
@user-wl8pv2qk2h,2024-02-08T14:14:44Z,1,"Hey Josh, thanks a lot for this video. It was so so helpful. Little background, why this is so awesome: I'm native German, but quite okay with English. I'm already 40 now and never had to do anything with Stochastics or even Statistics, in fact I was very bad (or let's say uninterested) in Maths back in school. Like 20 years later I finally got a job in AI, but need to do a lot of learning sessions to understand at least the basics (I have other skills that contribute :P). When the course came to Bayes I was totally lost. I watched several videos (of course in German, because that seemed obvious for such a complicated topic) and ended up, watching your Quest. And it nailed it. It's so clear to me now. BAM, Thx so much! You're great at explaining!!!",True
@oscararmandocisnerosruvalc8503,2024-01-29T20:11:03Z,0,"How would you tackle similar probabilities of being spam but at the same time not spam ??. Btw amazing explanation , thanks a lot !!",True
@pebre79,2024-01-28T02:11:50Z,1,Great explanation. Thanks for posting! Keep up the great work,True
@charalamposkatsoukis8694,2024-01-16T18:26:23Z,0,What if there are two words in the email that have zero probability? Do I add 2 to every other word or 1 ?,True
@manaskulkarni2075,2024-01-15T04:20:43Z,1,Just awesome explanation üòä,True
@MTRFRK,2024-01-08T19:57:19Z,1,Your teachings are really really really good,True
@judemungcal1131,2024-01-08T00:41:06Z,1,"Just about to have my thesis defense, i was planning to use Naive Bayes in my modelling and really didnt understand it. Now this video might be the biggest help for my defense, THANK YOU!!!",True
@paulafont4862,2023-12-12T10:04:47Z,1,thank you! very useful,True
@chrisg0901,2023-12-11T20:16:58Z,1,This was helpful. I'm going to eat a can of Spam in honor of this video.,True
@mohitnayak3483,2023-12-10T21:00:25Z,1,"I like how you say ""spaaam"" :). Great lesson",True
@exoticcoder5365,2023-12-03T11:18:06Z,1,"12:32 ‚ÄúSPAM !‚ÄùüòÇüòÇüòÇüòÇ I love the BAM changed to SPAM, loving these small details bringing us joy ! Thank you Josh !",True
@mdeliasinulislam7663,2023-11-30T06:54:08Z,1,"To learn ML concepts, best channel ever.",True
@thequeenofspades,2023-11-22T16:45:26Z,7,"I swear to god authors go out of their way to explain topics like this in the most complicated way possible. I reread the section on naive bayes in my textbook many times, and didn't understand it. Josh made it crystal clear in under fifteen minutes.",True
@nzerusocia9232,2023-11-22T16:39:14Z,2,I wish our education system helped us learn the way you teach. I am so thankful to have stumbled upon your channel in this perilous journey of machine learning algos,True
@aryangurav3528,2023-11-09T07:37:18Z,1,"hii, I just started my ML journey and i didnt find anyone who teaches this good, i mean u ARE BAM!!",True
@anaums413,2023-11-08T17:16:53Z,1,"you're an excellent teacher, thank you",True
@anishaggarwal3007,2023-11-07T00:21:48Z,0,"Is the addition of 1 count to everything, laplace smoothing?",True
@ricardogomes9528,2023-11-02T17:16:05Z,1,The take on Probabilities vs Likelihood was your hero moment for me. I haven't seen such a crystal clear explanation long time. Thks!,True
@knt2112,2023-11-01T03:03:48Z,1,"Thanks sir! I've been able to refresh and gather content to write in concept  before the exam just because of your awesome videos, thanks again. BAM BAM BAM, Oh no  SPAM! üòÖ",True
@anilerdemulusoy,2023-10-28T14:27:55Z,1,I lost it when he said SPAM!!!,True
@minerodo,2023-10-21T16:36:47Z,1,great!! thanks!! impossible to explain it better!,True
@parasraina9470,2023-10-21T14:48:32Z,1,Beautiful Explanation! Thanks,True
@saulmendoza1652,2023-10-20T23:12:20Z,1,Best channel on YT for stats!!!,True
@tommy9x,2023-10-20T13:57:17Z,2,easy to understand,True
@TizedesCsaba,2023-10-18T12:02:20Z,0,"Really great, but how we can teach Bayes Classifier if we have no classifier yet?! :)",True
@jones4540,2023-10-15T08:46:29Z,1,"Josh, thank you very much 2 days I could not understand about Naive Bayes, thanks to You I figure out how it works",True
@mbrz1970,2023-10-14T23:20:37Z,1,Your videos are awesome. Definitely one of the best teachers here on YouTube. Thank you for making such great content for us.,True
@YesEnjoy55,2023-10-12T12:25:48Z,1,"Wow such a good videos, Thanks from my bottom of heart!",True
@lucando27,2023-10-12T09:32:21Z,1,"Really good explination, thanks",True
@starlight5531,2023-10-08T14:16:48Z,1,I heard that pee poo peepooo peepooooo multiples ttimesüòÇüòÇ,True
@siridubbaka6068,2023-10-05T06:40:36Z,0,in the first example why are we taking total number of words to calculate probability why shouldn't we take total number of mails,True
@xpt5oo186,2023-10-01T08:50:50Z,1,next time i want to hear: beep bob bim boossh pip pim pom pom pom pim pip bom bib,True
@YYchen713,2023-09-27T02:15:32Z,2,Can only be so clear if the presenter has a profound understanding and thoughts about the topic! Superb!,True
@agila.p9807,2023-09-25T10:31:03Z,0,"At 5:53 can you please explain how the score that dear Friend gets if the message is normal is proportional to the message is normal given Dear Friend, I tried the calculation but I am getting a different answer, please help.",True
@ani_vaidya,2023-09-14T12:20:10Z,0,amazing explaination but stop saying pip poop pip lolü§£,True
@sinkseeker,2023-09-11T08:02:50Z,0,"Hi John! What do you mean by ""divided by 17, the total number of words in all of the normal messages""? The way I understand it, if it's the total number of words in all of the normal messages, it would be more than 17 (assuming that the length of each normal message isn't that short). I think I have misunderstood it. I hope you can clear up my confusion. Thanks for the video!",True
@bhushandhuri2660,2023-09-10T07:00:25Z,1,"Even though I love your videos and get a lot of knowledge from them, I still not not support you financially as right now I don't earn anything. But believe me one day when I get a job, I'll buy something from your shop.",True
@yourvirgochild,2023-09-05T00:09:44Z,1,Very nice!,True
@willw4096,2023-08-31T19:47:33Z,0,1:26 3:45 4:22 4:49-5:07 Prior Probability-5:14 7:15 8:49 9:53 10:41 12:35 13:39 13:48‚ùó,True
@advithabobba1679,2023-08-19T22:05:32Z,1,You are god for me !!! Thank you for such an amazing video,True
@davidspence4054,2023-08-16T17:33:02Z,1,"Excellent, thank you.",True
@1DanielMacedo,2023-08-14T09:49:02Z,1,"This channel is fenomenal, thank you Josh.",True
@LogicVertix,2023-08-11T07:31:31Z,0,"I have observed that the value of P(Lunch Money Money Money Money|Normal) at 10:49 is 0.000002, which then decreases to 0.000001 at 12:19. Please correct me if I am overlooking something; I am new to this.",True
@Rkanu13,2023-08-03T09:55:42Z,0,Is 4 the total unique number of words in your example?,True
@ThierryHenryStyler,2023-07-31T18:57:10Z,0,"One Question: You diveded in minute 2:14 with 17, the total number of words. What if i have this: I have the word dear, its 50 times in spam and 10 times in not-spam (legimate). i have 100 spam mails and 50 not spam mails  If i would calculate the probability that the word dear is spam then i would calculate 50/100 - right?  100 the total number of spam mails.",True
@vedparulekar478,2023-07-17T03:49:55Z,1,"Just one doubt , At 2:13 shouldn't the probability of word dear given a normal message be=no of times the word dear appeared in a normal message/no of normal messages. But overall a great video ,loved it.",True
@abdullahsalama9474,2023-07-09T13:36:58Z,0,"Great video as usual!.  I just think speaking about why naive Bayes is called naive should have included reasons with other types of data, not only text (ex: Assumption of conditional independence of features). Also, not considering the order is more of a limitation of the bag of words which can be used with other types of models.",True
@SlyFluffyFox,2023-07-09T08:04:08Z,1,"Very well explained, should i say, Clearly Explained !!!!",True
@esotericwanderer6473,2023-07-08T18:42:55Z,1,BAM!!!,True
@VideamRaveeni,2023-07-01T08:44:41Z,1,maaaaaaaaaaan !  I had been stuck on this topic for a MONTH trying to get the hell out of it...  you just cleared everything in 15 MINUTES!  You are an absolute HERO.,True
@akshaybaviskar1582,2023-07-01T04:47:42Z,0,"*when you say BAM , it's very irritating*",True
@EmirTT,2023-06-23T14:33:39Z,1,Thanks a lot!,True
@VinayakSinghYT,2023-06-21T09:54:55Z,1,great teaching,True
@gabrielconceicao3956,2023-06-11T06:40:16Z,1,You just saved me from getting a 0 on my assignment. Thanks!,True
@TheoOliver-cn1lo,2023-06-10T12:35:02Z,0,Great video to understand naives bayes !,True
@jakestbu8692,2023-06-09T05:11:09Z,3,"I've learnt a few ML algorithms, and this was the first one that I learnt with StatQuest. It was definitely the easiest.",True
@coldbrewed8308,2023-06-07T11:37:59Z,0,"Why do we need to take the weight of normal vs spam? Because spams are in the minority class, it's automatically given a lower probability.",True
@ExamSolutions.e,2023-06-04T18:10:12Z,1,I like the shameless self promotion part! luv you‚ù§‚ù§‚ù§‚ù§,True
@jeffgalef121,2023-05-30T19:47:06Z,1,Spam!!!  ü§£ü§£ü§£,True
@andrre1987,2023-05-23T11:30:48Z,0,"hey! your videos are great! There is something I am missing about the way you compute P(N | word). Shouldn‚Äôt we also divide the product P(N) * P(word | N) by the probability of the word, i.e. P(word)? Also, what value would you assign to P(word)? Hope to get an explanation from you soon! thanks a lot for the videos, they are brilliant :)",True
@jessewerth4858,2023-05-23T01:38:06Z,0,This seems to suggest N is the total # of words in normal emails... I though it would be the total # normal emails? (And the conditional probability would be the fraction of normal emails containing a given keyword),True
@yashsaxena326,2023-05-21T15:54:01Z,1,awesome,True
@tonycardinal413,2023-05-14T18:56:45Z,0,Awesome ! So clear and your pronunciation and English skills are so great !...rare on here. One question please: WHere does bayes theorem equation fit in with this presentation? I mean (P(A|B) = {P(B|A)P(A)/{P(B)})   ?,True
@developementdiscourse3358,2023-05-08T07:33:59Z,0,"Hi Josh starmer, I guess the formula for Naive Bayes in your illustrated guide has an issue. The numerator for normal and span probability formula got switched (page 135). Please correct me if I am wrong.",True
@hlo0o0o0le,2023-05-06T13:24:56Z,1,"Its really simple but school makes this so complicated, thank you for this amazing explanation",True
@youmeforever8801,2023-05-05T05:14:52Z,1,Spam-less self promotion.üòä Hats off Josh.  Thank you for your great work.,True
@Gwen_Sable,2023-05-04T19:32:29Z,1,"Josh, your channel has tremendously increased my confidence in my quest to enter the field of data science. I hope you never stub your left pinky toe ever again. Keep being awesome!",True
@shantanu556,2023-05-03T15:24:12Z,1,Super handy when you have exams tomorrow ,True
@AkshatKalra-gw5cx,2023-05-03T01:27:32Z,1,Thank you for explaining this so beautifully ! Bless ya,True
@jojosharma635,2023-05-01T10:33:20Z,2,amazingly taught Sir!! Thanks a ton!,True
@serge9259,2023-04-24T12:16:50Z,1,"I jut love you so much, thank you",True
@bay-bicerdover,2023-04-18T13:14:40Z,0,Pls stop that horrible beeping!,True
@KrishnoSarkar,2023-04-17T03:39:01Z,1,Very clear explanation with excellent visuals and witty explanations :),True
@shivjani1992,2023-04-16T09:25:56Z,1,Love the humor :-)  Definitely gonna subscribe to your channel soon,True
@xiaojukemg944,2023-04-15T06:11:07Z,0,thank you but it wastes time by making unnecessary sounds,True
@vinayp171,2023-04-13T03:18:16Z,0,"thanks for the video and the awesome content , my 2 cents I felt like this video should talk more explicitly about the feature independence assumption in Naive Bayes, it's sort of implicit in the word order description you gave , but would help to make that explicit.",True
@AshaiiOokami,2023-04-12T14:56:27Z,1,"Was expecting a fullblown BAM! at 12:32 and then the SPAM! caught me off guard, it was perfect, now I gotta update my own prior about your BAMs! Thank you, you beautiful man, for the best ml explanations ‚ù§",True
@gillbates9668,2023-04-01T21:44:21Z,1,thanks for this great tutorial,True
@jaskaransingh0304,2023-03-30T12:34:28Z,1,Amazing,True
@justinkirkland140,2023-03-29T10:20:49Z,0,"I have a question (for anyone really) regarding the use of likelihoods in the Naive Bayes algorithm: when you perform the calculation for ""Dear friend"", you use p(Dear | N) * p(Friend | N) (and the corresponding p's for S), but should you not also multiply by the probabilities of the other words NOT being there? As in, p(Dear | N) * p(Friend | N) * (1 - p(Lunch | N)) * (1 - p(Money | N))?",True
@FreeMarketSwine,2023-03-28T08:06:09Z,0,"For the first example where the calculations were 0.9 and 0.1, would it be accurate to say that there's a 90% chance that the message is not spam?  Or is there a more complicated calculation there?",True
@siri0699,2023-03-28T00:38:17Z,1,"Josh , ur the best ‚Ä¶. U break it down to simpler ways and teach it well ‚Ä¶. Tq so much",True
@kasperhannberg4209,2023-03-26T14:16:09Z,1,Great video! Thanks guys. Where would i find a video on Bayes Optimal ??,True
@tankkinnari,2023-03-22T05:39:37Z,1,Thank you so much. Great explanation,True
@adityagoyal4299,2023-03-21T17:31:42Z,1,"Apart from the cringy intros, your videos are amazing!",True
@adzictanja,2023-03-21T10:27:25Z,1,"12:32 Almost spat my tea from laughing. TRIPLE BAM execution of the result, Josh!",True
@sreedharreddy472,2023-03-17T14:31:32Z,1,Simple and clear Explanation Sir. I just Enjoy watching your tutorials and learning. The way you explain makes complicated stuff so simple. My respect and humble gratitude to you Sir üôÇ Thank you for your teaching. Keep teaching and keep inspiring many.,True
@samsonmak3272,2023-03-14T08:07:26Z,1,We appreciate your time and effort into explain ML to us. Best wishes!!!,True
@datahouse24,2023-03-12T09:15:18Z,1,very cool intro,True
@technotroll,2023-03-06T13:11:59Z,0,"Looks really easy to game this approach by inputting ""Dear friend"" into spam messages... :p",True
@syedarsalan7000,2023-03-04T09:33:41Z,1,"Find the probability that you get a youtube comment, SPAM!",True
@mohsinkhalid2375,2023-02-22T22:01:59Z,0,"Hello Josh, to get the probability that the message is Normal given the message is ""Dear Friend"". Shouldn't we be dividing the P(N) * P(Dear|N) * P(Friend|N) by P(Dear)*P(Friend) ?",True
@sidhantnahak4481,2023-02-22T11:11:40Z,2,"I just loved the way he was saying,I got all these things. Thanks man!",True
@sajawalhassan1f12,2023-02-21T03:29:18Z,1,"Good video, but that is not a histogram, it's a bar chart. A histogram only works when it is quantitative data.",True
@thishasntbeen1,2023-02-14T20:35:53Z,0,"I just wonder where is the combination number (i.e. the factorial thing with the total count of words in the message and the number of draws of each word) is. It is normally a part of multinomial probability function. Is it just ignored because it is a constant, same for both categories?",True
@rahmatulakbar5005,2023-02-11T09:40:51Z,1,You really mastered art of theaching,True
@KeithHeilner,2023-01-27T22:02:53Z,0,"I'll pick up some merch regardless but can you get ""Bays Has a Posse"" on a shirt? I have way too many bags :)",True
@KeithHeilner,2023-01-27T22:00:19Z,1,"OMG!!...subscribed, hit the bell, and provide feedback.",True
@kridsumangsri964,2023-01-25T09:11:20Z,1,very very thank you,True
@jshd456,2023-01-23T16:09:03Z,0,How do i get which variables are significant under naive bayes,True
@akshayhegde5003,2023-01-04T21:26:20Z,1,Awesome explanation !,True
@neworldemancer,2023-01-03T00:32:28Z,0,thats not what is called likelihood.,True
@keonscorner516,2022-12-28T23:41:22Z,0,Naive byes,True
@thev1l564,2022-12-16T00:53:15Z,1,This is awesome!,True
@anirbanpatra3017,2022-12-15T03:09:51Z,1,You Are The best a man can be.,True
@crazzzykhunsh69,2022-12-12T14:34:01Z,1,Thank you for explaining it so easily. I have my exam tomorrow. üò©,True
@yosrmanai8830,2022-12-04T12:06:23Z,1,"12:33 should've said  ""SBAM!!!""",True
@jacobkanyi5046,2022-11-26T08:34:23Z,2,Why am I finding this a week before exam. This video was easy to understand. Thank you! Triple  BAM!!!üòÇ,True
@burcukoculu9934,2022-11-21T08:10:28Z,1,You're the best :) Thank you always Josh! BAAM!,True
@stevelck90,2022-11-15T12:15:09Z,1,"Oh my, this is a lifesaver. Thanks for the sharing.",True
@computersindia,2022-11-12T08:14:57Z,1,Well explained,True
@yeoucheoub3535,2022-11-12T03:39:48Z,1,"very clear instructions, thanks!",True
@advaithsahasranamam6170,2022-11-09T20:39:43Z,1,Hooray! Now my spam and ham detection model has a 97 per cent accuracy. BAAAAAM.,True
@spagussy,2022-10-31T19:28:40Z,1,i love you american internet man,True
@lch99310,2022-10-28T13:54:57Z,0,Does hyperparameter in Naive Bayes exist that we can adjust to improve the model performance?,True
@maikfranke2303,2022-10-23T10:52:55Z,1,Nojoke best intro so far! Todays new catchy tune*-*,True
@sunnyrawat931,2022-10-20T14:22:01Z,1,Done very well..,True
@f44nboy,2022-10-20T06:58:58Z,0,Correct me If im wrong but at 02:30 you talk about the probability of seeing wach word. And then you create the  fraction 5/17 where 5 is the number of times the word occured in a normal message and 17 being the total number of words in all normal messages. But shouldn't 17 represent the total number of normal messages?,True
@jimilpatel2983,2022-10-16T12:50:24Z,1,Excellent Efforts in making the video !!! Thanks,True
@mars5617,2022-10-09T19:17:45Z,1,I am new to the channel and clearly all videos deserve likes just because of the quality of intro.,True
@yugal3556,2022-10-03T20:45:17Z,0,"Your humour is too creepy and cringe, don't try to make jokes there in between of studies, i didn't laugh for single time for your all of your cringe jokes",True
@andyrojhn9235,2022-09-30T04:53:55Z,1,"new subscriber, amazing explanation",True
@velunatarajan2649,2022-09-25T16:03:50Z,2,"Deep information with a simple learning approach. Thank you, Josh !!",True
@hkshetty,2022-09-25T03:58:23Z,1,One of the best explanation that I have come across!! ü§ó,True
@bhuriwataunguranun6371,2022-09-25T00:48:24Z,1,Amazingly clearly explained.,True
@jdlopez131,2022-09-23T03:21:22Z,1,thank you for your efforts in putting this together. Great contribution!,True
@54mjaime,2022-09-21T18:44:55Z,41,"The brilliance of StatQuest is how the use of prosody and cartoonish visuals can tap into my ""child"" brain which in turn allows me to stay focused, entertained, and makes these concepts seem so simple. Genius!",True
@birvasavalia743,2022-09-18T00:39:37Z,1,Thank you!! It's really helpfull. Glad to have your videos.,True
@578n.jhansisri5,2022-09-12T01:54:59Z,1,Awesome lecture üòé le,True
@us.reza.abbaszadeh,2022-09-11T15:42:20Z,1,why Naive Bayes is naive!!!! HAHAHAHAHAHA...!!!üòÇüòÇüòÇüòÇ,True
@ibrahimislam845,2022-09-09T18:43:21Z,1,"very nice video, i love you, myself nic curr",True
@hanadibinmujalli965,2022-09-09T15:59:47Z,113,we are lucky that you decided to create this channel & teach us ML in very clear & entertaining way!!,True
@SPLICY,2022-09-07T13:27:38Z,1,"""SPAM!!!"" gets me every dang time üê∑",True
@ritikvaidande4683,2022-09-04T13:38:25Z,1,BAAAMMMM!,True
@nintishia,2022-08-31T14:11:33Z,1,"Very lucid. Appropriate example. Great introductory material. Improvement suggestion: remove ""BAM"" from your narrative.",True
@pabloruiz5407,2022-08-22T16:52:08Z,3,"I actually laughed out loud at 12:29  üòÇ Amazing videos, thank you so much!",True
@lashlarue7924,2022-08-15T08:35:04Z,1,"StatQuest with Josh Starmer is so 'effin' Starmery AND so 'effin' Joshy that I can after problem Bayes no drunk be Naive watch alcohol intoxication consumption learn classification still.  And THAT, my friends, accomplishment teacher Bayes a is be of proud good.",True
@yudhaken2095,2022-08-15T02:39:57Z,1,"All of your tutorials will really help me in making my final project, and I just want to say thank you very much.",True
@abhinavjoshi14914914,2022-08-10T10:48:32Z,1,Amazing video!,True
@junyiwu9341,2022-08-08T15:28:57Z,0,I can't tell weather it's AI or human that talk,True
@DM-py7pj,2022-07-30T09:47:27Z,0,Great. Studyguide link now broken however.,True
@pablomarquesi,2022-07-27T18:12:11Z,1,Amazing explanation,True
@gpietra,2022-07-23T20:22:29Z,1,Perfect explanation!,True
@raviyadav2552,2022-07-13T16:33:46Z,1,"this is hilarious, i just opened this video and at the same moment my mom just entered and as usual the intro of statquest was going on ,she thought that I was listening some songs instead of doing studies and she without even a second thought slaps me , then for proving my point I have to show her all the other video of josh starmer to convince her that I was studying , and now she is like whatever ü•≤.  she never heard of a professor enjoying teaching so well and singing.üôÉüôÉ and as i m writing this my back still hurts üòÇüòÇ",True
@designcredible8247,2022-07-04T03:36:58Z,0,"Hi Josh, I had a query at 8:23. I was wondering if the formula was divided by P(Dear) * P(Friend) somewhere or not. Please let me know if it was! PS: I love your videos, they are quick to understand!",True
@ramazanyel5979,2022-06-27T13:00:25Z,0,"hello josh. thanks for the explanation. i wonder how to handle the case where  there is a word that does not appear in both ""spam and ""not spam"" messages.",True
@arieljiang8198,2022-06-26T21:18:37Z,1,clap clap clap,True
@LITHIUMINWATER,2022-06-23T14:14:25Z,1,bip boop bip boop bip ‚ù§,True
@chauvoluuhuong7485,2022-06-21T09:43:09Z,0,"Dear i have a question, why does the total number of words  you gave in example alway a constant (for ex: 17, 12..) ? it should be differ over messages  OR u  ""total number of words""  it isn't words in one message",True
@abhisheks5666,2022-06-18T17:38:39Z,1,"Only If i had teachers like you, I'd be attending Univ everyday!",True
@fumax3035,2022-06-18T13:52:06Z,0,Great video Sir. Have a doubt...  You said that adding black boxes won;t change our intiial guesses in normal messages.  But at 12:28 the probability that the message is normal has changed to 0.00001 from the initial guess at 9:50 which is 0.000002  For verification I calculated the normal guess using blackboxes and it came out to be  0.000002 as one should expect. So is there a printing error at 12:28?,True
@harshparadkar,2022-06-12T20:28:02Z,1,Best explanation I've seen YouTube.,True
@shreyashavaldar2076,2022-06-12T05:56:16Z,1,BAM! My lecturer should watch this before teaching to us.,True
@donharris8846,2022-06-10T12:15:59Z,1,"You Sir, are the King of explaining machine learning concepts. Many thanks!",True
@tanyaahuja8923,2022-06-05T13:05:28Z,1,The best tutorial out there!,True
@frankchen4229,2022-06-04T23:26:31Z,1,triple spam,True
@eddpalenciavanegas6739,2022-06-02T13:47:20Z,1,This content can easily be just an educational fun tik tok....Amazing!,True
@thanhquocbaonguyen8379,2022-05-28T09:40:33Z,0,"hey josh, your video is just totally awesome. I just spot a typo of the word 'contains' in 09:10. hope this helps!",True
@thivuxhale,2022-05-22T05:56:46Z,0,13:50 what's the difference between 'high bias' and 'low variance'?,True
@mohitzen,2022-05-21T09:19:57Z,2,Thanks a lot!!!,True
@rabiajaved4228,2022-05-19T08:48:14Z,1,Superb,True
@chaosjoerg9811,2022-05-17T17:16:01Z,1,Friend is probably the most common word I see in spam messages.,True
@ninhdang1106,2022-05-17T09:31:13Z,2,I love the videos! It's like ELI5 of Machine Learning. It really helps me a lot since I'm struggling with the course in university,True
@clashlogs1854,2022-05-12T03:55:22Z,1,What a briliant way to teach topic.,True
@ARBESProductions,2022-05-10T02:02:22Z,1,"Oh, my god! You probably saved my semester under 15m üòÇ. Thanks for the content!",True
@zivlazarov,2022-05-10T01:06:55Z,0,Does anyone know if he's cynical?,True
@maitekolarik,2022-05-08T19:51:22Z,9,I love your explanations and humor. Thanks a lot for making complex things so much easier.,True
@hatemecauseuaint,2022-05-02T16:19:55Z,0,"Where did the 4 come from at 4:59? I get where the 8 but where is the four, shouldnt it be 2?",True
@Josh-di2ig,2022-05-01T18:52:42Z,1,you're amazing. thanks for an awesome vid!,True
@TheDroidMate,2022-04-26T08:38:46Z,1,you deserve so many more subscribers...,True
@footballistaedit25,2022-04-26T06:00:55Z,1,"The way you present each topics is amazing. Thank you so much, Sir",True
@nachiketbatwe633,2022-04-25T03:14:48Z,1,best channel of stats,True
@korreyj,2022-04-22T14:28:01Z,1,Tip of the hat to you my good sir üé©,True
@Hevletica,2022-04-17T23:46:54Z,1,Gotta love this channel!,True
@bea59kaiwalyakhairnar37,2022-04-15T06:02:04Z,0,4:58 I think there is the mistake while taking the probability of P(N). The p(N) should be:- P(N) = 8 / (8+2) because we have two dear in spam.,True
@GonzaloEyzaguirre13,2022-04-04T23:22:51Z,0,"Why do the probabilities add up to 1? If we are referring to the probability of finding Dear in the message, given that all mails have more tan 1 word, should the sum of all probabilities be more than 1 (or if it's only a subset of words, shouldn't it be different to 1?)",True
@vig9737,2022-04-02T09:22:42Z,0,"Hey Josh, why do I see it written as naive and na√Øve bayes in articles? Are they both correct?",True
@arvindchandar2732,2022-03-28T15:31:31Z,1,wow! amazing content!,True
@Acha413,2022-03-26T19:55:19Z,0,It is called naive because of conditional independence.  That is p(class|word1) x p(class|word2) x ... x p(class|word n) = p(class|words word 2 ... word n). It does not consider joint distribution of words in sentences. That is why it is naive,True
@lindeanchuang8115,2022-03-25T02:25:30Z,2,"As a non-English native speaker, thank you for the cc subtitles and the pictures.üôè",True
@carlosforza5411,2022-03-20T19:33:29Z,1,You're the best Josh,True
@khoerulumam8316,2022-03-15T01:54:23Z,0,awesome! but how if the input word not in normal message neither spam message? for example 'hello friend',True
@beautyisinmind2163,2022-03-13T13:50:22Z,0,it would be awesome if you provide us that presentation slide.,True
@gaplekicoy,2022-03-12T16:52:28Z,1,"Hello Josh. My lecturer mention ""posterior"" when he was teaching this. Can you please explain what it is and how to calculate it? Thank you. Amazing video btw.",True
@danverzhao9912,2022-03-07T09:58:24Z,8,"This is what I call quality teaching, stuff that I expect from unis that are offering online lectures because of covid, with nice animation and thoughtful explanation, not just lecturer talking in the lecture hall but recorded like usual improvising.",True
@vanessaking1855,2022-03-07T05:03:41Z,1,Clear explanation,True
@dante3578,2022-03-05T07:06:49Z,1,spam!,True
@divyamishra4911,2022-03-03T13:41:40Z,1,Amazing explanation. I like your shameless self promotion.,True
@bokunzhao9346,2022-03-01T20:36:20Z,1,"Thanks man, you explain stuff to people like they're idiots, just what I need, subbed.",True
@prof.angelinagokhale6004,2022-03-01T09:47:00Z,3,Thank you Josh for this wonderful explanation. It will give me a head start into understanding the mathematical-statistical underpinnings of the algorithm. I am going to strongly recommend your tutorials to my students!,True
@aniketsingh4953,2022-02-27T11:44:13Z,1,My school teacher literally showed us your videos in class to teach us lmao,True
@Marshmachu,2022-02-26T12:49:20Z,0,Why is P(N) x p(Dear | N) x p(Friend | N) proportional to p(N | Dear Friend)?,True
@DaveGogerly,2022-02-25T16:11:54Z,1,"Hi Josh, I bought some of your study guides today, they are the icing to the cake, well done they are really good and look very cool too!",True
@vigneshreddyjulakanti7583,2022-02-25T15:48:08Z,1,hurray,True
@DaveGogerly,2022-02-24T23:07:01Z,0,"Hi Josh, I have a question please, what do you mean when you say total number of words in all of the messages=17, are you referring to an average here please ? or are you saying that if you had ten messages the accumulated total of all the words in those ten messages is 17?",True
@mahdimohammadalipour3077,2022-02-24T10:03:59Z,1,Thank you for this content and the effort you put on it. I 'm wondering that could we only use Laplace smoothing only for the class (or classes) that has problem by becoming 0 or we have to use it for all classes and treat them equally ?,True
@kartikgupta370,2022-02-20T05:07:46Z,1,best explanation,True
@CrewNDx,2022-02-19T15:03:12Z,2,You're so good at explaining,True
@dakotahanemann-rawlings4566,2022-02-19T06:37:32Z,1,SPAMMM!!,True
@aravindkramesh,2022-02-09T22:00:33Z,0,*Don't sing. Just teach.*,True
@solidwaterslayer,2022-02-09T17:45:48Z,0,"One important correction: The reason why you work in log space is because if you multiple a bunch of small numbers together, python might think that it is 0.  Underflow is not a good reason. I did an school assignment incorrectly because of this.",True
@distrologic2925,2022-02-07T18:18:24Z,1,I genuinely laughed at the SPAM!!!,True
@gavinr1425,2022-02-06T17:58:55Z,1,We are all fortunate that this channel is free,True
@harshalbhoir8986,2022-02-06T17:42:46Z,1,we love your videos,True
@julianmoller2148,2022-02-06T00:32:59Z,1,great!,True
@mayurmulik1647,2022-02-05T11:36:16Z,1,14:00 Shameless self promotion...........So funny,True
@hansenmarc,2022-02-03T22:36:07Z,1,"Very nice explanation, Josh.  Thank you!  I hope someday you can give us a similar explanation of categorical na√Øve Bayes.",True
@mprawn02,2022-02-03T01:33:48Z,1,Thank uuuu!!,True
@62294838,2022-01-30T02:13:20Z,43,This is just soo amazing. I did spend quite a long time reading the lecture notes of other universities online but never get to understand it as thorough as this. Soo good thank youu!!,True
@NocturnodeArkania,2022-01-30T00:02:13Z,0,"11:47  hello sir, you applying Laplacian smoothing method in the 2nd case (alpha = 1), but my prof said, it will also affect the count for the normal and spam, such p(normal) = (8+1)/(12 + 1(2)). What do you say?  Thanks in advance.",True
@shichengguo8064,2022-01-26T16:32:41Z,0,"Dear Josh, any plan to talk about ""Bayesian hierarchical modeling""? Thanks.",True
@bestest43,2022-01-25T10:20:31Z,0,Hey Josh! I was wondering what the more complex (not Naive) version is called Naive Bayes? I tried to look it up on the internet but couldn't come across anything reasonable.,True
@victormachadogonzaga1898,2022-01-21T16:44:49Z,1,You are awesome!,True
@_zeppeh,2022-01-18T15:05:59Z,1,god this video is honestly one of the best videos that I saw in the recent time. Such a joy to watch!,True
@taotaotan5671,2022-01-17T19:03:56Z,0,"Awesome quality, Josh! I recently found many ML algorithms, including Naive Bayes, are strictly based on something called ""conditional independence"". Can we possibly cover this in the future?",True
@arnolddaniel6844,2022-01-16T00:24:57Z,1,I LOVE YOUR CONTENT!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! But the INTROSSSSSSSSSSS are like a bad opening to an anime.,True
@elevatexd,2022-01-10T22:10:13Z,2,Wow this is impressively easy to understand when u explain it,True
@Rahul-fq9kf,2022-01-08T15:19:58Z,0,I have been looking around the web to understand the practical understanding on which is better Bayes or Naive Bayes Classifier.. but I am unable to find anything that explains in a simple way to make me understand...Hoping your videos can...,True
@Rahul-fq9kf,2022-01-08T15:17:59Z,0,Love your sessions... you simplify them so well. Thank you. The only thing I don't understand is the use of word 'BAM'. Is that a way of Americanising the video. We usually do not hear such words.,True
@oreocookie9775,2022-01-06T04:57:18Z,0,"Overall really good video, but it is slightly cringe tho :D",True
@lucasbandeira5392,2022-01-06T00:29:32Z,0,"Josh, your explanations are awesome! I just got one question from this video: In the examples, shouldn't you have divided the final probabilities you found by P(dear)*P(friend). I mean, it should be Bayes theorem, right?",True
@negarvahid3429,2022-01-05T18:00:30Z,1,dude thank you so much for these videos. They're legit a life saver!,True
@georgioalbani6312,2022-01-04T00:13:25Z,0,"Wow ! Awesome video! I have a question though. If there are two words with zero probability then what happens ? Is everything increases its value again, and what about the new words how much are they increased ?",True
@yt-1161,2022-01-03T14:01:17Z,0,"...Given total number of ""words"" or total number of spams @3:17",True
@zahiyoussef2551,2022-01-01T23:06:34Z,2,"Baaaaaaam i've just discovered that i still a kid , and i feel happy when you explaine with this BAAAAAM thing  , it works broo hhhhhhhhhhh.",True
@saunaknandi1814,2021-12-31T18:53:10Z,1,"In search of Gold , I found diamond",True
@saunaknandi1814,2021-12-31T18:13:10Z,0,Hey do u have ur own paid courses?,True
@guardrailbiter,2021-12-29T15:01:21Z,1,"StarQuest's favorite Flintstones character:      Bam Bam   (or as he calls him ""Double Bam"")",True
@sohambasu660,2021-12-20T23:38:11Z,0,Any video on Maximum Aposteriori Hypothesis ?,True
@travel6142,2021-12-18T14:49:21Z,0,"I have a question. What about if we have data of continuous values? How multinomial naive bayes would work in that kind of problem? For example, considering the gene data of previous videos. Thank you for this video!",True
@Foxie-1,2021-12-14T21:28:07Z,2,"I like your ML spam filter. Even it's naive version has the benefits: if your friend proposes you a lunch, the message is classified as a good one, but if he asks you for some money, it's thrown to the spam folder)",True
@stoufa,2021-12-14T00:23:29Z,1,"Naive Bayes, Beautifully Explained!!! You sir got a new subscriber! üñêÔ∏èüòÅ",True
@peronianguy,2021-12-13T01:20:44Z,1,Cleveland from Family Guy turned out to be a great professor,True
@AnasMations,2021-12-11T18:26:18Z,1,"This was so helpful and fun, thank you!",True
@ia0722,2021-12-08T20:32:23Z,2,"Omg this was so good, being spoken to like a five year old is seriously the only thing my fried brain can handle at this point.",True
@ahmed-abubakar,2021-12-06T21:35:42Z,2,Honestly the best way to learn to Machine Learning and stats - THANK YOU you made my DAY!,True
@codingstyle9480,2021-12-04T19:23:08Z,0,"Thank you very much for the awesome videos. BTW, do you offer any coupon code for the study guides?",True
@sauravkokane3490,2021-12-02T11:02:11Z,0,Please tell us about word naive in naive Bayes?,True
@ominhquanho3860,2021-12-02T07:46:07Z,0,"I know there are two functiona in the sklearn library which are TfidfVectorizer and CountVectorizer, do their function are to produce the 'frequency dictionary', if not, which function in python should we use to create a frequency dictionary like in your tutorial, or we have to write our own code. Thank you so much",True
@jma42,2021-11-25T04:31:37Z,1,double bammm,True
@petril48,2021-11-22T21:41:13Z,0,4:56 Why are they 8 messages in Normal and 4 in Spam? In the introduction at 1:50 it looks like probabilities are computed over 17 Normal messages and 7 spam.,True
@neniscarlet3880,2021-11-22T17:52:38Z,1,"Please adopt me as your student! Amazing content, you make it sound so easy!",True
@montgomeryfrenwheringwerth5584,2021-11-20T16:45:18Z,0,"Seems like a bit of a complicated way to do things. I think just oversampling the minority group to create equal classes in the training set and then adding the probabilities of the words together in the normal and spam cases, then seeing whether the normal or spam gets the highest number would have the same effect. Naive Bayes seems to have a lot of arbitrary scaling going on.",True
@NoOneIsHereRightNow,2021-11-16T12:42:02Z,1,so funny and understandable immediately <3,True
@yashchaube2815,2021-11-14T13:41:43Z,1,everyone - he really loves the audience as he is replying to all comments  PLOT TWIST - he trained a model that replies to the comments based on their contents...  ps- this is my first vdo and i subbed instantly ... really lover the content tho,True
@chelseavancoller6317,2021-11-13T09:41:22Z,1,Thank you! Thank you! Thank you! This was absolutely phenomenal!,True
@moisesdiaz9852,2021-11-12T11:26:08Z,1,"Needed to refresh, thanks again!",True
@christopherreif3624,2021-11-10T21:23:23Z,5,You are the reason why I'm getting through my graduate level statistical courses. I'm glad I had found you!,True
@ozcanf6680,2021-11-07T10:03:01Z,1,"Awesome explanation, thank you Josh.üëè",True
@muhtasimbari8951,2021-11-06T02:04:15Z,9,Why can't my professors be half as good as you in explaining this stuff? I owe you my project marks StatQuest!,True
@rex3616,2021-11-02T08:20:44Z,0,"Awesome Video. But is it a bit fairer to set the equal amount of spam and normal mails in the training dataset? I mean, set the initial guess equal in both categories.",True
@emineaysesunar5122,2021-10-29T20:24:08Z,1,shameless self promotion :D,True
@fcolinavid,2021-10-28T13:52:25Z,0,"Thank you Josh, awesome explanation as always.  At the 7:08 mark you insert a symbol that looks like alpha between 0.01 [symbol that looks like alpha] P(S|Dear Friend).  I haven't come across this symbol before.  Would you mind explaining what it's called?",True
@addisonmcghee9190,2021-10-28T02:05:26Z,0,"Hi Josh, I wanted to ask about the calculations that you did to find P(Dear Friend | N)  Did you assume that Dear and Friend are conditionally independent given N?  My initial thought would be that P(Dear Friend | N) = P(N) P(Dear | N) P(Friend | Dear, N)",True
@thientruongvan7388,2021-10-27T15:02:27Z,0,"Hi Josh, You are super with amazing lessons, I learnt a lot, many thanks! I just wonder how you prepare the slides, which tools do you use?",True
@federicoarias9687,2021-10-27T00:16:54Z,0,Your f√≥rmula implies that the probability of every Word is independent. You Are Not using bayes Theorem,True
@ghazimaamar1876,2021-10-25T11:12:16Z,1,This Guy is the best <3,True
@cece8723,2021-10-24T12:51:14Z,1,"loved how you introduced the ""Shameless Self-Promotion""!",True
@lyricass7810,2021-10-19T14:38:16Z,1,i love statquest,True
@TheIronStarks,2021-10-19T04:35:29Z,1,"I have a test tomorrow and I‚Äôm cramming some information, your video has literally saved me so thank you.",True
@user-cc8kb,2021-10-15T17:11:15Z,1,Clearly explained! Thank you! :),True
@franciscocordeiro8080,2021-10-14T22:34:31Z,1,thanks for saving me once again,True
@hyeongsoo-nim719,2021-10-13T20:09:19Z,1,"From South Korea,  Im really appreciated your dedication for the work. I have been struggling with understanding it, but its completely understandable .",True
@gutierrezraphaele.4178,2021-10-09T12:20:55Z,0,"This is by far the best introduction video to Naive Bayes. I have few questions tho: 1) Where can we insert the alpha value to the formula? I've seen many Naive Bayes formula not having that value, and also 2) can we implement this with continuous variables? How?",True
@abelmescua6569,2021-10-09T07:58:50Z,1,"You are awesome, I love your videos!!",True
@elloquendero100,2021-10-08T15:06:36Z,1,"King, this was so helpful!",True
@tirsapambayun3539,2021-10-08T00:04:58Z,1,"Amazing video, thank you...",True
@RobertWF42,2021-10-06T13:36:12Z,0,"Very helpful, thanks Josh!   I've also read that Naive Bayes is ""naive"" because it assumes the conditional probabilities in the formula are independent when in fact they likely are positively or negatively correlated: P(""Dear"" and ""Friend""|N) > P(""Dear""|N) x P(""Friend""|N).",True
@TJ-wo1xt,2021-10-06T12:38:04Z,0,but how is the message divided into spam and ham in the first place??,True
@Anthestudios,2021-10-02T16:49:02Z,1,Dear friend. Money money money money!!! Thank you so much!,True
@AshutoshPandey-se8vt,2021-09-29T11:17:01Z,1,"Great Video, watching for my exams at IIT Delhi",True
@vnrbhat,2021-09-28T12:23:02Z,0,1.25x,True
@ParminderKaur-zm4kw,2021-09-25T06:10:34Z,2,"whenever i open your channel video, it seems I have come to the world of watching cartoon series. you start your videos in amazing ways and I just love your voice.",True
@arpitkakkar2780,2021-09-24T12:35:00Z,2,You taught a naive person a Naive Bayes. BAM!!!! Awesome,True
@chadgregory9037,2021-09-20T19:40:20Z,1,"323,323 viws",True
@suniljain7793,2021-09-17T07:38:42Z,0,"I read somewhere, naive bayes works well with large amount of features.So,why naive bayes works well with large amount of features ?",True
@vidura911,2021-09-12T18:08:07Z,1,"""Shameless self promotion"" got me off guarded XD XD",True
@alepel792,2021-09-09T18:23:08Z,6,Amazing teaching abilities. This is now a complimentary source for my machine learning course - thank you!,True
@antoniosalomao6634,2021-09-09T16:13:59Z,7,"The world needs more videos like these, thank you Josh!",True
@BrunoAraujo-po2lm,2021-09-09T03:18:24Z,1,"OH MY GOD! Best explanation I've seen so far! I MUST see the other videos! Got one more subscriber man, thank your so much for your work!",True
@jovisyang,2021-09-04T05:48:33Z,1,Can you please publish a book? Your videos are so helpful to me!,True
@lennyatomz8389,2021-09-04T01:26:04Z,1,"Hi John, thank you for creating this *very* helpful video! I would respectfully like to point out the typo at 9:09 (""countians"" instead of ""contains"").",True
@user-cv6xl8cb1n,2021-09-02T13:36:57Z,1,Your videos are so meaningful and comprehensive! I‚Äôm your Chinese fans! Thank you so much!,True
@aparnavarshney4970,2021-09-02T03:45:06Z,1,U are funny JoshüòÇ,True
@vigneshvicky6720,2021-08-31T19:15:18Z,2,Love frm indiaüíñüíñ,True
@HannyDart,2021-08-31T08:01:44Z,1,SSoPAAAOm,True
@hussainalasmawi5373,2021-08-31T05:32:35Z,1,BAAAAAM!!,True
@anggipermanaharianja6122,2021-08-28T03:14:02Z,1,Sbam!,True
@seckinbilgic,2021-08-22T22:12:39Z,1,Double Baaam,True
@iftikhar3609,2021-08-20T07:09:04Z,1,Bam!,True
@axeside2110,2021-08-18T07:36:41Z,1,Never feel so smart learning this thing.. thank you..,True
@sandranickita124,2021-08-16T19:43:23Z,1,Learning in a funny way and almost forgetting that I have a heavy exam tomorrow,True
@vinitabaniwal685,2021-08-15T13:19:30Z,1,Thank you very much for such an explanatory Video!,True
@cmfrtblynmb02,2021-08-15T10:58:31Z,0,Isn't naive bayes naive because it ignores joint distributions? I know word ordering is kind of part of joint distribution but I think people may get the wrong idea by just thinking naivety is all about the word ordering.,True
@jaivratsingh9966,2021-08-14T13:12:16Z,1,Guru!,True
@tanyongsheng4561,2021-08-01T06:28:00Z,0,Great video. But I have a confusion ln 4:59. Why use P(N) = 8/(8+4) instead of P(N) =17/(17+7) as the probabilities of the NORMAL message? Is P(N) = 8/(8+4) the data coming from smaller sample dataset depending on each person's probabilities on receiving normal message?,True
@vivekmankar5823,2021-07-31T17:40:59Z,1,"@StatQuest with Josh Starmer, You have literally replied to each and every comment   !!!!!!!!!!!!!!!,   I don't have words to describe your dedication !! TRIPLE TRIPLE  TRIPLE BAM !!",True
@Gauravjhakr1,2021-07-30T06:49:13Z,1,brilliant explanation.,True
@abd_alkader,2021-07-29T10:27:10Z,0,i think the rule of Bayes is P(spam | word ) = (  P(word | spam ) * P(spam) )  / P(word)  .... and you ignored the denumerator ( P(word) ),True
@ushadevialankar6595,2021-07-28T10:26:12Z,1,Ohh my god..how patient u re..rplied to everyone..nd thnku for ur easy explaination..i have an final examination tomorrow...it helps i hope!!üòä,True
@leekev1048,2021-07-26T12:23:38Z,0,"Neat! Thanks for having such a great video.  But I‚Äôm still having a question. While plotting the Bayesian networks of Naive Bayes, why do arrows point out from the dependent value we want to predict to features we have? Since we basically get the output refers to features, I think the direction of arrows should be reversed.",True
@aaishag5309,2021-07-23T11:44:46Z,1,Thank you so much for explaining all the concepts clearly. I love watching all your videos. DOUBLE BAM :),True
@wiczus6102,2021-07-18T14:48:20Z,0,Why are the values multiplied instead of multiplied and summed like P(S)*P(S|Dear)+P(S)*P(S|Money)+... ?,True
@Han-ve8uh,2021-07-17T02:37:08Z,0,"Something I'm confused about is trying to fit this video in a tabular framework. Because this seems like a discrete problem and discrete probability exercises usually study P(X=x|Y=y). I mean like a table with 4 features. X1=Dear X2=Friend X3=Lunch X4=Money, and the values being the count of each word from the bag of words of all Spam mixed, and all Normal mixed.  1. Is this a possible way to model the problem? Would this turn into a gaussian bayes problem where we must estimate mean/std for each X (how does this work? In your other video they are already given). 2. Another thing i don't understand is why is Lunch,M,M,M,M is calculated with power 4 instead of P(Money=4|N/S). Seems like there is something binary about this problem and it is only measuring presence/absence, but it is also not binary, measuring not just presence/absence because money got accounted for 4 times instead of 1. 3. At 11:00, +1 count got added to each word to make lunch non-zero, doesn't this tweak the proportions of Dear:Lunch:Money in 2:1:4 to become 3:2:5. Somehow I have a feeling the proportions should be maintained by adding a different count to each word for this naive bayes theory to work accurately? Or is this not necessary because conclusions will not change in the end when you apply the same hack to all classes and it's computationally easier to add the same number to each word? 4. Why did the Normal messages get black boxes too when the missing word problem was only among the Spam (Lunch word specifically)",True
@alexwang2586,2021-07-17T02:17:03Z,1,Anita,True
@ajkbox,2021-07-13T08:13:14Z,0,Something I did not get: why multiply the probability of the different words? Why not add them? The more matches the higher risk of spam I would imagine.,True
@yusufahmed2233,2021-07-11T01:48:30Z,0,"5:32 for dear friend, he has done p(n) * p(dear | n) * p(friend | n). But what about lunch and money? Shouldn't it be p(n) * p(dear | n) * p(friend | n) * p(not lunch | n) * p(not money | n)? Because the later is what our prof did, so I am confused with the two. Any help would be appreciated. Thanks.",True
@koreanbroadcastarchive306,2021-07-09T01:14:42Z,2,"Josh, you are the greatest thing that happened to me in 2021. Thank you for the walkthrough.",True
@siwarbelhajmohamed4612,2021-07-08T09:53:49Z,1,Thanks  great explanation,True
@nergizahmadova5679,2021-06-29T19:45:21Z,0,"Great Video! I just have one question, that I would be glad if you could answer:). In many books, it says that when the response variable is binary, as the same case span or normal, Bayes classifier corresponds to predicting class one if probability is higher than 0.5. However, in your case, the probabilities were quite low in both classess, so that made me a bit confused:)",True
@sooryaprakash6390,2021-06-28T17:40:12Z,0,"From this video ,I understand that Naive Bayes works very poorly for unbalanced datasets given that they consider prior probability .  Why don't we just don't consider it and achieve better results ? @StatQuest",True
@user-pk8hn6zw8m,2021-06-23T21:48:59Z,1,BAM!,True
@eugeniomercuriali2206,2021-06-21T16:37:51Z,1,Bro I think I love you,True
@marcel7593,2021-06-13T12:05:12Z,0,"Hey, very nice video. Ty very much :)  However, I have one question about the slide at 5:41min: Doesn't it shall be: We can think of 0.09 as the score that Dear Friend gets AND it is a normal message? Because the score to get Dear Friend if it is given that it is a normal message is p(Dear|N)*p(Friend|N). No?",True
@phillipuchen,2021-06-11T01:36:52Z,1,"You deserve to have one million subscriber, too easy to understnad",True
@luisguilhermesilvarodrigue8242,2021-06-08T12:09:02Z,0,"Oh, no! Another dreaded teminology alert! The pseudo count shown after 10:24 to adress the zero probabality problem is also known as additive smoothing or Laplace smoothing!",True
@sephdisclosed,2021-06-08T10:29:48Z,0,"Sorry for this dumb question, why do we say the 'total number of words' is 17 for Normal and 7 for Spam, when there are only 4 there ?",True
@hamedgholami261,2021-06-04T17:39:14Z,1,I love the way you teach things! thanks for being this much helpful.,True
@YawAnsongSnr,2021-06-01T18:40:16Z,0,I know this doesn't make sense but I keep rewinding from @0:12 to @0:00. Good music lol,True
@nyotowijaya7949,2021-05-31T08:46:56Z,1,ty somuch,True
@zahidshoumik8967,2021-05-28T21:37:16Z,1,bip..bip..bip..bip..amazing,True
@dulangikanchana8237,2021-05-28T13:45:43Z,0,"hi, i watch your videos.they are awesome.also can you please do video on min-max normalization,z-transformation,unit vector normalization, log transformation?",True
@dhirendrasingh6071,2021-05-24T16:05:05Z,1,You're amazing!! Keep it up.,True
@TheRony999,2021-05-22T09:27:47Z,1,SPAAAAAM!!!,True
@yuhanglu7391,2021-05-21T09:27:57Z,1,I love you!,True
@BurgerFred1,2021-05-20T20:10:20Z,1,"Missed chance to call the study guides ""Stategy Guides""",True
@AM-is9bi,2021-05-20T06:15:19Z,0,You could've summed all this up in 5 min if you hadn't spoken to your audience like they were children.,True
@kartikanand6348,2021-05-19T19:10:19Z,1,BAM!!,True
@61_shivangbhardwaj46,2021-05-19T05:06:36Z,1,Thnx sir,True
@DarkNevrozz,2021-05-18T22:43:07Z,1,You save my exam thanks,True
@allyourcode,2021-05-17T05:19:42Z,1,I never thought I would be laughing my head off at a stats lecture. You are truly gifted and a gift to this world. Wow.,True
@subhasreegupta8669,2021-05-16T12:44:39Z,1,"Being a Data Science aspirant and having zero knowledge in stat(bcz I m from a tech background), you save me in the middle of the ocean. I owe you everything I learnt till now in statistics. I dont know how to thank you.....Thanks a lot and keep making awesome videos for us...",True
@Jo-tr3ci,2021-05-16T10:13:08Z,1,"Thank you very much. English is not me native language, but your explanation is very good to understand.",True
@user-ff7lg7md9s,2021-05-15T12:53:34Z,1,"Man, You are surely talented! Voice is so good that it was just perfect for me to understand the content. Thank you.",True
@RamsLiff,2021-05-13T04:20:23Z,1,"This video is the best way to explain, by far",True
@Th3Moody,2021-05-11T22:46:30Z,1,He even received sBAM messages,True
@Rahulkumar-wp6ve,2021-05-10T08:23:39Z,1,Most unique and interactive way of teaching!!!!,True
@ShashwatAgarwal007,2021-05-06T14:22:38Z,1,"Man, you taught some parameters which I was looking for why they exist. Thanks.",True
@sarvagyaiitmadras8727,2021-05-05T14:36:26Z,1,wow :),True
@maefiosii,2021-05-04T18:37:30Z,0,"you explained this to me as if i were a learning impaired person, which is a good thing",True
@namratakelkar1102,2021-05-04T14:47:21Z,1,Woahh.. thanks for this!,True
@lmkne3124,2021-05-04T09:15:03Z,1,fantastic video,True
@adanvivero702,2021-05-02T15:58:04Z,0,"4:58, I still don't understand where the 8 comes from....",True
@ahindrilasaha5850,2021-05-02T05:52:58Z,0,How will the conditional probabilities be calculated if we term frequency or TIDF instead of simple count?,True
@kshitizchaurasia5777,2021-05-01T20:09:12Z,0,"Josh, I am working on a project where I need to classify survey comments in multiple categories, I was thinkimg of leveraging Naive Bayes but now after this tutorial I feel for my case I require a model that can also take into account relationship between words and phrases while classifying. Can you suggest me some other model I can use and it will be very helpfup if you have a video on that algorithm. Thanks in advance.",True
@fibowibo5359,2021-04-29T13:35:58Z,0,"Hey Josh, we learned the formula: P(w_k|v_j) + (n_k,j + 1) / (n_j + |VC|) where VC is the vocabulary, n_k,j the number of word w_k in class T_j and n_j is the number of all words in T_j. this seems to differ to what you show in the video? can you clarify on this one please?",True
@sachi5008,2021-04-25T07:29:16Z,1,"Thanks, this is the best explanation of the NBC.",True
@Baron-digit,2021-04-25T06:46:51Z,1,"Mate, someone should dedicate you his firstborn :-)",True
@saiakhil4751,2021-04-22T17:52:02Z,1,It turned from BAM to SPAM in no time... :),True
@karannchew2534,2021-04-14T07:17:00Z,0,"Notes for my future revision.  Use ""Prior Probability"" and ""Conditional Probabilities"" to estimate a new and related probability.  Use ""Prior Probability"" and ""Conditional Probabilities"" to estimate probabilities of a related event being True and False. Yes and No. Spam and Bam.  Prior Probability = Initial guestimate. Conditional Probability 1= Probability of an observation (or data) if event was (or assumed to be) True. Conditional Probability 0 = Probability of an observation (or data) if event was (or assumed to be) False.  Probability of Event being True = Prior Prob x Conditional Prob 1. Probability of Event being False = Prior Prob x Conditional Prob 0.  Compare both and decide (or best guess) event as True or False.  Except that the ""Probability of Event being True"" and ""Probability of Event being False"" aren't the actual probabilities. But they are good enough for the like-for-like compare-to-decide purpose.  Naive: observations (or data) are simply assumed to be independent (eg. relationship among data, influence on the next best guess, or sequence of words is ignore etc). But good enough for the purpose.  Naive Bayes Classification -> high bias, low variance.",True
@ishansahni8361,2021-04-11T14:25:59Z,0,"Cannot thank you enough for this Josh! Crystal clear explanation üôå  I just have one question - If we use all the covariates available for the 'i'th condition, would the sum of all the likelihoods for that condition be 1?",True
@sog2531,2021-04-08T19:30:27Z,2,Lol SPAM~!,True
@python_by_abhishek,2021-04-08T05:08:09Z,1,"When i google for best statistics tutorials, this channel was at top of the list. Thank GOD, it wasn't a spam üòäüòä",True
@sehrishmunawar6281,2021-04-07T05:54:51Z,0,I need these slides,True
@fruitgummy4935,2021-04-06T02:19:20Z,1,"the way free youtube education teaches so much more clearly than my 70k tuition university,,,,,,,,",True
@anitha6g,2021-04-01T07:03:10Z,0,What if there's a new word(say 'dollars') in the new email to be classified. What would the probability of that word be. ü§îü§î,True
@alann346,2021-03-31T02:08:23Z,0,"The words (dear, friend, lunch and money) are attributes or values of the attribute WORDS.",True
@tramynguyen3450,2021-03-29T15:09:44Z,0,I dont really get the point why p(N)xP(Dear|N)xp(Friend|N) = p(N|Dear Friend). Please explain for me. Thank you,True
@duyngoc6760,2021-03-28T03:50:39Z,1,Awesome ! Very clear explained Josh Starmer!!!,True
@aryamohan7533,2021-03-26T17:09:13Z,0,"Bam! This video cleared up a lot for me. Thank you Josh!  Could you do a video on probability calibration and how it varies from Logistic regression models to Naive Bayes models?  I understand the topic is used more to answer prognosticative questions over the diagnostic questions we usually see in a ML problem, but I would love to see ""Probability Calibration, Clearly Explained"" :)",True
@ClosiusBeg,2021-03-23T19:22:24Z,1,"I wanna smoke with you dude, you are awesome! :3",True
@julioribeiro1909,2021-03-20T08:54:48Z,1,baaammm.. easy to teach even 5th grade students,True
@danlufu2310,2021-03-14T00:44:02Z,1,This video answers all my questions! This is indeed clearly explained!,True
@purplegalaxy5265,2021-03-13T19:34:04Z,1,"I was doubtful about going for a masters, but you made me reconsider it!!! Thanks, hope I do pursue masters after all!  Also, your explanation was TRIPLE BAM!! LOVED ITTT!! THANK YOU AGAIN!!",True
@manojrag3059,2021-03-13T15:46:14Z,1,infinite bam!!!!!!!!!!!!!!!,True
@mochamadwahyuhidayat6269,2021-03-11T05:45:53Z,1,"You know what, I'm a newbie and feel enlightened. Subscribed!",True
@amrezz9695,2021-03-11T00:10:12Z,2,"I don't knew how many times I have to say ""Thanks I love You""",True
@mambakind,2021-03-06T23:01:26Z,1,"Finally! I understand SOMETHING that has the word ""Bayes"" in it! Great way to start! Thanks a bunch!",True
@adnanhaybe4551,2021-03-05T16:14:31Z,0,I have a question why did you say 12 messages and why are you dividing the values by 4 i can't seem to understand where you got 4 from when you were calculating the probabilities of normal or spam messages??,True
@chriskang7119,2021-03-04T18:44:35Z,1,"okay, I lost it on ""SPAM!!!"". so good.  edit: now I got the Triple Spam, I'm dead",True
@VernonSwanepoel,2021-03-02T18:46:37Z,1,I know what YouTube channel my son will have to watch to learn stats and probability...   BAM!,True
@jennytamboli5740,2021-03-01T00:02:02Z,4,This was so so so so so sooooooooo amazingg!!!!! I have my oral exam tomorrow and your explanation worked like a charm!! Thank you so so much Josh for creating such a video with crystal clear explanation. It is NOT BORING at all rather very interesting and kept me hooked till the end of the video!! :) Bam Bam Bammm!!!!!,True
@ianscottuk,2021-02-23T21:17:58Z,14,you are genuinely getting me through a masters....,True
@devanshmesson2777,2021-02-22T11:50:25Z,1,"14:04 üòÇüòÇüòÇ. Seriously man, This teaching style is golden.",True
@stamas02,2021-02-22T10:36:03Z,0,"When you calculated the likelihood, did you not forget to include the information we gain from the word's absence in your first toy example? For example, you calculate the likelihood by P(Dear | N)P(Friend | N). Still, I think it should be P(Dear | N)P(Friend | N)P((not) Lunch | N)P((not) Money | N) that would give you the correct likelihood, which is 0.47*0.29*(1-0.18)*(1-0.06) = 0.105 and then the ""score"" that is proportional to the probability is 0.105*0.67. Am I not right here?",True
@malinkata1984,2021-02-19T01:39:47Z,1,This is brilliant! <3,True
@sushrutdhiman1776,2021-02-18T14:33:32Z,0,What if a completely new word appears while testing your model and that word has never been seen by it will it's probability not be 0 then?,True
@KansasFashion,2021-02-18T05:20:42Z,0,"Hi Josh, I love you and I have a question. In your tutorial, Probability of money as normal message is what you showed. However, some other tutorial, people also get the probability of money as spam message. Then they calculate final Probability as 'P(Money | Normal) / (P(Money | Normal) + P(Money | Spam))'. What's the two difference between these two probabilities? For second type of possibility, if money appears four times. How do I calculate probability definded in my second way?",True
@zchasez,2021-02-18T01:30:43Z,0,"Hi Josh, I used the Baysian Theorem explained by 3Blue1Brown to do the normal-spam example here. the result I got is different from yours. my P(N | ""Dear Friend"") = 0.7821 instead of your 0.09. I also noticed that the way I calculated it involves the sum of ""friend"" and ""Dear"" likelihood given they are Normal Messages before they are multipied by the P(N) and divided by the P( all likelihoods of Dear Friends). Why is that?",True
@joaomoreira5794,2021-02-18T01:23:41Z,0,"Hello, i am using your videos to help me study for my masters' exam, by watching them and seeing if i understand my college lectures after, and I have a question. In my college lectures to get the probability P(Normal | Dear Friend) after calculating P(Normal) * P(Dear | Normal) * P(Friend | Normal) we would have to divide the result by P(Evidence), which would be P(Dear) * P(Friend) in this case, however this division is not done in the video. What am I not understanding correctly? Thanks in advance!",True
@tirkdiamond,2021-02-17T06:27:55Z,2,5:32 just for the beep boop beep you get a subscribe! LOL,True
@tymothylim6550,2021-02-17T03:20:18Z,9,"Thank you Josh for this video! It gave me a great and clear understanding of the Naive Bayes Classifier, as well as a good workaround the '0 probability' problem by using alpha!",True
@basantmounir,2021-02-16T20:59:41Z,1,Excellent explanation! Keep going!,True
@Shionita,2021-02-14T18:47:03Z,0,"I was so OMG with the ""lunch money money money money"", that I had to test in Python (I was scared because I've never done that ""extra step"" of ""adding 1""). But everything went well, just using MultinomialNB (from sklearn) to fit the original bag of words (so that the count for lunch is 0 considering all ""spam"" rows), and then predicting ""lunch money money money money"", the prediction was 1 (spam) (0.99186829 probability). I suppose the class methods fix this internally, so that we don't have to add 1 ""manually"", but I couldn't find any info, so I just made the manual test hehe.",True
@jorgitozor,2021-02-13T15:58:08Z,0,3:31 lol,True
@ravenzy6121,2021-02-13T11:28:56Z,0,"Hi, thanks for explaining the concept of naive bayes theory clearly. however, my incompetence didn't understand the part in 5:45 clearly when you said it is technically proportional to the probability of it is a normal message given that it said dear friend. I would appreciate if you can explain it further ^_^ thanks!",True
@BlackSaix66,2021-02-10T15:27:49Z,1,This was outstanding! Thanks for this great great explanation.,True
@ataadi1460,2021-02-08T14:21:44Z,1,SPAM!!!!!!,True
@MiniIsPoland,2021-02-07T17:10:51Z,1,"I suposse now i will pass my exam on studies, thank you so much:)",True
@ruheenqureshi,2021-02-03T16:33:19Z,1,"Your videos are really helpful, easy to understand. I searched for how to use adaboost and now I'm binge watching all your videosüòÇ",True
@justingarzione293,2021-02-02T21:22:46Z,2,double bam,True
@arthurus77,2021-02-01T21:21:04Z,1,thanks,True
@thomasrobatsch2582,2021-02-01T05:28:40Z,0,"At 7:22, can we look at the ratio of the 'proportional probabilities' and say that, given this training data, there is a 90% chance that the test message is not spam and a 10% chance it is spam? Loved the video!",True
@WalyB01,2021-01-28T14:57:02Z,1,"Reading a paper comming accros this think oh help, finding a stat quest. Pfff I willl survive",True
@heteromodal,2021-01-28T09:59:59Z,1,"Wonderfully explained, thank you!",True
@pradipsodha6182,2021-01-27T15:24:08Z,1,perfect !,True
@TheBuzoTechie,2021-01-27T13:29:55Z,1,Nice video :) I‚Äôm planning on using naive bias for a Twitter stock market bot.,True
@federicojosegonzaleztoimil2895,2021-01-21T01:55:23Z,1,I love you StatQuest!,True
@ds5375,2021-01-21T00:45:42Z,0,Nice video. Critique: I think it would have been nice to lay out Bayes theorem and show how each step in the calculation related to each term in Bayes formula.,True
@xuehaizhou5503,2021-01-20T05:42:07Z,1,shameless makes me feel shamefulness...,True
@prithvirajgawande6150,2021-01-18T17:55:49Z,1,I think Lunch MONEY MONEY MONEY MONEY MONEY would make a great song!,True
@chrisspencer6502,2021-01-15T11:30:32Z,0,did you round your answers? for the lunch money question I got 1.562976*10^-6 or  0.000001562976?,True
@usamahussain4461,2021-01-15T08:03:07Z,0,"""technically, it is proportional to the prob that the message is normal, given it says dear friend""...i could not understand that..",True
@kireet5508,2021-01-13T16:29:40Z,2,"Legend says that Josh is ""Bam""ming every comment till date. (That aside, this was probably the best explanation of Naive Bayes I came across)",True
@Virtualexist,2021-01-12T15:36:46Z,0,When Josh said SPAM !! Then and only then --> Hi I am Shamanth and I am an aspiring data scientist... skips the adv.,True
@AmanKumar-jb2mk,2021-01-10T18:42:42Z,1,Yaar that's really awesome‚ù§Ô∏è !!! By watching this 15 min video I am able to clear my hell lots of doubts!,True
@hectorgomez7813,2021-01-06T08:17:49Z,0,What happens if we use the tf-idf instead of the word occurrence? what changes it needs to be made?,True
@nguyenucanh3248,2021-01-05T01:58:46Z,0,"If for example, I have extra problability for the whole word ""dear friend"" to appear and not appear, then how would the equation become. Thank you",True
@olofnuggets5936,2021-01-04T11:30:55Z,1,thx very much u r the best <3,True
@Samuel-wl4fw,2021-01-02T17:49:16Z,1,"Very impressive explanation, I love videos that make me enjoy learning, especially topics I have on exams :D",True
@hellomacha4388,2020-12-31T19:10:50Z,1,Hello Josh Welcome to the new calendar 2021 with a big double Bam,True
@danielbaena4691,2020-12-30T04:55:23Z,1,Thank you so much! üôè,True
@saisrisai9649,2020-12-28T13:03:43Z,0,You sound like Phoebe Buffay :),True
@ericazombie793,2020-12-28T09:40:14Z,0,"I have a question, does the study guide have the same content as the video, or does it have more content?",True
@noahrubin375,2020-12-28T04:56:38Z,1,Quadruple BAM!!,True
@prakharagarwal6237,2020-12-26T11:30:50Z,1,"Thank You, Josh for such an incredible video !!",True
@hatrez907,2020-12-20T22:32:25Z,1,You my boi just earned a subscription.¬† Thank you for you explaining it in such plain and funny way.,True
@varunjindal1520,2020-12-19T10:05:04Z,1,"This is an awesome explanation. I first did some match from Prof. Andrew course and then this, and this made it completely clear.",True
@JainmiahSk,2020-12-18T09:09:10Z,0,Is this Bernoulli Navie Bayes Classifier ?,True
@flel2514,2020-12-15T23:46:05Z,1,great!,True
@Roboticdock,2020-12-14T01:25:47Z,1,Awesome!,True
@ajaytomar369,2020-12-12T06:37:53Z,1,"My uni teachers are great but i miss the music i find here. Also, your content rocks!!!!",True
@jessicafb5398,2020-12-12T02:32:32Z,13,"When it says ""shameless self promotion"" the thought didn't even cross my mind of it being self-promotion, more like ""helpful to the viewer"" :)",True
@bhargavnakkina2568,2020-12-11T16:20:54Z,0,is it a bot?,True
@MarioMedinaaa,2020-12-08T22:09:34Z,0,"thanks you for this video , greeting from Chile uwu",True
@piyushborse3085,2020-12-08T14:27:16Z,2,SPAMM!!!!,True
@DrTurt-uq3kc,2020-12-03T12:46:26Z,1,Sometimes I just really need someone to explain something to me like I'm an idiot.. And then I understand.. And I feel less and more like an idiot,True
@vincent4926,2020-12-01T17:35:24Z,0,Put spped to 1.5 and thank me later,True
@escape0707,2020-12-01T08:30:22Z,0,"Little typo at 9:11, contains rather than countians~",True
@fisherh9111,2020-11-29T04:55:06Z,1,"All this talk of ‚Äúlunch‚Äù, ‚Äúdeer‚Äù and ‚Äúspam‚Äù is making me hungry.",True
@sarmisthadas2617,2020-11-27T16:37:07Z,1,Indeed this is the most easiest and intelligible way to remember. Thank You soooo much.,True
@danielwiczew,2020-11-25T22:16:24Z,0,"I don't understand still, why do we call conditional probability given a class (e.g. Normal) as a Likelihood. Likelihood is probability, that given data support a given statistical model (as far I understand), but it does not fit to Naive Bayes. What do you think @Josh Starmer ? Or maybe it comes from the Bayes's terminology?",True
@Wazzanx,2020-11-24T13:10:10Z,0,"why are we dividing by the number of words and not the number of emails? Isn't the aim to predict whether it is spam or not, and not to predict the next word in the email?",True
@michaelmayor9666,2020-11-23T15:12:35Z,1,thanks I understood now,True
@niazmakhdum3562,2020-11-22T07:13:17Z,0,I am still confused that how did you get 8/(8+4) and 4/(8+4). Can you explain please?,True
@congdatt,2020-11-21T14:03:36Z,0,"Explain why the NB classification belonged to the Bayesian Network type,",True
@avazB,2020-11-20T11:53:39Z,1,SPAAAAM!   You are awesome maaaaaan!,True
@raghavendramani9795,2020-11-20T07:51:52Z,1,thank you sir.,True
@azkarehman8366,2020-11-19T16:26:20Z,1,Tripple Dayymmm ! awesome video,True
@147Ptm,2020-11-18T17:23:20Z,1,i love the shameless self promotion alerts lmao,True
@SnoSixtyTwo,2020-11-17T22:41:05Z,0,"Thank you for your incredibly hard work that must go into these videos. Allow me to make a suggestion: I think the difference between P(Spam | Message) and P(Word | Spam ) is a bit too subtle here and might be missed by viewers. By putting it next to Bayes' theorem, it would be clear that these are different things, but the comparison of scores is valid bc the P(Message) is the same for both Spam and Normal.",True
@Birdsneverfly,2020-11-16T07:21:11Z,1,So clear and concise!,True
@rahuldey6369,2020-11-12T07:31:41Z,1,This was so clearly explained.. So many Bams and spams actually worked. Thanks a lot Josh,True
@ibnuaffan2000,2020-11-12T07:06:47Z,1,"subscribe materials, BAM!",True
@mojtabasardarmehni453,2020-11-11T22:52:26Z,1,"You are the best. Thanks, man!",True
@israahs365,2020-11-11T15:05:21Z,1,CAN'T THANK YOU ENOUGH!,True
@venkatadineshjeedigunta510,2020-11-07T18:38:50Z,1,"Great session, very well explained.. Thanks Josh, you are amazing ..",True
@ersanefesemerci,2020-11-06T14:12:48Z,1,BESTT CHANNELL EVERRR!!!!!!,True
@DuongPham-sm5qc,2020-11-05T04:00:42Z,1,Thank you for this amazing video. Much better than college lectures in any way!,True
@MLGGCrisps,2020-11-03T17:34:40Z,1,"As always, thank you for the video :D",True
@Abhinav-tk1bt,2020-11-02T06:41:54Z,0,Do we add black boxes only when we have a 0 frequency word? Please answer fast,True
@MrBemnet1,2020-11-02T03:15:03Z,0,I have one question Mr Statquest  why don't we get 1 when we add  all posterior probabilities?,True
@thomasgamble5566,2020-11-01T16:02:48Z,1,Big FAT like! üëç,True
@rohitvadlamudi,2020-10-31T23:14:55Z,1,absolute legend.,True
@suyashgupta1180,2020-10-31T20:07:23Z,1,Lucky me! Found an awesome channel for ML. Bammm!!!,True
@josevillalobos547,2020-10-29T14:19:23Z,1,I hope one day I get lucky enough to have a research advisor or mentor of any kind like yourself. All your videos are worth gold. Thanks for the all the time you put in to us.,True
@pavittarkumarazad3259,2020-10-28T20:07:09Z,1,So good and clear. I am so happy to find this channel :),True
@p.kartik4309,2020-10-28T17:06:25Z,1,That's like a cake walk.. will be happy to see if u will make videos on theese algorithms implementation in Python or any relevant frameworks...  But yes ur videos are so impactfull that we can go on practical with theese itself..  Nice intuition :),True
@kurtissac,2020-10-28T14:22:31Z,0,your singing reminds me of fallout 4 nuka world radio,True
@kurtissac,2020-10-28T14:19:34Z,1,bammmmmmmmmmmmmmmmmmmmmmmmm,True
@KansasFashion,2020-10-27T18:52:26Z,1,This is an awesome tutorial! And your voice is hot af!!!!,True
@kanmani294,2020-10-27T07:20:45Z,1,You made it so simple to digest Thanks a lot,True
@jeremyhofmann7034,2020-10-25T15:05:27Z,72,I subscribed immediately after ‚Äúbeep boop beep boop‚Äù.  Honestly this may help me find a love for statistics after a bad college education experience with it in the 90‚Äôs.,True
@Divinemadness2,2020-10-23T19:03:45Z,1,"I normally get a bit annoyed with sponsorships in videos and/or self-plugs for other videos, but you are one channel where I have 0 problem with it. You put out such an amazing product and clearly care so much about the viewers that you can get away with anything haha. Great job on this video! Beautifully explained.",True
@jongwonlee4399,2020-10-19T15:11:47Z,0,Does spam/ham filtering project require the dataset to be labeled?,True
@jakecake51,2020-10-19T02:09:14Z,1,You're great.,True
@mattoh1468,2020-10-16T08:42:03Z,1,Lol I lost it at 5:33: Beep Boo Beep Boo Beep. I was not expecting that in a Naive Bayes tutorial haha. You're pure gold Josh :),True
@mwahmed9519,2020-10-14T04:30:08Z,0,Hi Josh. Can you please create a video on how to use EM algorithm in R,True
@dlargent,2020-10-13T20:46:02Z,1,"Thanks, this is SO GOOD!",True
@namansood1489,2020-10-12T04:46:58Z,0,"Hello, amazing video, Just wanted to understand the concept of the P(Word|Normal). The denominator mentioned was the total number of words in the normal messages. What is the major understanding behind taking it as the denominator. My initial guess was P(Normal) would just be the total number of messages and P(Word) would be the number of messages containing the given word in it.  Thanks a lot",True
@madhuvarun2790,2020-10-11T15:15:06Z,1,I just learned Bayes classifier thanks to you. Keep up the amazing work.,True
@xukiomi,2020-10-09T20:54:23Z,0,you guys can watch this in 1.25x and it will feel natural,True
@howaikeong7768,2020-10-09T08:14:52Z,1,I got no doubt to subscribe you,True
@mansoorbaig9232,2020-10-05T07:45:57Z,1,This is simply awesome! BAM!!,True
@anuragshukla1891,2020-10-04T03:38:21Z,1,"Loved your video, brother. Thank  you",True
@jerodzimmer1781,2020-10-02T15:12:14Z,1,Thank you so so SO much! This is so much more understandable than my professors throwing formulas at me with variables I don't understand. When will profs learn that you can't learn a concept by looking at a definition without an example?,True
@toituxu749,2020-10-01T03:27:36Z,5,"When you explain something complicated to everybody and even a 5-year old kid can understand,  you BAM DAM genius  Thanks DAM much!!",True
@krzysztofkolmus6936,2020-09-30T17:30:31Z,0,Nice explanation :) Could you give an example of the application of Naive Bayes to solve biological problems in R?,True
@jeffery_tang,2020-09-28T13:46:09Z,1,"Dear Friend,  Lunch Money",True
@doyel5,2020-09-26T18:19:41Z,0,"Hi Josh, Thanks a lot for your videos. I follow them regularly. Now, if I can ask a question on this video please. It is regarding the basic naive bayes probability framework. At 1:38, you mention that we need to calculate the probability of seeing each word, given that it was in a normal message. So I am taking event A to be ""A word w appears"" and the event B as ""This is a normal mail"". We need to calculate P(A|B). So the event ""A and B"" = The word w appears in normal mails. So, P(A and B) = # of times word w appears in normal mails / Total no. of word counts in the corpus. Am I right till now? My main question is how is P(It is a normal mail) = Count of words in all normal mails / Total no. of word counts in the corpus. Shouldn't it be P(A normal email) = # of normal emails / Total no of emails in the corpus?  Thanks a lot!",True
@glaswasser,2020-09-24T11:58:04Z,1,"I like how you pronounce ""spam"" similar to ""bam!""",True
@haneulkim4902,2020-09-22T06:03:07Z,0,"Thanks for an amazing video! From my understanding multinomial naive bayes is useful when data are categorical and gaussian naive bayes is useful for continuous data(since we need to distribute them), is it correct?",True
@VarunKumar-pz5si,2020-09-20T17:42:49Z,2,spam BAM.......!!!!!!!!,True
@jakem55531,2020-09-20T00:08:32Z,1,Love your humor!,True
@chandanmishra308,2020-09-19T10:22:57Z,1,BAM! finally understood this topic.  I can't thank you enough. You were such a big help! You are real gold and this channel a gold mine.,True
@avakhonsari3673,2020-09-18T15:52:46Z,0,"just a suggestion, was hoping to see Bayes' Theorem in this video",True
@nvfpv,2020-09-18T05:05:49Z,1,You're total awesomeness,True
@AmanKumar-hr5dc,2020-09-17T02:13:42Z,1,I search any machine Learning or Statistical topic on StatQuest before google,True
@judahgoldfeder3626,2020-09-17T01:00:31Z,0,The reason it is called Naive bayes is because it assumes all the probabilities are independent. It has nothing to do with being a bag of words model.,True
@shivamtyagi9936,2020-09-16T22:56:12Z,1,Waiting to get a job!!!! First thing to do after jobü§îü§îBUY ALL UR SONGSüòâ,True
@omermohammed5838,2020-09-16T20:28:35Z,1,I love your positive vibes,True
@saravananraja6369,2020-09-16T14:12:13Z,1,Triple Spam was surprising! Keep up the good work dude.,True
@huakun,2020-09-15T08:42:07Z,2,Thank You so much. If I saw this video earlier it could save me so much time. I took machine learning courses several months ago and the concepts and terminologies were so confusing. I hope more videos like this are made in the future. Thank you again.,True
@swaralipibose9731,2020-09-14T10:07:26Z,1,Naive bam!!!  You received a new subscriber .  Deserved itüòä,True
@tenmins9355,2020-09-11T07:51:19Z,1,"I love your videos, thank you for making ML easier for students all over the world!!!",True
@anzei331,2020-09-09T09:30:25Z,1,Great tutorial as always! When will you be covering Bayesian Bandits? that would be a great addon. Thanks,True
@yuribruxel6074,2020-09-08T16:05:42Z,0,What are the similarities and differences between the Naive Bayes Classifier and a Hidden Markov Model? Both methods tries to predict and unobservable state given prior probabilities. In HMM the problem is framed as a time series whereas in NB multiple features are used. But they seem fundamentally the same approach for me. Am I missing something?,True
@shubhamlahan,2020-09-05T10:05:24Z,5,"Josh is a Godsend for noobs like me. Why Google it when we've got StatQuest!¬† Please accept my BAM, milord.",True
@swapnilasawa1730,2020-09-04T02:12:56Z,0,"Please correct this, Likelihood doesn't mean P for discrete! The likelihood is a generic term which means P(evidence/hypothesis), or P(data/parameters)",True
@mrunalinarkhede4477,2020-09-03T04:33:46Z,0,Can you please have a video on bernoulli naive bayes,True
@tahirkhalil2471,2020-08-31T17:07:41Z,1,BAM,True
@GamerBat3112,2020-08-30T08:54:23Z,5,The 45 dislikes are from the Nigerian Prince!,True
@sruthiparvatha8790,2020-08-30T05:39:31Z,21,"Thank you so much for this great explanation. You're an awesome teacher, keep up the great content. Can't wait to see more :)",True
@kumijos,2020-08-30T02:39:41Z,0,if the calculation of the probability of normal messages is 0.50 and spam is also 0.50. How do you decide the message normal or spam?,True
@mohitnagarkoti4086,2020-08-27T20:43:01Z,1,great work :),True
@rohansaxena2710,2020-08-27T12:58:21Z,1,"Explanation was very good, loved the Shameless self promotion part too... üòÇ",True
@nathankomer8699,2020-08-26T18:22:52Z,1,"Great introduction, awesome visualization, and some extra BAM! Thank you so much for your hard and very entertaining work!",True
@soruzein2988,2020-08-26T07:05:20Z,0,But how if the result of the probability the massage was Normal and Spam is the same? Which one do we choose?,True
@zerge69,2020-08-25T22:21:55Z,1,I love your pedagogical style: learning advanced stats a la Sesame Street :),True
@RhymesWithPorridge,2020-08-25T19:38:35Z,0,"WTF is this silly ""BAM"" all the time?  Why make it so damn childish??",True
@xinniu3145,2020-08-25T14:23:26Z,0,Thanks for this video but well... I feel a bit awkward by being treated as a 5 years old child LOL,True
@Ajax4Hire,2020-08-24T12:43:04Z,0,"Message that contains ""Dear Friend"" is spam is almost 100%. Good description of Naive Bayes, bad example.",True
@MirGlobalAcademy,2020-08-23T08:04:30Z,2,GOOD,True
@yasseralallawi5515,2020-08-22T20:47:08Z,1,"I like it, when the video started I thought ‚Äúseriously!‚Äù, but it turned out to be really useful and fun",True
@plekkchand,2020-08-20T11:41:04Z,0,"We could do without the word ""Bam"". The format is sufficiently non threatening and informal.",True
@christopherellis2663,2020-08-20T08:37:06Z,1,"Dear Friend, I have an amazing  opportunity for you.... a typical spammer. One would be very Na√Øve to assert otherwise.",True
@diaojun161,2020-08-20T07:01:34Z,1,"I really like the video, so good at using examples",True
@HhhHhh-et5yk,2020-08-19T10:28:11Z,1,Best Teacher ‚ô•Ô∏è,True
@karandewani6614,2020-08-19T07:30:38Z,1,awesome video!!!!!!!! keep up the good work,True
@yanhuang5494,2020-08-18T14:14:20Z,0,"At 5:28, when you wrote ""p(S) x p(Dear | S) x p(Friend | S)"", I guess your intention was to compute ""p(Dear Friend | N) x p(N)"" which is proportionally to ""p(N | Dear Friend)"" by the Bayes formula, and there seems to be an unspoken assumption that p(Dear | N) and p(Friend | N) are independent of each other, so p(Dear Friend | N) = p(Dear | N) x p(Friend | N). Overall, I am grateful of this video, but would hope the above to be explained clearer and more explicitly.",True
@gustavshochat7792,2020-08-18T05:26:44Z,4,"My ""DOUBLE BAM!"" senses started tingling around 12:25 but were abruptly quashed by your ""SPAM!"" sneak attack. So very grateful for the information you provide and the methods by which you deliver.",True
@fawadmahdi,2020-08-17T16:31:11Z,2,Thank you so much. Finally understood naive Bayes theorem,True
@neillunavat,2020-08-15T15:33:32Z,0,"Because of adding a Pseudocount, the total probability is coming as 1.01. Can you elaborate? Edit: The total probability of spam content is coming as 0.99. Uhh...  P.S. I know you have rounded off the values for simplicity but probability cannot be above 1.01. I was cross checking my naive-bayes algorithm in python... That's how I found this out. Thank you for ""Clearly Explaining""... Because of your amazing work, I am able to understand algorithms and write them from scratch! Thanks again!  :D",True
@shashankgpt94,2020-08-15T15:29:36Z,1,12:32 SPAM!!,True
@Laura-jl7vc,2020-08-15T12:10:53Z,2,"best channel I could have find in the last month. Thank you so much. I'm doing my master thesis in machine learning and I've never done anything with it before, and this is helping me so much",True
@itssatvik,2020-08-15T06:46:55Z,0,"SPAMM. Loving your videos Josh...Got the to the point Learning Resource. If possible ,pls do mentions where we can have the implementation of the concept",True
@DarshanRamRox,2020-08-13T04:02:26Z,1,Beep boop beep boop beep,True
@mohammedmunavarbsa573,2020-08-12T13:23:36Z,1,BAM BAM DOUBLE BAM,True
@alfredoespinozapelayo,2020-08-12T05:17:26Z,0,x1.35 video speed the goldilocks zone,True
@nasibnaimi,2020-08-10T14:50:53Z,0,"Dear Josh, is it correct to say that that it is naive because we are considering the different features, or in this case words, to be independent of each other?",True
@venkatasubramaniangopalakr7880,2020-08-08T03:22:47Z,34,Your videos are one of the best out there in the market on YouTube. You explain such complicated technological concepts in very simplified terms that a layman can understand. Thank you for taking the efforts and time to teach ML and stats. You deserve an award! :-),True
@melodic_martialist,2020-08-07T12:20:59Z,1,Subscribed... üòÑ,True
@franciscoaragao9672,2020-08-06T16:34:58Z,1,"You are a great teacher, man. BAM, (Voc√™ √© um grande professor, cara. )",True
@dante2707,2020-08-06T15:03:39Z,0,What happens if the score is the same for both classes? Would the class with higher prior be assigned?,True
@Aditya_Kumar_12_pass,2020-08-06T07:33:45Z,0,war machine: boom! i was looking for this,True
@ichimatsu8422,2020-08-06T06:36:25Z,1,"Your vids are amazing man! I'm getting my degree in Data Science and this channel has been super helpful! I'm a little nervous because the classes are so stats heavy. I hope you keep putting out videos and study guides. I have Statstical Methods II, and Statistical learning I and II, left before I finish my degree. If you could put out vids that cover some of those topics that would be Awesome! Thank you bro!",True
@OElitecorp,2020-08-04T16:12:57Z,1,How haven't I found this before? - Weak sauce awareness for quality resources that's how... Great vid,True
@rhn122,2020-08-02T06:14:41Z,0,When Oversimplified become a Statistics YouTube channel,True
@vivekkalyanarangan9629,2020-08-02T01:53:34Z,1,"Hey Josh, any chance you have LIME/SHAP/Explainable AI lined up? The way you explain it will be perfect for understanding these topics - even the research paper doesn‚Äôt give the step by step for LIME & SP-LIME... thanks a ton in advance!",True
@jonathanisakov6469,2020-08-01T19:42:15Z,0,"hi josh, thanks for the video helped allot in my course.  i made it into a python script, I'm hoping you'll use it to help others understand the subject if they need some more help. https://github.com/jonisakov/naive-bayes",True
@sandeepkumawat4982,2020-07-30T21:19:57Z,2,Shameless  Self Promotion.üòÇüòÇ,True
@nastarankianersi104,2020-07-30T11:51:47Z,4,Oh my God learning has never been more fun. And when you say clearly explained you mean we can understand both the words and the meaning so thank you! :D,True
@tanishadas9420,2020-07-29T14:52:19Z,1,I wish you  made videos about every topic under the sun so that it would be as easy as stats and ml,True
@kristophert541,2020-07-29T13:19:32Z,1,You just save my ass again. Thank you so much StatQuest!,True
@khineyinmon9323,2020-07-28T14:52:34Z,1,"Thank you very much for an enjoyable explanation!  By the way, what is ""Bam""??",True
@mengyaowang6300,2020-07-28T12:10:20Z,1,"""Ohh~No~!""",True
@ramanjaneyuluthanniru1428,2020-07-27T09:33:46Z,0,"sir, please provide some notes..for future reading,,, writing notes is taking more time.. unable to watching all videos..getting bore while writing notes...  please provide some notes for revision purpose... mail id: ram04420@gmail.com  üôèüôèüôèüôèüôèüôèüôèüôè    Thanks",True
@MatheusAlves-uy7yt,2020-07-27T03:54:15Z,1,"Josh thanks for the content, mainly for the subtitles cause i am from Brazil and this helps a lot.",True
@kashyapbarua7578,2020-07-26T18:09:33Z,0,"Josh. Please do Bayesian Approach for A/B Test. I have gone through numerous resources on the internet, I am very confident that you would be able to teach that far better than the other materials.",True
@Geza_Molnar_,2020-07-26T14:51:50Z,0,"Hi, great and patient explanation! At the beginning of the video, when you calculate the values for the histogram - Aren't those values 'distributions', as they are calculated from the sample? - Then those calculated 'distributions' are used as (estimates of) 'probabilities', well, because there is no better value at the moment that woukd estimate the real values of the probabilites. (It's about the phrases -and the underlying concept-, and maybe I want to be more accurate than enough accurate :-) )",True
@prof_shixo,2020-07-25T14:52:57Z,1,"Nice one! I would just add a missing ***terminology alert*** about why the Naive Bayes algorithm is naive? to say it assumes i.i.d (independent and identically distributed) samples, which is not suitable for sequences such as the ones in language models. Usually, that's what an interviewer would want to hear in an ML/data science interview.",True
@UzairKhaskheli,2020-07-25T11:44:54Z,1,Subscribed! My teacher explained that in a very confusing way. Thanks to you <3,True
@joicejoseph2176,2020-07-24T17:21:50Z,1,"Josh Starmer, God among men.",True
@cguser,2020-07-24T12:05:44Z,2,you know good things gonna happen when he said BAM!,True
@rafareis5957,2020-07-23T19:01:36Z,1,"I'm at home during lock down, studying machine learning through StatQuest yeah!!",True
@shubhamgupta6567,2020-07-22T18:07:15Z,0,Could you please suggest any good document for Bayesian network. Thanks in advance üòä,True
@mingcui2426,2020-07-22T03:36:48Z,1,funny and informative!,True
@sidisu,2020-07-20T22:23:35Z,1,"Are you missing dividing by P(word) in your examples? Without adding that component, my probabilities for both pos/neg are 0 since my documents are long.",True
@srujananeelam8487,2020-07-20T07:23:45Z,1,"Hi Josh, I have learned  a great deal in machine learning primarily through your videos.The way you simplify things shows your command on the subject.Thank You Very Much!!I have request,Can you start explaining the concepts of Deep Learning as well(Neural Networks).",True
@rafsananwar3308,2020-07-19T07:39:57Z,1,Shameless self promotion ü§£ü§£ü§£,True
@wew6438,2020-07-18T14:46:40Z,1,"the level of your understanding is astonishing, our children in the future will thank you for your work.",True
@LynN-he7he,2020-07-18T03:35:10Z,1,I had to switch browsers because I'm not signed in to Youtube on the previous browser just so I could like and subscribe to your page. Thank you for breaking it down in a way that would stick! And for the sporadic chuckles while watching this video!,True
@prathameshpradipdatar2003,2020-07-15T06:46:43Z,1,"Josh, cannot thank you enough!",True
@yacoubhossam7542,2020-07-13T19:07:12Z,1,NICe,True
@shubhamsarin2263,2020-07-12T07:50:32Z,1,Your videos are the best thing to have happened during this lock-down (I've lost my job and trying to utilize this time to learn more and more). Thank u so much.,True
@akshatshrivastava4524,2020-07-12T07:00:37Z,3,"I am so lucky , I found this channel, at very start of my Data Science Journey. Thanks Josh.  Psst - I am gonna buy your study guides. Hope that is the least I can do to support your effort :-)",True
@tomas5296,2020-07-12T02:14:24Z,1,i love u,True
@samirkhan6195,2020-07-11T20:48:42Z,1,You are my favorite Teacher since i can watch you even when my mind full of stress.,True
@akshaykothari9946,2020-07-10T20:05:42Z,1,Awesome explanation Keep it up !!,True
@soumojitchowdhury,2020-07-10T18:31:33Z,1,Great example. Thank you good Sir :),True
@JFDA5458,2020-07-10T09:57:22Z,0,"Is the number 17 the total number of words in each message, or is it the number of times specific words of interests (""Dear"", ""friend"" etc) occur in each of the 12 messages?",True
@liranzaidman1610,2020-07-10T09:54:27Z,1,F.A.N.T.A.S.T.I.C,True
@abhishekm4996,2020-07-09T17:30:57Z,3,I enjoyed it and learnt....Even a kid can clearly understand your video provided the basic probability stuff taught in his/her school...,True
@jiunyen5586,2020-07-09T03:23:02Z,54,"The second I saw that we're gonna talking about spam, I waited for you to say ""SPAAAAMMMM"" like a 5 yo child. And you deliver as always! Thank you!",True
@pana1599,2020-07-07T11:27:04Z,1,hi Josh. how to understand  the part that we have 17message at the start but at 4:51 the message is 12,True
@gowthamprabhu122,2020-07-07T10:29:40Z,2,This data set is considered discret yes?,True
@jankova0013,2020-07-06T19:44:04Z,1,When you make the histograms you get all of the words that occurred in all of the messages and count them in each category? That's why is lunch 0?,True
@ashishagrawal4906,2020-07-05T22:09:36Z,2,This is the first tutorial related to Machine Learning I found that is actually clear :),True
@chhabiacharya307,2020-07-05T14:24:15Z,1,Double SPAM!!!,True
@akulmaheshwari977,2020-07-05T08:09:51Z,2,This explanation was insane thanks Josh,True
@AKMailing,2020-07-04T06:01:04Z,1,"Thanks but sorry... While your superb explanation of the theory, the Q that I'm left with... is it naeeve or nave bayse?",True
@dwj489,2020-07-03T14:02:21Z,2,BAMMMMMMM,True
@abdosoliman,2020-07-03T02:22:45Z,7,"dude, you should never ever stop making videos üòÇüòÇüòÇ",True
@albert_chen,2020-07-03T00:35:58Z,3,I started listening to your music and I was like:  This is the same dude who makes machine learning tutorials. Lmao.,True
@albert_chen,2020-07-02T20:23:05Z,7,I just stared machine learning and your songs just bring a smile to my face.,True
@sairohithpasham,2020-07-02T08:04:32Z,2,"man, Thats some awesome explanation right there.  wish my teacher was as cool as you.",True
@nermienkhalifa5997,2020-06-30T22:52:46Z,2,"to the point, thank you!",True
@ritikparida3104,2020-06-30T10:21:36Z,1,When are you  going to make videos for Deep Learning?  I always search YT for a Neural Net StatQuest but sadly never find one!,True
@mrifat13,2020-06-30T06:53:28Z,2,what is statquest? sorry just my curiousity :D edit: ooh nvrmind ahahah,True
@RealSlimShady7,2020-06-29T07:02:01Z,1,"TRIPLE SPAM!!! For the first time, I have understood this topic so well. NB is so confusing because it is not taught the right way. With you, its different :D This made me read more the alpha value (which we kept 1 as default, we could have kept it anything, like 0.01). Is this a hyperparameter and is it also valid in Gaussian Distribution NB? I mean from a programming perspective, the ideal way of finding alpha would be to do Grid Search and find it out?",True
@kiranchowdary8100,2020-06-28T15:18:29Z,13,I didn't expect that spammm at 12:33 instead of bam I am like literally baaammm,True
@shanthgaitonde,2020-06-28T09:30:25Z,0,"How is this multinomial and not binomial since we have two classes, spam and ham respectively?",True
@ShermanSitter,2020-06-27T18:10:23Z,0,"at 5:50, isn't that likelihood technically the probability that it is normal given that it includes the words ""Dear"" and ""friend"" and not the phrase ""Dear Friend""? This is probably what you meant, but just thought it would be worth it to clarify for your viewers. I like that you are making these videos btw. Statistics understanding is so important for modern society.",True
@franciszhang1809,2020-06-27T14:33:45Z,3,"I don't understand why still people dislike such a great video....well, triple spam to them",True
@magelauditore333,2020-06-26T16:07:18Z,2,You are legen- wait for it -dary.,True
@TheVintagePiano,2020-06-26T14:30:53Z,1,"Every single time I click on your video, I immediately like it because of the jingle at the beginning :)",True
@ethiomachineleaning8487,2020-06-26T09:56:06Z,1,it is interesting explanation!!! did you have quest about ensemble method?,True
@dinariad5117,2020-06-26T04:53:18Z,2,"I think I am missing something here. Could you please explain to me why when we calculated the ""prior probability"" we used '8' to be the total number of normal messages and '4' to be the total number of spam messages while at the beginning when we calculated the ""likelihood probabilities"" the total number of normal messages was '17' and '7' for spam messages?",True
@nicolascortegosovissio2824,2020-06-25T09:45:18Z,8,"Great and funny explanation! Just love the terminology alerts: ""Oh nooooo"", keep going please!",True
@hasnainkhan2959,2020-06-24T20:30:44Z,2,"Thank you so much, tomorrow is my exam and thank god I found your channel, you explained naive bayes classifier in 15 minutes and my teacher couldn't explain it very well in one hour. Thank you sir.",True
@chetanhireholi,2020-06-24T07:54:58Z,2,BAM! Thanks for the simplest explanation present on the internet! Double BAM!,True
@kushagrakumar1282,2020-06-23T18:28:00Z,0,could you please explain why do we multiply with overall probability during the calculation. for e.g. P(N) * P( Dear | N ) * P( Friend | N ). Why P(N) in the multiplication factor?,True
@gayathridevi311,2020-06-20T16:18:50Z,0,"Hi Josh , I am a fan of your videos , obviously you !! You have made learning experience as a joyful one. I have a question. When we calculate  probability of individual words , after adding alpha , don't we need  to  increase total word count  from 11 to 12?",True
@auzaluis,2020-06-18T13:30:35Z,1,"Who the f...didn't like this video? Let's be honest, envy!",True
@euseikodak,2020-06-17T22:25:19Z,1,"Man, I loved your language in this video!",True
@hellasleepypal,2020-06-17T15:40:31Z,1,"Shaaamelesss Self Promotion! It‚Äôs all gravy baby but you gotta lead the fish to the water, am it right? This guy gets it...",True
@monishajayanmoni4135,2020-06-17T12:29:07Z,2,I went through 2 tutorials but never understood naive bayes clearly. But after seeing ur video I understood it clearly. Thanku so much!,True
@zhichensong986,2020-06-17T06:54:33Z,996,"Honestly Josh, I feel like you could teach machine learning to a 5yr old. This is amazing content, thank you.",True
@jazzenasombrado330,2020-06-17T05:40:07Z,1,SPAM! :),True
@hlamzar,2020-06-16T11:25:47Z,2,wow thank you man this was a great educational video. now i can avoid my lecturer's 3 hour online lecture on this,True
@BeSharpInCSharp,2020-06-16T04:12:59Z,2,Thank you. You are wonderful teacher.,True
@BeSharpInCSharp,2020-06-16T04:10:36Z,0,Why do't we add alpha to dear in the normal message?,True
@arnoyang4017,2020-06-15T10:52:53Z,157,"This tutorial is actually better than my college lecture, paid $$$ for a paper of degree, sucks",True
@tanyaneeraj8798,2020-06-15T07:26:32Z,14,"well, I have an exam tomorrow, and I am binge watching statquest videos and loving the songs and the animated explanation. who says higher-level things need to be boring. Thank you statquest. BAM!",True
@aakashkhadka3835,2020-06-13T11:01:17Z,2,Just in time. Thanks for the video,True
@bangnguyen9412,2020-06-12T17:52:15Z,1,"Hi Josh. I've just discovered your channel and it was amazing!!!! Every video is so easy to understand and entertaining at the same time! I just wanted to clarify about the word ""histograms"" used in this video; all of the graphs used seem more like bar charts for categorical variable (words). The x-axis of these graphs are not continuous so I wouldn't consider them histograms. Is there any reason why you call them histograms?  Thanks!",True
@carlosalexramossouza7439,2020-06-12T17:16:01Z,3,I don¬¥t know why someone can deslike a great video like that. I say great because it is very well explained and very well produced.,True
@dec13666,2020-06-12T17:07:50Z,81,"You know when the author of the video cares about their viewers, when you see him replying *every single comment* on this section",True
@karanbhuva7733,2020-06-11T18:31:03Z,1,Best exampleüëåüëåüëåüëå,True
@muhammadsaad8531,2020-06-11T15:47:16Z,1,So naive ...,True
@PRSHNTTKMR,2020-06-11T13:28:33Z,2,That is an awesome clear and simple explanation worth a big thumbs up üëçüëç,True
@stushapiro2745,2020-06-11T11:02:45Z,2,Great video!,True
@sudarshanbandyopadhyay9052,2020-06-11T07:52:39Z,3,"Such nice explanation of the concept, from now onward will follow this channel to learn more stats and probability concepts.",True
@dleivam,2020-06-11T02:36:50Z,2,"Awesome video, thanks",True
@ladgrove,2020-06-11T02:33:06Z,2,Beep Boop beep Boop made me lol,True
@franciscoaragao5398,2020-06-10T06:26:51Z,2,Muito bom,True
@tooljerk666,2020-06-09T18:13:16Z,0,So using alpha = 0 is sort of like the bias term in neural networks?,True
@AOLFlyersNewsletters,2020-06-09T16:43:10Z,6,Josh - this was so good!  Awesome job.  I went to your site to see Study Guides. Do you have package price?  That way we get all.  And maybe in future come up with a visual study book focussed on ML topics.,True
@ABC2691,2020-06-09T07:08:28Z,1,The worst part about this video is that the song lasts for only about 15 seconds hahahaha..  :D :P loads of love to you Mr. Starmer from India <3,True
@adiflorense1477,2020-06-09T04:19:00Z,0,"11:42 sir, why total number of message is 7+4 not 8+4",True
@adiflorense1477,2020-06-09T04:09:49Z,6,7:31 DOUBLE BAM!!! .cool explanation sir.. i just understood  term prior probability after seeing this,True
@icp133,2020-06-08T20:17:47Z,1,how does this only have this many views?,True
@ssshukla26,2020-06-08T05:15:12Z,4,"Somebody, give this man a SPAM!!!",True
@NoOffenseAnimation,2020-06-07T21:51:19Z,2,Money money money money money....  SPAM!!!,True
@BM-uf4pp,2020-06-07T01:57:00Z,1,Just an amazing explanation,True
@anirudhsilverking5761,2020-06-07T01:35:12Z,1,"OMG YES! I WAS LOOKING FOR THIS AND HOPING A VIDEO FROM YOU!! FUCK YEA, YOU UPLOADED 3 DAYS AGO JUST WHEN I NEEDED, GODSENTTT",True
@archerd3344,2020-06-06T22:02:32Z,1,"I have a question. Bayes formula says: P(A|B)=P(B|A) P(A)/P(B). In your example for P(Dear|N), shouldn't there be a denominator for P (Dear)? Also, shouldn't the P(N) be the number of words in N over the total number of words in N, rather than messages? Thanks",True
@jiheonlee4065,2020-06-06T06:00:38Z,2,Triple SPAMMMMMMM !!!! :),True
@barankaplan4308,2020-06-05T15:42:32Z,2,i don't know what to say... YOU HAVE A PURE TEACHING ABILITY,True
@xiaoweijie952,2020-06-05T14:17:36Z,1,"where can I purchase your study guide, could you share a link?",True
@sachinarthanatt4638,2020-06-05T07:53:51Z,0,Can someone help me with the probability? P(Money|Normal) should be  1/8 right? and P(Not Money|Normal) should be 7/8.,True
@jiaweitang5560,2020-06-04T22:44:49Z,1,"Shameless self-promotion, I like it",True
@eternalnik6578,2020-06-04T09:10:13Z,2,Thank/love you donators for this video and Josh. Maybe I will become rich one day because of u. <3,True
@reactorscience,2020-06-03T21:32:40Z,2,"Amazing sir. ""Shameless self promotion"". ü§£ü§£ü§£",True
@naveenkalhan95,2020-06-03T21:16:43Z,49,after seeing this video I feel like there are only 1503 (total views) people in the world who know what Naive Bayes truly means!!! plus Josh Starmer included ... thank you for your great work,True
@TheCJD89,2020-06-03T19:34:19Z,15,"Not sure why you say ""oh no"" before a Terminology Alert. They are usually quite helpful :)",True
@ezp721,2020-06-03T19:11:36Z,5,"Oh No!! Privacy alert!! ...since the algorithm needs to read the email to detect spam hahaha. Thanks Josh, great explanation!",True
@Velzen5,2020-06-03T16:45:35Z,2,Wait till the spammers know how your filter works!,True
@alecvan7143,2020-06-03T16:36:25Z,2,"Amazing video as per usual Josh, wow ! Loving the sections of the video by the way :)",True
@Loachie90,2020-06-03T16:22:32Z,18,"Just when you expect Bam, you get Spam...",True
@yanghe5948,2020-06-03T15:53:27Z,2,I have never been this excited!  Quest on!,True
@initdialog,2020-06-03T15:52:51Z,4,Neuronal networks next? Bam?,True
@edmundchan8923,2020-06-03T15:20:56Z,3,Simple and clear! Like it!,True
@Vivaswaan.,2020-06-03T15:19:55Z,4,"What you teach is very helpful, but how you teach is what makes you the best. What a beautiful way of teaching! comprehensive explanation and all the creative lines/ wit you put here and there.",True
@sa89879,2020-06-03T13:49:03Z,2,nice great work,True
@unchilenoypython1379,2020-06-03T13:48:03Z,2,I love your content!!!,True
@aikimark1955,2020-06-03T13:42:23Z,2,"sp: ""countains""",True
@cvryn7,2020-06-03T13:30:02Z,2,"Thanks a lot. Amazing content. Anyway, waiting for your intro song with tabla(drums). I saw them sitting behind you on a table in one of your promotional videos.",True
@NarendraWicaksono,2020-06-03T13:18:51Z,2,This man is Chef John of machine learning. BAM!,True
@thatrand0mnpc,2020-06-03T13:02:58Z,2,Triple BAM!!,True
@imshafay,2020-06-03T12:55:17Z,3,Your number1 fan here XOXO,True
@ccuny1,2020-06-03T12:52:24Z,2,Great treatment of Naive Bayes for people like me. Thank you Josh.,True
@petercourt,2020-06-02T11:02:58Z,8,"Wonderfully clear and helpful, thanks for all your hard work making this Josh! :D",True
@statquest,2020-05-29T11:58:13Z,98,"NOTE: This StatQuest is sponsored by JADBIO. Just Add Data, and their automatic machine learning algorithms will do all of the work for you. For more details, see: https://bit.ly/3bxtheb BAM!   Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
