author,updated_at,like_count,text,public
@statquest,2021-09-11T13:53:59Z,13,"The full Neural Networks playlist, from the basics to deep learning, is here: https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@nickmishkin4162,2024-05-31T01:45:08Z,0,"Nice video. If ReLU always outputs a positive number, how can the neural network produce a negative sloping curve?",True
@RomaineGangaram,2024-05-05T01:54:30Z,1,Bro you are a genius. Much love from South Africa. Soon i will be able to buy your stuff. You deserve it,True
@mahmutarican,2024-05-02T17:07:32Z,0,Please put an end to the songs in th beginning kind sir üòä,True
@nandakumar8936,2024-04-24T20:33:02Z,1,'at least it's ok with me' - all we need for peace of mind,True
@user-et8es9vg5z,2024-03-22T06:09:03Z,1,"As always everything is very clear but I still don't understand why is RELU function is currently the most effective function in machine learning. I mean, the shape seems so less natural than for the softplus function for instance.",True
@r0cketRacoon,2024-03-14T08:01:37Z,1,could you do another video on back propagation with 2 hidden layers combined with relu functions ? I really love ur visualization,True
@HarryKeightley,2024-03-10T20:42:17Z,0,Thank you for the very clear explanations - this video series is wonderful in its capacity to communicate complex topics in a very clear and understandable manner. I have a question: does the use of the ReLu function for this network result in a less accurate model overall due to its piecewise linearity? It seems the more elegant curve has been replaced by a more simplistic linear-looking triangle. Would this mean a larger network would be needed in order for the model to be more accurate - so that the non-linear relationship between dosage and effectiveness can be modelled more accurately through a more complete/complex interaction of nodes?,True
@discotecc,2024-02-21T17:39:37Z,2,The theoretical simplicity of deep learning is a beautiful thing,True
@juliusasherp.austria3667,2024-02-21T01:32:09Z,0,The only things i understand are the BAM and the Double BAM,True
@user-mz6lb2gy4t,2024-02-06T14:17:29Z,0,why do we use relu for the output layer?,True
@omerutkuerzengin3061,2024-02-01T11:00:23Z,1,Du bist toll!,True
@user-ry4nw1ez4e,2024-01-26T08:13:47Z,1,"Plz do more shameless promotion , I liked them only by you and only you",True
@manojsingh-nj7rl,2023-11-19T20:50:59Z,1,"Please brother don't  say shamless self promotion.....ita really worth more than any other youtuber present out thier and sell courses....Love you brother ...stay healthy and stay safe for usüòä  EVERYONE LETS DONATE HIM  SOMETHING , EVEN SMALL FROM EACH OF US COULD BE GREAT HELP FOR HIM AND HIS EFFORTS Please pin this.....COMMENT",True
@epistemophilicmetalhead9454,2023-11-14T07:59:15Z,1,note regarding relu: derivative at 0 is not defined so we assume the derivative at 0 o be either 0 or 1.,True
@sgrimm7346,2023-10-19T15:48:39Z,0,"So, in other words, if the output is anything less than zero, Relu outputs a 0. If it's 0 or above, the output is that value. Ex: -.49=0, .15=.15 or whatever, .5=.5, -15=0. Is this correct?",True
@whataquirkyguy,2023-09-08T03:30:01Z,2,t i n y  b a m .,True
@MrFindmethere,2023-09-07T14:41:06Z,0,How do we add the resulting graphs together,True
@Red_Toucan,2023-08-24T22:07:39Z,3,"I was struggling (well, I still am) with understanding activation functions and these videos are helping me a lot. And man, you answer every single comment, even in old videos. Thanks a lot and many bams from Argentina :D .  There's one thing that I did't quite get, and maybe this is reviewed in the next episodes.  I understand activation functions are used to introduce ""non-linearity"" to predictions, but to me they still seem very arbitrary. I mean, why would I, for example, with ReLU in mind, keep positive values and change negative values to 0? Am I not losing a lot of information there?  I know when it comes to deep learning sometimes the answer is along the lines of ""because some dude tried it 10 years ago and it worked. Here's a paper discussing it"" but I'd still like to ask.",True
@hozaifas4811,2023-08-20T10:45:31Z,1,Your explanation deserves a huge bam! that's great man,True
@Vanadium404,2023-07-30T09:19:57Z,2,That SoftPlus toilet paper and the Chain Rule sound effect in every video lol,True
@QuranKarreem,2023-07-20T15:39:06Z,1,"Very good explanation ,especially when you talked about the relu function which is not differentiable  keep up the great work brother",True
@user-jx7wp1ur2s,2023-07-15T11:16:26Z,0,dude your videos are simply very cringe .,True
@speedtent,2023-07-09T11:31:25Z,1,You saved my life thank you from korea,True
@chenzeping9603,2023-06-24T05:49:37Z,0,"if you add activation funct to the final layer, doesn't it restrict the possible output values? (i.e. if you add relu to the last layer before the output, doesn't it restrict outputs to pos? similarly if you use sigmoid, doesnt it restrict it to between [0, 1]",True
@Luxcium,2023-06-20T21:05:16Z,0,*I am already familiar with Neural Networks Part One* üòÇüòÇüòÇ So I this my quest will start here so it‚Äôs time to _Start Quest_ This time my quest is leading me to the *ReLU in Action* then I will unwind and back propagar üéâthe *Recurrent Neural Networks (RRNs)‚Ä¶* I will then learn What is ¬´¬†*Seq2Seq*¬†¬ªbut I must go watch *Long Short Term-Memory* I think I will have to check out the quest also *Word Embedding and Word2Vec‚Ä¶* and then I will be happy to come back to learn with Josh üòÖ I am impatient to learn *Attention for Neural Networks* _Clearly Explained_,True
@iiilllii140,2023-05-30T07:34:25Z,7,"This is such a nice and clear visualization of how activation functions work inside a neural network, and a perfect way to remember the inner workings. This is a masterpiece!  Before that I knew, apply an input, activation functions, etc, etc, and you will receive an output with a magic value. But now I have a much more deeper understanding of WHY we are applying these activation functions / different activation functions.",True
@flockenlp1,2023-05-19T12:27:57Z,0,"Hi, are you planning on making a video on Radial Basis Function Networks and self-organizing maps? Especially with self-organizing maps it's very hard to find good ressources, at least so far I found nothing that could really help me wrap my head around this topic. I figured, since this seem like your kind of topic and you have a hand for explaining these things in an easy to understand way there'd be no harm in asking :)",True
@Bbdu75yg,2023-05-06T10:44:39Z,1,Wow!,True
@seanlynch4354,2023-05-03T01:31:51Z,0,"I have a question, will they y output in a ReLU function always be the same as the x input if the, x input is greater than 0?",True
@Xayuap,2023-04-17T22:41:13Z,2,a tiny bam is just a declaration of humility,True
@jennycotan7080,2023-04-15T07:18:41Z,1,You said that ReLU sounds like a robot. My personification of this function is actually a robot who is simple-minded when solving problems! Coincidence!,True
@_1jay,2023-04-05T03:49:09Z,3,Another banger,True
@user-ko9ri6ve9x,2023-03-31T13:36:12Z,0,"me: how to pronounce ReLU chatgpt: ReLU (Rectified Linear Unit) is usually pronounced as ""ray-loo"" or ""ree-loo"".  me: how about ""rel-u"" chatgpt: ""Rel-u"" is also an acceptable pronunciation of ReLU. However, ""ray-loo"" or ""ree-loo"" is more commonly used.",True
@peki_ooooooo,2023-03-10T01:07:37Z,0,Why does the ReLU function design like this?,True
@ifargantech,2023-03-05T18:56:01Z,1,I always expect your intro music... I like it. Your content is also satisfying. Thank you!,True
@raul825able,2023-02-18T12:54:33Z,2,Thanks Josh!!! It's such a fun to learn machine learning from your videos.,True
@Intaberna986,2023-02-13T17:17:36Z,1,"God bless you, I mean it.",True
@carzetonao,2023-01-04T21:26:46Z,1,Really like your video and r shirt is nice,True
@anshulbisht4130,2022-11-09T05:00:25Z,0,hey josh m how u added blue and orange line @5.24. i mean how that blue line in -y axis came in +y axis ( we need to multiply it by something which is not shown in video ) ?.. hopefully u reply soon :),True
@anirudhsingh9025,2022-10-24T08:36:05Z,0,i have been watching your quest for NN for past few days and the way you explain is good but i didn't get one thing that you said about adding 2 lines on a graph ? So how do we add 2 lines on a graph and finde a third curve?,True
@Jerry-lp3uj,2022-10-04T09:50:40Z,1,Tiny Bam ü§£,True
@tagoreji2143,2022-09-29T05:31:45Z,1,thank you Professor,True
@hemantrawat1576,2022-08-17T14:39:03Z,1,I really like the intro of the video statquest....,True
@maryamsajid8400,2022-08-02T20:27:00Z,1,amazing job... understood clearly... now i don't have to search more for ReLU :D,True
@alrzhr,2022-08-01T07:00:07Z,1,biriliant,True
@lisun7158,2022-07-08T18:34:38Z,3,"[Notes excerpt from this video] 7:10 The reason why ReLu works. -- Like other activation function, the weights and bias on the connection slice, flip and stretch the function image into new shape. 7:40 The method to solve the problem that the derivative of ReLu function is not defined at bent point (0,0). -- Manually define the derivative to be 0 or 1.",True
@muzammelmokhtar6498,2022-06-06T10:37:08Z,0,"Great video, but i dont really understand about the curvy and bent ReLU on the last part of the video..",True
@adamoja4295,2022-05-28T08:01:53Z,1,That was very satisfying,True
@kanwarmuhammadareeb1335,2022-05-06T18:26:59Z,1,So good!,True
@slkslk7841,2022-05-01T12:29:32Z,0,At 7:45 could you please tell why gradient descent wouldn't work for a bent line? Gradient descent videos didn't help clear this doubt.   Amazing video btw! Thanks,True
@nikachachua5712,2022-03-30T13:41:04Z,0,how would that green line fit the data if we dont apply relu on last node ?,True
@edmalynpacanor7601,2022-03-13T15:50:21Z,1,Not skipping ads for my guy Josh,True
@chaoukimachreki6422,2022-02-12T11:04:22Z,1,Just awesome...,True
@Abhi-qi6wm,2022-02-01T05:30:37Z,0,There is an error at 3:15 because you change the dosage from 0.2 to 0.1 in the equation.,True
@superk9059,2022-01-31T12:45:41Z,1,Awsome,True
@BowlingBowlingParkin,2022-01-08T11:31:55Z,1,AMAZING!,True
@sohambasu660,2021-12-25T00:36:21Z,0,"I really the great content you make that helps us to understand such difficult topics. Also, if you kindly include the formula generally used for the concept and break them down in the video, it would be immensely useful.  Thanks anyways.",True
@abbastailor3501,2021-12-24T06:03:10Z,1,Take me to your leader Josh ü§≤,True
@adriangabriel3219,2021-12-19T14:27:58Z,1,Why does it make sense to use a ReLu at the end? Is it to reduce the complexity of the green squiggle from a curvy to a pointy squiggle?,True
@maliknauman3566,2021-12-09T19:13:37Z,3,Google should give you award for spreading knowledge to us all...,True
@rafibasha4145,2021-11-24T07:13:12Z,0,"Hi Josh,thank you for the excellent videos ..how the input data splitted across 2 hidden neurons",True
@6866yash,2021-10-31T14:14:14Z,1,You are a godsend :'),True
@harishbattula2672,2021-09-15T13:45:07Z,1,Thank you for the explanation.,True
@drzl,2021-09-11T15:31:02Z,1,"Thank you, this helped me with an assignment",True
@adityams1659,2021-08-27T13:28:09Z,1,*this video/ most of his videos have less than 100K views!!??* *ppl are missing out on a gold mine!*,True
@julescesar4779,2021-07-29T08:33:17Z,1,‚ù§,True
@surajjoshi3433,2021-07-26T13:54:54Z,1,"I was just gone mad by overthinking ,how neural networks work with relu activation function and even I wasn't able to get the real feeel of neural networks but u did it , amazing!! man just amazing,  From where do you learn such things I searched all over the internet but didn't got anykind of examples like this ,Is there anybook you would like to recommend to us which you use ? :)",True
@user-rt6wc9vt1p,2021-07-24T03:21:21Z,2,"How would we deal with the derivative of this function? I've read that the derivative is 0 for x < 0 and 1 for x > 0, but I'm having issues in that when weights are initialized below zero, (something like -0.5), the derivative of the activation function is 0. The chain rule would then make the entire gradient for that weight 0, and the weight would just never change.",True
@wojpaw5362,2021-07-22T15:38:24Z,1,OMG  - CLEAREST EXPLANATION OF RELU ON THE PLANET!!! PLEASE TEACH ME EVERYTHING YOU KNOW,True
@victorialeigh2726,2021-07-16T14:45:36Z,2,That shameless self promotion lol!,True
@Odiskis1,2021-07-13T14:23:08Z,1,How do we know values won't become really high above 0? I though the activation function contained the values in both negative and positive directions so that they wouldn't explode. Is that not a problem?,True
@ramyagoka9693,2021-07-12T07:41:57Z,1,Thank you sir so much for such an clear explanation,True
@vijaykumarlokhande1607,2021-06-23T07:58:15Z,0,"I think this video was expected to be on backpropagation, not on relu.",True
@brianmalone6484,2021-06-19T16:14:00Z,0,"I am more concerned that you pronounce ""axis"" like ""access"".",True
@makefly3305,2021-05-29T07:07:09Z,1,I died when I saw the toilet paper.,True
@pelocku1234,2021-05-28T19:40:59Z,0,"Is the derivative of SSR with respect to w1         -2(obs - pred) * ifelse(y1 * w3 + y2 * w4 + b3 <= 0 , 0, w3) * ifelse(x1 <=0, 0, 1) * input?   The ifelse is ifelse(if this is true, do this, else do this). All of these should have an i as a subscript as well. I feel like there is something that I am missing, and if this is too complicated of a question for the comment section, no worries. I was able to get a loop to do the backpropagation for the last couple of videos, but I am having trouble with this one.",True
@jamasica5839,2021-05-25T09:27:09Z,0,"With ReLU life is easier, you don't have to computing complicated THE CHAIN RULE :D Great series!!! I finally get it because of you Josh!",True
@FloraSora,2021-05-17T06:23:58Z,2,I love the toilet paper image for softplus... didn't catch it on the first watch but it became more and more suspicious as I went through this a few times... LOL.,True
@ofek8280,2021-05-08T15:24:12Z,2,Shameless Self Promotion is the funniest thing Ive seen in youtube!,True
@heplaysguitar1090,2021-05-08T06:41:14Z,7,"I come here every time I learn some new concept to understand it clearly.  Thanks a ton!!  Would really love to jam with you someday for the intros, and maybe we can call it a BAMMING session.",True
@Okkyou,2021-04-26T13:01:44Z,0,"Does that mean, I can chose  any activation function?",True
@naf7540,2021-04-05T20:12:07Z,92,"This is just so crystal clear and must have taken you some time to really deconstruct in order to explain it, really fantastic, thank you Josh!",True
@prasadphatak1503,2021-03-21T06:08:33Z,3,Tiny bam üòÇ omfg I couldn't stop laughing. It's like Ryan Reynolds is explaining Neural Networks üòÇ,True
@hongyichen8369,2021-03-10T22:49:00Z,2,"Hi, there are a lot of activation functions like relu and tanh etc..., can you make a video about the usage of different activation functions?",True
@dengzhonghan5125,2021-02-27T09:31:45Z,0,Can u also talk about CNN and RNN? U r my favorite teacher.,True
@TrungNguyen-ib9mz,2021-02-18T13:39:42Z,0,"Great video!! But might you explain more about how to estimate w1,w2,b1,b2,...? Thank you!",True
@aswink112,2021-02-12T15:55:11Z,9,"My mind is blowing. Triple Bam. Josh Starmer - a great thanks to you for making such amazing videos, educating others for free of cost.",True
@bibiworm,2021-02-09T21:22:32Z,1,Could you shed some light on the advantage and disadvantage of Relu vs Soft Plus please? Thank you. I didn't know there was soft plus until this video. lol,True
@bibiworm,2021-02-09T19:51:15Z,1,"at 8:17, could you please explain in details why it does not matter that RELU is bent? How does it related to gradient vanishing/exploding? The reason I am asking is that if like you said we can get around the non-differentiability at bent point by setting gradient to 0, then it leads to gradient vanishing during back propagation, right? Thanks.",True
@bibiworm,2021-02-09T19:44:29Z,4,"I like how you explain affine transformation rotates, scales and flips activation function with vivid illustration. Now I can relate to Lecun's deep learning class, which talks about this in abstract matrix form. Thanks.",True
,2021-02-02T20:17:30Z,1,"I gave ""like"" before watched it...",True
@user-pf3qf4cu7f,2021-02-02T05:45:34Z,2,Bip boop bip boop.,True
@onemanshow3274,2021-01-27T16:14:27Z,1,"Hey Josh, Can you please please make videos on Recurrent Neural Network and Transformers",True
@anishtadev2678,2021-01-24T10:50:04Z,2,Thank you Sir,True
@ThePanagiotisvm,2021-01-21T12:11:20Z,1,It's only me that I didn't understand where the values of weights and bias come from? Why for example the first weight w1=1.70?,True
@yacinerouizi844,2021-01-08T15:19:01Z,1,thnak you,True
@MaxDiscere,2021-01-05T13:14:38Z,1,It would be a good tutorial but I have to cringe all the time if I see theseüòÇüòÇüòÖ,True
@darshdesai2754,2021-01-02T21:57:11Z,1,"Hey Josh! Amazing content - as always. I have always found your videos to be very useful in understanding the fundamental ideas, rather than just accepting the 'theoretical definitions'. I just wanted throw out a suggestion that it would be great if you could collaborate with other open source/free for all learning mediums like Khan Academy. This would not only increase the viewer base for all open source platforms but it would also fill in the gaps where the content on your channel or their channel has not been created yet. ",True
@RubenMartinezCuella,2021-01-01T18:49:13Z,3,"Hey Josh, here is a topic you may be interested in making a video about. It is very relevant and I feel like not many videos in the web explain it:  - Which are the hyperparameters affecting a NN and what is the intuition behind each of them. Most packages (e.g. caret) run a grid of models with all combinations of parameters you have specified, but it gets very computationally expensive pretty easily. It would be great to learn about some of the intuition behind in order to feed that grid something better than random guesses.   Let me know what you think about this topic, and thanks again for your great job.",True
@kihongkim8726,2020-12-30T14:55:58Z,0,I want  about cross entropy !!!!!!!!!!!!!,True
@firattamur1682,2020-12-25T12:44:51Z,4,"Hi, I was really excited when I saw you start to neural networks after your great machine learning videos. Can you create a playlist for neural networks as you did for machine learning? It is easier to follow with playlists. Thanks",True
@sankalpchenna3355,2020-12-16T07:10:49Z,2,next video plzzz,True
@nkristianschmidt,2020-12-11T16:33:42Z,0,a selection of songs available https://music.youtube.com/browse/VLPLueprIarfLd9uawtXM7oNQ6ppFCfQN4sW,True
@matthewlee2405,2020-12-08T16:00:12Z,2,"Thank you very¬†much ¬†Starmer, very clear and great video! Thank you!",True
@user-bf6xb8xx8r,2020-12-04T01:20:56Z,5,How could you make difficult Machine Learning contents so easy? Incredible!,True
@mainakray6452,2020-12-03T12:59:46Z,2,your explanation makes things so simple ...,True
@user-oj6uc1kv6u,2020-12-02T09:30:14Z,0,Can you make a video about Recursive Feature Elimination ? I like your video style.,True
@miriza2,2020-12-02T00:11:08Z,2,Triple BAM!!! üí• üí• üí•,True
@MrAlb3rtazzo,2020-12-01T16:41:10Z,65,"every time a go the bathroom, and I use ""soft plus "" I think about neural nets again and this accelerates my learning process :)",True
@nonalcoho,2020-11-30T05:11:19Z,2,"Learning math is becoming sooooo easy and funny with your effort!  Thank you so MUCH! BAM~~~~~~ If possible,  can you make a video about the ""gradient vanishing problem"" in the future~?",True
@nirajpattnaik6294,2020-11-29T12:35:46Z,2,Awesome.. Awaiting CNN from SQ ..,True
@alfadhelboudaia1935,2020-11-28T20:59:39Z,2,"Hi, you are really awesome, would appreciate it, if you do a video on the (Maximum a posterior estimation (MAP))?",True
@user-bz8nm6eb6g,2020-11-25T05:01:09Z,2,Wow Wow,True
@ProEray,2020-11-24T20:16:45Z,6,I desperately need a recurrent neural networks video :'(,True
@sandmanjack948,2020-11-24T07:36:10Z,0,"Sir, I am an undergraduate student and one of your fans in China. Your video reference to Neural Networks enlighten me a lot, may I reproduce these video to bilibili.com and share with my classmates?(many of them can't watch video in youtube  :(  ) I promise that the source will be indicated!",True
@alimehrabifard1830,2020-11-23T21:35:17Z,23,"Awesome guy, Awesome channel, Awesome video, TRIPLE BAM!!!",True
@mounikyerusu5058,2020-11-23T20:07:57Z,1,"Josh, could you please do a video on NLP and its implementation in python?. Would really love that.  And about the video, it is awesome as always!",True
@DThorn619,2020-11-23T17:57:06Z,96,"Just to help with promotion the study guides he posts on his site only cost $3.00 and they are immensely helpful to refer back to. It's like having his entire video condensed into a handy step by step guide as a PDF. Yes, you could just watch the video over and over but this way you help Josh continue making great content for us at the cost of a cup of coffee.",True
@williambertolasi1055,2020-11-23T10:48:02Z,4,Good explanation. It is interesting to see how the ReLU is used to gradually refine the function that defines the probabilistic output.   Looking at how the ReLU is used reminds me of the use of diodes (with an approximate characteristic curve) in electronic circuits.,True
@ML-jx5zo,2020-11-23T10:06:40Z,2,Again a appreciation for u,True
@Anujkumar-my1wi,2020-11-23T09:17:56Z,1,i want to know that whether weights in neural network are linear relation between input and nonlinear output or is it something else?,True
@vishaltyagi5000,2020-11-23T09:12:45Z,1,"Hi really love your work. Any plans on doing the Recurrent Neural networks including the modern RNN units (LSTM, GRU)?",True
@ashutosh-porwal,2020-11-23T09:11:15Z,7,The way you explain is on another level sir..Thanksüôè,True
@arkobanerjee009,2020-11-22T09:16:40Z,3,Brilliant as usual. Is there an SQ on softmax activation function in the pipeline?,True
@kiranchowdary8100,2020-11-18T17:32:05Z,3,,True
