author,updated_at,like_count,text,public
@statquest,2023-06-05T10:51:46Z,8,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@brunocotrim2415,2024-05-16T00:24:54Z,1,"Hello Statquest, I would like to say Thank You for the amazing job, this content helped me understand a lot how Attention works, specially because visual things help me understand better, and the way you join the visual explanation with the verbal one while keeping it interesting is on another level, Amazing work<3",True
@sreerajnr689,2024-05-05T12:44:12Z,0,"Your explanation is AMAZING AS ALWAYS!! I have 1 doubt. Do we do the attention calculation only on the final layer? For example, if there are 2 layers in encoder and 2 layers in decoder, we use only the outputs from 2nd layer of encoder and 2nd layer of decoder for attention estimation, right?",True
@benmelis4117,2024-04-19T19:40:35Z,1,"I just wanna let you know that this series is absolutely amazing. So far, as you can see, I've made it to the 89th video, guess that's something. Now it's getting serious tho. Again, love what you're doing here man!!! Thanks!!",True
@ncjanardhan,2024-04-11T23:23:15Z,2,The BEST explanation of Attention models!! Kudos & Thanks üòä,True
@bencordell1965,2024-04-03T08:01:02Z,0,I hate it when it says clearly explained and I don't understand any of it,True
@Mars.2024,2024-04-01T17:55:35Z,0,"Hi :) Thanks for your great nlp playlist, but .  .  .  I still couldn't understand the concept of attention, lstm and encoder_decoder, rnnÿåetc :( All is vague to me. Not clear to understand. Would you Please introduce one or more conceptual  reference  about these  chapter ? That would be great .",True
@VadimChes,2024-03-29T13:32:37Z,0,"As far as I can understand, Attention is not only for Encoder-Decoder approach, and not only for text translation/understanding. I saw a paper where Attention is used as the first step of timeseries prediction model with LSTM. Maybe somebody could give me a link or something to learn that use case of Attention mechanism too?",True
@rafaeljuniorize,2024-03-26T22:54:37Z,1,"this was the most beautiful explanation that i ever had in my entire life, thank you!",True
@akashat1836,2024-03-25T02:30:23Z,1,"Hey Josh! Firstly, Thank you so much for this amazing content!! I can always count on your videos for a better explanation!   I have one quick clarification to make. Before the fully dense layer. The first two numbers we get are from the [scaled(input1-cell1) + scaled(input2-cell1) ] and [scaled(input1-cell2) + scaled(input2-cell2) ] right?    And the other two numbers are from the outputs of the decoder, right?",True
@orlandopalmeira623,2024-03-21T23:33:29Z,0,"Hello, I have a doubt. The initialization of the cell state and hidden state of the decoder is a context vector that is the representation (generated by encoder) of the entire sentence (input)? And what about each hidden state (from encoder) used in decoder? Are they stored somehow? Thanks!!!",True
@user-zt8vw7df4f,2024-03-08T16:16:32Z,0,"Sorry I can not quite understood,   1. why the output of decoding (0.9, 0.4) could plug in the attention values (-0.3,0.3)? What if the total length of them is not four? for example if I have 3 decoding output values and 3 attention values, the total length of fc layer is six unequal to the sequence length 4.  2. What does ""Do some math"" mean? how (-0.3, 0.3, 0.9, 0.4) became (-0.7,4.7,-2,-2), why the maximum 0.9 correspond to -2 ?",True
@dvm509,2024-03-02T00:15:46Z,0,"It makes negative sense to me at 13:49, I can see why you get -0.3 which is -0.76 * 0.4, but what is the calculation for 0.3 after that i multiply 0.01 * 0.6 and add it to -0.3 it wouldn't be 0.3.",True
@lunamita,2024-02-25T21:08:25Z,1,"Can‚Äôt thank enough for this guy helped me get my master degree in AI back in 2022, now I‚Äôm working as a data scientist and still kept going back to your videos.",True
@hujosh8693,2024-02-17T05:06:25Z,0,why is the first output of decoder is <eos>? is that correct?,True
@boriscrisp518,2024-02-14T10:30:58Z,0,nice video.. would be better if it wasn't aimed at 6 year olds,True
@juliank7408,2024-01-29T14:43:00Z,1,"Phew! Lots of things in this model, my brain feels a bit overloaded, haha But thanks! Might have to rewatch this",True
@Kamael2077,2024-01-16T15:35:04Z,0,"even though it's obvious that he spent a lot of time making these cute characters and interesting explanations, the most essential contents stil appeared to be very confusing for listeners. lol",True
@andrybratun7064,2024-01-10T22:46:49Z,0,What is difference between attention and self-attention?,True
@zlatanmessi2095,2024-01-10T12:59:54Z,1,Added on my AI Playlist,True
@ahmedmostafaismaelabdelmaq5277,2024-01-10T09:52:01Z,0,"i have a suggestion  , Make real Example by words and get their number and compute to result ,....",True
@sinamon6296,2024-01-01T21:08:01Z,3,"Hi mr josh, just wanna say that there is literally no one that makes it so easy for me to understand such complicated concepts. Thank you ! once I get a job I will make sure to give you guru dakshina! (meaning, an offering from students to their teachers)",True
@shaktisd,2023-12-18T13:55:32Z,0,"I have one fundamental question related to how attention model learns, so basically higher attention score is given to those pairs of word  which have higher softmax (Q.K) similarity score. Now the question is how relationship in the sentence ""The cat didn't climb the tree as it was too tall"" is calculated and it knows that in this case ""it"" refers to tree and not ""cat"" . Is it from large content of data that the model reads helps it in distinguishing the difference ?",True
@Betonn711,2023-12-05T21:53:45Z,1,BAM!!,True
@yoshidasan4780,2023-11-30T07:16:30Z,1,first of all thanks a lot Josh! you made it way too understandable for us and i would be forever grateful to you for this !! Have a nice time! and can you please upload videos on Bidirectional LSTM and BERT?,True
@gordongoodwin6279,2023-11-26T21:37:25Z,1,"fun fact - if your vectors are scaled/mean-centered, cosine similarity is geometrically equivalent to the pearson correlation, and the dotproduct is the same as the covariance (un-scaled correlation).",True
@rishabhsoni,2023-11-16T12:34:15Z,0,"Superb Videos. One question, is the fully connected layer just simply the softmax layer, there is no hidden layer with weights (meaning no weights are learned)?",True
@aoliveira_,2023-11-13T15:31:42Z,1,"This author has some interesting explanations  but the bams, etc are an annoyance.",True
@luvxxb,2023-11-08T07:43:58Z,1,thank you so much for making these great materials,True
@vohiepthanh9692,2023-11-06T14:24:41Z,1,BAM!,True
@patrikszepesi2903,2023-10-23T10:34:05Z,0,"Hi, great video. At 13:49 can you please explain how you get -.3 and 0.3 for the input to the fully connected? THank you",True
@ezrachua1317,2023-10-19T16:31:22Z,1,bam,True
@sunnywell264,2023-09-30T07:19:17Z,1,"Hi @statquest / @Josh ... This is an amazing video and i had been going through your content. All of those content are some of the best explanations of AI that I have seen till date. In this video towards the end where we are setting the input values of the fully connected layer, i am not able to place the values besides the value of one of the attention value. Please confirm below if I am right:  Value from Encoder Layer: let's : -0.76(1st LSTM) | 0.75(2nd LSTM) go: 0.01(1st LSTM) | -0.01(2nd LSTM)  Value from Decoder Layer: EOS: 0.91(1st LSTM) | 0.38(2nd LSTM)  Similarity Scores: Lets and EOS : (0.91 X -0.76) + (0.38 X 0.75) = -0.6916 + 0.285 = -0.4066 ~ -0.41 go and EOS: (0.91 X 0.01) + (0.38 X -0.01) = 0.0091 + -0.0038 = 0.0053 ~ -0.01   After Softmax Lets and EOS: 0.4 go and EOS: 0.6  Attention Value for 1st LSTM which is rolled twice(for lets and go): -0.76*0.4 + 0.01*06 = -0.298 ~ -0.3 0.75*0.4 + -0.01*0.6 = 0.3 - 0.06 = 0.24  Thus we get the following input values for the fully connected layer: 1. Value from 1st LSTM Layer(Decoder) -> EOS: 0.91 2. Attention Value for 1st LSTM Layer(Encode) wrt EOS -> -0.3 I suppose the following two values are what we get from 2nd LSTM layer which has a different initial values for initial Short term memory and Long Term memory: 3. Value from 2nd LSTM Layer(Decoder) -> EOS: 0.4  Let me know if my understanding is correct Josh.",True
@tangt304,2023-09-27T01:20:34Z,0,"Another awesome video! Josh, will you plan to talk about BERT? Thank you!",True
@jarsal_firahel,2023-09-24T19:10:55Z,1,"Before, I was dumb, ""guitar"" But now, people say I'm smart ""guitar"" What is changed ? ""guitar""  Now I watch.....  StatQueeeeeest ! ""guitar guitar""",True
@usser-505,2023-09-21T14:05:34Z,2,The end is a classic cliffhanger for the series. You talk about how we don't need the LSTMs and I wait for an entire summer for transformers. Good job! :),True
@aayush1204,2023-09-19T13:09:16Z,2,"1 million subscribers INCOMING!!! Also huge thanks to Josh for providing such insightful videos. These videos really make everything easy to understand, I was trying to understand Attention and BAM!! found this gem.",True
@Murattheoz,2023-09-15T12:58:01Z,8,I feel like I am watching a cartoon as a kid. :),True
@mehmeterenbulut6076,2023-09-13T16:20:33Z,2,"I was stunned when you start the video with a catch jingle man, cheers :D",True
@handsomemehdi3445,2023-09-12T07:21:44Z,0,"Hello, Thank you for the video, but I am so confused that some terms introduced in original 'Attention is All You Need' paper were not mentioned in video, for example, keys, values, and queries. Furthermore, in the paper, authors don't talk about cosine similarity and LSTM application. Can you please clarify this case a little bit much better?",True
@Anonymous-tm7jp,2023-09-10T16:19:12Z,0,"I can relate to Squatch so muchüòÖ. If he would have been a real person, he would have been a great friend of mineüòÅ",True
@hasansoufan,2023-09-03T11:36:35Z,1,Thanks,True
@MelUgaddan,2023-08-29T13:24:44Z,6,The level of explainability from this video is top-notch. I always watch your video first to grasp the concept then do the implementation on my own. Thank you so much for this work !,True
@navidghasemi9685,2023-08-24T14:42:34Z,1,great,True
@miladafrasiabi5499,2023-08-22T21:01:13Z,0,"Thank you for the awesome video. I have a question. What does the similarity score entails in reality? I assume that the Ws and Bs are being optimized by backpropagation in order to give larger positive values to synonyms, close to 0 values to unrelated words and large negative values to antonyms. Is this a right assumption?",True
@PratikShetty-sv9ig,2023-08-22T06:31:29Z,0,We are comparing the score then shoulnt we divide the denominator then?,True
@Thepando20,2023-08-19T02:18:44Z,0,"Hi, great video SQ as always! I had the same question as @manuelcortes1835 and I understand that the encodings are the LSTM outputs. However, in 9:02 the outputs are 0.91 and 0.38, maybe I am missing something here?",True
@bestsagittarius7925,2023-08-13T11:20:34Z,0,"Frankly, Josh , if take view of Transformer Self-attention, this video seems meaninglessful because Self-attention can do much  more  better than what you mentioned. If so, why we need to take this lesson?",True
@gahbor,2023-08-03T02:22:07Z,0,how does the content of this video compare to https://youtu.be/zxQyTK8quyY ? im about to watch both regardless,True
@kickcomedyfun4059,2023-07-29T03:20:13Z,1,"Bad content, Focus on content, clear theory and not on sarcasm. It isn't helping",True
@tupaiadhikari,2023-07-28T13:13:45Z,1,Thanks Professor Josh for such a great tutorial ! It was very informative !,True
@tupaiadhikari,2023-07-28T13:11:00Z,0,At 13:38 are we Concatenating the output of the attention values and the output of the decoder LSTM for the translated word (EOS in this case) and then using a weights of dimensions (4*4) to convert into a dimension 4 pre Softmax output?,True
@sciboy123,2023-07-26T20:15:18Z,0,I had a little confusion about the final fully connected layer. It takes in separate attention values for each input word. But doesn't  this mean that the dimension of the input depends on how many input words there are (thus it would be difficult to generalize for arbitrarily long sentences)? Did I misunderstand something?,True
@yizhou6877,2023-07-23T14:37:07Z,2,I am always amazed by your tutorials! Thanks. And when we can expect the transformer tutorial to be uploaded?,True
@abdullahbinkhaledshovo4969,2023-07-22T13:27:12Z,1,I have been waiting for this for a long time,True
@Rykurex,2023-07-21T20:58:05Z,0,Do you have any courses with start-to-finish projects for people who are only just getting interested in machine learning? Your explanations on the mathematical concepts has been great and I'd be more than happy to pay for a course  that implements some of these concepts into real world examples,True
@michaelbwin752,2023-07-19T19:19:03Z,0,Thank you for this explanation. But my question is how with backprogation are the weights and bias adjusted in such a model like this. if you could explain that i would deeply appreciate it.,True
@vinisilva5647,2023-07-19T01:39:52Z,1,it would not be possible to translate the other older videos you explain very well.‚ù§,True
@saschahomeier3973,2023-07-17T10:50:54Z,1,"You have a talent for explaining these things in a straightforward way. Love your videos. You have no video about Transformers yet, right?",True
@capyk5455,2023-07-17T08:32:35Z,1,"You're amazing Josh, thank you so much for all this content <3.",True
@sushi666,2023-07-15T19:26:02Z,0,"A video on Siamese Networks would be cool, esp. Siamese BERT-Networks",True
@arvinprince918,2023-07-13T14:05:50Z,0,"hey there josh @statquest, your videos are really awsome and super helpful, thus i was wondering when will your video for transformer model come out",True
@elmehditalbi8972,2023-07-13T09:11:50Z,0,Could you do a video about Bert? Architectures like these can be very helpful on NLP and I think a lot of folks will benefit from that :),True
@elmehditalbi8972,2023-07-11T13:16:22Z,0,Will there be a video about transformers?,True
@Travel-Invest-Repeat,2023-07-08T03:32:14Z,8,"Great work, Josh! Listening to my deep learning lectures and reading papers become way easier after watching your videoes, because you explain the big picture and the context so well!! Eagerly waiting for the transformers video!",True
@okay730,2023-07-08T01:08:15Z,2,"I'm excited for the video about transformers. Thank you Josh, your videos are extremely helpful",True
@umutnacak,2023-07-06T14:40:44Z,0,"Great videos! So after watching technical videos I think complicating the math has no effect on removing bias from the model. In the future one can find a model with self-encoder-soft-attention-direct-decoder you name it, but it's still garbage in garbage out. Do you think there is a way to plug a fairness/bias filter to the layers so instead of trying to filter the output of the model you just don't produce unfair output? It's like preventing a disease instead of looking for a cure. Obviously I'm not an expert and just trying to get a direction for my personal ethics research out of this naive question. Thanks!",True
@Sarifmen,2023-07-05T16:16:47Z,0,13:15 so the attention for EOS is just 1 number (per LSTM cell) which combines references to all the input words?,True
@theelysium1597,2023-07-04T17:44:57Z,0,Since you asked for video suggestions in another video: A video about the EM and Mean Shift algorithm would be great!,True
@midolion8510,2023-07-03T20:45:18Z,1,You made me really excited for transformers üòÖ,True
@weiyingwang2533,2023-07-02T14:18:26Z,1,You are amazing! The best explanation I've ever found on Youtube.,True
@clockent,2023-07-01T12:27:11Z,0,"One thing that eludes me, after watching the video once again is, on what basis can we compare hidden states of encoder and decoder. Why are they comparable at all? I understand we can compare word embeddings, but hidden states?",True
@koofumkim4571,2023-06-29T01:09:05Z,55,"‚ÄúStatquest is all you need‚Äù ‚Äî I really needed this video for my NLP course but glad it‚Äôs out now. I got an A+ for the course, your precious videos helped a lot!",True
@owlrion,2023-06-28T23:30:56Z,1,"Hey! Great video, this is really helping me with neural networks at the university, do we have a date for when the transformer video comes out?",True
@nogur9,2023-06-28T07:31:57Z,1,Thank you very much!,True
@SharingFists,2023-06-28T02:56:20Z,4,This channel is pure gold. I'm a machine learning and deep learning student.,True
@thanhtrungnguyen8387,2023-06-27T03:06:04Z,1,can't wait for the next StatQuest,True
@ArpitAnand-yd7tr,2023-06-26T18:56:45Z,1,Really looking forward to your explanation of Transformers!!!,True
@ArpitAnand-yd7tr,2023-06-26T18:30:20Z,2,The best explanation of Attention that I have come across so far ... Thanks a bunch‚ù§,True
@guillermosainzzarate5110,2023-06-26T16:15:07Z,1,"Y ahora en espa√±ol? Casi no lo creo, este canal es increibleüò≠ muchas gracias por tus videos !!!",True
@coldbrewed8308,2023-06-25T06:57:46Z,0,Hello! Can we get a video on Gaussian Processes? many thanks!!!,True
@gnorts_mr_alien,2023-06-22T04:18:05Z,0,waiting really hard for the transformer video.,True
@jana5384,2023-06-21T01:48:34Z,0,omg! where's the transformers video? my test is tomorrow ahhaha,True
@Luxcium,2023-06-20T20:36:11Z,0,Oups üôä What is ¬´¬†*Seq2Seq*¬†¬ª I think I will have to check out the quest and then I will be happy to come back to learn with Josh üòÖ I am impatient to learn *Attention for Neural Networks* _Clearly Explained_,True
@hasansayeed3309,2023-06-20T19:33:51Z,1,Amazing video Josh! Waiting for the transformer video. Hopefully it'll come out soon. Thanks for everything!,True
@alexfeng75,2023-06-19T18:10:21Z,0,"Fantastic video, indeed! Is the attention described in the video the same as in the attention paper? I didn't see the mention of QKV in the video and would like to know whether it was omitted to simplify or by mistake.",True
@tantzer6113,2023-06-19T17:30:25Z,0,Most videos on attention and the transformers start with recurrent neural networks and ignore LSTMs.  I don‚Äôt know enough to understand the pros and cons of this choice.,True
@clockent,2023-06-18T20:04:18Z,19,"This is awesome mate, can't wait for the next installment! Your tutorials are indispensable!",True
@seifeddineidani3256,2023-06-17T15:40:52Z,1,"Thanks josh, great video! ‚ù§I hope you upload the transformer video soon :)",True
@shofyansky,2023-06-16T14:14:25Z,1,cannot wait to transformer explanation.,True
@madjohnshaft,2023-06-16T00:19:58Z,1,I am currently taking the AI cert program from MIT - I thank you for your channel,True
@imkgb27,2023-06-15T13:16:08Z,2,"Many thanks for your great video!  I have a question. You said that we calculate the similarity score between 'go' and EOS (11:30). But I think the vector (0.01,-0.10) is the context vector for ""let's go"" instead of ""go"" since the input includes the output for 'Let's' as well as the embedding vector for 'go'. It seems that the similarity score between 'go' and EOS is actually the similarity score between ""let's go"" and EOS. Please make it clear!",True
@abrahammahanaim3859,2023-06-15T10:24:13Z,1,Hey Josh your explanation is easy to understand. Thanks,True
@oskarikaadinugroho1332,2023-06-15T05:22:12Z,1,please explain GNN or GCN,True
@SimoneVenturin,2023-06-14T16:34:25Z,0,When Transformer? üòç,True
@Fahhne,2023-06-14T15:20:56Z,1,"Nice video, can't wait for the video about transformers (I imagine it will be the next one?)",True
@Opinionman2,2023-06-14T14:45:50Z,1,You are the best!,True
@sagardesai1253,2023-06-14T11:29:43Z,1,great video thanks,True
@andrewsiah,2023-06-13T00:25:50Z,1,Can't wait for the transformer video!,True
@MartinGonzalez-wn4nr,2023-06-12T20:11:39Z,4,"Hi Josh, I just bought your books, Its amazing the way that you explain complex things, read the papers after wach your videos is easier. NOTE: waiting for the video of transformes",True
@user-fj2qq7cp2n,2023-06-12T14:09:24Z,1,Thank you very much for your explanation! You are always super clear. Will the transformer video be out soon? I have a natural language processing exam in a week and I just NEED your explanation to go through them üòÇ,True
@aizazkhan5439,2023-06-11T21:05:55Z,2,Anyone from MSc. CL IMS Stuttgart? I am leaving this comment for you peeps,True
@kaixuan5236,2023-06-11T07:10:51Z,0,Can you do videos on transformer network and multi-head attention? Love your vids!,True
@user-rs4bc8ij1d,2023-06-10T10:16:41Z,1,We love your video,True
@miro6470,2023-06-09T23:54:17Z,1,"People are going to change the world with the videos that you are making. Isn't crazy that you are literally chaging the curse of human history? like, for real.",True
@silver_soul98,2023-06-09T18:36:33Z,0,another great video. any idea when the transformer video coming out?,True
@notyet1213,2023-06-09T17:42:51Z,1,Thanks!,True
@Sergio.Freitas,2023-06-09T12:50:36Z,1,Valeu!,True
@nikolamarkovic9906,2023-06-08T23:11:23Z,1,for this video attention is all you need,True
@frogloki882,2023-06-08T20:53:03Z,2,Another BAM!,True
@abdullahhashmi654,2023-06-08T18:20:18Z,1,"Been wanting this video for so long, gonna watch it soon!",True
@jacobverrey4075,2023-06-08T10:01:08Z,1,"Josh -  I've read the original papers and countless online explanations, and this stuff never makes sense to me. You are the one and only reason as to why I understand machine learning. I wouldn't be able to make any progress on my PhD if it wasn't for your videos.",True
@sabaaslam781,2023-06-08T03:59:52Z,1,"Hi Josh! No doubt, you teach in the best way.  I have a request, I have been enrolled in PhD and going to start my work on Graphs, Can you please make a video about Graph Neural Networks and its variants, Thanks.",True
@manuelcortes1835,2023-06-07T19:35:06Z,0,"I have a question that could benefit from clarification: In the final FC layer for word predictions, it is claimed that the Attention Values and 'encodings' are used as input (13:38). By 'encodings', do we mean the short term memories from the top LSTM layer in the decoder?",True
@rathinarajajeyaraj1502,2023-06-07T13:27:57Z,1,Much awaited one .... Awesome as always ..,True
@ThinAirElon,2023-06-07T09:52:14Z,3,quadruple BAM !,True
@automatescellulaires8543,2023-06-07T07:58:40Z,1,"wow, i didn't think i would see this kind of stuff on this channel.",True
@JL-vg5yj,2023-06-07T03:58:40Z,1,super clutch my final is on thursday thanks a lot!,True
@carloschau9310,2023-06-07T02:42:51Z,1,thank you sir for your brilliant work!,True
@familywu3869,2023-06-06T22:35:17Z,2,"Thank you for the excellent teaching, Josh. Looking forward to the Transformer tutorial. :)",True
@hassanalmaazi3768,2023-06-06T16:08:34Z,0,"Hello professor, Great video! but could you help with key representation here as there are three representations query, key, and value. I cannot identify what is key representation here, with outputs from encoder as value and output from decoder as query.",True
@MrMehrd,2023-06-06T06:15:36Z,1,No shameless promotion,True
@user-wr4yl7tx3w,2023-06-05T22:54:04Z,0,can you consider making a video on Transformers?,True
@won20529jun,2023-06-05T21:55:21Z,1,I was literally just thinking an Id love an explanation of attention by SQ..!!! Thanks for all your work,True
@BboyDschafar,2023-06-05T21:09:17Z,1,Attention is all you need...,True
@RafaelRabinovich,2023-06-05T20:32:19Z,0,"To really create a translator model, we would have to work a lot through values of linguistics since there are differences in word order, verb conjugation, idioms, etc. Going from one language to another is a big structural challenge for coders.",True
@aquater1120,2023-06-05T19:05:03Z,3,I was just reading the original attention paper and then BAM! You uploaded the video. Thank you for creating the best content on AI on YouTube!,True
@naomilago,2023-06-05T18:28:16Z,1,The music sang before the video are contagious ‚ù§,True
@envynoir,2023-06-05T17:18:10Z,1,Godsent! Just what I needed! Thanks Josh.,True
@user-co6pu8zv3v,2023-06-05T16:52:52Z,1,Thank you! :),True
@d_b_,2023-06-05T15:52:42Z,1,Thanks for this. The way you step through the logic is always very helpful,True
@ivanzhovannik5419,2023-06-05T15:14:51Z,0,"Hey Josh, good job, thank you.  The only note I have is that the explanation of the second decoder step was way too quick... Other than that - great explanation!",True
@aiforeveryone2941,2023-06-05T14:52:22Z,1,i hope to see cross attention  BAAAAAMMM!!!!,True
@linhdinh136,2023-06-05T14:41:05Z,6,Thanks for the wholesome contents! Looking for Statquest video on the Transformer. ,True
@KevinKansas1,2023-06-05T14:03:35Z,6,The way you explain complex subjects in a easy-to-understand format is amazing! Do you have an idea when will you release a video about transformers? Thank you Josh!,True
@saranyav2581,2023-06-05T13:46:18Z,1,Thanks,True
@seriousbusiness2293,2023-06-05T12:43:46Z,1,This is like a Kids show for machine learning lol. Right in the intersection i am Looking for.,True
@rajatjain7894,2023-06-05T12:31:57Z,1,Was eagerly waiting for this video,True
@andresg3110,2023-06-05T12:12:57Z,1,You are on Fire! Thank you so much,True
@docteurlambda,2023-06-05T11:25:22Z,1,Yess,True
@AntiPolarity,2023-06-05T11:16:21Z,2,can't wait for the video about Transformers!,True
@statquest,2023-06-05T10:51:46Z,8,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@The-Martian73,2023-06-05T08:22:33Z,1,"Great, that's really what I was looking for,  thanks mr Starmer for the explanation ‚ù§",True
@lequanghai2k4,2023-06-05T08:01:56Z,1,I am stilling learning this so hope next video come out soon,True
@rikki146,2023-06-05T07:50:31Z,1,"When I see new vid from Josh, I know today is a good day! BAM!",True
@faysoufox,2023-06-05T07:47:55Z,1,"Thank you for this video. Just a comment, your website didn't display well on my phone.",True
@MrKrtek00,2023-06-05T07:28:42Z,1,"damn, you are good!",True
@xXMaDGaMeR,2023-06-05T06:38:52Z,1,perfect topic,True
@aleefbilal6211,2023-06-05T05:59:07Z,0,Still waiting for your video on Transformers you left 6 months ago.,True
@lakshaydulani,2023-06-05T05:58:09Z,1,now thats what i was looking for,True
@dylancam812,2023-06-05T05:45:17Z,21,Dang this came out just 2 days after my neural networks final. I‚Äôm still so happy to see this video in feed. You do such great work Josh! Please keep it up for all the computer scientists and statisticians that love your videos and eagerly await each new post,True
@yuanyuan524,2023-06-05T05:43:23Z,1,best tutorial in youtube,True
@vanilan3585,2023-06-05T05:33:25Z,2,can u do tranfromer too,True
@muntedme203,2023-06-05T05:27:03Z,1,Awesome!,True
@rutvikjere6392,2023-06-05T05:14:03Z,9,I was literally trying to understand attention a couple of days ago and Mr.BAM posts a video about it. Thanks üòä,True
@Xayuap,2023-06-05T05:09:26Z,0,so is that simple?,True
@markus_park,2023-06-05T04:58:39Z,1,Thanks! This was a great video!,True
@markus_park,2023-06-05T04:58:20Z,1,Where were you 2 years ago haha,True
@ichkaodko7020,2023-06-05T04:51:59Z,1,I just watched it bam!,True
@munchika7013,2023-06-05T04:44:45Z,1,Please make a series on transformers,True
@CatatanSiRebiaz,2023-06-05T04:43:31Z,3,Currently learning about artificial neural networksüòÅ,True
@mrstriker1847,2023-06-05T04:40:21Z,3,"Please add to the neural network playlist! Or don't it's your video, I just want to be able to find it when I'm looking for it to study for class.",True
@souravdey1227,2023-06-05T04:22:20Z,1,Had been waiting for this for months.,True
@technicalbranch99,2023-06-05T04:12:04Z,3,First :),True
@atharva1509,2023-06-05T04:08:07Z,121,Somehow Josh always figures out what video are we going to need!,True
@christosvoskresye,2023-06-05T04:07:29Z,0,Always be'sing and do'sing!  And never cheesing or choosing!,True
@Xayuap,2023-06-05T04:06:07Z,2,"weeeeee, video for tonite, tanks a lot",True
@rrrprogram8667,2023-06-05T04:04:58Z,1,Excellent josh.... So finally MEGA Bammm is approaching.....  Hope u r doing good...,True
@chessplayer0106,2023-05-29T02:30:42Z,4,Ah excellent this is exactly what I was looking for!,True
