author,updated_at,like_count,text,public
@statquest,2019-11-14T13:36:50Z,207,"NOTE 0: If want to learn more about The Chain Rule, see: https://youtu.be/wl1myxrtQHQ NOTE 1: The StatQuest Gradient Descent Study Guide is available! https://statquest.org/studyguides/ NOTE 2: A lot of people ask why we are using Gradient Descent to estimate the parameters in this video when we could just use least squares. We use least squares to produce a ""gold standard estimate"". This is the best possible estimate. We then attempt to derive the same estimate using Gradient Descent. This shows 1) how gradient descent works and 2) that the estimate is pretty good compared to the ""gold standard"". NOTE 3: A lot of people ask how I found the slope value, 0.64. In the example in this video, we can compare the estimates from Gradient Descent to those that come from another method called ""least squares"". For specific problems, we can plug the data into the least squares formula and the output is the optimal parameters. To learn more about the specific least squares formula see: https://en.wikipedia.org/wiki/Least_squares If you're wondering why, when we have least squares, would we want to use gradient descent... the answer is that least squares only works in specific situations and gradient descent can work in many more.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@JohnBittner-iw3qy,2024-05-30T22:20:26Z,1,i love you statquest :),True
@AhmedKhaled-xp7dm,2024-05-29T12:19:22Z,1,"Just Amazing, Thank you!",True
@jrlearnstomath,2024-05-24T08:40:02Z,1,"Awesome chain rule in a nutshell @~8:07, and also thanks for sharing a link to a more in depth video in Note 0 in your pinned comment!",True
@anushavishwanathan8276,2024-05-24T04:27:09Z,1,GRADIENTT BAMMMMMMMMMMMM,True
@ramonjales9941,2024-05-24T02:05:50Z,1,incredible!,True
@matheusmelo4078,2024-05-20T17:41:41Z,1,Expectation break for THE CHAIN RULE at 17:58 üòÇüòÇ,True
@jelenagaric1044,2024-05-15T19:15:51Z,1,Double bam,True
@jeevankumarreddyravuru3346,2024-05-14T18:50:51Z,1,Thanks for the great explination.,True
@charlesrainesmusic4783,2024-05-10T04:51:36Z,1,Amazing,True
@pradeepmallampalli4541,2024-05-09T20:35:27Z,3,Give this man the highest medal in teaching. Thanks a lot for all the effort you made to explain this.,True
@yycthenics,2024-05-09T02:39:35Z,0,Where did 0.64 came from ? :),True
@ge13r,2024-05-08T05:42:47Z,1,Saludos desde la UNET!!,True
@atteharrikari9959,2024-05-07T11:34:25Z,0,"So, gradient decent is basically Newton's method?",True
@leducphuclong,2024-05-06T06:22:05Z,1,Thank you so much !! You did it wonderfully !!,True
@JH-fs3lz,2024-05-03T20:46:52Z,0,You should use partial derivatives when dealing with multivariable functions.,True
@ivanstepanov3013,2024-04-21T19:19:17Z,0,10 gradient descents out of 10!xD Thank you a lot! But why do we use double multiplying by 0.5 at 18:06?,True
@sajadsh8260,2024-04-21T17:39:31Z,1,"it was soooooo great, thank you",True
@EvelinaPettersson-mk5ot,2024-04-19T10:00:15Z,1,"Thank you! Very helpful video, I really liked the way you presented it, made it fun to watch and easy to follow. I usually have a problem with keeping my concentration when watching videos like this, but I had full focus throughout the whole video! Cheers!",True
@no_kurisu_allowed,2024-04-18T01:21:23Z,2,Such a CLEAR explanation!!!!!! Thank you very much!,True
@saltedfish_is_good,2024-04-17T13:57:40Z,1,"What can I say except thank you for this gold tire free knowledge? I would love to ask question, but I have none.",True
@harrypounds456,2024-04-06T19:22:42Z,1,"At the end, 22:20, why are steps 1 & 2, not the other way around, surely you need to pick random values first in order to calculate the Loss function derivitive.",True
@nirmalarora2207,2024-04-06T17:29:57Z,1,thank you brother,True
@marinabaskakova2333,2024-04-06T13:44:51Z,0,"Can someone explain once again for blondes - at 10 min, why do we plug values into the derivative function?",True
@VikasSrivastava6,2024-04-06T00:38:20Z,1,Great video ! Exactly what I was looking for.,True
@vasundran2628,2024-04-05T15:37:44Z,1,Excellent explanation,True
@someshshridhar9998,2024-04-04T16:32:20Z,1,BAM !!! i loved this exapmle...,True
@wa1601,2024-03-31T21:44:22Z,1,"thank you so much for this help for free, you're an amazing person love from the whole world!",True
@tingweiwu4227,2024-03-25T06:01:54Z,1,So Bam helpful!,True
@robertpollock8617,2024-03-24T20:50:47Z,0,Why do you not treat the intercept as a constant when doing gradient descent derivative calculation for the slope parameter optimization since you found it to be 0.95?,True
@xiaozaowang5106,2024-03-24T03:20:31Z,1,You are a genius!!!!!!!,True
@Anonymous-fr2op,2024-03-21T11:12:27Z,1,"I have no words to say how amazing you are. Not only are your videos super easy to grasp, but also, wth, even after 4 years you're replying to your comment. Wow! Amazing",True
@binliu7645,2024-03-17T06:30:50Z,1,Thanks!,True
@ishikaverma5392,2024-03-13T12:31:04Z,1,You're the new Phoebe,True
@krishnaks4622,2024-03-12T08:13:06Z,1,thanks to you . I finally learnt gradient descend ü§©,True
@mandarmore.9635,2024-03-10T05:25:01Z,1,you are best,True
@lifeisbeautifu1,2024-03-08T19:32:07Z,1,You are the best!,True
@ManasNandMohan,2024-03-08T12:11:12Z,1,"Loved It, Loved it - Explanation at its best",True
@shivakumarjadipalli6823,2024-03-07T07:54:17Z,1,"I have took a data science bootcamp of $4000 but even they didn't explain it like you did great work , Please don't stop doing this",True
@radowanahmedbaized3597,2024-03-07T04:52:53Z,0,Hi Josh!  Great video. Can you explain why subtracting the step size from the old intercept gives us the new intercept?,True
@timusbelkin,2024-03-06T20:17:24Z,1,Gradient descent is like Achilles and turtle myth),True
@r0cketRacoon,2024-03-06T14:18:05Z,1,"great video, i always wonder why weights/bias have them subtracted by their corresponding derivatives of loss function, u gave me a clearly explanation",True
@lucamix5757,2024-03-06T13:20:45Z,1,I felt like he was teaching to a 5yo kid... and I was that 5yo kid. Loved it! thank you very much,True
@meow-mi333,2024-02-25T18:27:18Z,1,Dude explained things clearly. Huge thanks! Helped me review what I‚Äôve learned. I feel much better.,True
@gigachad2419,2024-02-25T12:18:07Z,2,BEST VIDEO ON GRADIENT DESCENT!!!! PERIOD.,True
@Alan-qb9qt,2024-02-23T14:08:27Z,1,the most intuitive and detailed in every point gradient descent tutorial i've ever watched,True
@utkarshashinde9167,2024-02-22T08:47:30Z,1,Thankn You thank You Thank You,True
@veeramaninatarajan876,2024-02-20T17:54:04Z,1,"My first subscription in youtube, even though i watch hundreds of videos every month. Such a quality content :)",True
@Kimuji402,2024-02-13T17:51:02Z,1,You are my favorite teacher.,True
@srinivasveligethi2380,2024-02-13T17:22:08Z,1,Superb explation,True
@SagarBastian,2024-02-11T14:19:05Z,1,Great video that effectively explain concepts,True
@jasonwang-wg8wu,2024-02-06T14:14:12Z,1,these videos are almost TOO good that it got me thinking: what if college/uni profs are holding out on us in an effort to push us toward content creators like Josh Starmer? Is Big Academia in bed with Big Tutorial?!?!?!,True
@issaczheng5067,2024-02-05T16:03:02Z,2,i love you thank u for this,True
@climbeverest,2024-02-03T23:06:00Z,1,Incredible,True
@jamil5522,2024-02-01T07:22:40Z,1,The voice effects are pure gold üòÇ,True
@MoumitaHanra,2024-01-30T16:36:52Z,1,"I think I only understand your lessons (and a handful of very few others) and it has brought about a new understanding of these algorithms and concepts, really appreciate your lessons!!!!",True
@aloysiocamposdapaz8041,2024-01-24T23:56:37Z,1,TRPLE BAAAMMM,True
@user-js7hx5hf4z,2024-01-22T03:29:05Z,1,ÈùûÂ∏∏Â•ΩËßÜÈ¢ëÔºå‰ΩøÊàëÁöÑÂ§ßËÑëÊóãËΩ¨ü•∫ü§≤üíñ,True
@mdimdadulhkhan3270,2024-01-22T01:26:09Z,1,Loved it !!,True
@jjj78ean,2024-01-21T23:32:00Z,0,"Josh, thanks for the great video, but one point is not clear to me. If I took the function y = 1 - x ^2, the rule for Xnew = Xold- lr* f'(x) doesn't work. At each iteration, both abs(x) and abs(y) get bigger.",True
@angelaniu5908,2024-01-21T20:00:35Z,1,man i love you thank you sm,True
@user-rw6gl8pw4l,2024-01-19T06:31:56Z,1,i don't know how i thank you  proüò¢,True
@patrikszepesi2903,2024-01-11T12:34:11Z,0,"Hi, Josh I love your content. I have your book too, I am a big fan. I do have a question. How can you optimize for 2 variables at once? Meaning how can you simultaneously update the slope and the intercept? what if the new value of the slope would minimize the problem with the old intercept, but now you have updated the intercept too, so you missed on having the best combination which was the new slope with the old intercept.",True
@kisaa0155,2024-01-10T10:09:47Z,1,Thank you so so much ü•π,True
@aakidatta,2024-01-09T06:34:56Z,1,I watch several videos and pretty much everyone explained conceptually well but not the math part. This material is gold.,True
@vishnuvardhan623,2024-01-07T08:32:44Z,1,‚ù§,True
@redreg777,2024-01-05T10:00:07Z,0,I definitely wouldn't have used 'weight' as one of the variables. Audience confusion guaranteed when the context of the lecture is neural networks.,True
@salakoyigo,2024-01-02T12:02:30Z,1,I was so disappointed at 18:00 !,True
@primaldsml,2024-01-01T14:10:13Z,1,The Chain Rule >>,True
@AnotherBrownKid,2023-12-29T22:28:20Z,0,"At 9:35, why can‚Äôt we just solve that equation for when the derivative equals 0, what does the intercept equal?",True
@user-yc6wd1ds3p,2023-12-25T15:37:58Z,0,"Hey josh, amazing explanation, I love to get these visuals‚ù§‚ù§. I have one doubt, in this case you applied the gradient descent to the sum of residual squares... My doubt is we already have an analytical formula derived for the least square fit's slope and intercept by finding the absolute Minimum of the sum of squared residuals, so in this context how is gradient descent helpful???   I love your statquests, keep uploading moreüéâüéâ",True
@YLIU,2023-12-19T19:46:01Z,1,Ë¨ùË¨ùÔºÅ,True
@mohsinqureshi6850,2023-12-10T11:30:22Z,0,How did you assumed the first slope to be 0.64?,True
@user-pb1dt9tg1q,2023-12-09T11:37:08Z,0,how do we tell it is just local min and far from global min?,True
@williamikennanwosu,2023-12-09T11:12:19Z,0,"1.4 - 0.32 = 1.08, just thought it would help with your effective example so people don't get confused.",True
@obensustam3574,2023-12-06T14:13:09Z,1,love it,True
@saruqara3725,2023-11-30T19:42:49Z,1,BAM!,True
@geetanjalighatak3913,2023-11-26T11:59:30Z,1,I've been trying to understand gradient descent algorithm for months but all I could find were videos filled with jargon and now i finally understood it within 25 minutes. I love how you simplified the whole thing! This is literally the best explaination I've ever seen! I loved every second!,True
@Yes-bm4vn,2023-11-24T18:50:30Z,1,"I appreciate you so much, I could've not wished for a better explanation of gradient descent, seriously. Have a great day!",True
@NarinderSingh-iw6tn,2023-11-22T16:49:15Z,0,Thanks for the nice video. Can you please explain how the gradient descent estimate for the intercept and least square estimate for the intercept is the same (at time 13:55)?,True
@nancypoulose1534,2023-11-22T11:05:14Z,0,Why didn't I hear the awesome song! I was waiting for it,True
@sebastianvlad1772,2023-11-19T14:41:34Z,1,"Great lesson! I have one question: when we use gradient descent with at least 2 parameters, when does the gradient descent stop? Is it when the step size for one parameter is close to zero, or when all the parameters are close to zero?",True
@ggk7619,2023-11-18T08:24:25Z,1,Mate you're mindblowing. People like you are a blessing to this world. Stellar explanation. So much better than even paid Udemy courses,True
@sadiakhalid92,2023-11-11T15:39:51Z,1,These videos are perfect for someone with ADHD like me!!,True
@chautuanbk,2023-11-05T10:48:30Z,0,"Your video is great because It 's easy to understand, but in reality, you cannot find optimum values for intercept and slope separately like that, you must calculate step size, new value of both in the same loop. At 6:06, where do you get 0.64 (value of slope)? it 's exactly the optimum value of slope.",True
@kaleemullah247,2023-11-01T12:42:30Z,0,"11:10 One very important thing to note is the slope sign tell us which direction to step, the negative slope sign (derivative = -5.7) means the minimum value of the function is at right side of the point the derivative is taken, so we should step to the right side, while positive slope sign means minimum value of the function is at left side so we should step to the left side that's why step size is subtracted from old intercept. Negative slope: 0 - (-0.57) = .57 (step right) Positive slope: 0 - (0.57) = .57 (step left)",True
@arunthashapiruthviraj2783,2023-11-01T10:46:47Z,1,Thank You. clear explanation.,True
@raghavaganesh9,2023-10-31T14:05:40Z,1,i have no words loved it!!!!!!!! thank you,True
@tremaineification,2023-10-28T23:30:28Z,0,"18:11 The derivative of 0.5 is 0, I thought?",True
@aryabartarout5697,2023-10-25T06:06:33Z,1,Bam !,True
@lightrao,2023-10-24T07:27:23Z,1,This is what I want :),True
@jimss596840,2023-10-23T04:07:32Z,0,"Thank you for the video, it's very informative. I have a question though: in the example, it seems that the least square method is much more straight forward than the gradient method as the former set the derivative to 0 and only need to calculate once, and the gradient method revolves around multiple calculation. Then why choose gradient method? What's the advantage behind?",True
@user-fi5kc4md3w,2023-10-22T08:17:33Z,0,"Really great explanation but it would be better without every ""BAM"" disrupting the flow.",True
@tunvas,2023-10-19T09:22:25Z,2,"this is amazing, thanks",True
@PresencyStudio,2023-10-19T06:04:26Z,1,"I wish you the best in life man, thanks for making your awesome teaching skills available to broke & desperate students like me for free !",True
@knayam1000,2023-10-15T10:47:09Z,1,This is the best explanation I have seen on Gradient descent.Thanks Josh,True
@izzatulummah1594,2023-10-14T04:20:15Z,0,"I‚Äôve been searching for a video like this. I like how you went down and dirty in explaining all the details. Thank you very much. Looks like I‚Äôve just found my kindred spirit. I‚Äôm gonna make a lecture note  based on this video. Anyway, at 19:17 I think it shouldn‚Äôt be called just gradient, but vector gradient (because it has two partial derivatives). Even if it only have one derivative (of one independent variable), it can still be called gradient. Am I right?",True
@davidgithaiga6993,2023-10-11T08:39:57Z,1,‚ô•Ô∏è,True
@namo5570,2023-10-11T03:07:27Z,2,I love you!,True
@020hamza2,2023-10-10T03:26:24Z,1,You had presented steps very nicely,True
@robertpollock8617,2023-10-07T08:34:55Z,0,I am not sure you are saying this correctly. At the 0 intercept the SUM of the slopes is -5.7 each point will have its own slope. I think this is correct.,True
@kirangill2015,2023-10-05T22:50:30Z,0,why do we need to take derivative of sum of squared errors?,True
@michel_reis,2023-10-04T19:56:56Z,1,"Want to relax, but also want to study? Statquest! Bammm",True
@alekseishkurin4590,2023-09-30T23:03:53Z,1,These series are just brilliant!,True
@Jos-97,2023-09-26T19:57:14Z,1,BAM! Thanks a lot man,True
@khoatrandang7015,2023-09-26T11:42:57Z,1,"Came for the great lesson. Stayed for the ""bip bop"" sound effects. Now if theres a spotify version of this would be great",True
@chillingAccount_,2023-09-26T02:02:30Z,1,BRO THIS VIDEO IS SO GOOD MY GOSH I LOVE YOU,True
@janilayturatkhankyzy1321,2023-09-25T19:27:42Z,2,I will pray for your health. So much hard work put into this channel. God bless you,True
@bahamutShin,2023-09-22T22:31:25Z,1,"how did you get first slope 0,64?",True
@jacqueshollands5630,2023-09-20T06:16:16Z,1,"StatQuest offers some of the best, if not the best, step-by-step instructional videos on YouTube, regardless of the topic.",True
@xzist2342,2023-09-18T15:57:20Z,1,clarity++,True
@sinkseeker,2023-09-15T06:15:13Z,0,"Hi Josh! One question regarding the application of The Chain Rule @8:25, why are we adding -1 in the equation? Wouldn't 2 times inside be enough?  EDIT: Or rather, why do we still need to find the derivative of what's inside? How did the Chain rule make it so?",True
@kainebeato,2023-09-13T09:49:28Z,0,"Hello, your presentation is great! I understood everything except one moment. On 8:22 you multiply result by -1. I didn't understand why, can you please explain?",True
@shrirangphadke6156,2023-09-10T11:54:47Z,0,"Please give Josh the World's best teacher award for the video content, Nobel price for the noble work he's doing and Oscars for the song! Triple BAM!",True
@cuckoo_is_singing,2023-09-09T05:35:13Z,1,"After searching alot on the net, finally I could find out what is the gradient descent and how I can use it in ML. Tnx alot‚ù§",True
@user-po4vs4ob7d,2023-09-06T07:17:33Z,2,BAM*4,True
@kartik_exe_,2023-08-30T10:57:04Z,6,to be honest i never ever saw a teacher like him which teaches us and after the sessions we don't have any doubts to even ask. Kudos to him and also thank you StatQuest <3,True
@user-fu3yg3cd4o,2023-08-25T14:34:41Z,1,"+1 thanks for the clear explanation! and the ""chain rule"" effects >^=^<",True
@phoneix24886,2023-08-20T08:01:04Z,1,You are a gift of the Gods. You should not be lost in the crowd. We need more teachers like you.,True
@lifehacks4everyone,2023-08-19T17:46:04Z,1,Thanks a lot for simplifying this so beautifully!,True
@sofarsogood1337,2023-08-18T17:36:54Z,1,Great video! I am very interested in Artificial inteligence and this is one of the best chanells to learn,True
@FarazYousuf,2023-08-17T12:14:43Z,1,I was literally scratching my head until I found this clear explanation! Thankyou so much üò≠üíØ,True
@pcooi7811,2023-08-11T14:48:32Z,1,TQ sir !,True
@carlosmerinosubercaseaux3360,2023-08-10T22:00:41Z,1,amazing video!,True
@jorgelopezcuns,2023-08-10T14:46:56Z,1,Brilliant,True
@Vanadium404,2023-07-29T13:09:38Z,1,I have almost watched all videos on Gradient Descent and the number game you carry along while explaining concepts Ahhh Sir!!!!,True
@gumpr4544,2023-07-29T09:19:58Z,1,thankku  :D,True
@ankitbhushan7379,2023-07-27T07:50:29Z,0,"Hi, a very basic question. What if the curve does not follow a 'U' pattern, but follows a 'W' pattern. How do I know that the gradient descent gave me the most optimal value over the nearest optimal value to the random starting point?",True
@mathiaseliasmusic,2023-07-24T08:41:12Z,0,"Where did you get the slope 0.64 for the predicted height? You said it's the least squares estimate, but who did you come up with it?",True
@RobinSingh-ms3zt,2023-07-23T09:56:20Z,1,"Mind blown, I've never seen such type of amazing explanation before thank you so much.",True
@diptoroy6203,2023-07-22T06:07:02Z,1,You are super .The way you taught is unbelievable,True
@Patrick-jm9rq,2023-07-21T06:36:34Z,1,this series is by far the best detailed deep-learning course on youtube,True
@13671994833,2023-07-20T10:39:44Z,1,This is really a awesome explanation! Saved my day when I struggled in understanding these things.,True
@ayodejidare212,2023-07-16T14:00:52Z,1,"Thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you!!!!!!!!!!!!!!!!!!!!!!!",True
@BoCao-iw4is,2023-07-14T08:20:42Z,1,Thanks,True
@hjtam88,2023-07-13T05:01:05Z,1,Thank you for the informative and heartfelt presentation!,True
@dolthhaven8564,2023-07-12T17:24:27Z,0,when will we have a triple bam?,True
@pritamsaha6836,2023-07-12T08:57:04Z,0,"9:12  The video posted 4 years ago but still The Best Sir.   I was confused about why we are not using least squares i.e., just equating the derivative to zero to find the minima. Thank you for addressing that.  So is this only because of the possibility that sometimes it may happen that we won't get the slope/derivative equal to zero? But when will that happen? We differentiate a quadratic function, then its a linear function and a linear equation will give us a real solution.",True
@laslodes2550,2023-07-11T10:22:49Z,6,"I usually don't comment under youtube videos, but I had to it this time. Currently working on a robotics project and I have to use the gradient descent method to solve inverse kinematics. I didn't understand the method as explained during my lectures and I ended up watching this video. Seriously, I could even explained it to my prof and submit a request so that this video is shown to students during the teaching of this topic.  You're amazing and your explanation is so much easy to follow and understand. Thank you so much for this masterpiece.",True
,2023-07-08T13:54:30Z,0,"In real applications how GD is calculated exactly? First calculating the dy-hat WRT to intercept and then WRT to slope or intercept and slope at the same time? Im confused because first example is on intercept and second takes into account both. If its done by second way, would learning rates be diffrent for slope and intercept?",True
@nirmithjainresearchassista849,2023-07-06T12:32:22Z,1,as always awesome video,True
@kartickdhali4352,2023-07-05T15:51:43Z,10,This is the best video on Gradient decsent on internet. I wish I could have a teacher just like you in my college.,True
@Parmisan,2023-07-01T21:48:59Z,1,Is there any way you could pls pls pls do a set of videos on linear algebra? Thank u!,True
@abuzarrezaee8992,2023-07-01T13:40:33Z,1,Great greatüëçüëçüëç. Thank you so much,True
@cffex3858,2023-07-01T11:15:24Z,0,"How do I update weights and biases, is it the intercept?",True
@suyashdongre,2023-06-27T11:32:19Z,1,THANK YOU SO MUCH!,True
@griffin8er845,2023-06-25T17:42:06Z,1,"Omfg this was sooooo much easier to understand. I was having such an issue figuring out how gradient descent connected to the loss function and I think I get it now. I‚Äôm just out of high school and am very interested in machine learning and can understand the math behind gradient vectors, matrix vector multiplication, activation functions, and so on. The issue is that sometimes I never get a good explanation as to how each are related to eachother and how they fit together in a neural network but this definitely helped out.",True
@ramanmahapatra1514,2023-06-17T06:48:51Z,1,This is the best explanation of gradient descent I have seen so far. Thank you.,True
@Luxcium,2023-06-12T14:38:54Z,0,The equivalent of _gender neutral_ but for weight and height üòÆüí¨ How tall are you? üòæüí¨ I am one point four üòºüí¨ Why??? ü§≠üòÖüí¨ Because now I know you weigh one half üôÄüí¨ How can you know I am only ¬Ω of a weigh üòäüí¨ *I watch StatQuest with Josh Starmer on a computer screen üì∫*,True
@Luxcium,2023-06-12T06:57:40Z,0,Cool I am back to watch this video I have been able to complete the other one even though it was in the past when people where watching on ¬´ Computer  Screens ¬ª so bulky that they where able to fit a Z axis inside their screens... And when people though all those cool things we are learning with Josh where about Statistics... Haha ChatGPT would be laughing at this  (but he is not permitted to have emotions because we are just in 2023)... üòÆ,True
@Luxcium,2023-06-12T06:23:57Z,0,Wow üòÆ I am on a quest to find out where this gradient descent will stop I just learned that I must watch the video *Fitting a Line to Data* also know as *Linear Regression* before I can watch the *Gradient Descent Step-by-Step!!!* so that I can watch the video related to *Neural Networks part 2* that I must watch before I can watch the *The StatQuest Introduction To PyTorch...* before I can watch the *Introduction to coding neural networks with PyTorch and Lightning* üå©Ô∏è (it‚Äôs something related to the cloud I understand)   I am genuinely so happy to learn about that stuff with you Josh‚ù§ I will go watch the other videos first and then I will back propagate to this video...,True
@amnont8724,2023-06-04T18:32:52Z,0,"Hey Josh, could you please explain intuitively why do we subtract the step size from the old intercept / slope and get the new intercept / slope? I wonder why computing it that way makes sense?",True
@reapingshadow2866,2023-05-30T20:24:52Z,1,very bad video i do not like it,True
@jubinkuriakose,2023-05-28T11:31:57Z,1,This was my favourite one‚úåÔ∏èüëå,True
@haj5776,2023-05-20T18:18:06Z,1,You are the greatest teacher holy cow!!!!!,True
@kharcasse255,2023-05-18T15:00:46Z,1,This is awesome. I am learning so much and you are lowkey making it fun. BAMMMM!!!,True
@ojaswighate2588,2023-05-17T11:34:11Z,1,Thank you for sharing üôèüòä,True
@Hosain_Ahmed,2023-05-14T17:00:41Z,1,Thanks a lot. I'm from Bangladesh. It means English is not my native language but I have learnt many things from the video. All the visually representation was really amazing.,True
@zhiyingwang1234,2023-05-12T09:43:02Z,0,"I have watched the video several times, still could get the equation: new intercept = old intercept - step size, the step size is the slope of the residual function, what's the relationship between intercept and the step size? I'm not convinced of the equation. Alternatively, I like the passive agressive algorithm which calculates the derivative as 6*intercept -5.7=0, intercept=0.95, meaning when the incept increase by 0.95, the slope of residual function becomes 0 given the three samples here, hence to minimize the residual intercept_new=intercept_old +0.95*learning_rate.",True
@nickstutzman8946,2023-05-04T22:30:31Z,2,Best explanation I have seen on this topic.,True
@rashikraj7112,2023-05-03T07:37:49Z,1,"great video, loved it!",True
@mercysamoei7350,2023-05-02T19:47:37Z,1,BEST TEACHER  EVER!!!!!!!!!!,True
@drachenschlachter6946,2023-04-30T16:51:02Z,2,Perfect explanation thank you from Germany!!!,True
@tech_buddy,2023-04-29T04:20:58Z,1,i was learning neural network from verious sources but got stucked at gradient descent as i was not able to get feel of this visually but your video explained it much easier and visually with proper mathematics and graphical representation. just loved it‚ù§‚ù§üéâ,True
@dumbfailurekms,2023-04-27T02:07:59Z,0,any convenient table for the actual values?,True
@user-oq4iy6md7m,2023-04-24T07:28:53Z,0,I don't understand where that -1 comes from 8:19,True
@mattycoze,2023-04-23T00:40:12Z,0,I can't understand why we don't just apply newton raphson method here - is it just computational burden?,True
@_corinthian,2023-04-17T19:13:50Z,0,"why do we substract (old_intercept - step_size(gradient*learning_rate))   we found first gradient as f'(0)=-5.7, dont we need to use opposite direction of gradient i mean why we did not use '+' but '-'  ?",True
@jimwest63,2023-04-15T13:13:27Z,0,"Your material  really is great, please keep it up. One question though, I'm guessing it is just a bit of a drafting glitch, but the line of best fit shown around 21:25 that we got to via gradient descent looks strange, with the  residual on the furthest right point looking a like it would cause the squared residuals to blow out unnecessarily. Am I missing something?",True
@jimwest63,2023-04-15T12:44:42Z,1,Thanks!,True
@Aaron-o_o,2023-04-10T00:09:41Z,0,"20:06 ""and that gives us two slopes"" Given we were previously using terms intercept and slope for two variables we are iteratively optimizing, using the term ""slope"" to describe the results here was very confusing. It took me a few minutes to realize that these ""slopes"" were characterizing the slopes of the loss function for each of the intercept and slope.  22:12 shouldn't the first step be to choose random parameter values (e.g. the initial slope ""guess""), before calculating the derivatives? How can you calculate the initial derivatives otherwise?",True
@DharmendraKumar-DS,2023-04-09T11:33:55Z,1,waao man....you have proven its not about the knowledge we have..its about how we teach it to others....Greatüëç,True
@kumfman,2023-04-08T04:16:08Z,0,how may Scaler people here from Sachin Sir?,True
@williamstephenjones3863,2023-04-06T12:46:45Z,0,"When taking the derivative at 8:41, moving ^2 to the front, where does the -1 come from?",True
@mohammadkaif8385,2023-03-27T19:03:29Z,0,let me wrap your balls with gold foil and microwave them please Thank you,True
@AdityaX2703,2023-03-19T18:38:20Z,1,"jesus christ thank you so much for explaining this, i was wanting for this type of explanation but i couldnt find it elsewhere",True
@freddierobinson9587,2023-03-10T14:55:11Z,1,"great video, easy to follow and entertaining",True
@alitouil9554,2023-03-10T10:23:00Z,1,tripple BAM! very nice video thank you,True
@SachinMalali,2023-03-09T02:26:40Z,0,"@StatQuest with Josh Starmer, Why did you take slope as 0.64?",True
@mr.shroom4280,2023-03-07T10:22:33Z,0,"Hey i got a question, hopefully you see this. With gradeint descent, how am i supposed to get the derivative for each weight and bias in a loss function dynamically? because surely for networks with more than 100 neurons there would be a way, i know there is i just don't know.  When i am calculating the derivative for one varaible in the loss function, to optimize it, i get some overly complicated function, but i see some papers on it and it isn't complicated.",True
@kei3300,2023-02-28T07:37:48Z,0,pov: ur in an upper division engineering lecture but the teacher explains things simply,True
@luvmutreja25,2023-02-26T15:18:35Z,1,You are AMAZING BRo thanks a lot for such awesome explanations,True
@athenacai6448,2023-02-23T18:25:09Z,0,i love you,True
@agila.p9807,2023-02-11T12:24:09Z,0,"At 25:53, you said, 'these parts don't contain a term for the intercept', can you please explain ,that would be very helpful to me.",True
@imanemoustati1455,2023-02-10T10:45:50Z,0,"Thanks a lot for your efforts, but I just can‚Äôt get the difference between the least squares and the gradient descent! Since we calculate the derivatives for both, is it just that in least square we see where the derivative is equal to 0 and in decent gradient we pick random values for the parameter ? Can u please clarify it for me üôèüèª",True
@nikitarvachev3129,2023-02-06T02:10:58Z,1,Thanks!,True
@deathByStupid,2023-01-31T05:02:11Z,1,Double bam. great vid.,True
@kevindave277,2023-01-27T12:15:25Z,1,"I shouldn't pay a single dime to my university, man. Way better off giving that money to you lot. YouTube educators for the win.",True
@demongamer9685,2023-01-27T09:07:47Z,1,"I'm trying to learn this topic since 2-3 days going through different videos but it got confusing after a few minutes and I used to get bored. But I could understand the topic without getting bored. Thanks to you.... And ya, my favorite part... :  'THE CHAIN RULE.....'",True
@smooth7041,2023-01-24T15:47:16Z,1,"Excellent presentation!, excellent work!. Thanks a lot!... your work is helping a lot!.",True
@Thekingslayer-ig5se,2023-01-23T17:46:58Z,6,This man is an icon!  He should be celebrated much more!  You have no idea of how well he teaches stuff!  Way to go my man,True
@aneeqduraiz4092,2023-01-22T18:28:22Z,1,THE BEST !!!!!!!!!!,True
@BlackHeart-AI,2023-01-20T13:52:16Z,1,"The Chain Rule üòÇüòÇ Thank you , I love you <3",True
@pravachanpatra4012,2023-01-19T14:26:14Z,0,5:50,True
@brandolosaria9611,2023-01-19T05:56:22Z,2,"Thank you for your video, I've been using this for years already and didnt really understand the mechanics behind, all i know is that I have a general idea that it's used for. With your video, can now understand and even explain it to other people without saying ""it just magically works""",True
@AbdulWahab-mp4vn,2023-01-18T05:21:15Z,2,You said if i liked it...... I LOVED IT ..BAM !,True
@josemanuelgil9618,2023-01-16T22:45:55Z,1,"the BEST FHUK1111NHG VIDEO ON THIS TOPIC I HAVE EVER SEEN, BEAAAAAAAAAAM!!! YOU TOTALLY SAVE ME!",True
@harshanarayana6937,2023-01-15T13:55:44Z,1,BIG THANKS!!,True
@nriezedichisom1676,2023-01-14T06:40:14Z,2,"Beautifully explained. For the very first time, I understand this",True
@baptiste270399,2023-01-07T20:08:10Z,0,wtf was that intro with the guitar,True
@priyaresapu8485,2023-01-04T16:53:30Z,1,Unbeleivable!!! Atlast gradient descent is crysrtal clear... hatsoff of to the content provided and explanation... I completely come from non math background and have very minimal knowledge of calculus which I have been learning on my own... I have been trying to understand gradient descent for my machine learning projects and here I am totally satisfied... I must say even with such basic math I could easily follow the logic behind the algorithm ... Thank you so much for your awesome work..,True
@sniper4627,2023-01-03T07:51:06Z,0,8:20 where did you get that  -1 on the right side?  I hope you answer this,True
@PranjalVerma-br1zx,2023-01-01T13:20:12Z,1,A great salute to you for your dedication in teaching.,True
@planetassem448,2022-12-30T19:36:14Z,1,Legend!,True
@mohammadidreesbhat1109,2022-12-27T10:31:18Z,0,"Sir, After getting derivatives of different parameters which you combienly called as Gradients. What is decent there. For example, [ -0.5 , 1 , 2 ] is the gradient vector of three parametrs. Since, -0.5 is small our new learning rate or values of new paramertes will be close to -0.5 is this the descent",True
@mohammadidreesbhat1109,2022-12-27T10:27:15Z,1,If Such Teachers are available on you tube why people/students are wasting time in Universities and Colleges. Really ow sum/BAM,True
@PABITRABADHUK,2022-12-27T04:17:04Z,0,"I want to use gradient descent. But in my case, the loss function is generated through solving a system of ODEs. Hence, getting the derivatives (ahem, gradient) of the variables is not straightforward. Can you please guide me on what method I can use in such cases? Thank you.",True
@alainleclerc4523,2022-12-26T19:28:23Z,0,"hello Josh! thank you very much for this wonderful video. i have a question please: is it possible that the intercept and the slope has different learning rate. for example learning rate of intercept: 0.0001 and learning rate for slope: 0.000000001, for examplke when the x axis is years (small number) and y axis is salary (very big number comparative to year). I will be very glad if you can give me an answer. thank you so much!!",True
@hieuho3019,2022-12-24T12:37:46Z,0,"Just curious, at 9:32, he says when it is not possible to solve where the derivative is equal to 0 by using Least Squares, can anyone help me to explain specifically how least square falls short? Or when exactly the derivative = 0 is impossible to solve?",True
@vedant6460,2022-12-23T11:28:30Z,1,thank you!,True
@alainleclerc4523,2022-12-20T19:43:17Z,1,"I had a great moment listening to your wonderful explanations. all is so clear, so neat, so simple when you explain. i am myself a teacher, and i learn a lot from the way you make every complex thing so simple. you are great!! be blessed!!",True
@nakedperuvian,2022-12-19T23:47:04Z,1,Thanks!,True
@itsmeatrin,2022-12-13T12:36:27Z,1,BAM!!,True
@PedroRibeiro-zs5go,2022-12-13T07:50:46Z,1,"Thanks StarQuest, you are THE BEST!",True
@srinivasabhargava4587,2022-12-06T15:12:16Z,1,,True
@currytim4639,2022-12-06T09:40:35Z,1,"Thank you so much man, you are my best stat teacher! You will get paid one day when I have a job!",True
@RachelPun,2022-11-28T15:41:11Z,0,"What if we are not using least squares but some other loss functions, will the gradient still look similar and how would gradient descent apply?",True
@rezamoosavi71,2022-11-27T03:08:46Z,1,"Josh Starmer, you are not human :) no humankind can be such a great teacher; WOW!",True
@mehtabbandesha5637,2022-11-21T22:24:24Z,0,"5:02 -- we are assuming the slope to be constant =0.64, while trying to find best intercept? Will it be right to ignore slope?",True
@yakubsadlilseyam5166,2022-11-21T05:55:14Z,0,"We are calculating step size in the SSR vs intercept graph by taking derivative*learning rate, but how are we calculating new intercept in the Height vs Weight graph from the SSR vs intercept graph?",True
@lenko_me,2022-11-19T17:39:57Z,1,Scary thing turned out to be very simple due to your clear explanation. Thank you!,True
@data-science-ai,2022-11-19T16:22:28Z,1,"Thanks Josh, amazing. What you've done here is incredible.",True
@circleAI,2022-11-19T08:40:59Z,1,okay so this is triple bam video!,True
@rishipatel7998,2022-11-17T13:37:19Z,0,Thank you very much SIR,True
@pomiro,2022-11-16T10:38:21Z,1,Thanks!,True
@aryana506,2022-11-16T04:29:54Z,66,Your videos are so well put together. Thanks for all the time you put into preparing them!,True
@muhammadtaha3773,2022-11-15T20:45:28Z,1,"wonderful explanation sir, the best explanation ever ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§",True
@harshpratapsingh1638,2022-11-12T14:15:58Z,1,You have explained such a tough topic in much easier way,True
@sumitprajapati797,2022-11-11T15:26:09Z,1,"Really amazing ü§©, you are great teacher...",True
@thezizo33,2022-11-10T20:46:13Z,1,i love the bam,True
@deletedfile7979,2022-11-09T07:47:16Z,1,THE CHAIN RULE,True
@juliuskadel7696,2022-11-07T14:55:27Z,1,"Great video, thanks",True
@zombieboobuu9233,2022-11-07T00:46:14Z,1,I bought your book!,True
@chloetang5839,2022-11-04T23:33:51Z,1,Now I finally get this topic!!!! Thank you so much for this video,True
@jbangz2023,2022-11-04T10:39:46Z,0,"maximum number of steps is 1000 or greater, or is 1000 the minimum?",True
@amirg9809,2022-11-01T14:23:08Z,2,"If you already have the derivative, why not solve for zero? Is it because most of the times there are more weights to solve for? That makes the function multidimensional. So how do you know you are at the best hole in this multidimensional space and not just a random pit which has a lower loss than it's surroundings but there are better pits somewhere else?",True
@TheLeoPoint,2022-11-01T12:15:20Z,0,Understand cyclomatic process of gradient is my big nightmare,True
@suhaibahmed1524,2022-11-01T03:52:02Z,1,BAM BAM BAM BAM !,True
@nagham96,2022-10-28T16:01:29Z,1,Thank you very much! <3,True
@sandyqbg,2022-10-27T17:46:32Z,1,Thanks!,True
@tamerosman774,2022-10-27T00:59:43Z,1,Just wow,True
@merebhayl5826,2022-10-26T19:05:21Z,7,"Josh, You are a beautiful creature and gift! glad you exist! your thought process, your explanation, your positivity in teaching this to us, your creativity through art, and your music and albums. Be glad you exist to us all. I am also an electrical engineer, working primarily in software engineering, with a passion for music, numerous instruments, and painting. I am glad I came across you as a person, besides the great education!!",True
@whatsup778899,2022-10-24T00:39:33Z,1,"Josh, please take my respect. You are the legend. Keep going!",True
@drumminactuary,2022-10-22T19:11:26Z,0,Is it possible for the gradient descent algorithm to land on a local minimum of the loss function vs the global minimum?,True
@vipulchaudhari810,2022-10-22T18:29:08Z,1,Explanation is crystal clear and spot on. Thanks Josh for investing your time and energy to create such wonderful learning material.,True
@panterplatinum,2022-10-18T07:32:25Z,1,Thx! Great video,True
@raa__va4814,2022-10-16T03:46:07Z,1,"Hey Josh, could you remind me again where 0.64 came from? I see that we took a random value for intercept and then kept changing it ... that makes sense. But how did we know of 0.64 ... i believe this is the gradient for the ""best fit line"" given the data and hen again if thats the case then how did we find this ""best fit line""and therefore its gradient. Thank you in advance!!!!!",True
@RoshanMore04,2022-10-15T05:57:39Z,1,"I had searched tons of books to understand this concept, but none explained the way you did ...BAM!!!!!",True
@bunnyheart1,2022-10-11T14:33:39Z,1,Great vid! So clear and fun. Thanks,True
@sidverma1888,2022-10-11T00:35:12Z,0,"Hello professor, thanks for the explanation! Bam! Quick question, why didn‚Äôt we just solve for intercept in the d/dx(SSE) equation, opposed to iterating in smaller steps?",True
@saadyousuf9884,2022-10-06T06:06:25Z,0,I have watched both the least squares video and this one. But I cant understand the difference between gradient descent and least squared? Is it the same thing? If not what's the difference?,True
@Mars7822,2022-10-04T05:25:31Z,1,Spectacular video,True
@blacksages,2022-10-03T10:07:33Z,1,"clear explanation, great job!",True
@AshishBangwal,2022-10-02T06:11:16Z,2,when you explained all the steps to find derivative i laughed but after realising you do all this just to make sure everyone understands i felt so blessed to have people like you ‚ù§,True
@BentArabo,2022-10-01T06:38:55Z,1,Truly appreciate the simplification and enthusiasm!,True
@TheFolkRevival,2022-09-27T15:59:07Z,3,"What's really golden about this video is that you don't assume the viewer knows f.e the Chain Rule (even though it's good) and also show all the steps clearly. A lot of Math ""tutorials"" skip some crucial steps that might not be obvious to someone new to an area/idea, which make the material inaccessible. For comparison, even if your videos lack the great animations of f.e 3Blue1Brown, I think some of your explanations are much easier to understand.",True
@tagoreji2143,2022-09-26T07:07:51Z,1,One of the Great Lectures I have ever attended.Thank you  Professor Josh stammer üôèüôèüôè,True
@lera6801,2022-09-25T21:31:42Z,0,what about step size when it's negative? how we can measure that it's smaller than 0.001?,True
@bodwiser100,2022-09-24T11:15:04Z,0,Great video as always! Just one minor correction: at 12:13 you said -2.3 instead of -0.23. Just trying to help you out :D,True
@Waffano,2022-09-22T08:28:45Z,0,Why is it we don't just solve the the equation dSoSR/dIntercept = 0 where SoSR = Sum of Squared Residuals? Wouldn't we be able to find the optimal intercept this way? Is this just not doable/feasible when we have more variables?,True
@houman007,2022-09-21T07:12:59Z,1,I remember there was a whole lecture(2 hours) to explain what gradient descent is when I studied at uni. They should really play this video instead.,True
@sinarokhideh6794,2022-09-20T10:46:43Z,1,Man! This one was sooooo good. Thank you üôèüèª,True
@gangavaramsharath9471,2022-09-18T05:43:51Z,0,"Hi Josh, I have a small doubt  In starting of this video at 3.17 min You have mentioned predicted height=0+0.64*height . How this 0.64 came ?",True
@MufasaToday,2022-09-14T19:11:28Z,0,How is the derivative of the stuff inside the paranthesis -1 and not 0 ?,True
@petrkalina138,2022-09-11T10:44:20Z,0,"I am sorry, how do we come up to the slope estimate of 0.64 at 2:10? Thank you.",True
@imskr2683,2022-09-06T09:30:33Z,1,‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§üòΩüòΩüòΩüòΩüòΩüòΩüòΩüòΩüòΩüòΩüòΩüòΩ,True
@xiangzhang7355,2022-09-05T18:21:27Z,3,I have never seen anyone explain gradient descent as clearly as you!,True
@sadpotato5111,2022-09-01T09:23:15Z,1,"I LOVE your simple teaching style, it is like some kind of art. I give warm wishes for you, be happy :)",True
@salahaldeen1751,2022-08-30T04:03:41Z,1,Brilliant!!!,True
@deema5535,2022-08-24T12:54:59Z,0,I don‚Äôt understand how GD finds the intercepts that give the lowest value of the loss function by calculating what value of intercept make the derivative close to zero because local maximums give that too so how are we sure that by doing that we are finding a minima not just any critical point,True
@XFT8,2022-08-23T20:00:24Z,1,"I wish Josh could do a video on every possible subject one might need to learn.  Let's crowdfund to clone Josh!  Thanks Josh.  Josh, what's your day job, I'm curious.  You make these videos just for fun?",True
@shubhambhardwaj6475,2022-08-23T02:35:42Z,0,Which step size to consider (in regard to stopping the gradient descent) as we have got two here - FOR INTERCEPT & FOR SLOPE ?,True
@mursyidelric4734,2022-08-22T01:32:48Z,2,"Thanks for explaining ML in such easy to understand way, what a chad you are sir",True
@anishchhabra6085,2022-08-12T08:08:53Z,1,"Great work Josh, amazing explanation. Thank you!! Respectüôå",True
@ManojKumardurai,2022-08-12T08:00:50Z,1,explained clearly,True
@juliocardenas4485,2022-08-11T23:27:24Z,1,I want to be like Josh when I grow up (I‚Äôm 42),True
@faezehshalbafzadeh1931,2022-08-10T14:19:26Z,1,Thanks a lot!!!!It was greatüëçüëçüëç,True
@simonetruglia,2022-08-07T15:00:44Z,1,"love this video, once among the best on the web about this topic, especially when you're a visual lerner. Thanks mate",True
@pooyanyaghmaieyan1392,2022-07-31T12:27:03Z,1,thanks a lot !! Great !,True
@kutlwanotshatiwa8798,2022-07-24T09:26:55Z,1,I now understand gradient descent better!,True
@namitharassi,2022-07-22T04:40:27Z,1,Thanking you and praises to you a ton.. what a video. Thanks buddy.,True
@zappist751,2022-07-13T04:07:14Z,1,Jesus christ this is so insanely good omg thank you!!!!!!!!! BAM!,True
@sukanya4498,2022-07-12T10:32:43Z,0,"Actually, The Gradient descent considering both Intercept and slope gets to zero when 0.9385. Just manually checked ! üò¨ for fun!",True
@anantgosai8884,2022-07-09T10:45:04Z,1,You are the best! That's all I can say..amazing,True
@RoelBaardman,2022-07-08T12:01:17Z,1,This video helped me implement and debug a gradient descent implementation in C++. Thank you!,True
@FredoCorleone,2022-07-06T12:40:18Z,0,17:03 why do you multiply by -1???,True
@emirhalilovic3362,2022-06-28T10:06:17Z,1,"The best explanation of this topic I've come upon so far, helps me a lot while learning the gradient descent. Thank you so much!",True
@zahraaelattar9960,2022-06-23T21:10:49Z,1,I cannot thank you enough!! Best explanation I have ever seen so far!!,True
@raoufkutaish1294,2022-06-23T13:01:53Z,0,Thanks for the video. I would like to ask if I can use gradient descent to fit exponential function,True
@michaelbehrman1479,2022-06-22T23:00:18Z,1,"This video is incredible, thank you so much!",True
@gix111,2022-06-22T20:03:27Z,3,You explain complex things to me better than my professors.  Thanks for saving my semester. ‚ù§Ô∏è,True
@c.nbhaskar4718,2022-06-22T12:56:18Z,1,"Hi Josh ,  i have finished whole ML series..Are you considering making new statquest for Natural laguage processing??",True
@tiago9617,2022-06-20T16:25:24Z,0,Richard Feynamn would agree that you know the Gradient Descent,True
@ainiaini4426,2022-06-20T14:52:34Z,1,Wow that was an excellent explanations.. That BAM and poopooo made this video extra fun to watch.. Thank you so much,True
@nikitanikitov9362,2022-06-20T14:13:43Z,0,"8:29 Anyone, why (-1)Intercept???",True
@mahima1219,2022-06-18T10:56:35Z,1,"Its unreal how easily i understood this from you,i have been trying since 2 hours to understand this ugh",True
@feiyangbai8913,2022-06-17T21:34:33Z,1,Thanks. This helps a lot!,True
@tysk_dt,2022-06-16T20:37:29Z,1,"i was confused at first because at 11:05 you say the step size is ""negative 5.7"" although negative 0.57 is displayed and you keep saying it like that, which makes me think that it's supposed to be like that. but i think it's not? but guess i'm the only one!! easily gets lost, when one is not a pedantic annoying person like me! ps love your videos",True
@user-ek7sz3mu3i,2022-06-15T13:56:24Z,3,I envy modern young math students in colleges all over the world since they can easily choose best explanation of complex things on YouTube( Your channel seems the best I found!). It;s not like studying in the beginning of 2000's when the teacher was drawing some ugly formulas with chalk. making so great thing as maths boring to us. Who knows may be someone who will treat cancer or colonize Mars will pick up some missing brick of understanding from your videos. Pretty much thanks for your effort!,True
@SuperPvenegas,2022-06-09T12:34:59Z,1,"Thanks  Josh,  I'm a first time viewer of StatQuest, and thought I'd leave a comment and say BAM!  Thanks for taking the time to appropriately distill key concepts (e.g. gradient descent) and make them more consumable.  Also, just purchased the paperback on Amazon, should arrive tomorrow DOUBLE BAM!",True
@marcusdidius3279,2022-06-09T11:08:35Z,0,Great videos thanks. However I am confused about the relationship between epochs and gradient descent. In one epoch is only one step of the gradient descent executed? Or are steps executed multiple times until step with < minimum in each epoch? How does the model improve with more epochs executed? As suggestion: I think it would be benefecial to provide a mini-project so that you can see how the concepts translate to code.,True
@silverth1002,2022-06-08T17:16:09Z,0,"man, dogshit lecturers charge thousands of dollars per unit, but I learned more in one youtube video than I did the whole trimester. Really hope my lecturer gets fired soon",True
@NikitaSharma-bs4gg,2022-06-07T04:03:11Z,1,The video was so helpful ü•∫ü•∫,True
@sergioberrospi1778,2022-06-07T03:46:25Z,1,Love you man,True
@sharathcg4697,2022-06-04T06:43:40Z,0,Hi Josh @ 4:16 some authors used weight(w) on x-axis. so weight and intercept are one and the same and can be used interchangeably?,True
@youmna4045,2022-06-02T19:19:17Z,1,"Great explanation thanks , I couldn't stop myself from thinking  that you are Flint Lockwood from ""cloudy day with chance to meet the meatball""",True
@adityaagrawal3576,2022-06-01T20:48:54Z,1,most bestest tutorial!!,True
@peilingliu,2022-05-28T18:30:43Z,1,"hi this is a great lecture, only if put the error at different learning rate, such as 0.01 0.05 0.1.  bring us a feeling about monitoring the result :)",True
@sharathcg4697,2022-05-28T13:36:10Z,1,This is the easiest possible explanation that one can ever or never give. great work in video editing as well,True
@yaygrace,2022-05-24T02:06:46Z,2,"Thank you so much for making this video!!! I'm struggling in my stats class but after having it broken down step by step like this, I understand it so much better! You are a LIFE SAVER",True
@just_a_viewer5,2022-05-23T05:52:12Z,1,I can't believe I've finally understood this. Thank you!,True
@alexchang4582,2022-05-22T18:59:22Z,1,I learned gradient descent before. This is by far the best 24 minutes I spent to grasp the essence of this topic. Triple like!,True
@darkkiler7373,2022-05-19T11:38:49Z,2,You saved me thanks üò≠,True
@abdelrhmanrhyaseen6194,2022-05-19T08:17:16Z,1,"Wow, the best",True
@joxa6119,2022-05-17T14:14:03Z,0,"By knowing this, now I understand how powerful is Linear Regression even though it is a simple model of machine learning. Can't underestimate Linear Regression anymore after knowing gradient decent.   Got question Mr Starmer.  1- So even though the model has undergo gradient descent, are we still need to do the Cross Validation to get the best parameter for model?  2-  You said that in ""in other loss function"" in the video. In logistic regression, is there loss function too? What other model that has loss function too?",True
@aleynasenozan6111,2022-05-14T12:27:41Z,1,the best video which  ƒ± have ever seen,True
@abitofeva494,2022-05-11T09:11:38Z,1,Rlly appreciate your work!,True
@aureliusnt,2022-05-10T20:06:30Z,1,Awesome! BAM!,True
@TheHawkingSolutionHD,2022-05-10T19:37:41Z,0,"Thanks Josh, really great explanation as always. I would like to add that a lot of people are not calculating the derivatives by hand and chain rule but prefer using numerical methods. But the intuition is the same nevertheless.",True
@venkatprasath01,2022-05-10T15:59:02Z,1,This is Gold! ü§Øüî•üî•üî•,True
@monoarul_islam_3,2022-05-09T20:58:10Z,1,"Hi josh, I hope you are doing great. Congratulations on your new book. I want to learn machine learning but I'm in a fix. I don't know from which order to maintain your video series of machine learning. Could you help? Thank you.",True
@edidiongesu4035,2022-05-09T10:45:15Z,1,"First time on this channel and Big shout out to you, man. Subscribed quickly too. Thank you.üî•üî•üî•",True
@alevelsos,2022-05-05T08:12:34Z,0,"So because changing the interecpt changes our values for predicted hight, is that the reason we do partial differentiation with the intercept. ( I mean because we only change the intercept)",True
@aloooshalsoumahi5124,2022-05-05T05:23:39Z,1,No words can describe your teachings I willüëèüëèüëèüëèüëèüëè,True
@yc6768,2022-05-03T14:48:15Z,0,"Great video! Thanks! I have two questions: 1) to find the minimum value of the two gradients, can't we take their derivatives and set to be zero? 2) when trying to find the minimum values for both gradients together, using the same learning rate seems not take into account the individual impacts for each of them? If slope has more impact on the loss function, maybe take a smaller step compared to the intersect? Thanks again.",True
@satviksrivastava6632,2022-04-28T10:51:53Z,0,"In sklearn SGDclassifier, gradient descent is used for finding weights.  In sklearn linear regression, which technique is used to find weights????",True
@whyareugae5528,2022-04-26T21:54:37Z,4,"I wish I saw this video before I attended a 3 hour lecture on GD. Professors have no idea how to explain the basics. Thank you so much, now it all makes sense.",True
@SL14285,2022-04-26T06:11:27Z,0,"I have a question, how is the learning rate chosen, why is 0.1, not 0.2 or 0.01? and why the step size is -5.7x0.1, what does that mean?",True
@caosdo8297,2022-04-23T20:30:14Z,0,is it just me or some formulae for the derivative of the slope got squared incorrectly at 20:54?,True
@RishabhGoyal,2022-04-22T17:59:35Z,0,19:32 - formula with respect to slope is incorrect and contains a power,True
@sejalwagh8150,2022-04-12T08:03:17Z,1,thisssss isssss extremelyyyyyyyy helppppfullllllllllü•≥ü•≥ü•≥ü•≥ü•≥.... thankkk youuuu sooooo muchhhhhhhü•∫ü•∫,True
@sajjadalhajji2069,2022-04-11T22:26:49Z,1,Thank Yooooooouuuu,True
@mdarifurrahmankhandoker6180,2022-04-10T21:37:08Z,1,"Very nice video, liked and subscribed!",True
@nikunjprajapati1445,2022-04-10T14:31:02Z,1,Bro you are fire :>,True
@sammyraul5402,2022-04-09T20:53:43Z,0,How did you arrive at 0.64 as the slope of the line?,True
@rusundevelopment7674,2022-04-08T12:35:52Z,0,"22:45 Perhaps, step 5 formula is New Parameter = Old Parameter + Step Size (plus sign instead of minus).",True
@haribattula5187,2022-04-08T06:36:22Z,0,First of all I'm very good for having you here and explaining it in very simple manner as possible. But I have one doubt.... Why you only take derivative of sum of squared residuals instead of whole cost function.. I mean you left  1/2M  at the beginning of the cost function.. I know you are only taking derivative of sum of squares of errors... why wont you take an average from it by dividing the derivative with 2M as it is the definition of Loss Function Cost=1/2m(sum of squared residuals).. Thanks for this video mate...,True
@realurah,2022-04-07T15:55:30Z,1,you are amazing man,True
@larasmoyonugroho9945,2022-04-06T14:11:44Z,0,so.. sum of squared residuals is THE loss / cost function.... Wow... how could i missed that in optimization class? ok.. i have question Josh... is it possible if the loss/cost function is not quadratic sum ?  e.g. sum of [ error of height = abs(observed height - predicted height)  ] ...,True
@Atarios,2022-04-04T18:39:05Z,0,"1.4 minus 0.32 is equal to 1.1 ,  I'll remember that",True
@PhantomUniverse,2022-04-02T02:41:47Z,0,"But I am not understanding sir, I am quite weak student in study may beüòîüòîüòîüòîüòî",True
@juanguang5633,2022-04-01T06:32:53Z,0,"I have a question. At 11:08, why do we add the negative of the step size to get the new intercept? Since the slope = delta loss / delta intercept, and we want an increment on the x-axis(intercept), wouldn't slope * learning rate gives us an increment on the y-axis(loss function)?",True
@mohsen865,2022-03-31T16:20:27Z,0,i wonder how you have understood math concepts so deeply that enables you to express and explain incredibly clearly. many thanks professor,True
@sabrinakadirova7084,2022-03-30T19:42:51Z,0,Can anyone explain to me why the stuff inside of the parentheses is always -1?! I've stuck on the 8.19...,True
@annasaralidze4099,2022-03-28T19:50:21Z,1,"NEVER FORGET,  u are the best person i have ever met in youtube, THX for everything i hope u are feeling great all day <3",True
@abdulghanialmasri5550,2022-03-27T00:35:16Z,1,"Man, you are great, thanks for the efforts you put in these lessons.",True
@motherisape,2022-03-24T14:07:49Z,1,Bammm,True
@raghus8535,2022-03-22T17:23:06Z,1,"Many Thanks, Mr. Josh, cant believe it. watched several videos on gradient descent but this one is the best i have ever watched. appreciate your  effort.",True
@kenyoon2769,2022-03-18T21:04:51Z,1,jesus I really love this video. So clearly explained!,True
@mathewmurdock5297,2022-03-17T11:00:18Z,1,Your teachings is extraordinary.. No other video i saw like these. Amazing. Thank u somuch,True
@abhijitharidas4471,2022-03-16T16:21:03Z,1,This is a crazy good explanation!,True
@vijayguttula8115,2022-03-09T05:53:55Z,1,Double Bam!!üî•,True
@wiktoriapoch2679,2022-03-04T13:10:39Z,0,Thanks for this video! I have one question. Why should we use gradient descent instead of checking for which values of slope and intercept both derivatives are equal to 0?,True
@sylashalder6202,2022-03-03T10:09:02Z,1,Thanks a lot. I have understand deeply. You are so great. Again Thank you very much. Because I was so worry about gradient. Now I have got solution. Love you very much brother.,True
@shivendrapandey2103,2022-03-02T01:40:50Z,1,super helpful to revise the basics :) Thanks!,True
@cameronsantiago3155,2022-02-27T20:57:27Z,1,solid work,True
@sandipansarkar9211,2022-02-26T19:07:33Z,0,finished watching,True
@billyin4771,2022-02-22T15:16:21Z,1,This is a really straightforward and clear tutorial :D,True
@ahmedal-kufaishi7156,2022-02-22T14:53:10Z,1,You deserve my tuition fees not my university.,True
@summernie4964,2022-02-22T03:06:41Z,0,why would you * -1 when calculate the derivates. How to decide when I need to *-1 or 1?,True
@bennybenbenw,2022-02-18T03:47:16Z,0,@StatQuest with Josh Starmer 19:18 - your chain rule forgot to remove square,True
@darkprince2703,2022-02-17T22:20:20Z,1,awesome thanks,True
@gauravsharma-sd2mg,2022-02-10T08:16:59Z,1,I love the way you explain. Your videos are awesome üëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëè,True
@abhinavsingh9720,2022-02-09T10:59:57Z,1,Thanks a lot,True
@nivi8319,2022-02-04T16:47:45Z,1,Thank you sooo much that I got to know your channel .... I won't forgive myself if I just leave this without saying thanks to you... It helped me a lot and moreover I enjoy studying with your video....I understood the concept really well and u deserve more üíú,True
@buchdev,2022-02-04T10:32:50Z,1,Very well explained!! Thank you very much. Gradient Descent is very helpful,True
@RAJIBLOCHANDAS,2022-02-02T13:26:58Z,1,Good explanations,True
@abhinavsingh9720,2022-02-02T08:54:09Z,1,Thanks,True
@quiaro21,2022-02-02T00:41:04Z,1,My research project says thanks...,True
@tanyaahuja8923,2022-02-01T18:39:59Z,1,Can't thank you enough.,True
@superk9059,2022-02-01T09:38:30Z,1,I have joined in partraon. Thounds BAM!!!,True
@nithyapalanisamy4904,2022-02-01T00:01:22Z,1,Very crystal clear indepth explanation. I thoroughly enjoyed of learning. Even I can teach 2 year old kid.thanks a lotttttttt sir,True
@rampat9296,2022-01-30T23:17:27Z,1,"The best,  extremely good.",True
@arafat_ar13,2022-01-28T12:27:27Z,0,"This was really amazing and educational. Best gradient descent tutorial I've watched so far. Quick question, we know that the optimal intercept is when the gradient of the loss function will be zero. So, when we had the equation with the intercept as a variable, couldn't we have just set the gradient to zero and solved for the intercept that way? If I'm missing any points please correct me; why pick the intercept when we are veryyyy close to zero when we can just solve for the intercept by setting the gradient to zero that way?",True
@douglasespindola5185,2022-01-27T20:23:43Z,0,"Does anyone knows what Josh wanted to say at 20:36 with: ""The good news is that in practice, a reasonable learning rate can be determined automatically by starting large and getting smaller with each step""? I didn't get what ""a reasonable learning rate can be determined automatically"" really means. Anyway, a TRIPLE BAM video as always! You're a master in the teaching art, Josh!",True
@kyl6292,2022-01-25T17:25:29Z,1,20:58 Question: We have two equations that we update with new values (intercept and slope). But which of them do we look at when we check if step size is small enough?,True
@calito44,2022-01-22T03:23:13Z,1,Your 23 minutes  explanation minutes worth  30 pages of examples and math explanations  in a hardcore Machine learning  book.,True
@udujibonnke4931,2022-01-20T16:58:29Z,0,please how was the -1 gotten with partial derivative,True
@hungp.t.9915,2022-01-13T04:40:30Z,0,"near the end of the video, the intercept and slope get nowhere near golden value through?",True
@waleedfarooq9437,2022-01-08T10:14:13Z,1,Beautifully Explained!,True
@ashokachoudhurybhattachary9137,2022-01-04T07:47:23Z,0,"Very good explanation, please check the derivative with respect to the slope at 19:30 instant",True
@shivakiranreddy4654,2022-01-03T17:14:30Z,0,"HI Josh , If you have already picked the least squared value for the slope, means that is slope of the best fit line, ( you got that best fit line by rotating the line and plotting the SS(distances) and getting the one which was close to zero, you are using this slope to calculate the best line using gradient descent now? Why?",True
@toyl6727,2021-12-24T03:02:46Z,1,"I am doing a academic research project, and I thank you for really showing me the significance and utility of Gradient Descent!",True
@Mr-7ou6gn,2021-12-23T03:20:51Z,0,"Learning rate is smaller, and the number of steps is higher in practice to get reasonable accuracy (two decimal points).",True
@vincentjonathan,2021-12-23T01:43:00Z,1,"I have question about sum of squared formula to be used as residuals formula. I hear from my lecturer if we do that to avoid minus in substracting the predicted and observed value. My question is why don't we only ""absolute"" the result value after subtracting, instead of squared it? So the value will not minus too",True
@Rahul-fq9kf,2021-12-20T16:26:26Z,1,You teach so so well. Very thankful to you for these videos. I wouldn't have understood anything had it not been through you. Can't thank you enough!  God bless you!,True
@TheHappymyo,2021-12-19T22:08:45Z,1,Thank you  very much,True
@hyperspaced77,2021-12-17T09:03:41Z,1,This video is a typical example as to why teaching in Comp. Sci. Universities should change. The professor only has to present this video. It's discussion with the students for the applications there on.,True
@armanmalkhasyan4765,2021-12-14T19:29:41Z,0,In 19:13 You forgot to leave out ^2 on the heads of d/d_slope parameters,True
@ddi1118,2021-12-13T16:12:00Z,1,"Perfect explanation, subbed!",True
@ahamuffin4747,2021-12-12T20:57:41Z,0,"Thank you for the clear explanation. However, I still don't understand why we can take the value of the ""slope"" of the loss function to change the intercept. I don't understand the connection there. Thank you!",True
@user-ir7sl4gh9h,2021-12-12T15:59:29Z,1,Very informative and fun video to watch! Great explanation!,True
@zach6383,2021-12-03T22:07:39Z,1,You are very helpful thank you,True
@pg4234,2021-12-03T07:07:51Z,0,"At 9:32, you mention that gradient descent is very useful because we can use it even when we can't solve for where the derivative = 0. However, since the update rule contains the derivative of the loss function, how would we calculate the updates if we can't take a derivative of the loss function?",True
@kumaronlineplay,2021-12-02T14:09:24Z,1,Good Work.. Keep it up.. Nicely explained .. Anyone can eaasily understand,True
@armanmalkhasyan4765,2021-12-02T13:51:51Z,1,Thank you bro !,True
@mdaquib6511,2021-11-30T18:25:51Z,0,"I have confusion between gradient descent and regularisation. As far I understand from both, we are optimizing the weight term and reducing variance. I didn't find any difference yet in both. Can you please help me out with this?",True
@slimyelow,2021-11-28T19:02:41Z,2,"This has helped me tremendously in the understanding of basic Machine Learning, where all materials I have found so far blow past this very issue, as if it were something so obvious to everyone. But at precisely 4:20 I got my 'Aha!' moment. yay!",True
@mimosveta,2021-11-25T01:17:12Z,0,"my only objection is that you used values instead of symbols, I know what symbols stand for, but it's hard to keep track of what values represent, other than that, BAM",True
@santiagocalvo,2021-11-24T19:27:39Z,1,"well, well, well... looks like i found a new personal god",True
@fedvgo,2021-11-24T10:11:57Z,1,Thank you for your videos they are funny and help clearify complicated stuff.,True
@rishisingh6111,2021-11-23T15:55:19Z,1,Simply awesome.... thanks a ton for sharing this!,True
@xMarious98,2021-11-17T18:12:15Z,1,Ti amo,True
@rediettadesse5488,2021-11-17T16:22:57Z,1,"Woow, this is brilliant, Thank you so much!!!",True
@brindhaganesan3580,2021-11-17T16:08:10Z,1,"Deivame! I was struggling with Gradient Descent and luckily landed on this YouTube page! I also bought this study guide! I can‚Äôt wait for your book sir, Thank you for making my journey in Data Science easy and fun üòÑ",True
@doaaahmed9730,2021-11-16T21:28:53Z,1,Best  EXPLANATION EVERüíõüíõüíõüíõüíõüíõ,True
@renanreismartins,2021-11-15T23:30:12Z,1,you know your game man... well done,True
@javierlara7756,2021-11-15T22:04:02Z,0,eres dios reycito! un vioh mi compa <3,True
@ibrahimnada4702,2021-11-10T12:34:47Z,0,"@2:15 from where did u get the 0.65 for the slop ? "" lets just plug in"" is not enough",True
@p-niddy,2021-11-09T12:48:15Z,0,Can you explain where the primal vs dual problem fits into this picture? That whole area of optimization remains a mystery to me (p.s. I'm quite surprised [bummed out] you still have no video on Optimization),True
@nhiduong2412,2021-11-08T14:17:26Z,0,How do we plug in a random value to the slope and make sure that the derivative of the sum of squared residuals actually go closer to 0? what if It goes farther away? Can anyone please explain. I'm confused over this point. Thanks a lot!,True
@ferashabib2838,2021-11-05T19:24:58Z,1,i love you,True
@ferashabib2838,2021-11-05T19:24:41Z,1,you are great,True
@thej1091,2021-11-02T21:15:23Z,1,legend tackling GD and SGD in one video!,True
@misha1998923ify,2021-11-01T15:20:09Z,0,"Finally, some good fucking explanation. Thank you !",True
@joyhumin1,2021-10-31T21:28:38Z,1,Cannot express enough appreciations of your amazing content! Please continue your work!,True
@hussmq,2021-10-31T09:13:56Z,1,Perfect explanation BAM,True
@chouawarasteven,2021-10-30T05:43:23Z,1,"""Triple BAM!!!""",True
@ratnakarbachu2954,2021-10-28T16:02:42Z,2,"Finally lots of search on the internet , again I reached to the same place where things are explained easily with heart. Really you are good hearted as like the concept. Thank you sir, we owe you.",True
@yihongliu7326,2021-10-25T05:04:48Z,1,truly amazing,True
@BillHaug,2021-10-24T16:50:58Z,1,thank you,True
@KidusYohannes,2021-10-21T00:49:02Z,1,"This was incredibly helpful, thank you!",True
@psrsanon,2021-10-18T22:23:30Z,1,saved my life,True
@asu4908,2021-10-17T22:14:54Z,1,"Cant compare this to standardized teaching in university as i‚Äôm still only in high school but this was a good video, thanks!",True
@pramodahetti1070,2021-10-16T14:38:46Z,1,great video sir well done,True
@jongcheulkim7284,2021-10-15T09:18:57Z,1,Thank you.,True
@azzordiggle5143,2021-10-12T02:58:36Z,1,"Please, Does anyone has experince getting refund from university, I need it so I can pay this guy.",True
@lilyli339,2021-10-05T18:20:13Z,0,Love it so much!!! This is the best gradient descent lecture I found!!! Hope there will be mini-batch gradient descent lecture in your channel too!! Thank you so much for your videos!,True
@kjlee8786,2021-10-05T04:35:36Z,1,the channel is golden,True
@user-ch2sd6fm5l,2021-10-02T23:50:47Z,1,goat......,True
@minipc123,2021-10-02T11:30:12Z,0,"Thank you for the wonderful explanation. Just one thing I don't understand, how do you derive the update rule, ie new parameter = old parameter - step size?",True
@RobertWF42,2021-09-30T17:36:41Z,1,I've recently used gradient descent to fit a complicated probability distribution (the generalized beta of the 2nd kind) to insurance claim amount data for my job. There were no simple solutions using maximum likelihood.,True
@declanchrist2497,2021-09-30T08:54:30Z,1,"Thank you so much! This is an incredibly helpful video, and you are great at explaining things! Great work :)",True
@fhypnos912,2021-09-29T15:02:15Z,1,I FINALLY UNDERSTAND GRADIENT DESENT,True
@trust4774,2021-09-29T11:34:09Z,1,It's amazing...I want to give 10 thumbs. It did help me understand from scratch. Thank you so much!!!,True
@GaMiNGYT-dc2cf,2021-09-28T19:16:52Z,1,"Wow,what a clear explanation!!! Never saw anyone before like this guy..Hatsoff",True
@cisantacruz,2021-09-28T16:31:05Z,1,"love u, thanks!! i finally got it",True
@AliNa-ex1ct,2021-09-26T14:21:28Z,1,Super :) Thanx,True
@jorgeblanco1637,2021-09-25T15:32:23Z,1,YOU ARE AMAZING!!! THANK YOU SOOOOOO MUCH,True
@vincentvaldinata2020,2021-09-24T01:40:26Z,0,why we need to multiply -1??,True
@mojojojo890,2021-09-23T14:30:33Z,0,How do we know the derivative is always convex ... coz if it wasn't then the algorithm will reach a local minimum (not the most optimal answer),True
@tejasbalshetwar9587,2021-09-20T12:39:43Z,1,This was the best explanation that I need ed to understand gradient descent. Really liked it üî•üëçüëç,True
@abhi9029,2021-09-18T12:08:45Z,1,You are doing a god's work.,True
@DaviAreias,2021-09-16T13:00:08Z,1,What I expected at 18:00:  *THE CHAIN RULE*  What I got: ·µó ∞·µâ ·∂ú ∞·µÉ·∂¶‚Åø  ≥·µòÀ°·µâ,True
@amilagoku6073,2021-09-15T17:37:22Z,0,can we make intercept and the slope both be zero?,True
@radhey04ec,2021-09-12T14:40:46Z,2,What a Beautiful Way of teaching..... Appreciate üôèüôèüôè,True
@thomasquaid6932,2021-09-12T14:15:07Z,1,I laugh harder at this video than watching standup. Thank you for blessing the nerd world with your genius.,True
@rlankela,2021-09-09T22:34:19Z,1,The best,True
@divjyotsingh7400,2021-09-08T04:13:02Z,5,Brilliant! 2 years into ML and I still find this the best explanation of GD.,True
@itssidhere,2021-09-07T15:54:39Z,1,What a genius this guy is,True
@manubhatt3,2021-09-07T05:52:31Z,0,"Why we just can't set the partial derivatives to zero and solve n equations for n variables, again??!",True
@AJ-et3vf,2021-08-30T11:36:05Z,1,"Bam, so awesome!",True
@marioestrada2233,2021-08-29T18:13:01Z,1,Man you are truly awesome!!! Thanks tons!!!,True
@dishantvyas977,2021-08-28T07:43:36Z,1,19:30 was 'BOOM' üî•üî• Amazing explanation!,True
@twilli1391,2021-08-28T07:32:05Z,1,THANK YOU !!!!!,True
@flyingpama,2021-08-23T17:25:19Z,1,BAM! God this is so helpful. Thank you!!!!,True
@61_shivangbhardwaj46,2021-08-21T06:45:23Z,1,Thnx sirüòä  for quality content,True
@kmmertes,2021-08-20T17:06:28Z,0,"Totally disagree with ""shouldn't have to worry too much about the learning rate"" statement.  Determining the learning rate is by far the most challenging aspect of gradient descent.  If you employ a fixed learning rate, then one must figure one out for each problem.  Otherwise you will need to employ an adaptive learning rate.  There is a tremendous amount of research on adaptive learning rates because it is so hard...especially in high dimensions.",True
@sivarajchinnasamy11,2021-08-15T06:58:59Z,0,"This 22 min video takes  almost 2 hours for me , due to again and again went through üòÇüòÇ",True
@jos4552,2021-08-14T00:17:21Z,0,3 important lessons ---------------------------------------------------------------------------------------- 1. Partial derivatives and a simple real-world application. 2. Algorithms - I can now easily implement this in a code. 3. Linear regression ( ML),True
@dibyajyotichowdhury7412,2021-08-12T17:50:48Z,0,"Hello this is a very nicely presented video. It answered most of my doubts. However , just one concept I could not get. Why is gradient descent preferable over ordinary least squares method when there are lots of features in dataset?",True
@EvgenSuit,2021-08-12T11:38:48Z,0,"18:50, i think you forgot to remove the second degree from the last two derivatives",True
@rogmes1380,2021-08-12T07:22:50Z,1,"Good, explain. just watch this video to quick overview before take an exam.",True
@gaurangpendharkar6324,2021-08-10T23:21:07Z,0,"I have a question. During gradient descent, wouldn't finding the optimized intercept affect the derivative of the slope (d of cost/d of slope) which causes us to find a new slope which changes the derivative of the intercept (d of cost/d of slope). Basically intercept and slope both affect the derivative of each other, so how does that either not cause a loop or some issues?",True
@gaurangpendharkar6324,2021-08-10T22:40:19Z,2,This is insanely helpful Josh!!! I'm trying to understand neural networks and made the mistake of resorting to academic papers which use complicated language and unhelpful explanations. I read those for hours and only understood bits and pieces but a 25 minute video on this youtube channel helped me understand better than ever!,True
@banuteja1406,2021-08-06T17:45:37Z,0,how did you take the slope as 0.64 out of thin air,True
@wolfywolfgang2498,2021-08-06T06:58:22Z,1,"Am I the only one who its the like button, just by listening to the intro song ???",True
@learnaiwithjoelbunyan4764,2021-08-03T14:02:14Z,1,Great video!!,True
@starkarabil9260,2021-08-01T19:36:34Z,1,such a great explanation!,True
@CT99999,2021-07-30T04:14:01Z,0,"Really great explanation! Just a few very tiny suggestions for improvement: - @ 19:19 ""when you have two or more derivatives of the same function"" would be clearer as ""when you have derivates of two or more variables of a multi-variable function"". with your wording, could think that taking the derivative of a single parameter, but for two different values, could constitute 'two or more derivatives'. you meant for 'two' to modify the number of parameters that you take the derivative of, not the number of derivatives one takes (which could all be for the same parameter). - @ 20:12 you overuse the term ""slope"". it's confusing that you are now referring to the *partial derivative* of the sum-of-squared loss function as a ""slope"" (e.g., -1.6 and -0.8), while one of your two parameters in your overall linear function is ""slope""",True
@twenteex2519,2021-07-29T07:46:12Z,0,Could someone explain how he got the equation new intercept at 11:12,True
@anastasiakhodyunova668,2021-07-28T21:56:05Z,1,"Great video, thank you so much !",True
@nandinikadre7883,2021-07-27T04:16:03Z,1,Thankyou for the efforts that you put in the video. The concept is explained very nicely. It cleared all my doubts regarding the concept. Keep it up! Good work!!!,True
@shilpajain3986,2021-07-25T13:08:17Z,1,You are a GOD!,True
@rossbrigoli6780,2021-07-25T10:45:30Z,1,Bam!,True
@TheProffromBelgium,2021-07-24T11:28:27Z,0,"at 20:08 : and that gives us two slopes .... is that a right statement ? it gives us an intercept and a slope onto which one applies the step size. Or am I seeing this  totally wrongly ? in my reasoning, the video then continues correctly referring to the new intercept and slope sizes",True
@catcat-jx2bz,2021-07-24T06:01:08Z,2,brilliant,True
@HaribanshAgrawalModi,2021-07-16T18:06:28Z,1,"if my professors and teachers would have taught us like Josh. i would have become ""Jack of all Master of all""",True
@georgetzimas6882,2021-07-16T03:06:36Z,0,"14:02 if the minimum step size would be 0.001, how can the step size be smaller, since that's the cut-off point? Shouldn't it say 0.001 or larger, or am I misunderstanding it?",True
@sandydsa,2021-07-15T10:33:38Z,1,Hello from the Philippines!  Thanks for this video.  I give it 5 out of 5 BAMS.  :),True
@Vitenuto,2021-07-13T21:19:39Z,1,Thats really awesome :O I would like to see it in the start of my course!,True
@seathru1232,2021-07-11T11:22:38Z,0,"Dear Josh, when I try to implement the principles of this lecture on some new data, I get some strange results that I can't interpret. It seems that if I increase the number of samples to, for ex, 50, the learning algorithm produces aberrant values for both slope and intercept. I had to reduce the step size to a very small number (0.001) to get a reasonable fit. Could you please help me understanding why so? How can I set an optimal learning rate, number of iterations so that it adaptes to any sample group, regardless of its size or entity of the numbers stored inside? Thanks a lot!",True
@golamchowdhury4165,2021-07-11T07:56:44Z,1,"You are the man, you know how to teach, I was banging my head for few weeks with google's crash course, then I found this vdo!! Thank you so much. Its so easy?? now I would hunt for what other golds you have put here in your channel",True
@gojosan1376,2021-07-07T11:06:11Z,0,at 9:26 why cant we put the  sum of square residual equal to zero ? i think that's least sqaure,True
@jasonfaustino8815,2021-07-06T14:20:29Z,1,Did you just trick me that partial derivatives are fun? Thanks man been binging your vids and not one vid has let me down. Thanks!!,True
@macunknown9173,2021-06-29T04:27:40Z,1,"Love how clearly you explained, the recap in middle sure helps too, doesn't let us get lost !! More power to you  üôåüèª",True
@knightganesh,2021-06-24T05:18:06Z,242,""" If you can't explain¬†it¬†simply,¬†you don't understand¬†it well enough"" - Albert Einstein  Can't get much simpler than this explanation. Thank you so much awesome work keep it going üëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëç",True
@balaji.j2024,2021-06-19T13:06:59Z,1,"After understanding gradient descent from this video,  My Brain: Bammmmmmmmmmmmmmm!!!!! Thanks a lot, sir",True
@sadrabakhshandeh1835,2021-06-17T05:21:51Z,1,you are amazing,True
@Chris-yq1fw,2021-06-16T03:07:47Z,1,"I had to relearn calculus courses to understand this throughfully, completely worth it !",True
@SuperVarunsharma,2021-06-12T13:41:36Z,1,"I like how you make fancy things easier, making simple folks like me understand the so-called ""rocket science"" stuff.",True
@Rainbient,2021-06-11T20:10:27Z,1,Bam!,True
@bactran7799,2021-06-11T07:49:14Z,1,super great,True
@vincent3542,2021-06-11T02:54:48Z,0,"Sir why the values of sum of squared residuals are decreased, along with y intercept has decreased at 4:44? is that because the sum that we calculate like intercept + slope.weight at 3:17?",True
@PaulColclough47,2021-06-10T15:40:18Z,0,"One thing I don't get (at 09:54) is the relationship between the value of the slope (-5.7) and the line on the chart that represents the slope.  How did you use the figure -5.7 to plot that red line?  I get that a slope of 0 corresponds to a horizontal line, but I just don't understand how -5.7 corresponded to that line at that angle.",True
@maierelias7315,2021-06-10T04:48:37Z,1,I've watched so many videos about this topic but this is the first time I understood it. Great explanation thanks for the video,True
@shivanshjayara6372,2021-06-08T20:35:14Z,0,"SGD uses randomly selected data means after evey new value of the parameters we get, we then going to take new data subset then those new parameters value will be going to insert in that new derivative func which has new data i.e. weight...am i right?",True
@nithinprakash1382,2021-06-06T06:08:08Z,1,you made me understand this which was a puzzle for me from long time.. Thank you,True
@vikasreddy7015,2021-06-04T19:40:29Z,0,The video is awesome but the sounds you make in between is kind of making it hard to concentrate on the subject. just a review no hate !,True
@cvic7143,2021-06-04T15:08:59Z,0,I wonder why we need to take so many steps in gradient descent. Why can't the computer solve the intercept value when the derivative of the intercept value equal to zero?,True
@voidknown2338,2021-05-23T16:59:00Z,1,Bam Double Bam Triple Bam The End........,True
@davidphan6499,2021-05-19T16:28:22Z,1,Mr. Starmer was born to be a teacher.  Thank you.,True
@Immerz,2021-05-19T13:42:13Z,1,"I love that you always put the entire script in the slides. This way we can pause and really let the things you just said sink in. My teacher is probably a good teacher, but sometimes the things he says go into my left ear and leave through my right. Would be nice if I could pause and replay him aswell lol.",True
@binduskumar3201,2021-05-17T10:48:32Z,1,Learning....understanding is happiness....you are my happiness....BAM ü•≥,True
@husseinfadin3354,2021-05-14T20:55:20Z,0,nice but I well use this video as guide for solving derivative MSE,True
@mohamedgaal5340,2021-05-14T06:00:19Z,1,Another fabulous video!! Thank you.,True
@kashyapmantri2342,2021-05-13T17:48:15Z,0,My Crush: *Says Hi*  Me to my brain: *Just say something smart*  My two little brain cells: 8:24,True
@allyourcode,2021-05-12T08:57:27Z,1,"The ""bee boops"" are pedagogically CRUCIAL! DOUBLE ""BAM"" XD",True
@nailashah6918,2021-05-11T08:32:39Z,0,before you calculated slope using derivatives of sum of the squared residuals with respect to the intercept but after that you calculated intercept in the same way plz reply,True
@wliw3034,2021-05-10T00:59:04Z,1,Perfect Video Thank u for It,True
@nguyentn007,2021-05-07T02:41:02Z,1,your video is TRIPLE BAM!!!,True
@tony-thai,2021-05-04T16:22:06Z,0,"On 6:12, you had taken the wrong derivative calculations on the 2 last terms, didn't you?",True
@jayghosh8898,2021-05-02T18:05:02Z,1,"I can't thank you enough for making these informative and immensely useful videos. I think this is the first time I am using YouTube to learn something useful, and your videos make it so enjoyable",True
@muhammadfaisalinayat9150,2021-05-01T18:35:10Z,1,This was really a TRIPLE BAM. Best lecture on Gradient Descent on internet.,True
@sebastianheuchler436,2021-05-01T16:09:26Z,0,"Nice video, good explanation. Small notes: Graphic at 21:40 uses the wrong intercept. And please stop it with the awkward ""bam""s.",True
@uditmahajan555,2021-05-01T09:58:33Z,1,I KNEW THAT THE VIDEO WILL BE GOOD READING THE COOMENTS BUT D.A.M.N. I am hella impressed how well you teach this. Thank you for this content!,True
@andresouza6711,2021-04-30T21:32:25Z,1,tysm for this!!,True
@PritishMishra,2021-04-26T15:24:42Z,1,You are best...,True
@pavan2926,2021-04-26T03:12:26Z,0,Can you please explain how did u gave slope values as .64,True
@waattzup,2021-04-21T20:45:09Z,1,Let's BAM these 142 people who disliked this amazing tutorial!!,True
@nguyenkhoituan9291,2021-04-21T09:40:14Z,0,3:55 I think you mean 0.3,True
@sousf3881,2021-04-21T06:48:34Z,2,I love you Josh Starmer!,True
@rogertea1857,2021-04-19T08:52:10Z,1,So great! This video helps evacuate all the fog surrounding Gradient Descent. Thanks!,True
@Muuip,2021-04-17T09:30:19Z,1,Great presentation and visualizations!  Much appreciated! üëç,True
@hai7263,2021-04-16T16:20:03Z,3,The beep boop really caught me by surprise. Laughed out loud in front of my computer...,True
@arpiharutyunyan8400,2021-04-14T12:40:08Z,0,Please make a full ML bootcamp on Udemy. We are ready to pay for your videos. You are the best lecturer ever.,True
@NuclearSpinach,2021-04-12T14:58:02Z,0,"11:04 misspoke -- written words were right, spoken words shifted a decimal",True
@erensarnic4058,2021-04-10T20:42:52Z,1,Insane way of teaching. Greetings from Turkey üëã Respect.,True
@Anime_cooks,2021-04-07T15:22:53Z,0,Can u plz make a video on admm plz üôèüôè,True
@charanteja1928,2021-04-07T07:42:00Z,1,Thanks for explaining very clearly. One video that cleared all my doubts. Thank you again StatQuest,True
@MarcelloNesca,2021-04-06T22:06:50Z,1,"Unbelievable, I'm so PROUD to support your Patreon, your explanations leave me slack jawed with how conceptually easy it is to understand. Meanwhile I'm reading a book that over complicates things, you should get a teaching award!!",True
@originof8557,2021-04-06T19:45:22Z,0,"Why we subtract from the intercept value the step value? why not adding it? Looking at the L2 graph is clear that we need to subtract to reach the zero slope point, but without it how you can say that we need to subtract and not add?",True
@jaydeeppawar3696,2021-04-06T14:59:24Z,1,"Extraordinary work sir, hats off, thank you very much!!!!!!!!!",True
@md.nahidhassan6485,2021-04-06T07:08:48Z,1,After two years passed I really understand this concpet first time...........Thank you!,True
@seanfeely7990,2021-03-28T18:47:47Z,1,TRIPLE BAM,True
@maingo9586,2021-03-26T04:03:21Z,1,Thank you so much,True
@DEV-LUX,2021-03-21T13:05:35Z,1,"Easy and funny, thank you",True
@vaishantsah,2021-03-15T05:32:07Z,0,Why do we only use the Sigmoid function instead of other functions?,True
@mohiuddinshojib2647,2021-03-06T13:39:16Z,1,thanks,True
@timoschittly7007,2021-03-06T12:52:40Z,0,"Good Job man, you saved my ass again!! I just wanted to mention that the derivative of the slope at 19:36 still contains the exponent 2 after the derivation if you want to change it.",True
@hengdao1507,2021-03-05T02:47:30Z,0,"Could u please explain how to get the slope 0.64 ? Is it just a guess? Or calculated out by another equation? Thanks a lot üôèüèªI'm beginner , hope my question not going to disturb your time .",True
@martynasvenckus423,2021-03-01T08:12:48Z,0,"Isn't the sum of the squared residuals a cost function, instead of a loss function?",True
@auzaluis,2021-02-28T16:05:12Z,1,Beautiful explanation,True
@karlazongmokom9674,2021-02-26T20:52:18Z,1,"this was so insightful and easy to understand ,thank you soo much",True
@ongong8209,2021-02-25T12:39:03Z,1,thank you,True
@ATULSHARMA-ei5qh,2021-02-25T05:12:02Z,0,"Hi Josh, I have a small question for video at 19.32 where the square term for (d/dslope) is visible after taking gradient. Shouldn't the square term will be removed as we already put -2 in the front?",True
@catherinehiggins4526,2021-02-24T00:07:33Z,0,At 8:30 why did you times it by -1 ?,True
@shubhashish7022,2021-02-23T07:48:58Z,1,Wow! This was amazing.,True
@catherinehiggins4526,2021-02-19T20:10:21Z,0,Thanks! how did you get the slope in the first place?,True
@ahjiba,2021-02-18T22:37:00Z,0,"if you already have the derivative of the loss function, why can't you simply solve for the values of intercept and slope that result in the derivative being 0?",True
@reneulloa2647,2021-02-15T11:19:05Z,1,Beautifully done :),True
@shujiezhang3494,2021-02-15T08:12:00Z,1,"Exceptionally well-explained! Thank you, Sir!",True
@mertkarabulut8026,2021-02-14T20:25:17Z,0,Could I learn the program you used to draw the graphics by any chance. Great video btw helped me a lot thank you.,True
@karina7uriel,2021-02-14T15:51:36Z,1,Report those who are doing the andrew ng¬¥s  course and come here to understand the gradient descent,True
@martynasvenckus423,2021-02-13T08:09:16Z,0,"At 17:40, I think that second and thrid term should not be squared, since we already calculated its derivative?",True
@spearchew,2021-02-11T12:00:31Z,0,"another great video. At 19:20 you ""Note: when you have two or more derivatives of the same function, they are called a gradient"".  You seem to have deliberately chosen the number ""two"" rather than ""one"", but can't really decide why. If it said ""when you have one derivative of a function, it is called a gradient"" - would that not also be equally true ?",True
@tymothylim6550,2021-02-10T05:27:31Z,2,"Thank you, Josh, for this excellent video! I really like how you explain the concept of Gradient Descent step-by-step, with the aid of visual (and audio haha) cues. I really enjoyed the video!",True
@igorszemela1610,2021-02-09T19:23:43Z,1,great vid  thank you man,True
@Chuukwudi,2021-02-08T22:08:02Z,1,"From the bottom of my heart, Thank you.",True
@srmsagargupta,2021-02-08T18:16:41Z,1,Josh should be awarded professor of the century,True
@aswink112,2021-02-06T15:02:04Z,1,"Bam! Now, I am your big fan! <3",True
@manybarrylow3627,2021-02-06T10:14:50Z,1,AWESOME ! I never heard befor such a perfect description. I,True
@iliasaarab7922,2021-02-06T01:12:33Z,1,Amazing explanation!,True
@ashishgoyal4958,2021-02-02T08:00:10Z,1,Very Great Video. Kudos to Josh Starmer for his amazing work:) Also is there any book available of yours related to Machine Learning to read ?,True
@_mimi0007,2021-02-01T08:13:02Z,1,This is just awesome!  learned and understood a lot from this video about gradient descent.,True
@prasitamukherjee5864,2021-02-01T07:42:26Z,2,Thank you so very much! This was the cleanest explanation of Gradient Descent and it answered all my questions! The greatness of internet lies here. Keep making such informative videos,True
@ladyhangaku2072,2021-01-31T19:11:35Z,0,"This is so helpful! Just one thing: can someone please explain to me what ""intercept"" means in this case? I'm german and I can't link the meaning of this word to the german one used in ML .-.",True
@victorzaragoza2818,2021-01-28T22:33:34Z,2,Pero donde explica lo del brazo robot?,True
@rikudoukarthik,2021-01-28T15:10:38Z,0,"Fantastic video, Josh, as always! But, I'm still confused as to how exactly it is different from the regular least squares?",True
@ghee-air-moe5775,2021-01-24T20:52:22Z,0,"Homonyms. In ML we use the word ""weight"" as parameters we optimize using gradient descent. So maybe using an example ""weight"" wasn't such as good idea since weight can mean a person's actual physical weight or  weight could mean a parameter to tune with gradient descent. We see this sometime when people say ""mean"" when they mean average, do you know what I mean? ;)",True
@lakshman587,2021-01-24T10:48:27Z,1,13:45 is what I was looking for BAM! This cleared my doubt! DOUBLE BAM!,True
@milikest,2021-01-21T13:33:53Z,1,That is most hilarous and understandable way of describing this algo. Thanks for such a good video.,True
@eugenebragin3812,2021-01-19T21:13:21Z,0,"Thanks for the awesome video! At 10:54, I don't quite get why the intercept should be changed by subtracting exactly dSSR/dIntercept (multiplied by the learning rate). If dSSR/dIntercept is a rate of change of SSR with respect to Intercept (dSSR/dIntercept = -5.7), shouldn't then dIntercept be = dSSR/-5.7? So new_Intercept = old_Indercept - (dSSR/-5.7)*learning_rate?",True
@amanrastogi1232,2021-01-19T08:47:30Z,1,"Kudos to you man, keep up the good work. This is an excellent explanation.",True
@Frost571,2021-01-17T17:49:31Z,3,Thank you for explaining this in a simple way I can understand. I'm almost in tears right now ü•≤,True
@victoryiron,2021-01-17T05:54:47Z,1,I really appreciate this video for enlightening an idiot like me in about 20minues.. BAAAMMMM,True
@mandulathrimanne8400,2021-01-17T03:56:06Z,1,This helped a lot. Thank you!,True
@neurojedi42,2021-01-17T00:51:42Z,1,One day i will publish a paper in neuroscience in which i use machine learning to research and i will put your name in it.,True
@patite3103,2021-01-15T08:59:57Z,0,"Thank you for the video! The error with the squares left in the derivatives begins at 17:44. You can erase these squares easily if you use Camtasia and upload again the video. At 20.37 you mention that the learning rate can be determined automatically by starting large. Just before you mentioned that the gradient descent would not work with the learning rate used at the beginning of the video. These two things are not congruent for me. What do you mean with ""the learning rate is determined automatically""?",True
@nobutaka2032,2021-01-14T21:21:52Z,0,Those weird sounds make the video unwatchable for me.,True
@maryamhemmati6871,2021-01-13T20:53:47Z,1,How AMAZING you are!!!,True
@monapatel4768,2021-01-06T01:41:03Z,1,"Wow Just wow! GD seems so difficult to me but after watching your video, my doubts are totally cleared. Thank you!!",True
@hafededdinebendib5974,2021-01-03T00:00:09Z,1,"subscribing and liking the video is not  enough to well consider your effort, thank you sir.",True
@lauramaass8098,2020-12-31T21:45:21Z,0,I'm confused.. is there a typo at 18:54?  Why is there still a square for the 2nd two formulas? Shouldn't that square be removed?  It also shows up in the following formulas as well.,True
@adelsalaheldeen,2020-12-31T14:02:55Z,0,"This video is one of the best explanations for gradient descent   Thank you so much :)  But I have a question  The example of the video has only 3 samples (which make the step size small for both the slope and the intersect ""in case we start with slope = 1 & intersect = 0"")  What will happen if our data sample is large? How we optimize the learning rate & the starting slope & intersect to avoid very large step sizes?",True
@deepakmehta1813,2020-12-30T13:35:16Z,2,"Amazing video. Thoroughly enjoyed it.  I was trying to anticipate that you would say  The CHAAAAAAAAAAIN RUUUULE at 18:00, but you ended up saying The Chain Rule.  Thanks again Josh.",True
@SachinRaneTheSKR,2020-12-30T12:18:55Z,2,"This has to be, without a doubt, the most understandable and clearly explained video on Gradient Descent on the internet! Thank you so much!",True
@aanya8834,2020-12-28T19:54:55Z,16,"I've never come across a clearer explanation for gradient descent,this is so cooool! Love this channel! Thank you so much for making us fall in love with ML!",True
@tsarnature6587,2020-12-22T07:43:05Z,1,Seen many videos.Nothing is as clear as this.,True
@maheshbisht2967,2020-12-21T07:31:02Z,1,Oohhh myy goddd . My biggest fear was gradient descent. Thanks for video. U r awesome. I want to ask one question.  R u data scientist or what ?,True
@wasilus,2020-12-19T14:37:19Z,0,"at 8:30 , I don't get it. Where does -1 comes from. How does the derivative of the stuff in the parateses get to -1?",True
@gabrielegrano8204,2020-12-19T10:20:20Z,1,One of the best video I have ever seen!,True
@anifhanifasetianingrum3322,2020-12-15T20:29:10Z,1,Tks for good explanation here but I am still learning step by step,True
@uzairov123,2020-12-15T18:41:53Z,1,BAMM !!! Love this !,True
@xingmingliu4344,2020-12-14T03:15:57Z,1,I bet that the video explains gradient descent better than 99% of teachers around the world.,True
@chrispap5496,2020-12-13T19:43:26Z,1,Where have you been all my life! Thank you so much. Liked and of course subbed!,True
@varunjindal1520,2020-12-12T13:17:47Z,0,"It is an awesome video for concept clarity. Thanks. :)  I had a doubt for square figure (Time 17.38), but I think it is already listed down in your comments.",True
@snoopkv,2020-12-10T03:06:50Z,1,Awesome,True
@AmirAli-id9rq,2020-12-07T18:23:51Z,1,Now am gonna say this that I have never said to a guy. Josh i love u <3,True
@NagarajanParamasivamPhD,2020-12-07T16:00:33Z,1,"Wow, this is a simple and great explanation for gradient descent. Thanks!",True
@bonecrusher1792,2020-12-02T05:11:22Z,0,Is the gradiant descent taught in the majority of the video the same as batch gradient descent?,True
@Igotit2445,2020-12-01T06:48:54Z,0,d/d slope contains square terms is that right. ?????,True
@Igotit2445,2020-12-01T06:05:09Z,0,How we get the slope value = 0.64 in the Predicted Hight Equation??  Help me Please,True
@szewach86,2020-12-01T05:34:05Z,0,"hi, can you please explain where does the least squares estimate 0.64 comes from?",True
@tahmidakter2034,2020-11-29T18:33:39Z,1,I believe no other teacher can teach as good as you.You are a great teacher.I was trying to understand linear regression & gradient descent's theory but couldn't understand properly after watching so many videos.But after watching your video it has became so easy for me.I request you to release a machine learning course.I believe you can prosper in life.,True
@RaviKumar-yd3sc,2020-11-27T15:07:04Z,1,Bam!!!!! the comments are already here that came to my mind...so i commented this....,True
@AJRedekar,2020-11-26T11:32:04Z,1,Brilliant work !!!!,True
@retroenergy,2020-11-25T19:24:39Z,1,"Awesome as always, statistics are fun now",True
@Kmysiak1,2020-11-24T20:30:26Z,0,"Is max number of steps ""n_estimator"" parameter in sklearn?",True
@FlayyYT,2020-11-23T16:47:06Z,0,What is this d in derivatives,True
@j8ahmed,2020-11-23T03:23:19Z,44,"One of the absolute best educational videos I have ever watched. Based on your explanations I have been able to: - walk myself through coding a gradient descent algorithm - understanding the concept of tuning the learning rate (alpha) - how to handle multivariable problems, - and more.  I'm studying machine learning right now and this has helped me so much. I can't say how great this was. Thanks a million!",True
@ehg02,2020-11-21T23:44:03Z,0,"Hi, Josh! Quick question: any examples where least-square cannot solve the derivative = 0 and where we would be better off using gradient descent? Thank you!",True
@nanamahmoud147,2020-11-21T01:06:48Z,1,"I dont know what to say , thank you very much for the impressive explaination!!",True
@Ben-ws9pi,2020-11-16T15:13:58Z,1,thank you so much fellow!  it helped a lot!,True
@anasmomani647,2020-11-16T00:37:58Z,1,"As lot of people say that u r better than Andrew Ng in this topic, I want to say that u also better than 365 too I'm taking a data science course with them on Udemy and I had bad time understanding Gradient decent but things r much clearer now   Thanks a lot :)",True
@kokologix9834,2020-11-15T11:02:10Z,1,This was super Helpful. You are the Best. Thank you so much,True
@kailashks901,2020-11-14T00:48:56Z,1,Thanks for the amazing explanation.,True
@jayjayf9699,2020-11-13T09:35:38Z,0,"I‚Äôm ganna be honest, your singing gets on my nerves , but none the less the explanation is there",True
@dr.loucifhemzaemmysnmoussa7686,2020-11-10T22:33:36Z,1,amazing !,True
@Stanleyt1107,2020-11-10T19:08:26Z,2,"This is one hell of a super awesome explanation of gradient descent. Thanks so much so much so much, Josh!",True
@mohammadpatel2315,2020-11-09T11:59:43Z,0,"explanation was wonderful, besides getting confused with weights. Since in neural networks the scope is referred to as weights and here the weights was referring to individual height, took me some time to realise the weights was referring to an individuals heights(as in the input) and not the scope(weights in neural networks). But anyways great tutorial!!",True
@anandgoel3188,2020-11-09T11:47:18Z,0,@ 8:20 why is that equation multiplied by -1??? that multiplication by -1 took me 20 hours to understand...still scribling my head...Firstly I started to look for some gradient descent explanation...Now I hate the gradient descent to the fullest. now I have started investigating about that -1 multiplication...Why did he made the topic soooooo much worst... I hate gradient descent after watching this video.,True
@shabbirgovernor,2020-11-07T09:15:22Z,1,"You are the best Josh!!!! So clearly explained, big fan",True
@AbhishekSharma-ij8sl,2020-11-04T05:26:23Z,1,BAM!!!!,True
@murselmusabasic4260,2020-11-03T08:19:20Z,1,The Chaaaain Ruuuuule ... I's just hilarious :) ... anyway great great lessons. Thanks a lot.,True
@mohitsoni-pc3pd,2020-11-01T17:31:43Z,1,simply great,True
@Ceyhus,2020-10-30T16:05:53Z,2,"One of the coolest explaining of such important and not easy (at the beginning) topic. Spent at around 4 hours doing all in parallel in excel and rechecking all my steps with huge notes...Now, will shift to the Python to code all my notes   Thank you very much!",True
@vikaschinchansur4322,2020-10-30T00:02:54Z,1,BAM!! I could finally understand what Gradient Descent actually is!!!,True
@farssamann7035,2020-10-27T21:22:10Z,1,perfect explanation and super informative video. God bless you,True
@chandthkkr7939,2020-10-26T17:52:07Z,0,"@josh starner, do u have any tecnhical report on gradient descent?",True
@waleedalmarshidi3494,2020-10-26T17:28:45Z,1,I swear this is the most clear and FANTASTIC explanation I've ever found,True
@TheLLMGuy,2020-10-25T12:16:44Z,1,I swear to god i almost follow your tutorials just for the intro,True
@Yousafkhan-gv7cs,2020-10-24T20:31:11Z,1,Teaching is an Art and you explain this dry and difficult topic in a very comprehensive way! thanks for sharing your knowledge and skill.,True
@musabayr1580,2020-10-24T14:16:05Z,2,"Whenever I get hung up about some hard to understand topic, I remember Josh waiting you in StatQuest; so be relax and enjoy learning something new with him; without any doubt.",True
@muratakyldz9188,2020-10-23T16:26:25Z,0,fucking amazing.,True
@vivekraj-iq8kr,2020-10-18T14:32:40Z,1,I found a diamond,True
@dakshtatomar1543,2020-10-18T08:22:30Z,0,"Hey you, BAAM you are the life savior keep going !! I have a doubt though when gradient descent reaches the minimum of the function the value itself becomes 0 and it will automatically stop then what's the essence of using the learning rate as it only helps in customizing how fast or slow the function would learn. Why don't we just leave it to the raw slope value? It will become 0 anyway?",True
@karinasevillano9820,2020-10-17T09:15:56Z,2,That was a great explanation of Gradient descent and in an amenable way !! Thank you for the great video!,True
@annie157,2020-10-16T09:48:52Z,0,No one explaines why are we taking this bloody curve as an example.. why intercept and why slope.. why!!!!!!!!,True
@hleyjr,2020-10-15T12:52:21Z,1,It became so clear thanks to you!,True
@Rotnisi,2020-10-14T19:58:21Z,1,Damn it felt like watching a statistics - Data science - Machine learning tutorial from a SpongeBob SquarePants episode! That was interesting and funny at the same time. Well done!,True
@cageybee777,2020-10-13T19:39:55Z,2,"I love his humor :) ""The Chain Rule!""",True
@guavavodka,2020-10-13T16:14:37Z,1,awesome explanation ...,True
@lianmccc,2020-10-13T06:07:41Z,1,I wish I could know this channel earlier.,True
@ranimercury,2020-10-12T14:29:21Z,1,You are the best teacher ever Josh... I'm going Andrew Ng ML course and dint understand what the hell was Gradient Descent.. so came to YouTube and found your video...BAM..Thank you so much for doing these videos.... Keep em coming...üíú,True
@amruthn3272,2020-10-10T14:57:22Z,0,"While considering sum of the squares of the residuals as the loss function, why can't we just equate the slope of loss function to zero to get best intercept instead of plugging in many intercepts and check",True
@ramkotha4726,2020-10-10T12:40:50Z,1,"I will give away a million dollars to you when I have say 10 million dollars so you can continue do good to the community (smiles). You're just amazing Josh, pls stay alive and dont let these videos disappear from here.",True
@saurabh_tayal,2020-10-10T10:14:33Z,0,Why don't we add step size instead of subtracting it from the old parameter? Why do we only subtract it? Is there any specific reason? Or can we do both and it wouldn't matter!!!,True
@aditilangar278,2020-10-06T15:38:49Z,1,I was in pain because of the concept of gradient descent and its relationship with the line of best fit. I just can‚Äôt believe that someone can explain it so well. Keep it up!,True
@Olivia-by2vm,2020-10-05T03:26:11Z,0,Explicaci√≥n para un no-matem√°tico,True
@marcuscheeweixing2059,2020-10-05T01:27:28Z,0,"are u using L2 norm? in l2 norm , its predicted - observed?",True
@adibhatlavivekteja2679,2020-10-04T00:11:27Z,0,"Hey Josh , @19:20 the last two elements of the derivative with slope shouldn't have a power of 2. Instead, they should be to the power of 1. Please respond if my understanding is correct. Thanks!",True
@Leonardo-jv1ls,2020-10-03T10:16:23Z,2,You explain things so slow and step-by-step as it was being explained for chimps. Exactly what i needed. Thank you a lot.,True
@ailanfarid7314,2020-10-02T16:24:37Z,2,Marvelous as always üëç,True
@xiongjiabin,2020-10-02T13:36:31Z,1,Great video that explains gradient descent perfectly,True
@sunitbehera597,2020-10-02T09:59:30Z,1,You are a Hero,True
@aination7302,2020-10-02T01:20:09Z,0,"Quick question - Do I have to know this maths if I am implementing an algorithm which is doing this for me? or this is just ""if you would like to know"" study?",True
@rakeshreddy1365,2020-09-29T05:15:58Z,0,Why don't we take just one step when the derivative is zero that is where we get least error right?#someone please clarify my doubt,True
@yixianwang863,2020-09-28T21:08:21Z,1,Very very helpful! thanks!,True
@kousthabkundu1996,2020-09-28T14:15:59Z,1,You are a superhero,True
@connerrice781,2020-09-26T18:06:12Z,0,How did you find the slope = 0.64?,True
@d.anjaneyakumarreddy3660,2020-09-26T06:07:36Z,0,some calculations and formula I didn't get could please explain with some examples,True
@mrunmayid1139,2020-09-24T18:03:01Z,0,Why is the slope value taken as 0.64 ?,True
@eklim2034,2020-09-24T10:25:26Z,1,"BAM, my life is changed",True
@paulbrown5839,2020-09-24T07:49:10Z,1,very good explanation.,True
@piyushborse3085,2020-09-22T13:50:50Z,2,God level Explanation... üòçüòçüòçüòçüòçüòçüòçüòçüòçüòçüòçüòçüòçüòçüòçüòç,True
@basiccoder2166,2020-09-21T13:59:27Z,0,"Hi Josh, on 9:21 why you didn't use the maxima minima concept and equate derivative to 0? it will give the intercept value immediately",True
@aadishjain6838,2020-09-19T10:05:54Z,1,how is this free !!!,True
@hrishikeshsharma6862,2020-09-17T04:13:29Z,1,there are no any better explanation than this...!AWESOME,True
@sophiazhang2585,2020-09-16T18:58:58Z,1,Best Video of gradient descent I've ever found in the universe!! Thanks for saving my life,True
@DanielVazquez,2020-09-15T07:19:23Z,1,After all these years I finally understand the magic behind gradients! Thanks!,True
@jayjayf9699,2020-09-12T08:40:49Z,0,How do you extend this ideas to logistic regression which has categorical variables,True
@codesandroads,2020-09-09T16:45:09Z,1,YOU ARE GOD,True
@IconoclastX,2020-09-03T21:54:52Z,1,Machine learning is this dam simple fr?,True
@kishansingh-vn6ic,2020-08-31T03:06:42Z,1,May you get all the resources you need to keep making these videos.,True
@piokany8566,2020-08-29T07:54:15Z,0,Gut gemacht. Danke!,True
@laraozyegen8966,2020-08-27T09:35:19Z,1,Thanks a lot:),True
@kannanparthipan7907,2020-08-26T11:01:32Z,0,Before calculating intercept how did you assume slope as 0.64 (differentiating wrt intercept) and vice versa ? please clarify,True
@ksrajavel,2020-08-23T00:41:23Z,1,"Thank you for the comprehensive lecture, Professor Josh :)",True
@adityavlogs8284,2020-08-19T17:33:43Z,1,Superb explanation sir. Love from India.,True
@jjoshua95,2020-08-18T15:37:55Z,0,"BAM!!! Great explanation of gradient descent. I too have a doubt on this, does the readymade packages of python and R like sklearn use gradient descent for calculating the linear regression slope and intercepts.",True
@pds7890,2020-08-18T08:14:42Z,1,"I just learned gradient descent, which I thought is only possible for a pro! BAMM!  you are amazing! it's soooo simple!!",True
@andreyelven8095,2020-08-16T15:20:53Z,0,"Why do we take 0.64 as a slope? I watched both videos on Linear Regression and Least Squares, but still have small idea how we got 0.64. Do we just put our line in an approximatelly good position to calculate Least Squares value and then get this magical 0.64? If so, why do we actually need a a Gradient Descent if we need Least Squares for it?",True
@SureshKumar-lc9ln,2020-08-16T14:30:51Z,0,Thanks a lot for super clear explanation.  But why 0.64 is chosen for slope,True
@JaspreetSingh-eh1vy,2020-08-15T08:09:59Z,0,I can watch your videos like a fucking TV show!,True
@techtak5948,2020-08-14T11:12:16Z,1,Excellent. Thank you,True
@akashsoni5870,2020-08-13T10:56:24Z,0,"Hello Sir, can you please make some videos on Neural network and CNN",True
@oscarcaro3163,2020-08-12T21:48:40Z,1,Respect!!!!,True
@matthewhaythornthwaite754,2020-08-12T20:40:14Z,0,"Josh, you mention at around 9:30 that gradient descent is useful when its not possible to solve for where the derivative=0. Do you (or anyone else) have any examples of when this may be the case. Whenever there is a minima in a function, I'm struggling to think how its derivative could be anything other than zero. Therefore, when is it not possible to use least squares to calculate where the derivative = 0? Is this where there are more than one minima?",True
@PavanKumar-fg3er,2020-08-12T12:56:42Z,1,"Josh, keep up  your excellent work and keep educating us like this with many more articles.. quadruple bam",True
@vipinchauhan9249,2020-08-12T08:57:31Z,1,"Thanks a lot for your tutorials, You are the best teacher :)",True
@rubdownful,2020-08-10T14:01:06Z,0,Can anyone explain to me where the least Squares estimate for the slope (0.64)  comes from?  If I understand correctly it's the coefficient of the slope where the sum of the squared residuals is the lowest?  Why do a Gradient Descent for the slope if you already calculated it with the least Squares estimate?,True
@navaneethakrishnan755,2020-08-10T07:31:49Z,1,Wonderful explanation thanks for the video,True
@ExplorerSpace,2020-08-07T16:49:06Z,1,how are you so good at this man? you are killing it. love your videos. The most awesome man I have ever seen. ‚ù§‚ù§‚ù§,True
@Amf313,2020-08-05T21:30:42Z,1,Thank you so much Joshüôèüåπ,True
@jessicafb5398,2020-08-05T00:43:14Z,1,Thank you for explaining this so clearly!!,True
@programmer4047,2020-08-04T08:06:54Z,0,How it works in neural networks?,True
@julianpadilla9411,2020-07-31T15:23:43Z,2,Ive been using your videos to learn statistics... really you are an amazing teacher!!! thanks so much for your channel. Its amazing.,True
@JupiterChamsae991102,2020-07-30T22:46:17Z,0,"sorry this is probably a stupid question but for 11:23, a new intercept = last guess - step size, is it the case that we use gradient ""descent"" so it is a minus sign instead of a plus sign ?",True
@greatmanokonkwo656,2020-07-29T21:43:41Z,1,"All in favour of Youtube paying StatQuest, 3bule1brown and other educational channels more for all the great work they are doing for essentially turning this platform into an online university.",True
@greatmanokonkwo656,2020-07-29T21:35:09Z,256,"I love how this guy cares to explain every single detail, not assuming any prior knowledge whatsoever. I was genuinely shocked when he started calculating the derivatives in the video. Most resources will skip over such minute details and calculations, only focusing on the concepts. These videos are the most beginner-friendly resources on the web for ML, simply amazing.",True
@greatmanokonkwo656,2020-07-29T18:39:30Z,1,This guy's explanations are on par with Richard Feynman's.,True
@koolmo,2020-07-27T15:51:53Z,0,"Hello Professor Starmer, could you please create a video that explains how Gradient Descent avoids local minimum? or GD perhaps cannot find the global minimum..?",True
@Viralvlogvideos,2020-07-26T17:22:37Z,1,Awesome video,True
@manishpathak69,2020-07-26T16:27:31Z,1,Gradient really seems Decent post your explanation !! BAMM,True
@QuanNguyen-oq6lm,2020-07-26T04:04:45Z,1,BAMMMM! dude you are amazing,True
@ani4787,2020-07-25T13:41:35Z,0,"Excellent explanation! Thank you very much for making this concept super lucid :-) Just one observation - @19:12 the expression for the derivative of the sum of squared residuals wrt ‚Äòslope‚Äô shows 2 ‚Äòsquared‚Äô terms, which I guess is a typo. Kudos to you for making this so straightforward to understand!!",True
@RandomGuy-hi2jm,2020-07-23T16:52:56Z,133,there are real institutes which takes 4000-5000$ to teach you this and then comes statquest which saves your tons of money and provide u better explanation...... SUPER BAMMMMMMMM!!!!!!!!!!!!!!!!!!!!!!!!!!!,True
@vinayak186f3,2020-07-20T09:09:18Z,1,Bam !!! I got it in one go üòé,True
@gowthamprabhu122,2020-07-19T14:20:53Z,0,I have a question. Is there a maximum number of unknown variables in a function that Gradient Descent can solve ?,True
@shanthgaitonde,2020-07-19T11:18:15Z,0,How did you calculate least squares without knowing the slope and intercept before gradient descent gave an estimate value?,True
@robocop30301,2020-07-18T08:10:04Z,1,I wasn't sure about this video until I heard the intro song.  But now I know it's legit.,True
@nammap8169,2020-07-15T11:03:52Z,1,thank you,True
@haohanzhao1601,2020-07-13T23:33:29Z,0,"Hi in 8min40seconds, could anyone tell me why it is -2 not 2? Thank you a lot >.<",True
@StrikerTide,2020-07-12T04:12:59Z,0,"An excellent demonstration. A question, why do we take a constant 0.64 for the slope? Why not 0.6 or 0.54 etc?",True
@barshabasuraychaudhuri42,2020-07-11T11:56:20Z,1,Very Well Explained! Had fun listening to it too!,True
@hunters.dicicco1410,2020-07-10T12:40:31Z,3,i gained new intuition on gradient descent that i never realized during university because of this video. thanks a MILLION josh!,True
@duckha1011,2020-07-10T12:24:09Z,1,"OMG love you man, clearest explanation i've ever watched. Keep up the good workkkkk!!!!!!",True
@abdallamukhaimar7350,2020-07-10T10:45:18Z,1,thanks!!,True
@maclanphere5674,2020-07-10T00:31:45Z,0,"@2:13, the least squares estimate of .64, how was that calculated?",True
@taraprasadmishra3839,2020-07-09T18:26:57Z,287,This man can teach ML to a 5 years baby üòÖ,True
@nilswestphal8046,2020-07-09T10:01:50Z,1,"Hands down one of the, if not the best, explanation of GD.",True
@amangautam2658,2020-07-08T15:22:18Z,1,the best video on gradient descent BAM!!!!!!,True
@ramsunil4317,2020-07-08T06:37:29Z,3,"Loved the way your teaching, u made mathematics  my favorite  subject",True
@billsianipar2302,2020-07-07T12:37:12Z,1,"Hi Josh, thank u so much for the very clear explanation not just in this video, but also in other videos you've created. I wanna ask, is the intercept and slope values update method you explain in this video basically the same as backpropagation?",True
@akshairamesh2136,2020-07-06T20:49:29Z,1,"best explanation, thanks a lot!",True
@ahmedelsharkawy1474,2020-07-06T15:00:59Z,1,"can't describe how I feel,  you are amazing",True
@tshaylatte9502,2020-07-06T09:55:01Z,1,Thank you so much!,True
@KW-md1bq,2020-07-06T03:23:39Z,0,"There seems to be a mistake from 18:55 onwards. The partial derivative of the loss function wrt to the slope has not decremented the power of 2 in the second and third data point. You just left the ""squared"" in there.",True
@keshav2136,2020-07-04T19:56:46Z,2,Wow!,True
@vaishanavshukla5199,2020-07-04T17:57:29Z,1,too good explaination,True
@ismailasmcalskan2552,2020-07-04T11:23:14Z,1,Great explanation thank you man..,True
@raphaelbonillo2192,2020-07-02T22:34:20Z,0,"Se voc√™ n√£o consegue entender o Andrew (Entendedores entender√£o) explicando, voc√™ recorre a esse v√≠deo.",True
@manishkanyal4003,2020-07-01T06:54:32Z,0,"It was really great explanation ,can you provide a the presentation pdf please ...",True
@NeoZondix,2020-06-29T09:00:53Z,1,I can't express it strongly enough how much I'm thankful for what you do. You're the best. Cheers from Ukraine,True
@shankarmoorthy8912,2020-06-29T04:26:10Z,1,Awesome job!,True
@danvtoppo,2020-06-28T22:36:41Z,2,T H E  C H A I N   R U L E,True
@barankaplan4308,2020-06-26T23:28:10Z,1,masterpiece!,True
@pranavm002,2020-06-25T20:04:26Z,1,This video was amazing......I wasn't able to understand what Andrew Ng was teaching in his course. But this video has definitely made me understand why and how powerful Gradient Descent is  !!!,True
@lk2215,2020-06-23T20:40:15Z,1,That Chain Rule and Boop Boop Beep Booop killed me xD Go on Dude,True
@bzaruk,2020-06-20T22:50:06Z,1,WTF Was that MAN?!?! The BEST explanation ever for anything in the world (not just math)... I am thankful!,True
@yoshi_from_the_turtleland,2020-06-19T19:13:39Z,1,Crystal clear! Great tutorial!,True
@shaoming886,2020-06-19T02:59:37Z,1,hahaha I really like the beginning,True
@leoncheneyon7317,2020-06-18T04:50:28Z,1,"Please someone give this guy another stimulus check. I deem your teaching essential to America. Stay available!!  Thanks, now I know how to find alpha and beta using Gradient Decent algorithms based on the loss(cost function).",True
@yuqiu7207,2020-06-18T04:04:13Z,1,"Thank you, your videos help a lot!",True
@shankarkantharaj8697,2020-06-17T08:35:31Z,0,"Hi Josh thanks for the video. Quick question, when you update the intercept, shouldn't you use the updated intercept value for the update in the slope? Otherwise the update for the slope does not take into account the step made for the intercept and you end up at two different points on the graph of the loss function",True
@pranitadas3479,2020-06-17T07:31:54Z,0,"The approach which you used to explain gradient descent, is it same the same way Batch Gradient descent works?",True
@zhcterry,2020-06-17T04:21:55Z,1,bam,True
@Waffano,2020-06-16T18:26:49Z,1,"Just to be totally clear: If you have 300 data points for example, then the loss function will contain 300 squared residuals ((observed-predicted)^2) right?",True
@avandibenardi2770,2020-06-16T03:43:00Z,1,5:20 in my country its call bakso beranak  you put gradient on gradient( base error plot ),True
@leonardomauro7894,2020-06-14T22:40:08Z,1,You are amazing. I loved your videos. Thanks!,True
@liranzaidman1610,2020-06-12T20:43:42Z,1,"This is a method of a gifted person (Newton / Raphson). Josh, great video.  Can you give an example of an equation where it is difficult to find the optimum?  I think that the usual viewer will ask himself - if I have a Linear model, than OLS will solve it. If we have a quadratic equation than we'll use the method we study in high-school to solve it,  where do we see something to use with gradient descent? Thanks a lot",True
@dharmrajsharma3017,2020-06-12T19:09:31Z,1,Awesome Explanation I have viewed many other videos non of them helped to understand the concept the way you have done wonderfully explained love the way you explain and crashing  the subscribe button wonder full job...,True
@eugenebragin3812,2020-06-12T13:39:28Z,0,why not simply calculate best intercept from the derivative function = 0?,True
@echoway2002,2020-06-10T16:52:27Z,0,"Your videos helped a lot of people who learn better with visuals and examples. Have you thought of adding videos for CNN, RNN, Recommender and NLP please?",True
@erwinerwin3169,2020-06-10T02:39:18Z,1,Awesome Explanation!,True
@niloychatterjee1603,2020-06-08T04:01:39Z,1,This is awesome,True
@claudiusandika5366,2020-06-06T18:23:08Z,1,fourple bammm,True
@HearMeRawls,2020-06-04T11:20:51Z,1,"Excellent explanation, thank you!",True
@spikymuzikmaniac,2020-06-03T13:43:39Z,1,Thankful and grateful *wiping tears*. You are doing god's work!,True
@justinli19901027,2020-06-02T02:56:32Z,1,can't appreciate enough how clearly the concept is explained. thank you,True
@justinli19901027,2020-06-02T02:48:45Z,1,very good. thanks,True
@ChaitanyaKrishnabodduluri,2020-05-30T15:58:42Z,1,"a ton of love from india....great video...clear explanation ...""DOUBLE BAM""",True
@yogeshyadav-te8sg,2020-05-30T11:25:13Z,0,Sir please do this in pyhton  plz help me,True
@imranmahmood6105,2020-05-29T15:08:13Z,0,"I hope this is the right place to ask this but how would this work with a multiple regression where you are dealing with multiple parameters (and if I‚Äôm understanding everything correctly, multiple step sizes). How would you know when to stop at that point? Hope this makes sense. Thank you!",True
@ke5683,2020-05-28T01:04:48Z,1,I feel blessed to find this Video finally after not understanding this concept from many others...Best Explanation of this topic....Huge Respect to you Sirüòä,True
@Alicia0Cramel,2020-05-24T20:42:44Z,1,Thank you SOOOO MUCH!!!! I am studying this on my free time and you make it so fair to learn. Love it. So funny too.,True
@sarabjeetsingh5033,2020-05-24T19:22:41Z,0,"Hey, very well explained.  At 17:45, the equation has typo error. After taking derivative, there is still power of 2. Rest is excellent, thanks man.",True
@delwarhossain43,2020-05-23T08:20:54Z,1,"Hard words but easy explanation by you. Thanks a lot, BOSS.",True
@lifeisbeautiful882,2020-05-23T08:20:38Z,1,This is the best explanation of gradient descent.  DOUBLE BAM !!!,True
@balajibalu9094,2020-05-22T16:22:15Z,1,Love from India :),True
@skab111,2020-05-21T20:24:44Z,1,"such a nice and detailed and simple explanation, i couldn't ask for anything better. plus made me laugh with all the ""pi pou"" sounds :)",True
@ayushjaiswal6009,2020-05-21T12:36:50Z,0,sir why you multiply -1 in d intercept and -0.5 in d slope please explain,True
@DreamCodeLove,2020-05-20T14:30:15Z,3,Can you do a series on mathematical optimization so as to put the whole thing in perspective...,True
@WVZEIJL,2020-05-20T10:23:10Z,1,"Thank you, this is so much better than uni.",True
@MecchaKakkoi,2020-05-19T23:12:12Z,3,"There are many great maths teachers out there in schools and colleges. But also some that struggle with explaining things. So just to be sure, I think they should all be made to watch your videos! :)",True
@gabrielt2681,2020-05-18T17:34:05Z,1,Wow. Beautiful explanation!!! thanks!,True
@anamaybelekar,2020-05-18T13:29:58Z,0,"One question... in the first part of the video, while calculating only the intercept (considering the slope to be 0.65), can't we put derivative of the sum of squared residuals equal to zero and calculating the value of the intercept, rather than just putting another value of intercept and waiting till the derivative comes closer to zero... it will be quick i think...",True
@erichuang9224,2020-05-17T14:14:59Z,1,"pretty good explanation, way clear!",True
@Itsjinhuang,2020-05-17T12:51:26Z,1,u r just so good!!!! pls keep doing this we need/love u !!!,True
@picklestirfry,2020-05-16T17:23:44Z,1,You are robbing me of the experience of banging my head against the wall reading walls of confusing equations and acronym. You are making statistics too intuitive and clearly understandable,True
@anubhav12qw,2020-05-15T06:07:42Z,1,This channel just won my heart!!,True
@Kevin-cy2dr,2020-05-11T13:56:29Z,0,Im gonna implement this in python wish me luck,True
@eaglei2505,2020-05-10T22:11:14Z,0,"Hello, first of all thank you very much for the videos. But I was just wondering why anyone would use gradient descent to do least squares if they can use matrices  instead ( solving for line equation using linear algebra). I think it‚Äôs a lot faster, does using GD when doing least squares have an advantage over using linear algebra?  Thanks in advance",True
@arunavabhattacharya571,2020-05-10T14:58:50Z,0,Just a dumb question maybe but how did we assume the slope to be 0.64 in the beginning??,True
@manuelplank5406,2020-05-10T14:19:44Z,3,This video is just perfect to gain an intuition of gradient descent (what math heavy lecture slides fail to deliver),True
@mohamedaitahmed2842,2020-05-07T02:12:19Z,30,"Of all the explanations I've watched, this is, by far, the best explanation of gradient descent. Thank you for existing !",True
@mujeebrahman5282,2020-05-05T14:13:41Z,1,üò≠üò≠üò≠üëçüëçüëç‚ù§üáÆüá≥,True
@ShivamSharma-wm9wc,2020-05-05T12:56:12Z,1,Thank You !!!!!! BAM!,True
@Shubhamkumar-ng1pm,2020-05-03T14:08:34Z,1,subscribed your channel bam shared it to my friends double bam they also fell in the love the way you are teaching triple bam.,True
@sushantkarki2708,2020-05-03T13:47:20Z,0,"hello there, how did you get the slope 0.64?",True
@fangyuwang9134,2020-05-03T03:46:42Z,1,"thank you! Josh. Again, you helped a lot !",True
@dimitrioskolokotronis5083,2020-04-30T10:39:16Z,0,"Hi Josh. Your videos are perfect, they have helped me a lot. I have a question though. Why we use SSR in order to optimize our model? Is it a little bit contradictory? What I want to say is that we estimate the best line using OLS (a function that finds the line with the least errors) and then we optimize our model again based on the least errors. In the end, we are going to end up with the initial line we found using OLS. Am I right?",True
@user-bz8nm6eb6g,2020-04-30T02:52:58Z,1,Thank you !,True
@neptunesbounty1786,2020-04-28T16:28:42Z,6,Andrew Ng needs more songs at the beginning of his lectures,True
@Mohabpiano,2020-04-26T02:14:25Z,0,do mi sol mi si,True
@denys3211,2020-04-25T15:41:19Z,1,BEST EXPLANATION EVER! Well done!,True
@rbwebcom1658,2020-04-25T06:09:10Z,0,sir....why at first you put slope=0.64?????????????,True
@2002budokan,2020-04-24T23:09:46Z,0,"""Backpropagation"" made in StatQuest, please...",True
@rohithvishaal,2020-04-24T14:57:56Z,1,you and richard feyman were colleagues? right,True
@rohithvishaal,2020-04-24T14:50:45Z,1,you explained so clearly my dog started calculating gradient descents!!!!thanks man,True
@farukbulut5319,2020-04-24T11:08:48Z,1,"A fluent, plausible, logical explanation. Thanks a lot.",True
@fishermanwithfishes2286,2020-04-24T03:57:38Z,31,"Teacher: ""what we knew about Gradient Descent?""  Me:""THE CHAIN RULE!!!!!!!!!!""",True
@lsd9138,2020-04-23T22:24:45Z,1,15:41 Double THAANKS :),True
@hashiska.5358,2020-04-23T08:22:45Z,31,I will summarize what's written in the comments section: WE LOVE YOU.,True
@maazrana9121,2020-04-23T01:00:26Z,0,"What is the difference between OLS and Gradient decent, Since you mention in the video that we get the values of intercept and the slope that are same as the the ones got from least square estimates? Doesn't least square estimates does the the sam ething tries to find different values of slope and intercept and gives the best fitting line ? @josh Starmer",True
@gouripeddivenkataasrithbha5148,2020-04-22T12:13:00Z,0,"What a beautiful explanation of the gradient descent concept. However, I have only one doubt. It is very counter intuitive, when the intercept value is being updated, based on the slope of the cost function at a given point. How is that we just choose an arbitrary but intelligent learning rate to apply to the slope and UPDATE the intercept with it. We are somehow linking the intercept value with the slope of the cost function. How do we intuitively understand this, when there is no defined relationship between the two. Mathematically, the intercept that we choose defines the cost function. In gradient descent, the intercept selection is based on the slope at a particular point of the cost function. How to understand this mathematically. What if the residuals are so large that the cost at the point 0 is 1000. Then, even with the learning rate, the update comes up to 100. So, we directly update the intercept from 0 to 100?",True
@nataliebogda6554,2020-04-20T03:23:08Z,1,You are a gift to this earth,True
@eminbaturdizdar9018,2020-04-19T13:47:33Z,1,"The 2nd BAM and the 4th BAM, blew my mind. Finally I got some key confusing points. Ty!",True
@hiteshsondhi,2020-04-19T11:58:43Z,1,Amazing way and to the core,True
@vinaypalnati8117,2020-04-18T05:47:55Z,0,Does your t-shirts import to India??,True
@gasmikaouther6887,2020-04-15T12:01:56Z,1,Thank you for your efforts,True
@HungBya,2020-04-15T05:44:05Z,1,"Okay I don't know who you are, but I will find you and give you a cookie.",True
@sris1986,2020-04-15T02:22:39Z,0,"Can you please share an example where we cannot take derivate=0? Also, in all scenarios where we can equate slope/derivate=0 - Can we simply use least square method instead of gradient descent?",True
@mehtabbandesha5637,2020-04-14T23:06:47Z,1,"Double BAAAM, your awesome ill send u 5 bags of weeeed",True
@Azuremastery,2020-04-14T10:03:12Z,1,Thanks Josh. Clear explanation.,True
@OmerBoehm,2020-04-13T21:56:25Z,1,Brilliant presentation - thank you,True
@wanrongchua4333,2020-04-13T13:50:56Z,1,thanks Josh!!! this is amazing! and your teaching is great and easy to understand! LOVE YOU!,True
@sakshamjain580,2020-04-12T06:14:39Z,1,Thank you sir !,True
@techwizpc4484,2020-04-11T17:43:32Z,0,8:21 Where did the x-1 go?,True
@tatisespindola,2020-04-11T17:24:51Z,1,Very well explained! Thank you very much!,True
@kennethroark917,2020-04-11T10:09:44Z,0,"Hi Josh. Trying to connect everything and how you solved this step by step. Will you explain 8:08, what do you mean the derivative of the first part? Thank you!",True
@coldstone87,2020-04-10T12:54:40Z,0,I think you have the derivatives thing wrong. Derivative of x power n is nx power n-1. Here i see a different thing altogether.,True
@kennethroark917,2020-04-10T04:20:15Z,1,Best explanation on Gradient Descent!!! Thank you Josh!!!!,True
@frsalima8312,2020-04-07T21:35:29Z,1,thank youu,True
@user-wc3qo2yi3p,2020-04-07T13:51:17Z,1,"I have no idea what gradient descent for a year, and now i finally know what it is. Thank you so much.",True
@mohsenakhavan3500,2020-04-05T00:28:51Z,0,"Mr Joshua Starmer, Thank you very much. Deeply Appreciated. would you please start a series of deep learning you tube from the concept of vector and matrices with your nice explanations. I think all of the students have this request for you. Please do it to help us. Thank you again.",True
@yonghojung5446,2020-04-04T18:56:42Z,0,"Hi Josh Starmer  Can you also upload a video about gradient descent detail calculation when there are more than one input variable?(=multivariable gradient descent?)  As a result, I would like to coding multi-variable gradient in detail. Thank you.  (code image=> https://docs.google.com/document/d/1Lg1QtjrtQ_c4nOvlAdAX-pLQHAygn3gTarD_N5bObOs/edit?usp=sharing)",True
@alecvan7143,2020-04-04T11:39:27Z,1,"If only things were always explained in layman terms rather than needlessly complicated technical language, thank you  :)",True
@DutGi,2020-04-04T11:02:04Z,1,"oh wow, that was the greatest opening song of tech videos in youtube haha",True
@AliG.G,2020-04-01T23:24:36Z,1,absolutely amazing!,True
@kamelakriche6506,2020-03-30T17:43:29Z,0,"Excellent explanation. But, why not use the least square method to estimate the parameters ? I find that it is easy and practical tool .",True
@abhishek-kk4ju,2020-03-29T07:07:22Z,0,"Hi Josh, please make a video on recommendation system / collaborative filtering",True
@faustopf-.,2020-03-28T18:23:30Z,1,This channel seriously save my life,True
@kshitijpemmaraju4177,2020-03-27T06:19:06Z,0,"Does slope and intercept also exists for other ML models using regression like support vectors,decision tress,random forests,Gradients boosting etc where ever ML models  use regression technique is it same parameters , slope and intercept for all?",True
@kshitijpemmaraju4177,2020-03-27T05:50:14Z,1,"Very lucid and beautifull explanation ,let alone andrew,coursera etc, that has some ambiguity of these concepts .The quality,content and time of this video content is better than  MIT videos!!!!",True
@TheJackimaru,2020-03-25T05:54:10Z,1,Love your videos! Keep up the great work,True
@kc_good_luck,2020-03-22T19:30:05Z,1,Bam!,True
@wei2674,2020-03-14T19:33:55Z,57,"‚Äúit sounds fancy, but it‚Äôs really no big deal‚Äù Thank you for marking me feel the same way about those fancy named methodologies after watching the videos!! I do feel more confident now in learning new stuff! If I don‚Äôt get it, it‚Äôs not because Of me, it‚Äôs simply because the book/paper/course note is not as good as statquest :)",True
@jayasreemangalagiri971,2020-03-13T04:32:13Z,1,"Definitely the best explanation, can't wait to watch the rest of them videos man!",True
@ashokrajur09,2020-03-09T16:29:10Z,1,loved the style of teaching man.. wonderful.. thank you very much.,True
@bigyabajracharya6463,2020-03-09T14:58:08Z,1,"Really appreciate this, thank you",True
@varunsaagars,2020-03-09T06:42:59Z,1,Omg! i was literally happy with explanation,True
@user-ci1oj3xo6h,2020-03-08T18:05:28Z,1,"Damn! Now that's what you call explanation! Thanks, man! Extremely helpful content!",True
@oliverhahn9737,2020-03-08T16:20:11Z,1,So many lightbulb moments watching your videos,True
@8Narcis8,2020-03-05T17:31:24Z,1,Thanks! your explanations are just AWESOME!,True
@user-xt9js1jt6m,2020-03-05T11:24:21Z,0,Wonderful Sir!!  Can we use Newtown Rapson method ???,True
@Dhayadhanu,2020-03-05T11:04:55Z,0,may i know how you gave the slope as 0.64(why you assumed that),True
@useless0ful,2020-03-03T01:46:59Z,1,"Sir, i love you!! Much much respect and love! I'm crying!!! Thankyou tons Sir!!",True
@guoamy3213,2020-03-02T15:08:32Z,1,thanks! easy to understand,True
@dhartishkhatri6240,2020-03-01T07:58:02Z,1,I am addicted to your videos,True
@blmppes9876,2020-02-29T09:27:24Z,0,"Thank you for all, your explanation is great. I understand all except the Chain Rule!!!!!!!!!, what is it ?",True
@ArexHopeRX,2020-02-29T07:02:15Z,1,The most useful and understandable for a bird head like me explanation i've ever heard so far! Thank you,True
@vatsal_gamit,2020-02-27T02:49:44Z,1,You are the best üëç,True
@sehejwahla410,2020-02-23T08:48:22Z,1,Great work bruh,True
@sehejwahla410,2020-02-23T08:30:28Z,1,I came here for the intro,True
@canalepico7922,2020-02-23T01:05:46Z,1,Hi. How do you do a confusion matrix  when you get no integers  values in predictions with RF?,True
@sachinhaldankars,2020-02-21T21:08:37Z,1,Very well explained...Pure Gold,True
@codebyuk2210,2020-02-21T10:55:42Z,0,You forgot the eliminate the power of 2 of the result in 17:42 !!!!,True
@gauthampughazhendhi8173,2020-02-20T10:32:58Z,2,"Josh Starmer, you are really good at making videos that explain the difficult concepts in a simple, understandable and engaging fashion. THANK YOU",True
@jacquelinevenigalla3664,2020-02-19T03:21:09Z,1,Thank you so much. I really appreciate this video.,True
@wilsonacero6722,2020-02-19T02:39:21Z,1,Thanks a lot for your video... I've been trying to understand this for a long time. Muchas gracias un saludo desde Ecuador,True
@fernandotorres4463,2020-02-16T03:14:24Z,1,"Amazing!! , beautiful explanation.",True
@luciac5993,2020-02-15T15:47:15Z,0,Hi! Does anybody know which computer program is best for plotting in my data and creating simple graphs just like the ones on the video so that I can follow along with my own data? Please let me know! I'm an IB student struggling to complete my Math HL IA! Thanks!!,True
@yassertalebi1026,2020-02-15T15:34:37Z,1,"Bravo bravo , you made it very easy for me and also others , i really appreciate it , you can not imagine how our professor would have explained it , it was horrible and i am pretty sure that even he could not understand himself when was teaching to us",True
@mansoorbaig9232,2020-02-13T12:31:12Z,1,Wonderful explanation. Using the numbers made it much easier to understand rather than going with x and y. You made it look so simple...,True
@marcoscarballal2002,2020-02-12T03:46:03Z,2,Thank you!! You just explained in 20 minutes what took my professor 3 hours to do half as well,True
@jeremyherbert9919,2020-02-11T05:36:22Z,1,This was so helpful thank you!!!,True
@luzanfero3398,2020-02-08T13:56:09Z,1,"Great like all your videos, god bless you!!!",True
@gundeepsingh3021,2020-02-08T10:37:53Z,1,you are the best !!! :),True
@maheshjayaraman6856,2020-02-06T20:01:42Z,1,Super super super explanation.Really fantastic,True
@youssefkafa757,2020-02-03T12:26:57Z,0,ÿ¨Ÿàÿ¨Ÿà ÿßŸÑŸàÿ≠ÿ¥,True
@viniciusdeoliveira7637,2020-01-30T15:39:03Z,1,"Man, what an awesome video! I hope my professors in the university would have that ability to explain tough subjects! Thank you so much!",True
@louben5573,2020-01-27T16:26:51Z,1,"-thanks again, indeed you r pure platinum! best regards from Amsterdam Science Park",True
@coldnlonely,2020-01-27T05:21:11Z,0,"Thank you for the video! One question - maybe I missed something. You said one possibility for stopping is if the step size becomes small. So does that mean that when there are multiple parameters/dimensions, the algorithm could stop updating some of the parameters while continuing with those that are still above the minimum step size? Thank you.",True
@kenway346,2020-01-24T15:35:27Z,1,"What's the required GRE score for getting admitted to the University of North Carolina at Chapel Hill, where this magnificent Prof. Starmar teaches?!??",True
@yuv54,2020-01-24T12:33:16Z,1,Oh thanks a lot man! You've saved life. Really appreciated your efforts.,True
@oscara2030,2020-01-23T00:42:09Z,1,Amazing video...I finally understood gradient descent...thanks a lot. When a video about neural networks?,True
@gauravlotey4263,2020-01-22T09:27:09Z,1,"With sincere gratitude to you sir, I would like to comment that, I wish your 3-D animation graph would have been more smoother in animation and labelled with the axis names.  I am a serious follower of yours and have learned a lot from generous people like you, contributing towards a better society. I believe you'll take my comment as a genuine feedback as a follower and improve it !  Love from India sir !",True
@user-pk3fp1yq4w,2020-01-21T01:41:49Z,1,Thank you,True
@kantshribaronia3513,2020-01-19T16:50:05Z,1,You just made this concept so easy to understand. BAM ON!,True
@rohitamalnerkar2152,2020-01-19T05:11:12Z,1,wonderful as usual.,True
@prashantdalmia8830,2020-01-18T13:10:22Z,0,"I am to able to understand the concept very welll!! I got one question here :- At 8.24, you said we will be multiplying the equation by -1 . Can you please let me know like why we are multiplying the terms with -1? It would be very helpful if i'll get the answer here.   BAM!",True
@MascariciGangster,2020-01-17T00:03:59Z,2,I love you,True
@MrDiegonolovich,2020-01-14T20:38:36Z,1,This was a very well explained video. Clear and to the point with easy to follow examples.,True
@whysoserious-yj1ku,2020-01-14T18:37:54Z,1,You just got an Indian fan!!,True
@vikrantchauhan5176,2020-01-14T00:58:03Z,1,"Peep peep poop peep poop and gradient descent is done, thanks a lot man, you are a life saver",True
@RealSantahunter,2020-01-11T23:55:11Z,1,bupbupbibup!,True
@wonggran9983,2020-01-11T01:02:52Z,1,"you are a blessing, you give a high level explanation and with a little background knowledge, we can _descend_ into the depths of formality that university courses go into ourselves!",True
@gauravpathak7081,2020-01-09T22:33:25Z,1,Great explanation for Gradient Descent. Would request you to do a video explaining lightgbm.,True
@surajthallapalli4227,2020-01-09T18:22:21Z,1,The best explanation ever!!. I rate this tutorial infinite.,True
@SnehBhandariSnehbhandari,2020-01-09T08:16:33Z,1,I love sond effect!,True
@wenyange2607,2020-01-07T01:03:34Z,1,Another great StatQuest! Thank you!,True
@jaliu,2020-01-07T00:30:05Z,3,you accidentally taught me how to take derivatives in a few seconds which i never understood for years... mind is blown.,True
@sajjadabdulmalik4265,2020-01-05T20:14:08Z,1,"Josh Sir I am slightly stuck and not able to BAMMM. How you got negative - 5.7  the SUM OF  SQUARE RESIDUALS on your Y axis? The quadrant belongs to (+, +) . Could you please reply so that I get a BAMMMMM!!!!!",True
@chethankumar.s4556,2020-01-05T07:04:29Z,1,whats bam,True
@anujsaboo7081,2020-01-03T07:05:53Z,63,"So let me see, you taught this like teaching a kid by repeating the same thing over and over again and covered such detailed calculation in 20 minutes. It is just amazing how you make short videos and even revise prior concepts at the beginning and still manage to keep everything short. The songs, the BAMS! everything unique about this channel. Amazing learning experience. THANK YOU!",True
@sajjadabdulmalik4265,2019-12-31T19:57:04Z,1,Josh you are awesome!!!! Just had a doubt how gradient descent will work  when  intercept is negative since you are choosing intercept random starting from 0. What if the intercept is negative. Your reply will be very helpfull.,True
@Tiger26279,2019-12-31T07:40:56Z,1,Learned a lot here. Much better explanation than Andrew ng. I have also watch bluebrown explanation which is more on visual level but complicated. Here crisp and clear video üëå,True
@mohan007ish,2019-12-30T09:33:31Z,0,"Hey Josh, How do you get the first value of slope 0.64??",True
@somnathdey107,2019-12-27T15:03:41Z,2,"Hello Josh, can you please tell me how you get the first value of slope i.e., 0.64?",True
@123gregery,2019-12-26T18:34:34Z,11,"I'm following Andrew Ng's Machine Learning Course. I thing he should give a reference to your video for an explanation. That is superb, provided that we know some elementary calculus.",True
@rickandelon9374,2019-12-26T11:43:07Z,1,Bruh thankyou so much I don't know how to express my feeling! ;) thx for your service to the humanity,True
@poojamahajan6169,2019-12-24T05:27:54Z,1,very well explained.. was unable to find such kind of simple explanation till now .. Thanks a lot :) Keep making videos like this,True
@gesunonerabianco8016,2019-12-23T00:30:26Z,2,"Straight to the root of each subject, thanks a lot for your crystal clear explanations my friend!",True
@biklimbs1133,2019-12-22T06:35:41Z,1,Awesome video üëå really helpful,True
@anushlaila,2019-12-21T06:19:03Z,1,Crystal clear explanation! Thanks a lot. Keep doing these amazing tutorials. TRIPLE  BAM!!!!,True
@lobarbadalova493,2019-12-20T15:59:43Z,1,"Great explanation! I appreciated how hard working you are, it was very useful when u showed each step one by one. Thank you)))",True
@ericklestrange6255,2019-12-20T10:41:38Z,0,"you lost me a bit by changing the parameters used so frequently. now slope, now intercept now... in my book they do the partial derivative of cost with respect to each weight instead of calculating both separately and combining them as you do, is it the same?",True
@kerolesmonsef4179,2019-12-19T16:55:41Z,1,so much useful . and also subscribed,True
@carlosmoreno4212,2019-12-19T15:05:13Z,1,I¬¥d like to thanks for your great explanation about a such complex subject.,True
@joesimmons3133,2019-12-19T06:26:56Z,1,Dude you‚Äôre a Genius !,True
@mohsenhs,2019-12-13T18:46:12Z,2,"Great presentation, fun and easy, most importantly, You made easy a very hard topic (at least as lots of courses say) well done, thanks a lot Josh",True
@kalanagayan5283,2019-12-13T17:25:22Z,7,"I'm currently proceeding to my Final year of Bachelors degree, This is  the  minimalist most clear ,  interactive and funny session I've seen , Love your work, its been  a great help for the studies. Love and respect all the way from Sri Lanka !!",True
@wabisabi7727,2019-12-12T13:56:33Z,0,"hi Master, can i be your student?",True
@sergioorozco7331,2019-12-11T04:45:17Z,1,"Just Subscribed! Beautiful video. I do have one question, however. When you choose random values for your slope and intercept, you will get the local minimum of your function, but you are not guaranteed to get the absolute minimum of your function, provided that there exists more than one minimum, correct?",True
@sachabloem9936,2019-12-10T11:59:47Z,0,harde intro neef,True
@nikhilpradeep336,2019-12-10T03:03:57Z,4,"Hi Josh, I an a new entrant to the field of Machine Learning and  was really struggling to get a hold of the topics; understood the concept of Gradient Descend by watching this one video. Thanks a lot Josh , you are the best!",True
@mbayogesh,2019-12-08T06:15:25Z,1,Great Explanation.. Thanks Josh Starmer :),True
@mimitk6642,2019-12-02T04:38:07Z,0,"this excellent work man, thank you ! where can I download the slides ?",True
@bbow4972,2019-12-01T18:18:30Z,6,Normally I don't like when tutorial vids try to be funny or cute in their vids but this video was great. It was just enough to make the lesson not feel so monotonous or droning without getting away from the lesson at hand. It was also surprising easy to understand. Thanks for the vid.,True
@datoubi,2019-11-28T15:33:28Z,1,"so you know often people say how a channel deserves more attention even though already having millions of subscribers. This channel, however, is one of the few channels i know where I would actually agree with such a statement.",True
@markusvonderluehe5468,2019-11-27T06:53:59Z,3,Great explanation starting with the basics and explaining it step by step! Brilliant!!!,True
@cool20guitar,2019-11-26T19:20:52Z,1,Your videos are the best. No one could have explained it better. Love the sense of humour in between to spice things up. Thank you so much!,True
@nobeldhar7689,2019-11-25T09:32:07Z,1,You always bring the clarity and i like your style! You are awesome Josh.. Keep it going. <3,True
@JoeWong81,2019-11-24T14:25:47Z,1,That was a great explanation Josh,True
@grantpezzolesi1636,2019-11-23T20:10:49Z,0,Could you make a video on kernel PCA,True
@liar.2390,2019-11-21T13:23:13Z,2,Ahh thank you so much! I recently started working on (or more like preparing to work on) my MD thesis and there's a lot I need to learn by myself. Channels like yours really are lifesavers - you explain everything so well and it's actually entertaining to watch. You have a new subscriber. :),True
@BackToBackSWE,2019-11-21T03:58:47Z,1,"Bless your soul to its very core my good sir, may the internet flourish.",True
@akshayawate7239,2019-11-20T14:50:38Z,1,can we get python code if possible sir ?,True
@akshayawate7239,2019-11-19T18:17:24Z,0,how to choose slope value ?,True
@IgraphyRupage,2019-11-19T10:18:43Z,1,This is Brilliant!,True
@francescopaolodilorenzo2803,2019-11-17T15:30:05Z,1,Thank you from Italy,True
@HarpreetKaur-qq8rx,2019-11-15T23:32:12Z,0,why do substituting with value 0 for intercept and value 1 for slope gives us to slopes that is confusing,True
@karanjadhav6681,2019-11-15T05:56:42Z,1,You are too good sir :) ... Thank you for the video,True
@statquest,2019-11-14T13:36:50Z,207,"NOTE 0: If want to learn more about The Chain Rule, see: https://youtu.be/wl1myxrtQHQ NOTE 1: The StatQuest Gradient Descent Study Guide is available! https://statquest.org/studyguides/ NOTE 2: A lot of people ask why we are using Gradient Descent to estimate the parameters in this video when we could just use least squares. We use least squares to produce a ""gold standard estimate"". This is the best possible estimate. We then attempt to derive the same estimate using Gradient Descent. This shows 1) how gradient descent works and 2) that the estimate is pretty good compared to the ""gold standard"". NOTE 3: A lot of people ask how I found the slope value, 0.64. In the example in this video, we can compare the estimates from Gradient Descent to those that come from another method called ""least squares"". For specific problems, we can plug the data into the least squares formula and the output is the optimal parameters. To learn more about the specific least squares formula see: https://en.wikipedia.org/wiki/Least_squares If you're wondering why, when we have least squares, would we want to use gradient descent... the answer is that least squares only works in specific situations and gradient descent can work in many more.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@Jamesyoung450,2019-11-14T09:53:39Z,0,"Good video! at 19:21, should the last 2 derivatives not have the squared term any more?",True
@haohuynhnhat3881,2019-11-11T07:18:36Z,1,"""The Chain Rule""",True
@happyharmonyhangout,2019-11-10T13:32:24Z,1,"The minute you told why we call it ""Gradient Descent""...That was great :))))) Thank you!",True
@samuelnatamihardja2570,2019-11-10T11:13:28Z,1,awesome explanation for dummy like me.... thanks a lot!!!,True
@sagarsitap3540,2019-11-08T16:35:05Z,0,"In calulating both sope and intercept, suppose step size of slope has come to 0.0001 and step size of intercept has come to 0.3. Do we keep step size of slope (0.0001) as it is and calculate new step size of for intercept until it also become close to zero??? Or we hvae to do step size calculation for both even though slope step size is very small.",True
@swathikakandan4749,2019-11-04T10:34:19Z,1,"Wish every learner gets a teacher like you.You are awesome,Thank you so much",True
@yeparthepirate,2019-11-03T22:37:19Z,7,I'm struggling so much in Machine Learning right now. After watching just two of your videos I am feeling so much more confident. Thanks so much!,True
@MinecraftLetstime,2019-11-03T01:56:48Z,1,This guy is a legend.,True
@ericrudolph6615,2019-11-02T20:53:45Z,0,"I think you forgot to eliminate the exponent 2 at 18:55 & 19:14  at the derivative with respect to slope.  I'm a little bit confused.    But all in all please keep going on, your videos are awesome!!!",True
@Nikhil-jj7xf,2019-10-31T14:47:11Z,1,Great Job Brother..!!,True
@djin5395,2019-10-30T11:38:25Z,1,Thank you for your effort!,True
@palashvishwas9835,2019-10-29T19:32:31Z,0,Do we use Sum of squared error or Mean squared error ?,True
@LorwaiTan,2019-10-28T22:45:10Z,1,Thank you for clearly explaining this concept. Pure genius!,True
@pollafattah7062,2019-10-28T01:04:23Z,1,I am going to steal this video for my lecture :),True
@vasiliv2000,2019-10-24T13:40:58Z,2,BAMM!!! Very detailed explanation. Perfect,True
@tracyliu8762,2019-10-23T01:34:48Z,1,Love this video. You save my day!,True
@theconstantchange,2019-10-22T11:02:41Z,2,Man! You are the *ONLY* hope for humanity.  I hope you live for a 1000 years so that you can teach the humanity and build exponential knowledge graph.,True
@thomascrosbie-walsh2770,2019-10-22T10:22:56Z,1,I love you,True
@pavlo_hadzheha,2019-10-21T16:15:04Z,1,Gradient Descent has to do with Newton's method which is used in many applications. You can read about the method in the book Calculus by Gilbert Strang from MIT. It's so exciting!,True
@karalius000,2019-10-21T15:45:03Z,1,BAM,True
@OlliePage,2019-10-21T14:28:58Z,0,at 19:13 you‚Äôve left the squared powers on the derivates for the squared residuals with respect to the slope for the 2nd and 3rd examples. That‚Äôs incorrect isn‚Äôt it?,True
@kashifrizvee4727,2019-10-21T00:39:43Z,0,why he is multiplying derivate result by -1 ??,True
@vuphong2003,2019-10-20T08:27:36Z,1,*T H E C H A I N R U L E*,True
@markr9640,2019-10-19T15:26:36Z,1,@Josh. How nice to hear you sing just like Lou Reed :),True
@prashant4814,2019-10-18T09:04:07Z,2,woow clearly explained . thank youu,True
@JibranAbbasi_1,2019-10-18T01:47:18Z,0,"Great video man! As always, thank you. I had one question though, why can we not set the derivative of the loss function equal to 0 to calculate the optimal value for the intercept or gradient? Wouldn‚Äôt the value we get from that be ‚Äòexactly‚Äô where the loss function is minimized?",True
@umairalvi7382,2019-10-16T19:49:04Z,2,Cant be explained more easily.,True
@leadeeeeer,2019-10-15T18:47:56Z,1,"it is really amazing explanation!, Thanks",True
@crvnse,2019-10-15T07:44:01Z,1,Just one word Awesome.,True
@sriramshandilya2470,2019-10-12T05:05:43Z,1,Josh I love you man!! You're my Guru!!!,True
@mohitsrivastava5880,2019-10-06T22:27:13Z,0,"Just a thought in mind, why does this algorithm needs to guess a value for the unknown variable and find the step size? Since, after taking the gradient of the loss function with respect to one variable we get a linear equation with one unknown, so can't it be simply solved for that unknown after keeping the RHS i.e. d/dx(loss fn) equal 0?",True
@mohitsrivastava5880,2019-10-06T21:48:29Z,2,"Josh Starmer, I am a fan. Just keep doing whatever and however you are doing.",True
@youngzproduction7498,2019-10-05T18:49:52Z,1,Your vids are all fantastic. Keep doing it.,True
@quantaali543,2019-10-02T18:28:13Z,1,Ohhh man I  don't know how would I thank you.,True
@Android-cm5gn,2019-10-01T09:27:20Z,1778,I cannot believe how many teachers get paid in schools and in universities to make students feel stupid just because they cannot explain a very important topic in a similar way like you! So much respect!,True
@akshaykumarkurapaty8926,2019-09-30T01:08:11Z,1,You are Awesomeeeeee,True
@nivu,2019-09-29T20:22:19Z,1,"Great Man, Great Video",True
@jaelbutler7966,2019-09-28T05:39:30Z,6,So helpful!!! Thank you so much! The imagery and step-by-step walk-through were just what I needed! It makes so much more sense now!,True
@amittapas2429,2019-09-24T19:21:36Z,1,I was struggling with this for days! You saved me!!,True
@gordonlim2322,2019-09-24T12:53:22Z,0,For revision 14:51,True
@fatihkaya6817,2019-09-24T10:36:47Z,1,"This is really helpful video, concepts are clearly explained. Thanks!",True
@vinodjadhao5947,2019-09-19T05:13:04Z,1,Amazing !!,True
@filizcamuz8119,2019-09-18T14:42:51Z,1,"Thank you, the content is amazingly clear and straight to the point. Graphics are very helpful. Thanks for your time. Please keep going.",True
@khuongthe4155,2019-09-18T04:45:08Z,2,hay qua,True
@dipanshunegi9805,2019-09-17T06:59:09Z,1,a lot of love for you man.. indebted... totally,True
@res4421,2019-09-14T17:15:17Z,0,So what should we do if the loss function is not differentiable? Like in the case of least absolute error= abs(y^hat-y),True
@diencai1812,2019-09-13T20:23:35Z,5,OMG this is pure gold !!! Thank you so much for the time in compiling the video and sharing the insight,True
@SasidharKhambhampati,2019-09-09T14:52:05Z,1,"awesome video, Thank you so much Josh",True
@venkateshvardhineedi9571,2019-09-09T04:12:06Z,0,How you estimated slope is 0.64 for least squared errors,True
@justinwuu,2019-09-07T23:31:28Z,1,18:09 Pen Pineapple Apple Pen,True
@nixenze,2019-09-07T10:47:45Z,0,"Your video made me clearly understand how gradient descent work with a slow, step-by-step, explanation, I love it! Btw, I have a question. Why the loss function (sum of square residual) that I've seen from many places are divided by number of the elements, but in this video is not? Am i missed anything?",True
@sandeepm625,2019-09-07T00:37:21Z,1,Very insightful . Thanks . I never understood why we needed gradient decent when least squares technique could find the best fit . I now see the need for gradient descent .,True
@johnbowers8747,2019-09-05T19:58:08Z,0,Is there n error in the video at 18:54 when taking derivatives of the slope? Shouldn't the expressions in parens be to the first power?,True
@yugix901,2019-09-04T19:41:29Z,3,"Great explanation! Really glad this video was the first one to pop up when I searched for gradient descent, thanks:)",True
@AK-ud4ur,2019-09-04T08:46:12Z,0,"Awesome !, What is explosion of Gradient Descent",True
@Sgoose105,2019-09-04T08:27:33Z,1,ÂìàÂìàÂìàÂìàÂìàÂìàÂìàÂìàÂìàthe CHAIN RULE DUDUDU,True
@pavlo_hadzheha,2019-09-03T20:55:52Z,0,Why by multiplying the slope by the learning rate and substracting that from an old intercept gives us a size of a step?,True
@FlexerPivot,2019-09-03T14:24:56Z,1,"This has been helpful, currently learning machine learning in Coursera with Prof. Ng!",True
@Razan3bdullah,2019-09-02T04:56:55Z,0,Im lost,True
@every_instant,2019-09-01T06:20:45Z,1,wut kind of magic is this !,True
@nickweimer6126,2019-08-31T18:27:32Z,2,Boop-beep-beep-boop boop-beep-boop,True
@lukkydivvi,2019-08-30T09:21:50Z,0,Thank you for your amazing work. Can you please help me understand  - In this case the sum of residuals is first decreasing and then increasing. Since we are randomly choosing intercept values and plotting residuals vs. intercept graph - what happens if we have 2 or 3 minima - like a sine curve or something? is that possible? (not even sure my question makes any sense). How do we know when to use/not use GD?,True
@brianathomas1661,2019-08-29T19:56:45Z,2,This is freaking amazing! That is all.,True
@kk10-,2019-08-28T16:43:56Z,228,You did what Andrew Ng failed to. Explaning me THE GRADIENT DECENT algo.,True
@manasadevadas8685,2019-08-28T09:22:45Z,2,This is the first time I am adding a comment to a video :) Glad that I found you!,True
@shivamsrivastava416,2019-08-28T04:42:57Z,1,Awesome explanation.... literally awesome.....hats off sir,True
@Asylum_M,2019-08-27T20:49:09Z,0,"Thank you for you hard work! Why does professor Andrew Ng starts with this concept, when there are tons of prereq knowledge required to understand it more or less clearly?",True
@frankichan7690,2019-08-27T09:43:32Z,0,how did you know slope = 0.64 ? please,True
@biswajitpattnaik3433,2019-08-26T17:26:22Z,0,hi josh post one on SVM,True
@ivanjonathan4240,2019-08-24T09:39:33Z,1,The clearest explanation i've ever watched! thank you mastah!,True
@anandruparelia8970,2019-08-22T14:08:40Z,1,Sir you don't know what have you done by explaining this so simply ! Love from India,True
@himanibhatia6213,2019-08-22T13:45:09Z,1,Best tutorial I've ever watched. Please make a video on SVM and kernels .. A humble request üò≠,True
@anur749,2019-08-18T13:46:21Z,4,You are a saviour! Whole University wasn't able to make this so simple..you should be given highest honorary award in Data Science,True
@saicharan9202,2019-08-17T06:58:35Z,1,I saw so many ML&AI Videos on YouTube and no one explained one concept this much simpler yet intuitively. Kudos Bro.,True
@MarvijoAnimatedBookSummaries,2019-08-16T04:43:26Z,0,"Brilliant, clear explanation! Thanks. Quick one, at 8:19, the ""derivative of the stuff inside the parenthesis"" is -1, how did that come about? Isn't it:  g(intercept) = (1.4 - (intercept + 0.64 x 0.5)) g(intercept) = 1.08 - intercept dy/dx = 1 / (1.08 - intercept)",True
@lamanaghabayova2704,2019-08-15T14:13:04Z,1,The best explanation ever! Thank you!,True
@suwarnachoudhary8591,2019-08-11T14:30:55Z,73,"When ever I feel I am not able to understand any concept after reading any book or any literature available on internet, I open Statequest. This channel should be awarded as the best ever tutorials on Machine Learning concept. You are unbelievable Josh Starmer!!! Thank you <3",True
@graju2000,2019-08-11T11:45:26Z,1,BAM!!!,True
@MukeshArethia,2019-08-10T09:47:33Z,2,Awesomeness lies in simplicity!  Triple Ton BAAAAM!!!!!!!!!!!!!!!!!!!!!!!!!,True
@achrafsaad,2019-08-10T00:29:57Z,0,"For a more complex function/scenario where we have multiple local minimums, does the Gradient Decent only get to the first local minimum on its way? or is there a way to get it to find the global minimum? thanks.",True
@sachinrathi7814,2019-08-08T13:53:54Z,1,Thank you.. 23 min of video help me to crack the interview. üòä Thank you again for sharing your knowledge with the words..,True
@machhindrabasnet9488,2019-08-07T19:22:27Z,1,Best tutorial. Couldn't be better than this..Thanks Josh!,True
@jrpatnugot29,2019-08-07T11:51:43Z,1,Gosh! where have you been this entire semester? Many thanks for your awesome video! keep it coming!,True
@dmytro1667,2019-08-06T14:45:27Z,2,"My man, you are a unique!! This video is the best tutorial of all the tutorials I've ever seen! No bullshit, just the content and the key concepts! Thank you!",True
@AdityaFingerstyle,2019-08-05T12:22:36Z,6,There couldn't be any better explanation for Gradient Descent !,True
@carolinegh601,2019-08-03T18:29:12Z,1,That was AMAZING! Thank you.,True
@ravindudesilva1593,2019-08-03T14:35:20Z,1,Ugh üìëüòç,True
@MelodiousEntertainer,2019-08-02T01:06:11Z,0,"At 8:25,  what is the chain rule that make the power of 2 as being multiplied by 2 and -1?",True
@YouUndeground,2019-07-31T00:58:44Z,1,Thanks Josh! What a great video :D,True
@shashankmadan,2019-07-28T14:25:34Z,0,i didnt get how we arrived at -1 https://youtu.be/sDv4f4s2SB8?t=509,True
@LetWorkTogether,2019-07-28T02:29:21Z,1,"This is an outstanding explanation!! Thank you so much, guys!!! I will donate your channel once next month salary coming. :D",True
@cosimocuriale8871,2019-07-27T16:38:49Z,1,Josh you are an amazing teacher! Your tuturials are helping me so much! Thanks a lot!,True
@nntun03,2019-07-26T15:31:15Z,1,..very clearly explained..thanks! ..love the BAMs!!s!  :),True
@mohdhamdi6726,2019-07-24T23:17:25Z,2,How can you make something that seems complicated so so so easy?? Man I looovvve you,True
@reshaknarayan3944,2019-07-21T18:09:50Z,2,One video of yours is enough to become a subscriber. May God give you long and happy life!,True
@lahtamlahtam1172,2019-07-20T03:57:33Z,1,t h e c h a i n r u l e,True
@Privacy-LOST,2019-07-15T19:24:52Z,1,"You. Again. Stop making everything so easy and accessible, you will put dull stat teachers out of jobs. Did you ever think about their families ?",True
@omarleo2785,2019-07-15T07:16:35Z,1,thanks man.finally understood the logic,True
@wind1050ful,2019-07-09T13:13:45Z,0,Surely a great Video! Your example is just a bit confusing because it‚Äòs very unusual to name the input values weights.,True
@tictoc5443,2019-07-07T14:51:15Z,0,please give some practical examples of how this could be used,True
@sharathnagendran3754,2019-07-05T19:31:58Z,1,Awesome dude !,True
@plekkchand,2019-07-05T16:20:16Z,1,"Thanks, but could do without  BAM!!, etc.",True
@PinkFloydTheDarkSide,2019-07-05T15:29:33Z,1,You are the result of my good karmas in my previous birth and so I found you in this birth. You are a God sent :D,True
@gmo2827,2019-07-02T16:14:13Z,2,"Beep beep boop, best explanation. Thanks",True
@Adventurer_Deepu,2019-07-01T06:21:03Z,3,Thanks a lot sir  Clearly explained  Awesome video.,True
@OrlyDanielov,2019-06-30T12:49:47Z,0,"there is mistake in the calculte of the drived slope, no power by 2 in the 2 end's part",True
@alexpehers3033,2019-06-30T11:15:37Z,3,"The best explanation I¬¥ve seen so far. Slow, clear and super easy to understand!",True
@adolfomotanavarro6553,2019-06-30T00:52:08Z,103,"Boop beep beep boop boop (Translation: Excellent explanation, 5 out of 5)",True
@billykotsos4642,2019-06-29T19:56:57Z,1,Amazing,True
@gabrielvs12345,2019-06-28T22:26:28Z,1,"Good night from Brazil.  Congratulations, an excellent class. Very funny without losing focus!",True
@hemreozgur,2019-06-28T09:37:15Z,2,Thank you very much for clear explanations. It is very helpful.,True
@TechieParmar,2019-06-26T23:02:01Z,1,Best ever explanation. Thank you so much.,True
@rameshkumartamilarasivelus470,2019-06-24T13:47:17Z,1,"just wow!! thank you so much for such a lucid explanation!!! It would be really great if you can make videos on Levenberg-Marquardt and Gauss-Newton too to solve the non-linear least squares problem!!   But also the derivative d (SSE)/dSlope at 19:11 should not have squares, right??",True
@diptobarman10,2019-06-24T06:31:42Z,1,"You Sir, are awesome.",True
@hairuoxu1189,2019-06-17T18:08:33Z,1,hahahahaha the Chain RUUULE!!!,True
@jhtch,2019-06-15T01:03:32Z,0,How can't my teacher teaching like this?! arrr,True
@ice7mayu,2019-06-12T03:24:08Z,2,"Thank you, now I know what the gradient means, it helps a lot with machine learning for beginners.",True
@TheKimhansu,2019-06-12T01:39:12Z,0,"Thank you for the elite-level video explanations. These are fantastic! At 4:52 you ask, ""is this the best we can do?"" You are referring to f(intercept) = 0. Looking at the 3 data points for weight and height, I would think that, actually, an MSE of zero would be impossible. It would only be possible if your data points were actually sitting right on the line, correct? Maybe I'm splitting hairs here, but I want to make sure I have a good understanding. If my understanding is correct, then when would you know that you have the lowest MSE and how would your gradient descent algorithm know to stop there and provide you with that value?",True
@haslan4885,2019-06-11T15:18:30Z,1,this is awesome,True
@theja63,2019-06-11T07:34:01Z,1,W.O.W!!,True
@MisterDerban,2019-06-10T20:36:49Z,1,"AMAZING, THANK YOU SO MUCH !!!",True
@MrTalanten,2019-06-10T20:24:55Z,2,TRIPLE BAAAM!!! BIG LIKE FROM ME.)),True
@rupeshgelal1542,2019-06-10T05:44:27Z,1,BAMM!!!,True
@_sahair,2019-06-10T01:17:28Z,1,Amazing!,True
@AakashOnKeys,2019-06-08T20:28:45Z,1,statquest to the rescue indeed!!!!!! <3,True
@will100smith4,2019-06-08T12:32:54Z,0,r u josh from lets game it out  ???????????,True
@hoangsinhngo9905,2019-06-07T05:20:46Z,1,"just came from the Learning from data lecture, this vid really helps",True
@johanneszwilling,2019-06-05T19:38:04Z,1,"4:27 üò∂ I just had an Aha-Moment! üôÉ Thank you, Sir!",True
@vikas1988able,2019-06-02T09:10:37Z,1,Thanks a lot!,True
@christoherright6430,2019-06-02T07:04:32Z,1,This is the best and best and best in the world.,True
@AshwaniKumar-tn7rl,2019-05-27T04:45:54Z,2,BAM!!,True
@kimberlysilva2946,2019-05-26T14:10:30Z,0,Thank you this was so helpful!,True
@smrazaabidi1495,2019-05-25T15:37:52Z,1,"Hi, Jooossshhhhhhhhh, I am speechless, I am wordless, I was doing PhD and weeping for knowing the answers about Gradient Descent, took many courses from various MOOCs but zero or may be I couldn't understand but you are the blessing for us, really I can not express and define you. Forgive me. Infinite BAM. Thanks, sir.",True
@gulamahsan5902,2019-05-24T16:22:13Z,0,The best!!! Gradient Decent exaplain ever.. Thanks a Ton,True
@bleardloshaj3692,2019-05-24T13:41:58Z,1,A modern hero!,True
@slomo4056,2019-05-24T08:42:40Z,0,"at 17:45 the last two therms are squared, thats wrong i guess",True
@siddharthkhandelwal933,2019-05-22T10:49:08Z,1,Better explanation than Andrew Ng Stanford course,True
@Mario-ud2xj,2019-05-20T23:49:10Z,0,At 7.28 you say that we are given an equation of the curve. Don't quite get that. We plugged in a bunch of points and can plot. But how do we get the equation? And what is the equation for this curve? Thanks!,True
@EB014200,2019-05-20T21:55:09Z,0,This video was very helpful!! Thanks a lot :-),True
@MichaelJamesActually,2019-05-20T19:10:33Z,0,why is the learning rate not proportional to the 2nd derivative of the s.sq. residuals?  wouldn't this ensure that we didn't overshoot our next guess?,True
@pawsh9542,2019-05-18T09:02:28Z,0,Why cant we find the intercept with absolute minima once we have the curve?,True
@chagantisubhash,2019-05-16T18:14:13Z,0,Thanks a lot!,True
@TheKimhansu,2019-05-16T03:36:54Z,0,"Thanks, pulled me out of a rut. Going to see if you have one for SVMs, if not, it would be great if you did.",True
@sw3922,2019-05-15T15:23:03Z,1,This channel deserve 1 billion subscribe!,True
@sw3922,2019-05-15T15:10:57Z,0,best,True
@elgabotb1,2019-05-15T04:24:17Z,7,"Me encanta! Eres todo un tezo como decimos aqu√≠ en Colombia. Love it, you are such a ""tezo"" (brilliant) like we used to say here in Colombia",True
@blueboystudios,2019-05-14T06:34:57Z,3,Best stats teacher on the planet!! Thank you for your videos Josh!,True
@kayae6659,2019-05-13T15:43:16Z,0,Love ya,True
@pnachtwey,2019-05-13T04:20:23Z,0,Why gradient descent?   Why not Levenberg-Marquard or BFGS? or even something simple like Nelder-Mead?   I never have a function where I can take the derivative.   The derivative is computed by taking small steps from each node to find the gradient.    Nelder-Mead does not require derivatives or gradients.   I often have to find 5 to 15 variables to 5000+ data points.  Yes this takes awhile.,True
@girrajjangid4681,2019-05-11T14:57:09Z,1,best video ever. Thank you man,True
@sankalpsoni5021,2019-05-09T15:41:58Z,1,"You are the best, man.. U can make even the most dumb topic interesting af",True
@bargars4374,2019-05-09T10:28:48Z,0,"Nice explanation..hats off...How do I know the optimal values for the intercept & slopes  were obtained as it requires multiple iterations since I hv 1 million obervations & 32 features ( columns) .  Since we are using gpus we don't mind using batch gradient descent.  One more correction,. Only the mini batch gradient descent takes the subset of the data,  correct me if am wrong",True
@rameeshraja5364,2019-05-08T14:34:38Z,0,Why dont you explain backpropagation derivative in your next video,True
@mariovollbracht7785,2019-05-06T18:32:28Z,0,"Hi Josh - thanks for the video. Very helpful. Do have a question. Once you get the sum of the squares graph, you say we can now have a function. How do you actually calculate that function?",True
@ghaliahmed,2019-05-05T23:26:37Z,2,Double BAM!!! Excellent Video!!! I respect you SIR   ;-)  .... please keep doing video like this,True
@ghaliahmed,2019-05-05T23:19:19Z,6,best of the best explanation in this word !!!!!!!!! Thanks a lot!,True
@youtubevanced8291,2019-05-05T19:16:56Z,4,I'm final year student (Master degree in Mathematics) and this is my   Project ü§Æ . Now little bit clear what gardient decent is.,True
@frankribery3362,2019-05-05T02:48:18Z,1,your intro song is better than entire hip hop history,True
@feelsgood3209,2019-05-04T10:15:12Z,0,I don't understand the word BAM!! :D,True
@danmburu4053,2019-04-27T12:40:06Z,4,Question: How did you come up with 0.64 for slope at 2:15,True
@ac11dc110,2019-04-26T20:16:25Z,0,wtf intro lol,True
@simaykazc1508,2019-04-24T08:30:39Z,0,8.24 is so cute :D,True
@rajeshgudikoti4266,2019-04-21T10:01:17Z,1,Excellent,True
@kaisartitoniran1776,2019-04-15T21:57:43Z,1,I love this explanation,True
@youtubewatch4339,2019-04-13T08:55:08Z,1,BAM!!!üòÇüòÇ,True
@davidhofmann4857,2019-04-12T16:00:52Z,1,"The Chain Rule e e e e e e..Awesome videos, thanks!",True
@ghosh5908,2019-04-11T23:21:10Z,4,This channel is awesome...It should have millions of subscribers....I have become a fan Josh...big fan...,True
@halestothesea,2019-04-11T20:40:59Z,1,"Hi!  How do you know when gradient descent arrives at the correct answer (you said that the learning rate was too big, so the gradient descent didn't come to the right answer -->  so that's a sign to change the learning rate, but how do you know when gradient descent didn't come to the right answer?) - Would it just be when it gets to about .00001?   Follow up question - What is a good rule of thumb for setting and adjust the size of the learning rate? What's a good starting point and by how much should it be changed? Here the learning rate was decreased, is it ever increased (is it possible to start with a learning rate that is too small)? Thank you!",True
@douglas191848,2019-04-07T07:35:25Z,0,"Great video to understand Gradient Descent, but I have a question. Why new intercept = old intercept - step size?",True
@yami9977,2019-04-05T17:25:40Z,1,My thump is up... BAM,True
@chuanqichen4951,2019-04-05T16:10:31Z,0,This sooo goood material! It would be better to put the 3D RSS  graph with Intercept and Slope variable  since it can help audience understand high dimension and  compare with the quadratic curve before when fixed Slope only move Intercept.,True
@davideconti3228,2019-03-31T19:58:59Z,1,Thank you very much for  making this video!  You explained everything in detail and clearly,True
@zohar1998,2019-03-30T09:34:24Z,1,Double Bam!,True
@samskyverareddy3135,2019-03-27T12:34:31Z,5,I remember what gradient descent does but your explanation on slopes and derivatives gave me a big 'AHA!' moment. Thank you!  Instant Subscription! Looking forward to more videos.,True
@khemirimonem6001,2019-03-26T16:40:59Z,2,"That was perfect ,Thank you",True
@CC-um5mh,2019-03-26T02:02:15Z,0,"After all the frustration, I finally found your videos. Best explanation on gradient descent I‚Äôve seen. Lookin forward to your neural network video. And it would be nice if you could do an xgboost video too. Thanks for all your work!",True
@2002budokan,2019-03-23T18:54:27Z,1,I've found a new gem in Youtube.,True
@jolantaszkodon4245,2019-03-21T17:22:51Z,1,"Eeeeee so awesome, thank you!!!",True
@yangqiaozheng1555,2019-03-20T21:26:25Z,1,Double BAM!!! Excellent Video!!! Thanks a lot!!!,True
@smvigneshme,2019-03-19T14:11:22Z,1,Quest ONNN..,True
@emekaume,2019-03-18T22:15:59Z,1,this was very good,True
@bilinmeyenbiradam96,2019-03-17T20:09:32Z,1,Could you share your all intro musics?,True
@franciscovinueza5320,2019-03-17T19:28:36Z,1,"Once again, excellent video teacher!",True
@maxkrause7678,2019-03-16T14:01:30Z,1,This is a really good video.,True
@anmolraina772,2019-03-16T12:30:01Z,0,Hi Josh can you please post a video on Support Vector Machine.,True
@vivekkamble9525,2019-03-16T09:51:30Z,1,Best One so far... Very very useful...,True
@KapilKumar-pi9qv,2019-03-14T17:05:14Z,0,why is new intercept=old intersept-slope    ??? can any any one proove that???..,True
@hajhouj,2019-03-14T10:29:40Z,24,"THE CHAIN RUUUUUULE !!! :-D ...... I Love your videos, best courses I ever seen",True
@aditidhawan2545,2019-03-14T05:45:12Z,1,"Hi Josh, I love your videos and quite frankly they're way better than my college professors. What is the difference between Gradient Descent and Gradient Boosting?  Also can you pls pls take up support vector machines(SVM) and Kernels next OR recommender Systems/Collaborative filtering next? That'd be A HUGE HELP!",True
@craigsooman7901,2019-03-13T21:15:30Z,0,Great video. How do you get estimated standard errors for the intercept and slope using gradient descent?,True
@laughwithme1071,2019-03-12T21:46:57Z,0,"Hey, I'm self teaching data science and these vids are super helpful! Thanks!   I'm studying the ML course on coursera as well as other sources, why do I need to learn to implement gradient descent to do linear reg or logistic reg when there are already packages and libraries out there to do that?",True
@rubiskelter,2019-03-12T21:20:02Z,1,"This was very helpful. It allows me to follow the ""deeplearningbook"" more thoroughly . Thank you very much.",True
@kabirnarayanjha,2019-03-10T05:09:13Z,0,Awsmeeeeeeeeee,True
@kinwong6383,2019-03-07T17:49:21Z,3,Please keep coming up with videos. The possibility of getting my Master in Data Science is solely depend on you now  lol,True
@Anant1896,2019-03-07T11:01:07Z,1,I am the one with like number 555. haha. thanks for the great video,True
@katerynahorytsvit1535,2019-03-05T22:43:34Z,0,"Hi Josh, thanks for your amazing videos! I find them extremely helpful. Can you explain, please, how we can calculate ApEn (approximate entropy) by using R?",True
@safeeqahmed3306,2019-03-05T04:19:53Z,0,Thanks a lot for the awesome video. Is it correct to say that least squares is more efficient than gradient descent because it calculates the optimum value by finding the value at which the derivative is zero? (Just in a single step) but gradient descent takes many steps to arrive at the solution. I am probably missing something huge..just waiting to be enlightened:),True
@michakochanski818,2019-03-04T17:25:57Z,1,"Please add Patron so I can finally pay you man :D can't buy tees cause I'm from Europe, but I really want to give you money somehow!!",True
@Artificial_Intelligence_AI,2019-03-03T06:09:32Z,2,"I just gave you the like number 500. You helped us a lot with your Bams, double bams, triple bams, obese mice, musical intros... (and with all this ML stuff, almost forgot).     Not all heroes wear capes, some of them just have a YouTube channel like you  Regards from Spain.",True
@a2zinvestor587,2019-03-03T03:01:23Z,1,excellent presentation ...,True
@ibrahimyazicii,2019-03-02T14:37:48Z,1,"I think it should stop when the absolute value of stepsize <= 0.001. I also have a question. When you start calculating for both slope and intercep, does new slope and intercepts are used both in the formulas?",True
@zeio-nara,2019-03-01T19:13:18Z,1,"Perfect! Thank you, all is clear)",True
@shaz-z506,2019-03-01T16:55:18Z,1,"Great explanation, Thank you :)",True
@quenar,2019-02-28T17:58:19Z,0,Comparing to other topics... you lost me on this one... super freaking complicated...,True
@shahrukh1514,2019-02-27T06:39:13Z,1,"Out of the tons of materials, i found on the internet about gradient descent and loss function which also included videos on youtube and other MOOC, i finally have understood it here. No disrespect to other authors where they too tried to explain in the best possible way but for a person like me who doesn't have solid foundations in maths and statistics, this was the most effective explanation.",True
@navneeth93,2019-02-27T05:44:33Z,1,"Hi Josh, If there are more than one predictor variables, then how the intercept & slope would be considered ?  ( I think,  by independently plotting each predictor variable with the target variable would give one set of intercept & slope for that predictor variable and so on )",True
@damiandk1able,2019-02-26T12:44:37Z,2,"This channel is one of these many channels that explain topics I am interest in. However, StatQuest has great advantage over other channles: clear explanation, little or no information overhead and very well prepared examples. Everything stright to the point :)  Thank you very much for your work. It matters a lot. If I may propose a new topic, how about a series of videos about gaussian processes?",True
@user-xf1gm4en4e,2019-02-26T12:18:22Z,1,Your video is so sweet for  beginner like me. Thanks a lot :)  I can't wait until your next video about stochastic gradient descent!  Triple BAMMMM!!,True
@KatharineME,2019-02-25T23:47:43Z,1,"Hi Josh, I'm a big fan of your videos! I make computational biology videos and I'm wondering what video making and graphics software you use? I'm trying to decrease my video editing time. Thanks so much in advance and keep up the good work!",True
@Skandawin78,2019-02-25T19:05:18Z,1,Amazing as always .. triple BAM!,True
@syedmahmoodneuron9171,2019-02-23T19:32:22Z,1,"insane! sorry, its iinnnnssaaannne",True
@RamanShaw,2019-02-22T11:09:50Z,1,"Hi Josh, you have been doing a great job. Your videos are so simple and easy to understand. I am having difficulty in understanding the SVM Optimization technique. Could you please make one video on SVM explaining the mathematics behind it?",True
@ravitejavarma1307,2019-02-21T04:20:52Z,0,You promised on a video for neural networks to be posted earlier this year... Waiting for it bro... Double BAMMM,True
@kevindeng3576,2019-02-20T23:17:12Z,1,Do we have a video on partial least squares? I have difficulty understanding it intuitively.,True
@ceezar,2019-02-20T18:26:43Z,2,"You sir, are the master of death",True
@andrewc2876,2019-02-20T17:35:57Z,1,Triple bam!,True
@999Stergios,2019-02-20T07:44:25Z,42,"StatQuest. You're Gold, my friend. Pure Gold. 1000 Thanks and bravo. The way you explained it and the graphics was brilliant. Super simple super easy, very educational! Bravo, Bravo!",True
@SanthoshKumar-ur5wp,2019-02-19T09:35:21Z,1,Thanks bro,True
@randysong823,2019-02-19T03:44:14Z,1,I'm taking a graduate ML class and you have saved me so much time in understanding Gradient Descent. No other video shows this with an example. Amazing work.,True
@199720112015,2019-02-18T17:11:31Z,1,Great explanation. Literally saving me in my ML class.,True
@dominicj7977,2019-02-18T13:19:11Z,1,I think gradient is a common math/engineering lingo. Any one who did a graduation course in mechanical/electrical/civil engineering would know,True
@dominicj7977,2019-02-18T13:17:11Z,1,Hi  thanks. I was looking forward to this.,True
@sangramgaikwad7663,2019-02-17T05:10:39Z,1,Love your videosüòçüòç.. From PCA to TSNE to Logistics Regression... And now Gradient descent!!,True
@haroldsu1696,2019-02-16T08:21:10Z,1,awesome explain!,True
@FrequentC88,2019-02-15T22:46:19Z,23,"Love humor you've added to what can usually be such a dry topic.  The ""squiggle"" for logistic regression made me laugh.  Keep up the great content!",True
@un.ravellin4480,2019-02-15T07:44:48Z,1,Please make a tutorial on Expectation Maximization. Thank you :),True
@albertding9700,2019-02-14T13:42:35Z,13,Hi Josh this was really helpful. Can you do deep learning and neural networks next?,True
@windgentle1557,2019-02-13T21:33:50Z,1,Timely help! Looking forward to watching the video about SGD!!!,True
@Sansibarr,2019-02-13T09:23:58Z,1,"When i started with your videos i already loved them alot. BUT when i had to switch to other channels because ARIMA, SETAR, ARCH and so on are not covered here (yet?) i now even more adore your explanations :) Other presentations are soooo boring and much more difficult to understand. I hope there will be lots of more content in the future!",True
@bakrianoo,2019-02-12T22:40:37Z,5,Like before watching. Is there any option for DOUBLE likes to append an extra like after watching,True
@user-xw6sz1ys3i,2019-02-12T15:04:05Z,1,not enough to say it is impressive. thank you.,True
@RobertWF42,2019-02-12T13:20:27Z,2,"Very helpful, thank you! It's a little silly we have all these terms: statistics, data science, machine learning that describe the same thing. I think we ought to simplify and call the field of data analysis ""statistics"".",True
@gabrielemazzola9652,2019-02-12T12:57:04Z,2,"I wonder how you can still manage to improve the quality of your videos. You're the best, sincerely.",True
@123chith,2019-02-09T18:14:13Z,1,Just curious what videos are you planning to upload next?,True
@tumul1474,2019-02-09T16:03:12Z,2,thank you sir !!,True
@surajitchakraborty1903,2019-02-09T05:39:17Z,1,Thanks for the awesome video. Just wanted to confirm if the number of partial derivatives is equal to (Number of features +1(corresponding to intercept)) ?,True
@ashishk81,2019-02-08T14:43:49Z,1,Josh you should create one complete machine learning course on coursera or udemy platform i will definitely buy it ..and i am sure there many people like me,True
@MonABM,2019-02-08T07:18:30Z,1,Great explanation  as always ! Could you make a new video about Gradient Boosting or Support Vector Machine ? Thanks in advance.,True
@Tyokok,2019-02-08T00:24:59Z,2,"Not first time, nothing else to say but sincerely deepest Thanks! You  literally make all these videos The One-Stop for those knowledge! Really  appreciate it! God Bless!",True
@PeihuiBrandonYeo,2019-02-07T06:28:20Z,1,TTTHHEEEE CCCCHHHAAAAAAIIIINNN RRRRUUULLLEEEE,True
@ravitejavarma1307,2019-02-07T01:44:24Z,8,Double BAMMMM........ Please post more videos sir....,True
@vikasblue1,2019-02-06T19:20:32Z,1,"Thanks Prof. As always, nothing but the best. Do you plan to come up with a detailed follow up video on Stochastic Gradient Descent by any chance ?",True
@123chith,2019-02-06T17:00:05Z,1,Thank you so much can i also please get an explanation  for Support vector machines.,True
@yulinliu850,2019-02-06T13:04:01Z,1,Awesome! Thanks a lot!,True
@andreaperlato2813,2019-02-06T12:05:03Z,1,"the best explanation as usual, thank you!",True
@akmaleache4735,2019-02-06T03:33:56Z,1,It was a big BAAAAM.,True
@thongnguyen1292,2019-02-06T00:41:07Z,1,The chain rule at 17:59 is sooo anti-climatic that I keep replaying it and laugh to the screen,True
@thongnguyen1292,2019-02-06T00:26:27Z,260,"Clearest explanation in this universe, as always! Thanks a lot!",True
@omnesomnibus2845,2019-02-05T22:40:47Z,10,"Great presentation as always! One note, at 18:53 I believe you forgot to take out the ^2 for the second and third slope-derivative terms of the residuals. But the idea is clear anyway.    I'd like to see this implemented for clusters and logistic regression as well, and what loss functions are best for them and why.    Maybe also a bit more detail about why the slope multiplied by the learning rate is the best way to find the step size. You mentioned it, in terms of the desire to approach a slope of zero (therefore it makes sense to have a negative multiple of the slope be the penalty, since a large slope would lead to subtracting a large amount and a small slope would lead to subtracting a small amount). As I said, you mentioned it, but perhaps a bit more visual demonstration would be good. Great though!    P.S. As an additional source of revenue, you might 'sell' a zipped version of all your videos for a donation, or perhaps put it into a book that can be downloaded for a donation.",True
@reneeliu6676,2019-02-05T20:29:54Z,1,This is a long awaited one! THANKS!,True
@arelfony,2019-02-05T20:23:40Z,1,"As always, perfect! Can't wait for the next step_by_step piece.",True
@DanielWeikert,2019-02-05T19:25:33Z,1,Thank you. So if we have a net with 1 million weights we basically need to derive and apply the chain rule for each of those weights individually and adjust them (GD) like you did for 2 parameters intercept and weight1 right?,True
@arkasaha4412,2019-02-05T18:45:54Z,2,"""The Chain Rule"" is hilarious xD",True
@avinashkumarjha25,2019-02-05T18:39:39Z,2,Love you........ Thank you so much. May God bless you.,True
@pengxu8542,2019-02-05T17:38:16Z,2,You are the best,True
@rrrprogram8667,2019-02-05T17:30:28Z,2,Awesomeeeee joshhh.... Loved it,True
@cunningham.s_law,2019-02-05T17:14:01Z,1,I heard there are a bunch of other algorithms that do the same thing as gradient descent. Can you do a video on all the other alternatives to and their strength and weaknesses?,True
@biswajitpattnaik3433,2019-02-05T17:12:43Z,2,Pl post a video on gradient boost,True
@vamshi649,2019-02-05T17:04:25Z,1,Awesome explanation,True
@Tntpker,2019-02-05T16:48:13Z,1,The long awaited video,True
