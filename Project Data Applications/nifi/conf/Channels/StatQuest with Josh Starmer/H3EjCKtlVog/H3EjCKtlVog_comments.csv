author,updated_at,like_count,text,public
@statquest,2020-06-03T18:55:20Z,19,"NOTE: This StatQuest is sponsored by JADBIO. Just Add Data, and their automatic machine learning algorithms will do all of the work for you. For more details, see: https://bit.ly/3bxtheb BAM!   Corrections: 3:42 I said 10 grams of popcorn, but I should have said 20 grams of popcorn given that they love Troll 2.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@fluffyfetlocks,2024-05-07T23:19:43Z,1,Take a shot each time he say's troll 2,True
@qbaliu6462,2024-04-21T08:53:43Z,1,This channel has helped me so much during my studies ðŸŽ‰,True
@proggenius2024,2024-04-14T20:58:20Z,1,Quadruple BAM!,True
@user-pn3vw9sp5f,2024-03-27T14:03:37Z,0,The example is when y is categorical. Is this applied when y is continuous? Thank you!,True
@ahmedshifa,2024-03-15T12:47:47Z,0,"These videos are extremely valuable, thank you for sharing them. I feel that they really help to illuminate the material. Quick question though: where do you get the different probabilities, like for popcorn, soda pop, and candy? How do we calculate those in this context? Do you use the soda a person drinks and divide it by the total soda, and same with popcorn, and candy?",True
@sayanbhowmick9203,2024-03-10T19:20:24Z,1,"Great style of teaching & also thank you so much for such a great video (Note : I have bought your book ""The StatQuest illustrated guide to machine learning"") ðŸ˜ƒ",True
@jonathanjacob5453,2024-01-06T13:14:49Z,1,Looks like I have to check out the quests before getting to this oneðŸ˜‚,True
@furo.v,2023-12-28T03:12:02Z,0,"Running something like this `dnorm(x=1, mean=1, sd=0.3)` on R gives 1.329808. The program I made sometimes returns values like 28. Can I still use these numbers to multiply them as probabilities?",True
@introvert986,2023-12-26T18:04:16Z,1,how is likellihood calculated in 4:17 can you please clear,True
@user-ct3oi3sf7l,2023-12-16T09:14:55Z,0,"he was just there for food, anything he can eat.",True
@alanamerkhanov6040,2023-12-04T19:27:50Z,1,"Hi, Josh. Troll 2 is a good movie... Thanks",True
@leowei2575,2023-11-16T11:43:15Z,2,"WOOOOOOW. I watched every video of yours, recommended in the description of this video, and now this video. Everything makes much more sense now. It helped me a lot to undersand the Gaussian Naive Bayes algorithm implemented and available from scikit-learn for applications in machine learning. Just awesome. Thank you!!!",True
@patrycjakasperska7272,2023-11-14T17:49:29Z,1,Love your channel,True
@buihung3704,2023-11-08T17:49:34Z,0,"Hmm the way you calculate Likelihood in this video is different from the Likelihood vs Probability video. - In Likelihood vs Probability video, you calculate the Likelihood of a distribution we are assumed to have 2 characteristics (mean + S.D), GIVEN that you measure some data from that distribution. The Likelihood is used to determined the assumption is correct or not (if correct, it's the Maximum Likelihood). So Likelihood is used to find the distribution, given the measures data. - In this video, you calculate the Likelihood of a continuous variable (Popcorn, Soda pop, Candy) occurs, GIVEN that the person is either Like Troll 2 or Don't Like Troll 2. In my opinion, the term Likelihood here is very similar to the term Probability that you use in Multinomial Naive Bayes Classifier video, hence isn't same as Likelihood vs Probability video. Note that the distribution is fixed here.  Q1: So I guess your Likelihood = Probability in this case? Q2: May I ask if Likelihood applies to continuous variables like Popcorn, Soda pop and Candy... and Probability (interchangbly with Likelihood also) applies to discrete variables like the frequency of each word in a finite set of words from in an email like Dear, Friend, Money, ... ?",True
@samore11,2023-10-30T04:51:22Z,0,So you assume all the features are Gaussian no matter if they are or are not?,True
@YesEnjoy55,2023-10-12T12:59:13Z,1,Great so much Thanks!,True
@kumaransivan,2023-09-16T16:37:18Z,0,"Hi, in your P(""dear"" / Normal) use case, your answer was 8/17 (number of occurrences of the word ""dear"" in normal messages / total words in normal messages) . If we go with the traditional formula, P(""dear"" | Normal) = P(""dear"" and Normal) / P(Normal) = 8/17/8/12 = 0.70. Could you kindly clarify why the traditional formula did not apply here please?",True
@YolcuYolunda-kc2zo,2023-09-12T03:30:26Z,0,@statquest I didnt see cross validation video link in the description,True
@konstantinlevin8651,2023-09-07T18:53:45Z,1,"I'm a simple man, I watch statquests in the nights, leave a like and go chat about it with chatgpt.That's it.",True
@Zumerjud,2023-09-05T22:04:37Z,0,So the difference between multinomial and Gaussian naive Bayes is discrete data versus continous data?,True
@jeanpierremattei4073,2023-09-02T14:28:47Z,1,MerciÂ !,True
@michelebersani7294,2023-08-17T09:26:37Z,1,"Hi, I really like your videos. But I don't quite understand why you used likehood L instead of calculating the area under the graph by finding the probability",True
@riqardumilos4212,2023-07-31T02:45:01Z,0,"Hii again, I successfully made a Gaussian Naive Bayes model using data from a survey I made thanks to you <3, it predicts whether or not a person likes a youtube thumbnail based on the Self Assessment Manikin. I tested it out by using one of the respondent's ratings that I omitted from the training data and it gave the correct prediction. Right now I am lacking a lot of analysis and evaluations, do you have any ideas on what I should do to proceed? Also, I was considering using k-fold cross validation and confusion matrix. Are those suitable in this context or are there better routes I can take? (Sorry if it's a bit long, ðŸ˜…)",True
@pepitosuarez3258,2023-07-06T16:16:51Z,1,double baaam!,True
@roymillsdixton7941,2023-06-17T20:10:20Z,0,"A nice video on Gaussian Naive Bayes Classification model. Well done! But I have a quick question for you, Josh. I only understand that Lim ln(x) as x approaches o is negative infinity. How is the Natural log of a really small unknown number very close to zero assumed to be equal to -115 and -33.6 as in the case of L(candy=25|Love Troll 2) and L(popcorn=20|does not Love Troll 2) respectively? What measure was used to determine these values?",True
@olesyakryvoruchko4351,2023-06-14T06:19:56Z,0,"hm... a bit confused about the choice of ""really small number"" a) Loves Troll 2 L(candy=25|Loves) = log(really small number) = -115 b) Not Love Troll 2 L(popcorn=20|Not Love) = log(a tiny number) = -33.6  was there an actual number for Underflow? In my mind really small number is a tiny number....but the log() is different in those 2 examples. Does anyone understand what's happening?",True
@kartikmalladi1918,2023-05-31T04:37:46Z,0,"I've seen the cross validation video and the main thing that it does is consider diff training set and test model in a data set. In this video, are you trying to say cross validation helps for the accurate prediction and percentage contribution/coefficients give the decisive main important factor as candy? Thanks",True
@riqardumilos4212,2023-05-19T08:15:02Z,0,"Hi, I'm having some trouble because for one of my graphs the Standard Deviation is 0 so no graph is present, so for the likelihood do I put the value 0 or 1 (since there is no deviation it'll always be the same value?). Or should I add 1 to every standard deviation value like in the Multinomial Naive Bayes video?",True
@Nikhil-Tomar,2023-05-09T18:12:16Z,2,"at 3:42 you said 10 grams of popcorn, it should be 20 grams of popcorn given that they love troll 2",True
@vinaykumardaivajna5260,2023-05-03T07:45:35Z,1,Awesome as always <3,True
@Godofwarares1,2023-04-24T17:06:52Z,10,This is crazy I went to school for Applied Mathematics and it never crossed my mind that what I learned was machine learning as chatgpt came into the lime light I started looking into it and almost everything I've learned so far is basically everything I've learned before but in a different context. My mind is just blown that I was assuming ML was something unattainable for me and it turns out I've been doing it for years,True
@taetaereporter,2023-04-07T16:52:42Z,0,thank you for ur service T.T,True
@FreeMarketSwine,2023-03-28T08:12:00Z,0,Is it possible to flip this so that the predictive variables are binary/categorical and the target is continuous?,True
@deepshikhaagarwal4125,2023-02-17T06:23:35Z,1,Thank you josh your videos are amazing! HoW to buy study guides from statquest,True
@linianhe,2023-01-31T00:39:33Z,1,dude you are awesome,True
@anje889,2023-01-15T13:21:43Z,1,contents are excellent and also i love your intro quite a lot (its super impressive for me) btw. thanking for doing this at the fisrt place as a beginner some concepts are literally hard to understand but after watching your videos things are a lot better than before. Thanks :),True
@mikebatista7582,2022-12-30T18:37:11Z,0,Why is the likelihood that someone drinks 500 ml of soda pop given they Love Troll 2 (0.004) lower than the likelihood they eat 20 g of popcorn given they Love Troll 2 (0.06)? The soda pop amount looks to be higher on the y-axis than the popcorn amount.,True
@ernestjesly,2022-12-14T19:19:34Z,0,Is there any video for both continuous and discrete values?,True
@KirilKovacheski,2022-12-09T20:09:58Z,0,Is 'the score for Loves Troll 2' the same as the posterior probability?,True
@kirilblazevski8329,2022-12-08T22:50:06Z,2,"Since the likelihood can be greater than 1, doesn't that mean that we could get probability that is greater than 1?",True
@salma-amlas,2022-11-09T14:34:41Z,1,i love you statquest i do,True
@raa__va4814,2022-10-04T03:00:16Z,23,Im at the point where my syllabus does not require me to look into all of this but im just having too much fun learning with you. Im glad i took this course up to find your videos,True
@therealbatman664,2022-09-14T03:02:08Z,1,Your videos are really great !! my prof made it way harder!!,True
@WorthyVII,2022-08-04T17:13:49Z,2,Literally the best video ever on this.,True
@liamhoward2208,2022-08-04T15:32:32Z,0,This must be one of the methods that allows stores like target to know that someone in your house is pregnant or what other things they like and dislike,True
@liamhoward2208,2022-08-04T14:38:28Z,0,I cannot for the life of me figure out where you got 'CHECK OUT THE QUEST' from. I know its from an older commercial.....I keep thinking Taco Bell? Outback Steak House??  Sizzler??,True
@samuelschonenberger,2022-07-29T09:54:53Z,2,These gloriously wierd examples really are needed to understand a concept,True
@tagoreji2143,2022-07-10T13:52:38Z,1,Tqsm Sir for the Very Valuable Information,True
@mahdiamrollahi8456,2022-07-09T10:04:20Z,0,"Do we use this , when we have continuous data like here?  If any feature has different distribution (other than normal distribution, like gamma or beta distro) can we multiply likelihood like this?",True
@chonky_ollie,2022-07-03T20:57:12Z,1,"Your videos are more helpful than my Machine Learning lectures were. Man, you are Gigachad of Machine Learning",True
@tanishasethi7363,2022-06-09T06:21:42Z,1,"The shameless self promotion got me lol, u're so funny",True
@argonaise_jay,2022-06-01T15:36:09Z,1,One of the best channel for learners that the world can offer..,True
@mohammadelghandour1614,2022-05-31T18:54:21Z,0,Great work ! In 8:11 How can we use cross validation with Gaussian Naive Bayes? I have watched the Cross validation video but I still can't figure out how to employ cross validation to know that candy can make the best classification.,True
@pinesasyg9894,2022-05-09T17:44:33Z,2,amazing kowledge with incredible communication skills..world will change if every student has such great teacher,True
@anqiwu85,2022-05-05T09:18:23Z,1,"Hello Josh, I have a question:   In your probability and likelihood video, you said likelihood = L(distribution | data). But in this video, when you are counting the score for each class, the likelihood looks like L(data | distribution).   I am really confused. Can you help me figure this out? Thanks!",True
@Suk667,2022-04-21T16:37:46Z,1,Why is the L ( soda pop = 500/Loves) smaller than L(popcorn =20/loves)?,True
@sohambasu660,2022-04-08T13:24:22Z,0,from where di you get the value of L(popcorn=20|Loves) ?,True
@ruihanli7241,2022-03-11T00:45:07Z,0,"Then if the data have both discrete and continuous feature, we can use Naive Bayes (for discrete one in other video) and Gaussian Naive Bayes together to classify the data.",True
@mpierucci4,2022-02-26T17:13:39Z,1,Love you,True
@ArinzeDavid,2022-02-10T23:37:36Z,1,awesome stuff for real,True
@aparnaraj3342,2022-01-28T10:38:12Z,0,"can anyone please explain ... where do we use this formula P(xiâ”‚y)=1/âˆš(2Ï€Ïƒ^2 )  .e^(-((Xi-Î¼)2)/2Ïƒ2) in Gaussian Naive Bayes to predict new samples,  as this formula isn't explained here.",True
@rajeshnimma155,2022-01-26T11:14:57Z,0,How you are evaluating the corresponding y value from the curve? is it kde?,True
@megatr0nic,2022-01-22T12:56:23Z,0,"Based on the name of this concept, I was expecting a demonstration of how you map the problem statement onto the Bayes formula  p(a|b)=p(b|a)*p(a)/p(b)  Are you showing how to find: p(popcorn=20,soda=500,candy=25|Loves Troll2) ?  How to find the flip: p(LovesTroll2|popcorn=20,soda=500,candy=25)? Thanks",True
@har_marachi,2022-01-10T03:23:46Z,1,"ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜…It's the ""Shameless Self Promotion"" for me... Thank you very much for this channel. Your videos are gold. The way you just know how to explain these hard concepts in a way that 5-year-olds can understand... To think that I just discovered this goldmine this week.   God bless youðŸ˜‡",True
@ravirajshinde465,2022-01-08T11:13:51Z,1,"can you please tell me the difference of likelihood prob and normal Gaussian pdf  (prob), as we know we cannot find the value at a single point in Gaussian distribution , but here we are taking that",True
@TheRushilcool,2022-01-02T17:50:53Z,0,"Hey Josh,  I am confused, You calculate the probability for the popcorn to be 0.06 and the soda pop to be 0.004, this can surely not be correct considering that soda pop is perfectly in the centre of the distribution and popcorn is not. I read your older comments and found you use the dnorm function in R. I used the dnorm to calculate the soda pop probability, but changed from ml to liters and found an answer above 1, the conversion shouldn't change the outcome...surely. Is there an explanation I am missing? My guess is that the width of the dnorm should be scaled according to the size of the distribution ,i.e., mean of 500 shouldn't have the same dnorm with as a mean of 25.   In any case, thanks for the video, and please take my insights into account.   Cheers!",True
@sumanbindu2678,2021-12-28T03:06:32Z,1,Amazing videos. The beep boop sound reminds me of squid games,True
@peteerya7839,2021-12-26T21:43:18Z,0,"Hi  Your video is amazing!!! I have a quick question. When you said to use the cross validation to help use decide which thins, pop corn, soda pop and candy, I think the training data part can ""only"" help decide the prior probability, then use the testing data to do the confusion test comparisons, all the above are conditioned in each scenario, right? For example, we will have three confusion matrix from  pop corn, soda pop and candy based on the test data. What do you think?",True
@Darkev77,2021-12-16T08:35:14Z,1,"3:38, shouldnâ€™t the notation be L(Loves | popcorn=20), since weâ€™re given that he eats 20g of popcorn, how likely is that sample generated from the Loves distribution. Isnâ€™t that right?",True
@Mustafa-099,2021-12-13T19:39:17Z,1,"Hey Josh I hope you are having a wonderful day, I was searching for a video on "" Gaussian mixture model "" on your channel but couldn't find one, I have a request for that video since the concept is a bit complicated elsewhere  Also btw your videos enabled to get one of the highest scores in the test conducted recently in my college, all thanks to you Josh, you are awesome",True
@angelocortez5185,2021-12-08T16:55:32Z,0,"Hi, can you talk about Gaussian Process Regression?",True
@mahadmohamed2748,2021-11-26T15:15:07Z,0,"Thanks for these great videos! Quick question: In other resources the likelihood is actually the probability of the data given the hypothesis rather than the likelihood of the data given the hypothesis. Which one would be correct, or is it fine to use either?",True
@franssjostrom719,2021-11-23T09:27:26Z,1,Tough being a ML teacher these days with you around,True
@momolosi1689,2021-11-03T21:29:51Z,1,"I like the vide, quadruple BAM!!!!ðŸ˜‚",True
@konmemes329,2021-11-03T00:10:28Z,1,Your video just helped me a lot !,True
@bennyraichu,2021-10-21T00:07:28Z,0,"So, Gaussian NB is the same as NB, except that instead of calculating likelihoods from simple counting, we fit a Gaussian distribution over the features?",True
@samuelbmartins,2021-10-20T13:22:28Z,0,"Josh, a question about the formulation of Bayes' Theorem, especially considering the likelihood. For Naive Bayes, the formula is: P(class | X) = P(class) * P(X | class), in which the last term. is the likelihood  In your video, you represented the likelihood as L, so that, apparently, the formula would be: P(No Love | X) = P(No Love) * L(X | No Love)  (1) Is my assumption correct? Is it just a change of letters to mean the same thing? (2) Or is there any other math under the hoods? For example, something like: P(X | class) = L(No Love | X)  Thanks in advance.",True
@samuelbmartins,2021-10-14T18:21:09Z,3,"Hi, Josh. Thank you so much for all the exceptional content from your channel. Your work is amazing. I'm a professor in Brazil of Computer Science and ML and your videos have been supporting me a lot. You're an inspiration for me.  Best.",True
@gordongoodwin6279,2021-10-11T23:24:59Z,0,"Great videos and great channel, but I have one objection to how you characterize the likelihood. Itâ€™s not the likelihood of someone having popcorn of 20 give they love troll 2, itâ€™s the likelihood of Troll 2, inferred from the probability that they have a popcorn value of 20 given that they love troll 2. The p(popcorn of 20 | troll 2) is a valid probability, itâ€™s only a likelihood in the sense that we use that probability to evaluate which theta value (loves troll 1 or loves troll 2) is most likely. I know this seems like semantics but itâ€™s a very important distinction, and Iâ€™ve noticed itâ€™s misrepresented in all of your videos on Bayesian topics that Iâ€™ve seen. No negativity meant (:",True
@TJ-wo1xt,2021-10-09T10:56:22Z,1,bammmm!!!!,True
@sugarbeybiii101,2021-10-07T18:13:05Z,0,"Hi Josh, as always thanks so much for the very informative video!!! Quick question, how did you calculate for the likelihoods? :D",True
@AlejandroVidalesAller,2021-10-04T22:37:45Z,1,"""...and eat 25 grams of candy every day"" lol",True
@piyushdadgal,2021-09-24T16:42:31Z,1,Thanku bamðŸ”¥ðŸ”¥,True
@abir95571,2021-09-20T06:35:45Z,1,Imagine eating candy makes you hate a movie so badly lol,True
@chenzhiyao834,2021-09-10T14:28:18Z,1,you explained much clearer than my lecturer in ML lecture.,True
@amirrezamousavi5139,2021-08-30T15:41:19Z,2,My little knowledge about machine learning could not be derived without your tutorials. Thank you very much,True
@aasray,2021-08-08T06:46:55Z,0,"Hey Josh. Really love the video but I have one question. When you have a really small number you are meant to do the natural log. But in the calculator, what are you supposed to type. (i.e. ln(x), what is x meant to be when the value is extremely small. How exactly did you get -33.6 for the no love troll 2)",True
@MinhPham-jq9wu,2021-08-07T07:19:09Z,1,"So great, this video so helpful",True
@janeli2487,2021-07-28T03:03:40Z,0,"Hi @StatQuest, thanks for another awesome video. I have a question about posterior probability. Isn't p(class)*p(observation data|class) just the joint probability, and if we want to get p( class|observed data) we need to divided the joint probability with p(observed data)? Thanks in advance!",True
@diraczhu9347,2021-07-25T09:28:07Z,1,Great video!,True
@rahuldey6369,2021-07-22T16:10:27Z,0,So the idea of Gaussian Naive Bayes other than Naive Bayes is that Gaussian Naive Bayes assumes Gaussian distribution of the variables of interest right?,True
@MrRynRules,2021-07-20T00:19:06Z,1,"Daym, your videos are so good at explaining complicated ideas!! Like holy shoot, I am going to use this, multiple predictors ideas to figure out the ending of inception, Was it dream, or was it not a dream!",True
@vincentcatholic,2021-07-19T07:06:33Z,0,There is a bug in your example. They should eat 125 g canday instead of 25g!,True
@officialdvd,2021-07-15T02:11:05Z,0,"Hi!  i wanna ask about data training, is it any rule for value data training? like must using positive value or otherwise or anything? thankyou...",True
@justinneddie9437,2021-07-08T03:01:37Z,1,well the little intro made me cry laugh. I don't know why... awesome,True
@TheVijaySaravana,2021-07-03T20:11:10Z,2,I have watched over 2-3 hours of lecture about Gaussian Naive Bayes. Now is when I feel my understanding is complete.,True
@meysamamini9473,2021-06-30T14:25:10Z,1,I'm Having great time watching Ur videos â¤ï¸,True
@sairamsubramaniam8316,2021-06-13T06:36:07Z,1,"Sir, this playlist is a one-stop solution for quick interview preparations. Thanks a lot sir.",True
@itsfloofles,2021-05-24T17:21:09Z,0,"Hey Josh! Thanks so much for this informal video. I'm currently working on something that needs exactly what you describe here. However there is one thing I cannot find an answer to however hard I try. I sadly can't translate my issue to the example you give in the video but I'll try to make it as basic as possible.  Let's say that we have a couple of boxes that contain a random bunch of items. Some boxes contain similar items to others and beforehand I make some observations on this. I then create some PDFs of for example P(coin | box1). Let's say that I observed coins in a couple of the boxes but some boxes never had a coin in them. If we have a total of 4 boxes our prior would be 0.25 for each box. I observe a coin in the box that I have in front of me and calculate the likelihoods for each box. What do I do if I do not have a PDF for a box then? Because in that case the final likelihood will be off right?  For example: Box 1: log(0.25) + log(L(coin=1|box1)) Box 2: log(0.25) + log(L(coin=1|box2)) Box 3: log(0.25) + log(there is no likelihood!) Box 4: log(0.25) + log(there is no likelihood!)  The log of 0 doesn't exist, but if I simply disregard that part of the sum than the loglikelihood for Box 3 & 4 would be the highest (while intuitively this doesn't make sense!). What do I do? If the probability of a coin in box 3 & 4 is 0 then what is the likelihood?",True
@unfinishedsentenc9864,2021-05-23T18:56:11Z,1,Can we use logistic regression too to predict if a person loves the movie or not?,True
@61_shivangbhardwaj46,2021-05-19T09:08:47Z,1,Thnx sir ðŸ˜Š,True
@jffy9005,2021-05-08T17:16:18Z,0,should the two terms in the likelihood be flipped?,True
@jacquelineb.2468,2021-05-08T16:13:45Z,0,thanks for creating this helpful video! is your sample data available somewhere? would love to calculate things by hand for practice!,True
@adamkinsey3139,2021-05-06T21:48:46Z,0,"Error in Video (""Probably"")  Hey Josh, I think you should be saying ""Probability"" when you are saying Likelihood.  I know the distinction can be tough, but they are VERY different, so it is important to get right.  And I believe you explain it correctly in your other video, so I'm surprised to see it used incorrectly here.  When you already have a known distribution (not changing), and you calculate a value for an event based on that (known) distribution, that is a Probability.  ""Likelihood"" occurs when you have some real data, but you're not sure what the ""true"" distribution is that the data came from.  So you take a stab and pick one possible distribution, and then calculate the ""likelihood"" that the data came from that distribution that you guessed.  And seriously, I think that the difference between ""probability"" and ""likelihood"" is as big as consequential as the difference between differentiation and integration, so it's really important to clearly distinguish the two concepts.  Or am I wrong?",True
@rogertea1857,2021-05-04T06:41:57Z,0,"Hi Josh, could you plz make a video on Gaussian Mixture Model and Bayesian Gaussian Mixture Model?",True
@rogertea1857,2021-05-04T04:40:10Z,1,"Another great tutorial, thank you!",True
@jefflian9932,2021-04-17T08:50:20Z,1,"damn, are you here to save students on earth?",True
@steffens.1734,2021-04-13T12:58:55Z,1,"I don't know the movie, but I love popcorn. And I don't like candy. I should probably really see the movie, correct? :D",True
@RFS_1,2021-04-08T12:27:18Z,1,Love the explaination BAM!,True
@rohan2609,2021-03-06T03:51:07Z,123,"4 weeks back I had no idea what is machine learning, but your videos have really made a difference in my life, they are all so clearly explained and fun to watch, I just got a job and I mentioned some of the learnings I had from your channel, I am grateful for your contribution in my life.",True
@worksmarter6418,2021-03-02T05:18:26Z,1,"Super awesome, thank you. Useful for my Intro to Artificial Intelligence course.",True
@mukulsaluja6109,2021-02-28T18:09:36Z,1,Best video i have ever seen,True
@nafassaadat8326,2021-02-24T02:09:36Z,0,"how do you calculate the mean and variance, and standard deviation ?? when I calculate mean=24.3+28.2/2 =38???",True
@riniantony2325,2021-02-17T06:22:58Z,0,"Hello, what happens if the distribution is not Gaussian though?",True
@mahdiamrollahi8456,2021-02-16T14:39:13Z,0,"Hi, why in this example you multiply likelihoods instead of probabilities? Because in the example in â€œNaive Bayesâ€, you multiply the probabilities.",True
@peterkim3887,2021-02-16T04:58:14Z,0,Great tutorial again!! Do you have a tutorial on Gaussian Process?,True
@sheebanwasi2925,2021-02-14T00:23:52Z,1,Hey JOSH Thanks for making such amazing video. Keep up the work. I just have a quick question if you don't mind. I can't understand how you got the likelihood eg: L(soda = 500 | LOVES) how you calculating that value.,True
@aviinthewoods,2021-02-13T20:09:19Z,0,If I hear Troll 2 one more time I'm gonna cry in a corner...,True
@heteromodal,2021-02-09T19:01:51Z,1,"Thank you Josh for another great video! Also, this (and other vids) makes think I should watch Troll 2, just to tick that box.",True
@hli2147,2021-02-04T21:08:55Z,2,This is the only lecture that makes me feel not stupid...,True
@iamkrishn,2021-02-03T16:48:36Z,1,This intro is my favorite idk why! :),True
@johnel4005,2021-02-01T17:04:10Z,1,BAM! Someone is going to pass the exam this semester .,True
@Theviswanath57,2021-01-30T14:44:27Z,3,"In Stats Playlist, we used following notation for P( Data | Model ) for probability & L(Model | Data) for likelihood; Here we are writing likelihood as L(popcorn=20 | Loves) which I guess L( Data  | Model );",True
@SreekantShenoy,2021-01-24T14:53:14Z,1,oh noooo.. bammmmm.. this is greatt!,True
@santoshbala9690,2021-01-21T18:12:15Z,0,"Hi Josh.. Thank you very much for your tutorial video.  I am a big fan sir I have a clarification.  The P(Love Troll) or P (No love Troll) given the 3 variables - here in this example - we multiply the Prior Probability of the class with the likelihood of the variables given the class ... However as per Bayes's Theorem, it is also divided by the probability (or likelihood) of the variables...  which is not done in this tutorial, same with the Naive Bayes ""clearly explained"" tutorial... I am sorry if have asked something ""naive"" :)",True
@arjunbehl9771,2021-01-20T15:29:10Z,1,Great stuff : ),True
@ateebakber213,2021-01-05T23:42:54Z,0,How do we calculate the likelihood through excel?,True
@ADESHKUMAR-yz2el,2020-12-13T21:57:13Z,1,i promise i will join the membership and buy your products  when i get a job... BAM!!!,True
@jianshue9240,2020-12-09T18:57:57Z,0,Thanks dude,True
@Adam_0464,2020-12-03T14:05:22Z,1,"Thank you, You have made the theory concrete and visible!",True
@hawkiyc,2020-11-25T04:10:19Z,0,"Dear Mr. Josh,  I have taken another course have the following equation for the probability of Naive Bayes,  P( Loves Troll 2 | new data ) = [ P( new data | Loves Troll 2 ) * P( Loves Troll 2 ) ] / P( new data) P( new data ) called marginal likelihood,Â  and P( new data | Loves Troll 2 ) called likelihood  And then, the way to calculate marginal likelihood and likelihood is to calculate the probability nearby the data at a certain distance, and the distance is adjustable while you are building the algorithm. For instance, there is a circle in which the center is new data and you can adjust the radius if your data is 2-D data.  After watching both courses, I am wondering how can these two equations be equivalent?  I deeply appreciate your time for answering my question.  Sincerely, Gavin",True
@mohit10singh,2020-11-25T03:16:30Z,3,"I am a beginner in Machine Learning field, and your channel helped me alot, almost went through all the videos, very nice way of explaining. Really appreciate you for making these videos and helping everyone. You just saved me ... Thank you very much...",True
@aiavicii4243,2020-11-23T03:18:58Z,0,"can we make a video about Gaussian mixture model and EM algorithm, thx endlessly!!!",True
@yuniprastika7022,2020-11-19T09:14:39Z,1,can't wait for your channel to BAAM! going worldwide!!,True
@ateebakber213,2020-11-18T15:30:00Z,0,Can someone(in simple language without the underlying statistics) explain the difference b/w Multinomial and Gaussian Naive Bayes?,True
@kicauburungmania2430,2020-11-16T08:34:36Z,0,Thanks for the awesome explanation. But I've a question. Is GNB can be used for sentiment analysis?,True
@shichengguo8064,2020-11-12T02:27:27Z,0,Naive Bayes have an assumption that the variables should be conditional independent. What's the best strategy for feature selection in NaÃ¯ve Bayes prediction?,True
@thisisbarua,2020-11-09T07:31:54Z,0,Can anyone please tell me how he calculated likelihood of popcorn (0.06) etc. Please describe in details,True
@AmanKumar-oq8sm,2020-11-09T03:47:05Z,0,"Hey Josh, Thank you for making these amazing videos. Please make a video on the ""Bayesian Networks"" too.",True
@aicancode5676,2020-11-07T07:04:35Z,0,I dont even know why there is people disliking this video!!,True
@yuxinzhang4228,2020-11-06T11:22:43Z,5,It's amazing! Thank you so much !  Our professor let us self-teach the Gaussian naive bayes and I absolutely don't understand her slides with many many math equations. Thanks again for your vivid videos !!,True
@tianhuicao3297,2020-11-04T10:06:23Z,9,These videos are amazing !!! Truly a survival pack for my DS classðŸ‘,True
@ahhhwhysocute,2020-11-02T07:53:44Z,1,Thanks for the video !! it was very helpful and easy to understand,True
@MrBemnet1,2020-11-02T06:08:17Z,0,The y-axis in gaussian curve is probability density function  .I thought you need to find area under  interval to calculate probability. why did you assign the y value as probability?,True
@shabrina5863,2020-10-30T07:22:03Z,0,"Hi Josh! Thank you for your awesome content. Actually i wanna make thesis about tweets sentiment analysis. I wanna classify the sentiment into 3 categories (negative, neutral, positive). Also i wanna improve Naive bayes with Boosting technique (Adaboost). Which naive bayes is work best for tweet sentiment analysis?  Classical Naive Bayes, Gaussian Naive Bayes, Multinomial Naive Bayes, Complement Naive Bayes, Bernoulli Naive Bayes, Categorical Naive Bayes, or Out-of-core naive Bayes? I kinda confused to decide this.It is good to choose adaboost for improve Naive bayes? or  Do you have any suggestions/advice?   Thank you",True
@marisagonzalez679,2020-10-25T22:11:48Z,4,You always get the like after the intro song hahaha,True
@minweideng4595,2020-10-18T15:20:18Z,8,"Thank you Josh. You deserve all the praises. I have been struggling with a lot of the concepts on traditional classic text books as they tend to ""jump"" quite a lot. You channel brings all of them to life vividly. This is my go to reference source now.",True
@atrumluminarium,2020-10-08T14:19:53Z,0,5:45 hehe nice :D,True
@sudhashankar1040,2020-10-05T10:16:37Z,1,This video on Gaussian Naive Bayes has been very well explained. Thanks a lot.ðŸ˜Š,True
@shichengguo8064,2020-10-04T14:42:05Z,0,Looks like it also works when both multinomial and gaussian predictor existing in the prediction dataset.,True
@sakhawath19,2020-09-28T09:46:15Z,7,"If I remember all the best educator's name on Youtube, you always come at the beginning! You are a flawless genius!",True
@HardCorn12,2020-09-22T15:07:58Z,1,TRIPLE SPAMMMMMMMMMMM,True
@haneulkim4902,2020-09-22T05:58:32Z,0,"Amazing video! Thank you so much!  One question, What if distribution of candy or other feature does not follow normal distribution?",True
@haofu1673,2020-09-20T19:23:06Z,2,"Great video! If people are willing to spend time on videos like this rather than Tiktok, the wold would be a much better place.",True
@muffinman1,2020-09-09T11:21:51Z,0,"Hello Josh, i didn't get why Does Not Love Troll 2 Score of -47 was bigger than -124. Is that because 1/-47 is bigger value than 1/-124 (as i understand negative logs are divided?).",True
@dirshah,2020-09-06T01:46:21Z,0,"Forgive me for being Naive, but how is the likelihood being calculated?",True
@akashchakraborty6431,2020-09-01T18:39:00Z,1,You have really helped me a lot. Thanks Sir. May you prosper more and keep helping students who cant afford paid content :),True
@bigdhav,2020-08-25T17:17:10Z,1,"Wait...did he just intro with ""poopy-poop""? hahah",True
@vincentcaillet7670,2020-08-21T11:07:23Z,1,"Hi Josh,  First, amazing videos. Pffff I'm mind blown. Been bidge watching them today.  Second, I have a quick question...what if my data has some values that are discrete (0s and 1s for instance) and also non-discrete (that can be represented by a histogram)? This method explained above is not appropriate right? EDIT: just saw that you've already answered that question below - I had not scrolled down enough.You had shared that link: https://sebastianraschka.com/faq/docs/naive-bayes-vartypes.html",True
@Steve-3P0,2020-08-17T03:27:52Z,1,+5000 for using an example as obscure and as obscene as Troll 2.,True
@ajeelahmedabbasi,2020-08-15T15:29:31Z,0,"Hey Josh big fan, I'm having trouble understanding how you calculated the likelihoods such as L(popcorn=20|Loves) and the other ones like that, besides that though great job, I owe you so much thanks a lot.",True
@tcidude,2020-08-03T13:34:25Z,0,Josh. I love you your videos.  I've been following your channel for a while. Your videos are absolutely great!  Would you consider covering more of Bayesian statistics in the future?,True
@georgeruellan,2020-08-03T13:01:36Z,3,"This series is helping me so much with my dissertation, thank you!!",True
@camilamiraglia8077,2020-07-31T13:32:48Z,0,"Thanks for the great video!  I would just like to point out that in my opinion if you are talking about log() when the base is e, it is easier (and more correct) to write ln().",True
@shindepratibha31,2020-07-28T04:50:22Z,0,Naive bayes is based on probability and it is used for classification. Is there any Naive bayes method which we can use for regression?,True
@Geza_Molnar_,2020-07-26T15:56:50Z,0,Hi - another great explanation! I wonder what would be the result if you normalise the probabilies of the 3 values. - Would it affect the outcome of the example in this video? - Which areas of values are affected: different outcomes with non-normalised and normalised distributions (=probability or likelihood here)?,True
@sumsriv,2020-07-23T01:53:44Z,1,What happens when you have mixed data continuous and binary? Is there a form of naive bayes that would allow for this?,True
@imbored8699,2020-07-22T18:34:31Z,2,"Can we also say that this person can be an outlier? Because of having very high likelihood of popcorn and soda pop scores given that he likes troll 2 and only but high variance according to 3rd category we can also say consider him under the outlier category, can't we? Can you clear this doubt for me, please! And also thanks a lot for your effort and work..",True
@rajatshrivastav,2020-07-13T14:28:20Z,0,"Hey Josh!!  This channel is the best for all the algorithms in Machine Learning, Now how does Naive Bayes works when it has  a column in dataset which says the brand preference of Soda(Coke,Pepsi,Thumbs Up)  for those who love Troll 2 or not.In general how to deal with categorical variables? Thanks in advance!",True
@liranzaidman1610,2020-07-10T22:22:02Z,5,"How do people come up with these crazy ideas? it's amazing, thanks a lot for another fantastic video",True
@jackyhuang6034,2020-07-06T07:01:09Z,0,"Josh, can you summarize important algorithms for both supervised and unsupervised algorithms for interview preparations?  Can we buy in bundles at a cheaper price (affordable for students)?  Like $80 - $120. Thanks",True
@sejongchun8350,2020-07-04T23:16:36Z,2,"Troll 2 is an awesome classic, and should not be up for debate. =)",True
@BrianRisk,2020-06-22T21:15:15Z,3,People who don't like this video are obviously trolls,True
@auzaluis,2020-06-22T04:13:42Z,1,The world needs more Joshuas!,True
@ChoonHtooHan,2020-06-18T14:53:28Z,0,"Hello Josh, If the design dataset is sparse, let say based on your example, only 1 person likes Troll2   and the rest (15 persons ) didn't like Troll2. The prior probability of P(didn't like) will be much much bigger than P(like). When the new person comes, it is very high chance that he will be classified as ""didn't like"" regardless of the likelihood values of all the features.  How do you handle that situation? Thanks",True
@shubhamtalks9718,2020-06-14T13:29:19Z,0,Why do we assume data to be a Gaussian distribution here?,True
@ccuuttww,2020-06-11T06:23:14Z,0,some question I want to ask is can we pick the prior as a beta distribution and just multiply the likelihoods then we have the posterior   is it everything same as the video? and sometimes we also need to calculate the marginal probability if we don't use prior beta how is this refer to this video? and can u also explain MAP next time?,True
@indianarchangel,2020-06-10T18:25:29Z,1,I eat a lot of candy but haven't seen Troll 2 yet! I guess I wouldn't like it either :P,True
@taotaotan5671,2020-06-09T16:14:37Z,0,"Hi, Josh. Thanks for this clear explanation. Since this Naive Bayes could be applied to Gaussian distribution, I guess it could also be applied to other distributions like Poisson distribution, right? Then a question is: how to determine the distribution of a feature? I believe this will be quite important to build a reasonable model.  Thanks again for the nice video.",True
@adiflorense1477,2020-06-09T06:49:42Z,2,OH NO!!! IT TERMINOLOGY ALERT!!,True
@MrElliptific,2020-06-06T02:41:21Z,0,Thanks for this super clear explanation. Why would we prefer this method for classification over a gradient boosting algorithm? When we have too few samples?,True
@WillChannelUS,2020-06-05T04:32:42Z,2,This channel should have 2.74M subscribers instead of 274K.,True
@jiheonlee4065,2020-06-05T03:53:54Z,2,Thank you for another excellent Statquest !~,True
@vuhoangdung,2020-06-05T00:28:45Z,0,"nice video, but I was forced to watch a full 50s ad video, anyone got the same experience ??",True
@magtazeum4071,2020-06-04T20:23:54Z,1,"I'm a subscriber, but I didn't get your notifications even if I activated the notifications.. Apparently I have missed so many  videos..",True
@user-bz8nm6eb6g,2020-06-04T13:23:32Z,3,Can you talk about Kernel estimation in the future?? Bam!,True
@amitg2476,2020-06-04T06:22:30Z,1,"Hi Josh, I wanted to know as to how do we get the likelihood from the y axis ?? lets say in the video at 4:12 you get the likelihood from the y axis for drinks 500 ml of soda given the person loves troll 2 to be  0.004. So how are we getting 0.004 ?",True
@tassoskat8623,2020-06-03T23:23:45Z,57,"This is by far my favorite educational YouTube channel. Everything is explained in a simple, practical and fun way. The videos are full of positive vibes just from the beginning with the silly song entry. I love the catch phrases. Statquest is addictive!",True
@shailukonda,2020-06-03T19:21:38Z,1,Could you please make a video on Time Series Analysis (Arima model)?,True
@statquest,2020-06-03T18:55:20Z,19,"NOTE: This StatQuest is sponsored by JADBIO. Just Add Data, and their automatic machine learning algorithms will do all of the work for you. For more details, see: https://bit.ly/3bxtheb BAM!   Corrections: 3:42 I said 10 grams of popcorn, but I should have said 20 grams of popcorn given that they love Troll 2.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@Vivaswaan.,2020-06-03T18:19:41Z,1,The demarcation of topics in the seek bar is useful and helpful. Nice addition.,True
@juliorodriguez8984,2020-06-03T17:01:48Z,0,"In a deeper analysis, shouldn't you create a new column trying to remove the dependencies between soda pop and popcorn?",True
@initdialog,2020-06-03T16:08:41Z,0,"Finally worked up to the Gaussian Naive Bayes. BAM! ""If you are not familiar with ..."" :(",True
@nzsvus,2020-06-03T15:07:15Z,0,"BAM! thanks, Josh! It would be amazing if you can make a StatQuest concerning A/B testing :)",True
@anonimxwz,2020-06-03T14:29:28Z,2,BAAAAM,True
@dipinpaul5894,2020-06-03T14:15:48Z,1,Excellent explanation. Any NLP series coming up ? Struggling to find good resources.,True
@rrrprogram8667,2020-06-03T13:50:04Z,1,Thanks for the awesome video..,True
@danielasanabria3242,2020-06-03T13:49:30Z,2,So we use Gaussian when ALL our features are continuous and multinomial when  ALL our features are categorical?,True
@stepantuchin86,2020-06-03T13:13:07Z,1,Thanks for all your studies!  Are you going to dive into Reinforcement learning?,True
@seleldjdfmn221,2020-06-03T12:57:15Z,0,"Please remember me at 300 subscribers. Who Are your favourite youtubers? Oh, And happy repeat day! x3",True
@NoOffenseAnimation,2020-06-03T12:52:08Z,1,I'm early! small bam,True
@mildlyinteresting1925,2020-06-03T12:46:35Z,69,"Following your channel for over 6 months now sir, your explanations are truly amazing..",True
