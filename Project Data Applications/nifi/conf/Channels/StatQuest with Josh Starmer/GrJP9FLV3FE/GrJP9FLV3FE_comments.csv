author,updated_at,like_count,text,public
@statquest,2020-08-01T18:13:24Z,50,NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: https://statquest.gumroad.com/l/uroxo  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@mykindofgaming7345,2024-05-29T09:37:30Z,1,Your beginning intro song beats Nickelodeon. Everytime. lol.,True
@technicalknowledge9128,2024-05-18T18:27:32Z,0,I want to data analysis video python,True
@terryliu3635,2024-04-26T18:05:57Z,1,Quest on!!!ü•≥,True
@uwo7130,2024-04-23T00:25:36Z,1,Thank you!,True
@jasnaciemnica8132,2024-04-16T15:34:26Z,1,Love these Python tutorials after watching theory videos:D,True
@juliakuchno7308,2024-03-04T16:32:01Z,1,Hey! Thank you so much for all the work you have been doing! I have a question regarding the leaves values if we use xgboost in a regression problem. What do they mean then? Is it probability that the average of the observations that were segregated to that particular leaf has this and this probability of contributing to the loss function? Thank you so much for your help!,True
@purveshghedia542,2024-02-19T12:23:40Z,1,GOAT,True
@ribhavbansal2942,2024-02-09T14:25:47Z,0,Please share the code also..,True
@vivasjimmy,2024-01-30T18:49:14Z,0,"disliked just because the singing part :D  but i will continue watching the video, thanks for the effort",True
@coolmusic4meyee,2024-01-28T21:10:55Z,1,"Great explanation and walk-through, big thanks!",True
@humashehwana7419,2024-01-19T13:33:29Z,0,"Thank you for very nice videos. Very helpful indeed. I am working in R and I believe you dont have tutorials in R. I have one question to ask. I am using XGBoost for regression and instead of using one predicted value for each of the data point, I want to extract closest neighbors, hence I am trying to get all values from the terminal nodes. My question is. 1) If i have nround =100, should i get terminal nodes only from 100th tree because i believe that is the most accurate one. 2) Can Terminal node values using predict function(nodes = TRUE) be considered as final output value or should i use this formula previous probability + 0.3*T1 +0.3*T2+....+0.3*T100.",True
@forheuristiclifeksh7836,2024-01-04T14:30:59Z,2,0:09,True
@kandiahchandrakumaran8521,2023-12-27T15:47:33Z,1,Amazing. Wonderful videos. I started only 3 months ago and with your videos I am very confident to do nalysis with Python. Manny thanks.  Is it possible to create a video for Nomogram for competic#ing risks for Time-Event (survival analysis) based on CPH outputs?,True
@maskedvillainai,2023-12-18T15:04:15Z,0,I don‚Äôt ever use generic missing data. It‚Äôs not good for your accuracy. The data almost always exists as an inferred reference or indication elsewhere in the data. Eg country might be empty but all cities have a zip code which link to country. It‚Äôs better to synthesize those values as much as possible to improve benchmark prediction scores,True
@22niloc,2023-12-14T11:36:42Z,1,"Expertly explained. I've just come off a bootcamp style analytics course and I'm currently reviewing and building upon everything I've learnt. I'm finding StatQuest such a valuable ressource...Although, I think that XGboost just broke my pcüòÖ",True
@noazamstein5795,2023-12-05T17:05:59Z,0,something weird just happend: when I use XGBregressor with a binary objective  I get better results for a classification task than XGBclassifier. How can this be?,True
@noazamstein5795,2023-12-05T12:18:08Z,0,"for filling 0's instead of missing values, what is the rationale when it comes to *numeric* values?",True
@__mothership__8475,2023-11-30T15:50:50Z,0,"In reality when working on industry data,  both precision and recall does not get this better after performing cross validation and tuning on best parameters üò¢",True
@jiayiwu4101,2023-11-18T19:21:36Z,0,"Another awesome video! Quick question - as you put 0 for missing value during the input processing step, how did the tree graph know which is missing and which is no?",True
@aezazi,2023-11-14T19:19:14Z,0,"Hi Josh, First, thanks so much for the superb videos. I've purchased the notebook and am working through it. I noticed that when you create X_encoded, 'Zip_code' is missing from the columns list. Not sure I understand why you're not treating zip code as categorical. Did I miss a comment about this somewhere? thank you.",True
@user-hj6zn8js3i,2023-10-15T17:33:35Z,1,Thanks a lot!,True
@noazamstein5795,2023-09-08T18:21:41Z,0,"For the improved model - did the new parameters actually improve it, or did you simply use a different cutoff for classification? In other words, did the AUC improve?",True
@user-dj6fs7tx4l,2023-09-02T13:42:22Z,0,Thank you so much for your hard work! I've learn so much watching your channel. Could you please explain why I shouldn't use one hot encoding while doing linear regression and what should I use instead?,True
@kayvangharbi3242,2023-08-28T13:41:38Z,0,Can you use zeroes to represent missing data for continuous variables?,True
@codinghighlightswithsadra7343,2023-08-15T14:09:32Z,2,Thank you so much for the work that you used in step by step tutorial. it was amazing.,True
@jameswilliamson1726,2023-08-04T06:46:01Z,1,Another great tutorial. Thx,True
@gahbor,2023-08-03T03:39:04Z,0,"31:20 threw me off. From what I've read, XGB can actually handle categorical data and doesn't require it to be one hot encoded. The opposite is true for logistic...but here you say that one hot encoding wouldn't be applicable for data prep for a logistic ML? Could you elaborate?  Thanks for all your work, its extremely usefu :)",True
@Justjemming,2023-08-02T01:33:50Z,0,"Prof Starmer, I‚Äôve a question regarding what you said at 31:30 on OHE not working for regression models. Would you be able to kindly explain how to encode categorical data before training these models then?",True
@pedroramon3942,2023-07-19T06:58:28Z,1,Multiple BAMMM!!,True
@thomsondcruz,2023-06-19T10:19:43Z,0,Absolutely loved this video Josh. It breaks down everything into understandable chunks. Thank you and God bless. BAM! The only thing I missed (and its very minor) was taking in a new data row and making an actual prediction by using the model.,True
@user-xn3lf3dg1h,2023-06-12T22:15:38Z,0,First you‚Äôve saved me this is super clear! I love all your videos so much üòä I do have two questions‚Ä¶ 1. How would you handle a classification problem with time series data?  2. Is there any other evaluation test you should or could do to evaluate the effectiveness of your model?,True
@weihuahu8179,2023-05-22T08:32:54Z,0,Why not use enable_categorical=True to handle categorical variables more gracefully?,True
@TheSoonAnn,2023-05-21T12:53:55Z,1,very good explaination,True
@lolikpof,2023-05-12T13:30:19Z,0,how do you change the default prediction of 0.5 described in previous videos? I also can't seem to find anything about that on the internet.. :/ I'm just thinking that using something like the average for regression or the probability of observing 1 for binary classification might lead to better results for less estimators.,True
@matattz,2023-05-05T20:09:34Z,1,"hey first of all thank you so much for all your videos! I understood everything and i am hyped about trying out what i have learned, but i have one question. So after we built our model and we are happy with how it performs, how do we feed it with actual new unseen data? If i have for example just the data for one specific customer and i want to check if he/she leaves or stays, what would the code look like.",True
@AQ-jh5fr,2023-05-02T07:45:15Z,0,"Thank you John for the video, great job I have a question: I saw this quote in a datacamp article about xgboost: ""Normally, you would encode them with ordinal or one-hot encoding, but XGBoost has the ability to internally deal with categoricals. The way to enable this feature is to cast the categorical columns into Pandas category data type"" So which is it? Can we disregard OHE with XGBoost?",True
@Hi_howrudoin,2023-04-26T04:49:52Z,0,Josh u are great at guitar. But damn it ur singing is,True
@kahung33,2023-04-08T23:38:52Z,0,"Sorry sir, I am curious why you mentioned that one-hot encoding is not for logistic regression nor linear regression. I tried to google that, but seems no one mentioned it.  Thanks, your videos are really awesome.",True
@kaustubh135,2023-04-06T12:42:31Z,0,why my parmeter are coming none for same peice of code ?,True
@ericgong1361,2023-04-04T09:00:45Z,1,"Hi, Iike your video and purchased your code. But when i ran your code in the Optimizing the Parameter section where you commented out, it keeps generating errors, which says the model is misconfigured. The logic of how to use GridSearchCV() is pretty clear, but I couldnt find where your code went wrong. Do you mind taking a look at it? Thanks.",True
@SmithnWesson,2023-03-23T04:04:10Z,0,"Do you really have to replace a ""None"" value with 0?  Or will xgboost just handle that automatically?  I am trying to read the documentation.  It is not that clear.  I suspect it will handle a None or NaN automatically.  It is NOT necessary to replace that with zero.  It will treat it like zero regardless.  I have not confirmed this, but this is what I suspect.",True
@SmithnWesson,2023-03-23T03:58:14Z,1,BAM!  Well done.,True
@goelnikhils,2023-03-19T15:22:04Z,0,"Josh, I have a question. How to give embeddings as features to XGBoost.  Any kind of embeddings. Reason for asking is XGBoost is being used as classification model for personalized ranking in Recommendation Engines. Embeddings are generated for user-item interaction and then these embeddings are given to XGBoost. Kindly answer",True
@goelnikhils,2023-03-19T15:17:56Z,1,Amazing Content,True
@azingo2313,2023-03-17T15:23:09Z,1,This man deserves Nobel Prize for peace of mind ‚ù§‚ù§,True
@SmithnWesson,2023-03-17T05:01:27Z,0,BTW. You don't need regex to replace a space.,True
@simkort5799,2023-03-08T16:25:22Z,0,"Just incase anyone had issue with importin plot_confusion_matrix, it is decommisioned and u can use ConfusionMatrixDisplay instead <3",True
@sane7263,2023-02-18T18:56:33Z,2,That's the Best video I've ever seen. Period. TRIPLE BAM! :),True
@brandonterrell9680,2023-02-13T20:31:42Z,1,"# very helpful and informative, thank you!",True
@knightedpanther,2023-02-05T00:33:41Z,0,"Awesome Video. Thanks Josh.  I had 2 questions:   1. I think we can remove one 1-Hot encoded columns for each categorical variable. For example if we had a variable cat_or_dog with only 2 categorical values: Cat and Dog. Knowing that it is a cat means that is not a dog and vice versa. So we need only one column to encode this variable with 2 values. I think if we use all the one-hot encoded columns of a variable for linear/ logistic regression it will make the Hat Matrix non-invertible but dropping one column for every variable should work?  2. Also, I have seen people retain a validation set to evaluate the model when it is fitting and test data for final testing the model just so that they don't overfit to the validation set?",True
@tikendersingh1138,2023-01-30T17:17:35Z,0,"df.columns = df.columns.str.replace(' ','_') It's not working for me. What can be the reason?",True
@Toyotaman,2023-01-26T02:08:02Z,0,"38:05 stratify=y is not for yes is for dependent variable y. if you have a different variable, you gotta pass your response variable's name to stratify",True
@HardikShah17,2023-01-20T15:38:54Z,1,Excellent Video @StatQuest ! Can we please have more Start to Finish python videos? Like Lightgbm maybe?,True
@MikeKleinsteuber,2023-01-12T12:14:44Z,0,More difficult than it looks if your IDE is Pycharm,True
@lahirulowe4752,2023-01-11T03:38:34Z,1,"it would be a great idea to have an entire series of ""start to finish""",True
@ilona7051,2023-01-05T08:42:18Z,0,"Thank you for this amazingly detailed tutorial!   Unfortunately I wasn't able to run XGBoost at 41:30:  clf_xgb.fit(X_train,            y_train,            verbose=True,            early_stopping_rounds=10,            eval_metric='aucpr',            eval_set=[(X_test, y_test)]            ) since the 'eval_metric' and 'early_stopping_rounds' in the 'fit' method  are deprecated (python advises to use ""`eval_metric` and  `early_stopping_rounds` respectively in constructor or `set_params` instead"", but I'm completely lost in the xgboost documentation)  For some reason I also get a  ValueError: feature_names must be unique  I checked for feature_name (column) duplicates but didn't get any:  X.columns.duplicated() = array([False, False, False, ..., False]) (all False), similarly for X_train  The dataframe  X_train.loc[:, ~X_train.columns.duplicated()].copy() is exactly the same as X_train, so I'm not sure where the ValueError is coming from.",True
@williamTjS,2022-12-06T03:56:23Z,1,Amazing! Thanks so much for the detailed video,True
@McKeAnThEmC,2022-12-05T22:13:29Z,0,"Amazing video Josh, thank you for uploading!  I have a follow-up question - what if the target column we want the XGBoost to classify is categorical? For example, trying to classify genres of music where our target column has labels such as ""blues, rock, metal"" etc... Would we want to provide One-hot Encoding to this target column? Or could we just replace these categorical values with numerical? Thanks!",True
@georgepapachristou1100,2022-11-24T09:41:49Z,0,"Hey Josh! Thanks for your awesome material once again! I have one question, you mention that One-hot encoding is not suitable for Linear/Logistic regression, why are you saying that? Is there any better way for no-ordinal categorical features?",True
@angelitocatajan8449,2022-11-08T14:29:24Z,0,NameError:  name 'X_encoded' is not defined,True
@lesprevues8865,2022-11-06T13:37:32Z,0,"One question. Through which courses did u combine Data Sciene, ML, DL knowledge with the CS knowledge for example how XGBoost is using the memory etc?",True
@DikshaKadre,2022-10-16T19:57:46Z,0,Thank you! for the amazing tutorial. It is really helpful to me.  I have one doubt. Is one hot encoding only done for the independent variable? What if the dependent variable (y in this case) has categorical data and it contains more than 2 categories then?,True
@dr.kingschultz,2022-10-12T12:07:16Z,1,Another  very good video!,True
@erint.4917,2022-10-10T02:19:19Z,0,"Hi, Josh - fantastic video - thanks so much!!  I have a question about how XGBoost handles binary variables encoded in a single column (i.e., a column containing no missing values where 0=No and 1=Yes).  Is it OK for me to represent such variables using a single column (rather than use one-hot encoding)?  In that case, would XGBoost handle the zeros in that column as missing values?  If so, what would be the better way to deal with such variables?",True
@sahandkhamiri2621,2022-09-29T06:36:34Z,0,It was awsome Josh. Really helped me understand xgboost!  Is there a way you can hand me this jupiter notebook? I can't pay the price since I live in Iran :(,True
@AdamsJamsYouTube,2022-09-17T12:22:23Z,5,"Josh, this video is epic and really helped me understand the actual process of tuning hyperparameters, something that had been a bit of a black box until I saw this video. Your channel is awesome too - great jingles as well :D",True
@miguelbarajas9892,2022-09-16T08:39:07Z,1,Freaking amazing! You explain everything so well. Thank you!,True
@dillonmears6696,2022-09-09T15:52:10Z,1,Great video! You did a wonderful job of explaining the process. Thanks!,True
@nikhilgjog,2022-08-29T17:32:18Z,0,How to get output that you are showing at 42.50 in the middle of screen - XGBClassifier(...) ? thanks!,True
@nikhilgjog,2022-08-23T18:51:26Z,0,what is the purpose using cv and eval_set both? shouldn't we use just one of them? how they are used along with scoring='roc_auc' and eval_metric='auc'? is early stopping used for cv and on the eval_set both? how would the best best hyper parameters be chosen - based upon results of cv (only training data) or based upon performance on eval_set?,True
@tonyjoffre2909,2022-08-15T15:07:36Z,1,Is a modern Mister Rogers!!,True
@Vty358,2022-08-13T16:20:16Z,0,"with on-hot encoding...  you don't think we will get a ""MULTICOLLINERIATY"" issue?",True
@jeffschroeder8875,2022-08-13T02:56:08Z,0,"Thanks Josh!  How many trees are being built during your grid search, aka choosing hyperparameters?  Is there a default number of trees chosen when you are determining gamma, learning rate etc?  I get that you later determine the optimal number of trees (55), but what is that value during grid search?  Thanks!",True
@WondrousHello,2022-08-03T19:22:17Z,0,Can someone explain a bit more about the tree interpretation. Like what would you be able to takeaway from it in relation to who needs to be targeted when trying to retain customers?,True
@straboom9646,2022-08-03T11:02:30Z,0,"hey Josh! Great video, hoenstly this channel saved me! Could you please explain why one-hot encoding won't work well with logistic regression as you've mentioned at 31:20? Which type of encoding would you recommend for logistic regression?",True
@marcelocoip7275,2022-08-01T23:03:41Z,1,"Hard work here, I'ts funny how the responsabile scientist and the funny guy coexist, very useful lesson, thanks!",True
@willykitheka7618,2022-07-31T22:49:26Z,1,"Hey Josh, nice to get a face to a name!",True
@HasanAYousef,2022-07-30T07:37:47Z,0,"Great as usual, how can I save the model after getting it trained, and how can I consume it later for prediction without repeating the training process?",True
@aksharkottuvada,2022-07-28T21:52:25Z,1,Thank you Josh. Needed this tutorial to better solve a ML Problem as part of my internship :),True
@juanete69,2022-07-19T20:06:16Z,0,"What is the difference between the eval_metric and the scoring? Why are you using ""aucpr"" and ""roc_auc"" for them and not the same value for both? aucpr is better for unbalanced data, why are you using roc_auc too?",True
@juanete69,2022-07-17T23:41:19Z,0,"When should we use ""object"" type and when ""categorical""?  Why are you saying one-hot encoding is not the same for linear regression?   Isn't it the same than dummy variables? I think R converts does automatically the one-hot encoding when you fit a regression model containing a categorical variable (factor).",True
@shashidharmuniswamy2620,2022-07-15T13:52:13Z,0,"Hi Josh, Thank you for this very well-explained tutorial. I just bought a copy of your Jupyter notebook. I saw the application of XG Boost for three different datasets in the downloaded folder, but I'm wondering how different will it be for a regression problem? I just replace XGBoostclassifier with XGBRegressor? Do you have a Jupyter notebook copy for XG Boost solving a regression problem, please?",True
@tytaifeng,2022-07-14T13:13:20Z,0,"josh, i just absolutely love this video, after watching your videos about the theory part, then implement with real coding, is the best comb for learning. just have a few questions  1. when you use get_dummy, do we have to specify drop_first = True?  normally, when should we drop first? 2. after converting using onehot, too many features were created and making X very sparse, is there a conventional way to deal with this problem? (say if instead of using xgb, using random forest, too many categorical converted features will dominate in feature selection for building the tree, how can we handle this problem?) 3. for labels, do we have to oversample the 1's since it's only 26%? i think this is a very interesting topic. as always, love your video and i want to show you my support and my appreciation to your amazing videos",True
@elvisnizama5560,2022-07-10T18:17:10Z,0,"Great videos, I'd like to buy your book on Kindle, but the link doesn't work. Can you help me?",True
@nishkarve,2022-07-05T00:18:39Z,1,Is that a Tabla in your background?,True
@erichganz4605,2022-06-28T16:56:40Z,1,This guy is just amazing <3,True
@sabrinakhan952,2022-06-17T21:23:46Z,0,"I get an error at the following line: plot_confusion_matrix(clf_xgb,                       X_test,                       y_test,                       values_format='d',                       display_labels=[""Did not leave"", ""Left""])  Looks like plot_confusion_matrix is deprecated?   The error is here: --------------------------------------------------------------------------- XGBoostError                              Traceback (most recent call last) /var/folders/cm/vqsqnkqn19x8nz5_d5912hhrzh0cg1/T/ipykernel_37155/2937921683.py in <module> ----> 1 plot_confusion_matrix(clf_xgb,       2                       X_test,       3                       y_test,       4                       values_format='d',       5                       display_labels=[""Did not leave"", ""Left""])  ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)      61             extra_args = len(args) - len(all_args)      62             if extra_args <= 0: ---> 63                 return f(*args, **kwargs)      64       65             # extra_args > 0  ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_plot/confusion_matrix.py in plot_confusion_matrix(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax, colorbar)     261         raise ValueError(""plot_confusion_matrix only supports classifiers"")     262  --> 263     y_pred = estimator.predict(X)     264     cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight,     265                           labels=labels, normalize=normalize)  ~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py in predict(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)    1282         iteration_range: Optional[Tuple[int, int]] = None,    1283     ) -> np.ndarray: -> 1284         class_probs = super().predict(    1285             X=X,    1286             output_margin=output_margin,  ~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py in predict(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)     879         if self._can_use_inplace_predict():     880             try: --> 881                 predts = self.get_booster().inplace_predict(     882                     data=X,     883                     iteration_range=iteration_range,  ~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py in inplace_predict(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)    2032             from .data import _ensure_np_dtype    2033             data, _ = _ensure_np_dtype(data, data.dtype) -> 2034             _check_call(    2035                 _LIB.XGBoosterPredictFromDense(    2036                     self.handle,  ~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py in _check_call(ret)     216     """"""     217     if ret != 0: --> 218         raise XGBoostError(py_str(_LIB.XGBGetLastError()))     219      220   XGBoostError: [17:07:24] ../src/c_api/c_api_utils.h:161: Invalid missing value: null Stack trace:   [bt] (0) 1   libxgboost.dylib                    0x00000001222a1a54 dmlc::LogMessageFatal::~LogMessageFatal() + 116   [bt] (1) 2   libxgboost.dylib                    0x000000012229b28e xgboost::GetMissing(xgboost::Json const&) + 286   [bt] (2) 3   libxgboost.dylib                    0x00000001222a9454 void InplacePredictImpl<xgboost::data::ArrayAdapter>(std::__1::shared_ptr<xgboost::data::ArrayAdapter>, std::__1::shared_ptr<xgboost::DMatrix>, char const*, xgboost::Learner*, unsigned long, unsigned long, unsigned long long const**, unsigned long long*, float const**) + 516   [bt] (3) 4   libxgboost.dylib                    0x00000001222a8f28 XGBoosterPredictFromDense + 344   [bt] (4) 5   libffi.7.dylib                      0x000000010d9bcead ffi_call_unix64 + 85",True
@jingxuan104,2022-06-02T19:23:08Z,0,"Hi Josh, for the numerical variable do you still code missing data as 0? if so, this 0 could be misleading to a real zero, right?",True
@nikhilshaganti5585,2022-05-27T20:47:30Z,0,"Thank you for this great tutorial Josh! Your videos have immensely helped in understanding some of the complex topics. One thing I noticed while watching this tutorial is the handling of categorical features. I think the explanation you gave for ""Why not to use LabelEncoding?"" is applicable for models like Linear Regression, SVM, NN but not for Trees because they only focus on the order of the feature values. For example, in a set of [1,2,3,4], threshold < 1.5 would be equivalent to threshold == 1. Please let me know if my thought process in wrong.",True
@fernandes1431,2022-05-27T10:23:07Z,0,"Hi, I'm just playing with the code and I get an error in the 'Build a preliminary XGBoost Model' section. The ValueError: 2 different 'eval_metric' are provided. Use the one in constructor or 'set_params' instead. Can you advise on how to fix this please as I'm a noob :D",True
@cszthomas,2022-05-27T02:41:16Z,1,Thank you for the great work!,True
@cszthomas,2022-05-26T02:19:09Z,0,"Thank you for this XGBoost tutorial video. It makes people like me so easy to follow. In this tutorial, you printed gain, cover, total_gain and total_cover at https://youtu.be/GrJP9FLV3FE?t=3231. It reminds me that you showed us how to calculate gain at https://youtu.be/8b1JEDvenQU?t=457 and cover at https://youtu.be/8b1JEDvenQU?t=606 in your ""XGBoost Part 2 (of 4): Classification"" video. Can you explain which of gain, cover, total_gain and total_cover in this tutorial are gain and cover in ""XGBoost Part 2 (of 4): Classification"" video? I am still a bit confused after reading XGBoost Python API doc.",True
@felixzhao3435,2022-05-25T01:16:47Z,1,Thanks!,True
@fernandes1431,2022-05-24T14:08:09Z,3,Can't thank you enough for the clearest and best explanation on YouTube <3,True
@jongcheulkim7284,2022-05-19T09:48:16Z,1,Thank you so much^^,True
@kd6600xt,2022-05-17T14:40:56Z,4,"Quick note: At 43:30 instead of using the plot_confusion_matrix() which is now depreciated, you need to use ConfusionMatrixDisplay.from_estimator(). This can be done as follows: Include: from sklearn.metrics import ConfusionMatrixDisplay at the start with the other imports. Then when printing the confusion matrix you need to use the line: ConfusionMatrixDisplay.from_estimator(clf_xgb, X_test, y_test, display_labels=[""Did not leave"", ""Left""])",True
@alexpowell-perry2233,2022-05-12T13:58:01Z,0,Great video! Can you do a video on EBM (explainable Boosting machines) please?! This would be really helpful - thanks!,True
@lollmao249,2022-05-08T17:52:20Z,0,can i use this to predict stock market ? for time series forecasting ?,True
@harshbordekar8564,2022-05-07T11:16:56Z,1,Thank you for the awesomeness!!,True
@mehrdadzamirian,2022-04-20T05:35:28Z,0,"I have a question about handling the pd.get_dummies(). When it is used for gender, it turns out the ( M or F) to  a  single column of 1 and 0. But if get_dummies is used for a column that has 3 or more unique values, it creates columns equal to number of unique values. Isn't it better to use argument drop_first=True to reduce the redundancy. If not, when do you recommend to use this argument?  BTW, thanks a lot for these great videos! I have learned a lot.",True
@Mohith7548,2022-04-12T18:04:26Z,0,Expected to prune features based on their importance - feature selection!?,True
@Mohith7548,2022-04-12T17:36:57Z,0,XGBoost works with categorical data as well. Why did you encode those columns??,True
@yelyzavetatymoshenko1572,2022-04-11T15:12:29Z,1,Great one!,True
@navyasreepinjala1582,2022-04-06T04:29:15Z,2,I love your teaching style. Extremely helpful for a beginner like me. Really helped me a lot in my exams. No words. You are the best!!!!,True
@skyw_nker3723,2022-03-27T18:13:48Z,0,"50:26 very good representation of what my parents understand I'm doing thanks for the great tutorial! do you consider doing one about bayesian (or gradient descent) optimization to the hyperparameters tuning?",True
@marekslazak1003,2022-03-19T18:56:05Z,3,"Jesus, i just learned more over 10 minutes of this than i did throughtout an entire semester of a similar subject on CS. ++ tutorial",True
@tantalumCRAFT,2022-03-18T17:46:22Z,3,"This is hands down the best Python tutorial on YouTube.. not just for XGBoost, but overall Python logic and syntax. Nice work, subscribed!!",True
@v1hana350,2022-03-11T22:24:23Z,0,I have a question about the Xgboost algorithm. The question is how parallelization works in the Xgboost algorithm and explain me with an example.,True
@francovega7089,2022-03-06T01:24:35Z,1,I really appreciate your content Josh. Thanks for your time,True
@keyurshah8451,2022-03-01T06:57:28Z,1,"Hey Mate, amazing tutorial. Very complex problem explained in really simple and effective way.  I am using XGBOOST for one of the classification model and after watching your video it made me realise I can further improve my model. So thank you again and keep making those videos. Kudos to you and long live data science üôèüôè",True
@sunsiney7014,2022-02-20T21:10:02Z,1,Great video! Very informative and clearly explained! Could you please also present BART?,True
@teresitaeyzaguirre4741,2022-02-11T13:34:48Z,1,I love you statq,True
@gisleberge4363,2022-02-10T08:10:43Z,1,"Appreciate the Python related videos...helps to manoeuvre the code when I try to replicate the method later on...easy to follow the whole thing, also for beginners... üôÇ",True
@shahreyarnajeeb1006,2022-02-03T16:20:41Z,0,"Hi great video, Can't we use SMOTE to balance the data so we get good predictions for both 'leaving' and 'not leaving'?",True
@sandipansarkar9211,2022-01-21T14:00:53Z,1,finished watching,True
@samxu5320,2022-01-16T11:59:07Z,1,Your pronunciation is the most authentic and clearest that I have ever heard,True
@tillgrimminger8199,2021-12-20T15:30:08Z,0,"Hi Josh,  I'm only an undergrad so I can't be sure, and please dont crucify me :), but wouldn't stratifying the train test split result in passive data leakage because we ""know"" the exact percentage of people leaving the company in the test set? In Sklearns documentation stratify is mentioned as a parameter when used in cross-validation. This makes sense as you wouldn't want differenty balanced folds when hyperparameter tuning, however in the final evaluation wouldn't it be good to test the performance on differently balanced data?  Love your videos!",True
@tamirganor,2021-12-20T08:53:06Z,0,Grate video just one comment for the eval_metric = 'aucpr' it is the area under the percussion recall curve and not  the FP TP curve,True
@romanroman5226,2021-12-11T19:14:23Z,1,Awesome video! The cleanest xgboost explanation a have ever seen.,True
@ArunKumar-fg1yj,2021-12-08T15:32:23Z,0,"Hello Josh, I am trying to follow your steps. However,  plot_confusion_matrix(clf_xgb,                       X_test,                       y_test,                       values_format = 'd',                       display_labels=['Did not leave','Left'])  Throw an below error message:-  XGBoostError: [09:28:33] c:\users\administrator\workspace\xgboost-win64_release_1.5.1\src\c_api\c_api_utils.h:161: Invalid missing value: null  Can you please tell me what i am doing wrong",True
@Dollar123Bills,2021-12-06T00:08:14Z,1,That was amazing,True
@SergioPolimante,2021-12-01T19:57:02Z,1,This kind of content is SUPER HARD to produce. I really understand and appreciate your effort here. Thanks and congratulations.,True
@kaushikmahmud,2021-11-06T04:06:47Z,1,"DO NOT USE ""missing=None"" TO AVOID ERROR. Took me 1 hr to find the error.  clf_xgb = xgb.XGBClassifier(objective='binary:logistic', seed=42) clf_xgb.fit(X_train,            y_train,            verbose=True,            early_stopping_rounds=10,            eval_metric='aucpr',            eval_set=[(X_test, y_test)])  plot_confusion_matrix(clf_xgb,                       X_test,                       y_test,                       values_format = 'd',                       display_labels=['Did not leave','Left'])",True
@eskoo8396,2021-10-29T06:51:51Z,0,hi! at 41:30 it should be validation set and not test set. am I wrong? thanks!,True
@Learner_123,2021-10-18T23:47:44Z,1,"Many thanks for the wonderful tutorial.   Would please support in getting the error when I run the code line -> plot_confusion_matrix(clf_xgb,                        X_test,                        y_test,                        values_format='d',                        display_labels=[""Did not leave"", ""Left""])  XGBoostError: [10:35:55] /Users/runner/miniforge3/conda-bld/xgboost-split_1631904775127/work/src/c_api/c_api_utils.h:161: Invalid missing value: null Stack trace:   [bt] (0) 1   libxgboost.dylib                    0x0000000135d7ee74 dmlc::LogMessageFatal::~LogMessageFatal() + 116   [bt] (1) 2   libxgboost.dylib                    0x0000000135d7842f xgboost::GetMissing(xgboost::Json const&) + 271   [bt] (2) 3   libxgboost.dylib                    0x0000000135d86541 void InplacePredictImpl<xgboost::data::ArrayAdapter>(std::__1::shared_ptr<xgboost::data::ArrayAdapter>, std::__1::shared_ptr<xgboost::DMatrix>, char const*, xgboost::Learner*, unsigned long, unsigned long, unsigned long long const**, unsigned long long*, float const**) + 529   [bt] (3) 4   libxgboost.dylib                    0x0000000135d85ffe XGBoosterPredictFromDense + 350   [bt] (4) 5   libffi.6.dylib                      0x000000010bfc2934 ffi_call_unix64 + 76   THANKS, IT WAS RESOLVED WHEN I CHANGED MISSING=NONE to MISSING=1 in clf_xgb = xgb.XGBClassifier(objective='binary:logistic', missing=1, seed=35, use_label_encoder=False)",True
@pacificbloom1,2021-10-17T18:54:39Z,1,Wonderful video josh.....pleasee pleasee pleasee make more videos on start to finish on python for different models.....i havr actually submitted my assignments using your techniques and got better results than what i have learned in my class  Waiting for more to come especially on python :),True
@imranselim5978,2021-10-14T18:32:19Z,1,"Thank you for the excellent content and walk through. While One Hot Encoding the categorical variables should not we use k-1 variables for k categories? For example, for the Payment_Method column if Mailed_Check=0, Electronic_Check=0, and Bank_Transfer=0 doesn't that imply Credit_Card=1 and make Credit_Card column redundant?",True
@trendytrenessh462,2021-10-11T09:29:29Z,2,"It is really lovely to be able to put a face to the ""Hooray!"", ""BAM !!!"" and ""Note:""s üòÑ‚ù§",True
@rishikesh2598,2021-10-02T18:12:52Z,0,where to download this IBM data source from?,True
@zeusserch98,2021-09-28T08:36:13Z,0,Why you use one hot encoding? Does the xgboost work well with categorical data?,True
@yk4993,2021-09-26T21:06:20Z,0,"Could you please further elaborate on what you exactly mean by ""One-hot encoding is not for logistic/linear regression""?",True
@abdulkayumshaikh5411,2021-09-20T08:31:19Z,1,"Hello josh, you are doing amazing work keep doing",True
@hamidghaedi7202,2021-09-12T22:08:59Z,0,"Hey Josh, thanks for making great videso and  songs :)!  In my data I gam getting a large number of NaNs and want to run XGBoost to solve a classification probelm. Do you think it will be OK to use XGBoost to deal with that situation like having 50% of missing values in some variables with 200,000 observations (row numbers)? Based on the some of the replies you have posted I guess so.   In the data , I have some columns in them  I can observe data is between -5 to +5, if I set NaNs to 0 things will mess up right?",True
@NLarsen1989,2021-09-03T14:41:10Z,1,"Yikes, if I ever understand something enough to explain it as succinctly as you do then I'd be very happy. I've been smashing through a lot of your videos the last few days after spending countless months on python, sklearn and all the usual plug and play solutions and it's not been until I've started watching these that I've started to feel things click into place",True
@vianadnanferman9752,2021-08-30T19:00:43Z,0,"thanks a lot for this tutorial. please, what about multiclass classification? in GBM one vs rest is utilized but here as I read softmax and softprob can be applied but I don't know how they work.",True
@subhadipghosh5787,2021-08-28T00:02:07Z,0,"Can you tell me when i dump a xgboost tree it gives me ""cover"" for each perent node... What is that?",True
@shazm4020,2021-08-27T15:40:53Z,1,Thank you so much Josh Starmer! BAM!,True
@shazm4020,2021-08-27T14:44:49Z,0,I have a question that if the missing value in the variable is integer or float. How we should deal with it? We write it to be 0 or what else?,True
@algorithmo134,2021-08-26T07:21:21Z,0,Why are these videos not in 1080p or 4k?,True
@maurosobreira8695,2021-08-18T17:22:17Z,1,"A true,  real Master Class - You got my support!",True
@rohitrajora9832,2021-08-11T10:42:06Z,0,"Hi Josh, it would be extremely helpful if you could provide the link to the datasets that you use in these kind of videos ...bam.",True
@ruchi3491,2021-07-26T05:30:05Z,0,Hi Josh. one request can you do more of these start to finish series. Also please create a separate playlist for these start to finish videos. Thank you so much for your efforts.,True
@muskanroxx22,2021-07-23T15:17:00Z,1,You're a very kind human being Josh!! Thank you so much for making these videos. Your content is gold!!! I am new to data science and this is exactly what I needed!! :)  Much love from India! <3,True
@saikiran-mi3jc,2021-07-21T17:48:28Z,0,Could you please do videos pn catboost,True
@andrewxie9896,2021-07-21T13:21:39Z,0,"I can see how replacing with a zero for one-hot encoding is meaningful, but say for example, you had a continuous variable ""age"" to predict weight. In that case, you would cluster all the missing ""ages"" (replaced with 0) with the newborns. If age, foot_length, hair color and eye color were the features, and the labels were weight, I feel like 0 for age (probably expected to be of highest importance to weight) would break for the elements with missing and replaced data.age == 0?",True
@andrewxie9896,2021-07-21T13:05:53Z,1,"you are simply an amazing human being, also the notebooks are great! :D",True
@DavidTran23,2021-07-19T00:57:55Z,0,Do you have a version of this for RStudio?,True
@harshavardhanasrinivasan3125,2021-07-18T03:06:40Z,1,Reaaally amazing!!,True
@CalifornianViking,2021-07-15T04:03:45Z,0,"Hi Josh - great video. I have a question about missing data. I see that it is normal in statistics to ""fill out the missing data"" and I sometimes see the attempt to replace missing values with 0. This is a practice that I don't understand from two perspectives:  First, why is it neccesary if you have enough data to train the model? Wouldn't it be better to just eliminate the data?  Second, my experience is that unkown (missing) data is not the same as 0 (or any other data value). It has been the root of a lot of analysis issues that I have seen. It would be better to make a dataset where the data is truly is missing so that predictions can be made on what missing data means for the probability of an outcome. I understand that no charges is the same as 0 charges, I don't understand that a missing (unknown) age is a 0 age. I really like the way that SQL handles this with NULL values, which specifies that the value is unknown, the same is true for np.NaN.  Would you mind comment on these two perspectives?",True
@k44zackie,2021-07-10T15:19:43Z,1,Thank you very much for nice video! Very helpful for me.,True
@pengqian192,2021-07-02T00:55:50Z,0,I do really like your videos regarding GBDT and XGboost series. Could you please post a video regarding LightGBM?,True
@thiagotanure2212,2021-06-30T18:26:41Z,3,amazing tutorial Josh! Shared with my friends =D  Could you do one of these about pygam? It would be amazing :),True
@xuantrung13,2021-06-24T16:15:48Z,1,Triple BAM !!!,True
@alexblablabla5632,2021-06-22T21:33:56Z,0,Not sure if anybody has already asked this: why is it OK that One-Hot encoding makes k variables rather than k-1? Multicollinearity?,True
@Kal-Racso,2021-06-16T19:20:20Z,0,"I executed the first round of optimization but I got different outputs (not the ones commneted below first round), even though I used the same seed. Anyone knows what could be happening? And really, really amazing tutorial, thanks a lot!",True
@derek-kuang,2021-06-06T21:17:55Z,1,"Great lecture! I just purchased and downloaded the Jupyter Notebook. But I got a confusing error, hopefully, I could get some hints here. Thanks  After running the notebook in the XGBoost  1.4.0, if I initialize the clf_xgb by the following with missing=None,  clf_xgb = xgb.XGBClassifier(objective='binary:logistic',                              eval_metric=""logloss"", ## this avoids a warning...                             missing=None, seed=42,                              use_label_encoder=False)  I will get ""Invalid missing value: null"" error by calling   plot_confusion_matrix(clf_xgb,                        X_test,                        y_test,                       values_format='d',                       display_labels=[""Did not leave"", ""Left""])  I know it is because of the model since I got the same error by calling `clf_xgb.predict(X_test)`.  I found a related SO thread but not sure why we still have missing value after data cleaning.  https://stackoverflow.com/questions/67245147/getting-a-weird-error-when-trying-to-run-xgboost-predict-or-xgboost-score   Thank you!",True
@nepalm222,2021-06-03T22:44:52Z,1,"Great Content, subscribed",True
@TomerSalsa,2021-06-01T06:51:06Z,0,"Great video, love your content! I ran the notebook as-is, and got an exception during the ""plot_confusion_matrix(...)"" call  XGBoostError: [06:48:41] ../src/c_api/c_api_utils.h:161: Invalid missing value: null  Did anyone run into it? Thanks",True
@albinthomas7072,2021-05-19T06:35:36Z,0,Is there something wrong with my code if it stopped after building just 23 trees in the clf_xgb.fit ? as opposed to 55 in the video?,True
@kelvinhsueh5434,2021-05-18T17:37:14Z,24,You are amazing. Can't imagine how much work you put into those step-by-step tutorials.  Just bought the Jupyter Notebook code and it's beyond worth it! Thank you :),True
@albinthomas7072,2021-05-17T07:35:36Z,1,For the dataset used in the video -https://www.kaggle.com/yeanzc/telco-customer-churn-ibm-dataset,True
@mdaroza,2021-05-14T06:36:44Z,2,Amazingly organized and well explained!,True
@bishwarupdey10,2021-05-10T20:31:26Z,0,Love your videos ‚ù§Ô∏è‚ù§Ô∏è would you please make a video on ensemble learning techniques and also on time series analysis in python would be of great help.,True
@VarunKumar-pz5si,2021-05-09T12:02:55Z,0,Dataset link?,True
@VarunKumar-pz5si,2021-05-09T11:22:46Z,1,I'm very grateful to have you as my teacher.,True
@minseong4644,2021-04-28T03:14:42Z,1,Such an amazing job Josh.. Couldn't find any better explanation than this! Mesmerizing!,True
@jinwooseong2862,2021-04-27T06:48:40Z,1,I watched your all video for XGBoost. It helps me a lot. very appreciated!,True
@josephhayes9152,2021-04-16T23:05:17Z,1,Thanks for the great tutorial! You covered a lot of details (mostly data cleaning) that are often overlooked or skipped as 'trivial' steps.,True
@karannchew2534,2021-04-11T15:24:46Z,0,51:41 Send them StatQuest coupons instead. Better alternative to milkshake coupon.,True
@karannchew2534,2021-04-11T12:18:21Z,0,Question please.  Why isn't Zip_Code one-hot-encoded?,True
@Ankit-gq8gl,2021-04-08T03:33:11Z,0,"can we use xgboost for unsupervised learning as there is mandatory parameter Y_train. In algorithms like Isolation forest, we can set Y=none, is there such option here?   Also, is simply filling X_train twice in the .fit function is the way?",True
@znull3356,2021-04-06T05:03:09Z,92,"Please keep doing these long-form Python tutorials on the various ideas we've covered in earlier 'Quests. They're great for those of us working in Python, and they give me another way to support the channel. It has been a more-than-pleasant surprise that as I've grown from learning the basics of stats to machine learning and eventually deep learning, StatQuest has grown along with me into those very same fields.  Thanks Josh.",True
@mssnal,2021-03-29T20:16:29Z,1,Awesome man,True
@rubyjiang8836,2021-03-26T14:10:15Z,0,i always thought the voice is fake voice generated by machine... but it turns out to be real,True
@jwxdxd,2021-03-21T03:55:22Z,1,You are amazing! Thank you so much !!,True
@markrauschkolb5370,2021-03-20T14:17:11Z,9,"Extremeley helpful - would love to see more from the ""start to finish"" series",True
@srmsagargupta,2021-03-10T20:05:31Z,0,"Question Sir: how can we get display the tree with best auc score, i.e. 55th iteration. Also, can we use the rules directly or every time we have to run the model. Thanks in Advance",True
@daniloyukihara2143,2021-03-07T15:46:30Z,1,"hurray, i picture you totally different!  Thanks a lot for all the videos!",True
@gemon39,2021-02-19T13:11:59Z,0,thank you so much for the video! How could  you install kite with mac in jupyterlab ?,True
@farlem92,2021-02-17T15:11:17Z,1,"Hi Josh, thanks for the amazing content as always.   Maybe you covered this and I just missed it so apologies in advance but from roughly 41.47 in the video you use the training dataset to build the model and at the same time you are evaluating the model using the test dataset. This test data is used in tandem with the early stopping parameter and when the ""aucpr"" score stops increasing, the model then stops processing and choses the epoch that has the best ""aucpr"" score that resulted from the test dataset.   I am wondering if there is a problem in using the test dataset in order to chose the best iteration for the model to stop on? The test dataset is data that the model has not seen before so should the model be trained on a training dataset, and evaluated with a validation dataset, and then once the optimal epoch has been selected then you can predict the outcomes on the test dataset, which the model has not seen before.  Maybe I have misunderstood slightly but hopefully this makes sense.",True
@TheSpiralnotizblock,2021-02-15T10:53:08Z,0,"I just finished watching your video and thought it was very good. It was a nice introduction to XGBoost.  I wanted to use XGBoost to predict tennis matches, which player is most likely to win from a match. Is that even possible with XGBoost?",True
@tejaskulkarni5386,2021-02-13T13:17:32Z,1,"I wish I was allowed to pay you money, you life saver You will have to do with a sub for now :P",True
@bedirhangundoner9627,2021-02-10T11:00:46Z,0,"We made two possible parameter values. And firstly, since max_depth = 4 and has the middle, we got it. And since l_rate = 0.1 and it's the highest on its list, we also tried the higher values on the 2nd try, [0.1, 0.5, 1]. At this point I have a question; Isn't it possible for [max_depth = 3, l_rate = 0.05,] to be better than [max_depth = 4, l_rate = 0.1]?  I thought this cost was possible. But I think it's worth it: Is it better to avoid the possible cost of time than the possible cost of incorrect parameters? (of course by searching the optimum point) Am I correct or is there something missing?",True
@joshuanasr5226,2021-01-29T14:09:18Z,0,"Hi Josh, thanks a lot for a video! This Joshy needs help! I am new to all this programming stuff and am learning it for a university project. I downloaded anaconda with the numpy, xgboost, pandas.... but I am not able to import them into Jupyter. Any clue what the problem can be?",True
@nalidbass,2021-01-20T16:21:42Z,1,"Josh, won't there be target leakage when we evaluate 'aucpr' on the testing dataset to determine the number of trees?",True
@bjg4320,2021-01-11T01:12:56Z,0,I have a question why we need to keep the imbalance data and do not use oversampling or undersampling to handle the imbalance data?,True
@Pooja-ok4xt,2021-01-06T11:46:32Z,0,Video for difference between restricted estimate of maximum likelihood and maximum likelihood,True
@1386imran,2020-12-29T04:04:00Z,1,Do I see a tabla kept in the background....üòÖ,True
@sudheerrao07,2020-12-28T16:19:54Z,5,"Wow. Finally I see a face for the name. Your previous videos have had immensely helpul. I assumed you are a very senior person. I am not measuring your age. I mean, your way of explaining seemed like a professor with half a century of experience. But in reality, you are quite young. Thank you for all your simple-yet-detailed videos. No words to quantify how much I appreciate them. üôè",True
@santoshpanchal4644,2020-12-19T20:03:48Z,0,"Hi, nice video. Quick Question. How to output model for deployment on spark. Can you give some guidance on best practice for deploment in production.",True
@its_me7363,2020-12-18T11:14:25Z,2,"Can we use xgboost for multilabel classification? If yes, what parameters should be changed?",True
@henkhbit5748,2020-12-08T15:47:43Z,1,"Greatly appreciated this videoLike you said, telcos should gives more effort to tie the current customers. In real practice you want to know what the probability is that a current customer will no longer renew the subscription. You should then try to bind the customer with a high risk with incentives.",True
@abhishekchandrasekaran97,2020-12-02T16:50:25Z,0,"Hi Josh, Great video. Can't thank you enough for this.  One issue I am facing:  Whenever I run GridSearchCV and pass the param_grid, it is always returning the lowest value (magnitude) in the list for each hyper-parameter that I have passed.  Even when I change the range it keeps doing this. I never get the middle value and when I fit the model with the 'optimal_params.best_params_' output values after 2 rounds, the accuracy for 'Did not Leave' prediction increased but accuracy for 'Left' prediction decreased further, which is not awesome.  Any inputs on why this could be happening? One more doubt sir if I may, how do we interpret the leaves of the tree that we drew. I understand it is just one tree but can we guess an approx classification of 'left/did not leave' by looking at the leaf values? If yes, then how?",True
@Madsope,2020-12-01T14:23:06Z,0,"Really like the video! Clear explanation. I had already given XGB some shots and I used LabelEncoder from sklearn. That way, categorical data is not mapped to thousands of columns. My result (without optimisation) actually improves then over pd dummies. Is that essentially the same or am I missing something?",True
@hugochiang6395,2020-11-29T17:12:15Z,0,Great video Josh!  I have a question.  What if the missing data is numerical?  Wouldn‚Äôt setting it to zero throw off the calculation since we don‚Äôt one hot encode numerical variables?,True
@beautyisinmind2163,2020-11-28T08:12:05Z,0,"sir, during coding for multiclass classification, say y has 4 categories A B Cand D when I label the using label encoder using following code from sklearn.preprocessing import LabelEncoder ly = LabelEncoder() ly.fit_transform(y) then it assign labeling randomly, how can i label them on my desired way like this, A = (1 0 0 0 ), B = (0 1  0 0), C = (0 0 1 0), D = (0 0 0 1), or A =1, B =2, C =3 D =4",True
@saketnarendra8864,2020-11-27T11:12:56Z,0,"ValueError: Unable to parse string ""Afghanistan"" at position 0 what to do",True
@beautyisinmind2163,2020-11-26T17:00:23Z,0,do we need feature scaling for XGBOOST or not? and another question how can we optimize hyperparameter using heuristic algorithm like PSO?,True
@helttt,2020-11-24T02:32:09Z,0,Great presentation. How do you get access to the IBM sample data? Is there a link?,True
@danemerson6963,2020-11-22T19:47:44Z,0,It looks like the example tree that you selected to draw was classifying around the numerical value of zip code? Would it make more sense to one-hot encode this value since it is not appropriate to model zip codes as continuous data?,True
@nehabalani7290,2020-11-19T06:23:34Z,2,Good to also see you sing rather than just hear :).. i had to comment this even before starting the training,True
@jaikishank,2020-11-13T09:46:06Z,0,"Thanks Josh for the webinar and the Notebook is a great resource and ready reckoner with all the requisite tips.                                                                                                                                            One clarification on the Hyperparameter tuning as below is requested here... For ex. i did cross validation from a list of values of gamma and arrived at one value x as best parameter and do the next iteration of another parameter for ex learning_rate,fixing gamma vallue and then fix and do the third iteration for fixing both gamma and learning rate and search for best value of min_child_weight , and so on. In the process what we are getting is best values for these three gamma,learning rate and min_child_weight and will this remain intact for all combinations of other hyper parameters. Once we get the best parameter value we can freeze them for the final model. Do u have any presentation on xgboostregressor?? Thanks in advance...",True
@jimmyrico5364,2020-11-09T14:47:48Z,1,"This is a great piece of work, thanks for sharing it!  Maybe the only additional piece I'd add which I've found useful on the documentation of XGBoost is that one can take advantage of parallel computing (more cores or using a graphic card your machine or you could have on the cloud) by simply passing the parameter (n_jobs = -1) while doing both, the RandomizedSearchCV stage and the setting the XGB regressor type (XGBRegressor for example).",True
@tooljerk666,2020-11-06T04:35:23Z,0,Why do you not use xgb.DMatrix for test/train? I thought that was necessary for XGBoost.,True
@DrlGSN,2020-11-03T08:48:54Z,0,"Hello Statquest, thank you for this amazing tutorial. I just have one quick question on this. For One-Hot-Encoding, do I need to drop one categorical class for each categorical feature like what we are supposed to do in Linear and Logistic Regression?",True
@tooljerk666,2020-11-02T03:23:19Z,0,"When using the model on new, unknown data, do you put the new data through the same process of changing strings to nums, or do you use the transformed values, like they were used with dummy values. Do you instead gather a number of the new data points and go from there? Also, why wasn't the DMatrix attribute used?",True
@yogeshbharadwaj6200,2020-11-01T11:41:45Z,0,"Hello Sir, Thanks a lot for the video with step by step very detailed explanation. Have a doubt, understand that from data science models we can do predictions, eg like in this video we tried to predict whether customer would stop taking company service or not, but does model answers (or provides info) about what were the main reasons due to which customers are deciding to stop taking service i.e which inputs datapoint/observations are the main drivers of predictions. Reason to ask this question is, in practical scenario when we do such customer churn predictions and present it to senior leaders/Management, the question from management will be, do we know what are the top 3 reasons why our customers would decide to stop taking our service ?  Sorry for lengthy question, but got this doubt when I was thinking about how management would react when we present such predictions from model. Thanks in advance and for the awesome video once again.",True
@yoniziv,2020-10-31T13:21:58Z,1,Triple Bam! thanks for your great tutorial,True
@tas3159,2020-10-29T21:34:43Z,0,Thanks for that very clear tutorial.  A question. On 21:25 why do you use loc and not iloc.,True
@danielmagical6298,2020-10-19T10:24:46Z,2,"Hi Josh, great job really helpful material as I'm discovering XGBoost just now.   Thank you and keep you great work!",True
@nitishkumar-bk8kd,2020-10-17T10:11:31Z,0,"hey josh , loved your video , very informative  especially the hyperparameters  there is also a parameter ( min_child_weight  ) can u explain what that is ?",True
@tulanezhu,2020-10-16T22:23:28Z,0,This is very helpful. Could you explain how does XGB do multi class classification? Does it use a softmax method or one-vs-rest method?,True
@zheyizhao4865,2020-10-15T06:41:50Z,14,"Hey Josh, I just purchased all of your 3 Jupyter Notebook! I transferred from Econ major to Data Science, it was a nightmare before I find your channel. Your channel shed the light upon my academic career! Look forward to more of the 'Python from Start to Finish' series, and I will definitely support it!",True
@tooljerk666,2020-10-11T20:40:16Z,0,"Great video. I found a similar dataset on Kaggle, but where do you find the dataset used in the video? Tried Googling it but can[t find the exact dataset.",True
@udbharat_official,2020-10-07T05:13:02Z,0,Really!!!... u play tabla ?,True
@dcscla,2020-10-07T00:18:37Z,0,"Hi, Josh, I'm having troubles with this code. I bought the Jupyter Notebook. I'm windows 10 and I'm getting this error :  CalledProcessedError: Command '['dot', '-Tsvg']' returned non-zero exit status 1. [stderr: c'Format: ""svg"" not recognized. Use one of:\r\n  This is in the second to last cell of the jupyter notebook. Any idea about why I'm getting this error after installed and added graphviz to my system path? I have searched a lot, but did not find a solution to this.",True
@abulaitijiang,2020-10-03T02:43:29Z,1,"Hi, thanks for amazing video. How do you know which parameters you tune can help improve predict true positive(people Left) percentage? Is it the metric: aucpr parameters did the magic? Or I am missing something?  Thanks in advance",True
@user-tz9sr4fy1z,2020-09-25T06:51:58Z,0,I'm still stuck downloading xgboost The code you said (and any other code I found on the internet) does not work for me..ideas?,True
@Jopapy09,2020-09-19T09:37:31Z,0,"@StatQuest with Josh Starmer    what do you think about this discussion https://github.com/szilard/benchm-ml/issues/1 It seems that one-hot encoding isn‚Äôt necessary for tree-based algorithms, but you may run into a problem using this on linear models. I personally find ordinal encoding is more than sufficient and it significantly reduces your memory footprint if you have quite a few categorical features.",True
@levongaloyan7079,2020-09-16T09:14:37Z,1,Thank You,True
@narendrasompalli5536,2020-09-14T06:09:31Z,0,Sir can you please do a video on stacking algorithm,True
@sreejaysreedharan4085,2020-09-03T20:30:06Z,1,Lovely and priceless video Josh...BAM BAM BAM as usual !! :) God bless. .,True
@aaltinozz,2020-09-02T19:45:09Z,1,all week searched for this thank u very much <3,True
@anuradhadas8795,2020-08-29T04:56:28Z,0,Great Video!! One query though.. Why is one-hot-encoding not for logistic and linear regression?,True
@quasenerd5476,2020-08-27T18:36:12Z,0,"Excellent tutorial! Thank you very much  27:52 you mentioned that it's not proper to convert categories to numbers. The same applies even when the categorical data in question is naturally ordered? (such as income classes, where the next class always has a greater income level)",True
@satria5403,2020-08-25T15:24:40Z,0,"Hi Josh, great video as always.  Will support you with buying the Notebook. I wonder if you can walk through us building XGBoost model using spark?",True
@megalaramu,2020-08-25T10:42:56Z,0,"BAM!!! Another awesome stat. But josh one question though. While replacing the missing values with 0, just because it was categorical replacing with 0 helped us identify exactly which is missing, incase the column or the feature having missing value is numeric then as well replacing the missing value with 0 is a good idea ??",True
@queueoverflow,2020-08-23T14:53:57Z,0,I thought you were going to write a XGB from start to end...,True
@darksoul1381,2020-08-23T11:08:02Z,1,I was wondering how to find stuff regarding dealing with actual churn data and sampling issues. The tutorial addressed a lot of them. Thanks!,True
@haskycrawford,2020-08-21T22:47:39Z,1,I love the channel! Eu aprendo + aqui do que a Gradua√ß√£o! You great josh!,True
@obadajabassini3552,2020-08-20T16:19:30Z,0,We need more advanced statistics content :) how about stochastic processes e.g. Brownian motion. A lot of us would like more complicated topics to be explained by you :),True
@Fressia94,2020-08-20T15:13:19Z,1,many thanks to your great and so understandable video. It literaly helps me a lot in Python and XGBoost package,True
@andrewnguyen5881,2020-08-14T19:45:54Z,0,"Again another quality video, I was following along with your every word, which did bring up come questions: 1. When XGBoost deals with missing data, does it ever consider splitting the missing data in half?    --Using your example, would it ever do 1 blue and 1 green? What would happen if XGBoost encountered a data set with alot of missing values?                                                         2. When you ran your Cross-Validation, was there a reason you only used 3 values for each hyperparameter? Could you have done more if you wanted to? 3. When I ran my Cross-Validation, my scale_pos_weight didn't change even though I used the same parameters you did. What do you think the problem could be?",True
@godoren,2020-08-13T20:40:13Z,2,"Thank you for your job, the explanation of the topic is very clear and transparent.",True
@Krath1988,2020-08-13T17:51:20Z,102,"Liked, favorited, recommended, shared, and sacrificed my first-born to this video.",True
@azecem6187,2020-08-13T16:20:45Z,1,Thanks a lot Josh!,True
@RahulVarshney_,2020-08-13T11:58:54Z,3,"""25:36"" that's what i was waiting for from the beginning...Truly amazing.. You are providing precious information..CHEERS",True
@jessehe9286,2020-08-13T07:03:40Z,6,"Great video! Love it!  request that you do a comparison of XGBoost, CatBoost, and LightGBM, and a quest on ensemble learning.",True
@diegorueda3991,2020-08-12T19:28:41Z,0,"Hi, I have followed this tutotial step by step, but, when I get to the part of modelling, I keep gettin the following error message: ""'DataFrame' objects are mutable, thus they can not be hashed.""  Does anyone knows what this could be?  Something I've noticed is that when I use the function ""get_dummies()"", the dtype of the resulting columns is ""uint8"" instead of ""int64"" or ""float64"".",True
@MarshallDTeach-yr2ig,2020-08-12T14:56:19Z,0,Backpropagation please sir üò≠üò≠üò≠,True
@fgfanta,2020-08-12T13:42:34Z,0,"This is gold, thank you! I am a rookie of this stuff, still I am unsure one-hot encoding is the best to do, especially to encode the city; being a category with high cardinality, all those variables for 1-hot encoding will require many splits (I guess). Perhaps using a different encoding, like mean encoding or frequency encoding, would be better, may allow to have a good fit with fewer splits.",True
@yurimartins1499,2020-08-12T13:27:04Z,1,"Thank you Josh!! As a suggestion, you could do a StatQuest explaining the measures in market basket analysis?",True
@learnmandarinwithkaili1102,2020-08-12T01:57:12Z,0,play at 1.25x would be better,True
@kojsbarv,2020-08-11T11:37:46Z,0,"Love your work and a great webinar! Just one nitpick, you are not setting stratify to y for yes but passing it the target labels array. https://youtu.be/GrJP9FLV3FE?t=2279",True
@TheDhrv04,2020-08-11T11:15:32Z,1,Quadruple Bam !!!!!!!!!!,True
@KukaKaz,2020-08-10T04:48:02Z,5,Yes pls more videos with python‚ù§thank u for the webinar,True
@danielpinzon9284,2020-08-09T23:48:58Z,1,"Love u Josh.... you are a TRIPLE BAM!!!  Greetings from Bogot√°, Colombia.",True
@ketanshetye5029,2020-08-08T10:40:07Z,1,"could not help u with money right now , but i watched all the adds in video , hope that helps u financially . love u videos . keep up!!",True
@julieirwin3288,2020-08-07T20:27:32Z,3,What did we do to deserve a great guy like Josh ? Thank you Josh!,True
@saeedesmailii,2020-08-07T17:49:30Z,2,"It was extremely helpful. Please continue making these videos. I suggest making a video to explain the clustering with unlabeled data, and predicting the future trend in time-series data.",True
@Beyond90Days,2020-08-06T17:36:03Z,1,41:38-41:39  hahahahaha that was quite innocent mistake,True
@geoffreyzhang,2020-08-06T12:07:03Z,2,"I like your videos. Just one comment here. Most of time it is not good to use true test data as your validation data set. That is kind of ""treating"".",True
@parismollo7016,2020-08-06T11:27:52Z,3,I haven't watched it yet but I know this will be great!!!!!!!! Thank you Josh.,True
@abhinaym5923,2020-08-05T20:07:48Z,4,I am purchasing the Jupiter notebook to contribute to your work! Thanks a lot for this video! You are awesome! Will be very very happy to have more ML tutorials and thank you Josh!,True
@dagma3437,2020-08-05T19:55:30Z,0,I'm so glad you are a bad-ass stats guru and a teacher waaaaaaaaay before a singer and a guitarist ...Thank you! ;),True
@robingutsche1117,2020-08-05T18:14:35Z,0,"Dear Mr. Starmer, thanks for this amazing video once again. Could you explain why you use logistic regression as the objective parameter of the XGBoost classifier? Thanks and best regards",True
@OrionEx88,2020-08-04T18:35:58Z,1,"Thank you for this great video! By the way, do you have the link to download the dataset?",True
@anggapradikta1050,2020-08-04T08:04:12Z,1,Nice guitar,True
@hollyching,2020-08-03T22:56:52Z,0,Thanks Josh for another GREAT video! Just some sharing and minor questions. 1. try pandas_profiling when doing EDA. I personally love it. :) 2. some features are highly correlated (eg: city name and zip code). Do we need to handle that before running XGB? 3. Why choose 10 for early_stopping_rounds 4. What‚Äôs the difference between¬† - df.loc[df['Total_Charges']==' '] - df[df['Total_Charges']==' '] 5. What‚Äôs the difference between¬† - y=df['Churn_Value'].copy - y=df['Churn_Value']  Many thanks in advance!  H,True
@redaterjani8323,2020-08-03T17:00:48Z,0,"Hi, thank you for this amazing webinar. I have one question : Does Xgboost perform better with LabelEncoding than One hot encoding ?  I did some research and I find that one hot encoding is more adapted to linear models like ( linear regression, SVM .. ) and perform worse with tree-based models ( RandomForest,Xgboost .. ) Can I have some explanation ? Thanks in advance",True
@ramons.g5135,2020-08-03T11:50:13Z,0,"Hi Josh, great video as usual!   Just one question: when you run GridSearchCV, how do you tell the function how many trees to build? I see that when you use the .fit method in this classifier you can set the ""early_stopping_rounds"" which gives you control on the number of iterations being run, but when you use the GridSearchCV, how do you do this? For instance you know that when you use the GradientBoostingClassifier you can tell it to run ""n_estimators"", but seeing the XGBboost documentation I can't seem to find it anywhere...",True
@arnaiztech,2020-08-03T08:47:51Z,1,Really cool!! BAM BAM BAM!!,True
@Flaming88,2020-08-03T06:49:40Z,0,"Hi Josh,  Question on setting colsample_bytreee in XGBoost, is it will reduce the accuracy since the feature space expend on categorical data due to One-Hot-Encoding? In the end the model fails to evaluate the entire dataset while being trained because setting this parameter will randomly sample the column hence the model fail to grasp the actual context from the data due to One-hot-encoding. Is it doing something like LabelEncoding is much better option?  I am still new, but correct me if I am wrong on my point above.  -Ken",True
@shnibbydwhale,2020-08-02T23:06:34Z,0,"Hi Josh! Loved this video. I had a quick question about OHE vs Dummy Variables. I know that you used the pd.get_dummies method in this video, but this is a broader question about the two methods. I have a decent stats background and I know that when you are doing linear or logistic regression and you need to create indicator/dummy variables, you only need k-1 dummy variables where k is the number of unique categories for a given variable. I noticed that you did not drop one of the returned dummy variables for each column that you got from the pd.get_dummies method to create reference categories for your variables. Is this something that XGBoost handles just fine? And does this work for other common ML algorithms like SVMs or ANNs? Do you even need to create reference categories in ML algorithms? Which of the pd.get_dummies method or OHE generally yields better results? Thanks so much and keep up the great work! Your videos always put a smile on my face, and teach me a few new things in the process!",True
@starmerf,2020-08-02T21:45:07Z,1,Awesome hooray,True
@felixwhise4165,2020-08-02T18:46:27Z,1,just here to say thank you! will come back in a month when I have time to watch it. :),True
@SaurabhKumar-mr7lx,2020-08-02T12:19:05Z,0,"I have been tuning XGBoost for long, but it's like every time I look at your videos I will learn something new. One query on encoding. As per my experience, one hot encoding doesn't add extra information & also increases the feature space. Is it not suggest to use frequency encoding in such cases.",True
@haohuynhnhat3881,2020-08-02T08:53:02Z,0,i hear about category embedding instead of onehot encoding. Can you make a video about it?,True
@Azuremastery,2020-08-02T08:51:57Z,1,Thanks for sharing! Informative.,True
@rrrprogram8667,2020-08-02T08:14:54Z,0,When is xgboost with R coming out??,True
@metasoft0221,2020-08-02T07:00:03Z,0,"Thank you. Hi Josh, can you do the same program for LightGBM too?",True
@haohuynhnhat3881,2020-08-02T06:47:31Z,0,how about  multiple imputation using chain equation MICE? i am learning it for my project right now and i think that it is interesting.,True
@viniantunes5944,2020-08-02T00:42:12Z,4,"Josh, you're the didactic in person form.  Thanks!",True
@damienhood3306,2020-08-02T00:30:23Z,0,Is the Jupiter notebook lesson available anyway?,True
@dbporter,2020-08-01T21:47:31Z,0,could you mix the guitar,True
@theredflagisgreen,2020-08-01T21:33:12Z,1,This is magical.,True
@marceloherdy2379,2020-08-01T21:14:02Z,1,"Man, this video is awesome! Congratulations!",True
@zakkyang6476,2020-08-01T20:28:21Z,2,to check nuniquev value for each column use this: for col in df.columns: print(f'{col}: {df[col].nunique()}'),True
@PradeepMahato007,2020-08-01T20:20:08Z,2,"BAMMMMM !!!   This is awesome üëç Josh !! Thank you for your contribution, really helpful for new learners.üòäüòäüòä",True
@chiragpalan9780,2020-08-01T19:55:02Z,5,This guy is amazing.                         DOUBLE BAM üí• üí•,True
@arijitdas4504,2020-08-01T19:47:19Z,9,"If ""Stay Cool"" had a face, it'd be you :)",True
@sunnyp3484,2020-08-01T19:12:30Z,0,"Josh, next time you make such videos, please make sure that your cursor is visible. It helps a lot!!",True
@BiffBifford,2020-08-01T18:57:08Z,12,I am not a math geek. I am here strictly for the intro song!,True
@huseyin405,2020-08-01T18:29:07Z,2,Baaaaaaaaaaaaaaaaaaaaaaam,True
@MasterofPlay7,2020-08-01T18:23:15Z,1,"wow you are great instructor, very well explained! Are you also familiar with big data such as apache spark?",True
@ashrukhtf3183,2020-08-01T18:17:00Z,0,I am unable to open url to download jupyter notebook,True
@NoOffenseAnimation,2020-08-01T18:14:30Z,2,I'm early! BAM!!!,True
@RahulEdvin,2020-08-01T18:13:45Z,10,"Josh, you‚Äôre well and truly phenomenal ! Love from Madras !",True
@statquest,2020-08-01T18:13:24Z,50,NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: https://statquest.gumroad.com/l/uroxo  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@FlexCrush1981,2020-07-17T20:56:01Z,3,Very enjoyable webinar Josh. Thanks for posting. I'm not 100% sure how to interpret the leaves. The largest leaf value is 0.188 where Dependents_No<1 is true. Does this mean customers who fall into this category are the ones most likely to churn out of all the other combinations?,True
