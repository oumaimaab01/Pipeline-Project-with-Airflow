author,updated_at,like_count,text,public
@statquest,2022-05-09T20:43:48Z,0,Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@ricardoveiga007,2024-05-15T11:10:30Z,1,Highly educational and entertaining! Thank :),True
@alexhaowenwong6122,2024-01-02T21:37:32Z,1,Seems power is akin to resolution on a camera lense--being able to focus well enough to see that two small or distant objects that appear to be one are in fact distinct objects.,True
@user-yh4tx6ol9e,2022-09-18T06:39:08Z,1,Ugh!,True
@MDMAx,2022-05-12T08:31:13Z,1,"Lol, just do more experiments.   Very informative and descriptive lesson. Ty!",True
@MDMAx,2022-05-12T08:10:09Z,0,"11:21 ""to the one true mean"" :D",True
@stoic-999,2021-06-02T10:06:57Z,0,"Hi guys, has anyone of you gone through the probability moocs by NN Taleb where he critiques p-value and presents a very good content on p-value hacking ?",True
@esperanzazagal7241,2020-11-24T06:25:53Z,3,I want to give you a hug. Thank you for explaining this.,True
@jackyhuang6034,2020-09-24T14:18:08Z,3,"Josh, can you do a video on effect size? Thanks",True
@TheSpades21,2020-06-28T19:37:27Z,1,"Copy and pasted from wikipedia on Power of a Test: ""For a type II error probability of β, the corresponding statistical power is 1 − β. For example, if experiment E has a statistical power of 0.7, and experiment F has a statistical power of 0.95, then there is a stronger probability that experiment E had a type II error than experiment F. This reduces experiment E's sensitivity to detect significant effects. However, experiment E is consequently more reliable than experiment F due to its lower probability of a type I error."" Is this not saying, that the test with the lower power inherently has a lower probability of having a type I error and thus a lower p-value? Isn't this also saying that power= the probability that you will get a high p-value (the opposite of what Josh says in this video)? Please help me understand how I am wrong",True
@chrismalone8470,2020-06-06T06:14:46Z,0,you forgot to explain p hacking,True
@miguel.gargicevich,2020-04-21T03:33:26Z,0,"on 3.06 where you show all the p-values of the different sets and you mention the ocassional""false positive. Don't you want to have p-values smaller than 0,05?",True
@SoupCannot,2020-03-10T18:15:30Z,1,"I think this video has the potential to be confuse people about the concept of effect size. In some contexts, effect size can just be the difference between means, as it is presented here. In my field, it's much more common to talk about effect size as Cohen's d, which takes into account the variation in the data by having the standard deviation in the denominator; so ""effect size"" and ""variation in the data"" aren't necessarily two separate concepts, depending on how effect size is defined.",True
@pulutogo8266,2020-02-05T21:44:49Z,0,how did you calculate the 30% ?,True
@tanzine91,2019-09-28T13:36:32Z,0,"Hi Josh, I need some understanding on ""(1) Use your results as preliminary data and do a proper power calc"" and may be also on (2). What I understood from your vid was, we can say a t-test with sample size=N at 5% alpha works as intended if, when we perform many t-test say 1000 times on bootstrapped sample from the pop, 5% of the time there will be p<0.05 (correct?). So if with newly collected test set, I get like p=0.051, it might be because I used too little N, so that I wasn't able to detect the difference? So the action is, when this happened, we should do power calculation to determine the sample size using the desired level of power? Were these what this video is all about?",True
@jorgemedina8377,2019-05-07T21:05:08Z,0,"Thank you for the video. I had a question: How does one determine the power of a test relative to some other test? If we are deciding what test to use, how can we determine which test is the most powerful one?",True
@luisluiscunha,2019-01-31T14:19:49Z,2,Chorus... Stat...Quest...Stat...Quest  ahahahahah Thanks man!,True
@abhaydadhwal1521,2019-01-20T15:54:51Z,1,what is a good p-value Josh...?,True
@abhaydadhwal1521,2019-01-19T06:17:32Z,1,whats a t-test that u're saying again and again Josh...?,True
@brianp9657,2019-01-12T13:22:48Z,3,"Thanks again for all the videos!  I had a question about effect size.  If Effect Size = (mean of experimental group - mean of control) / standard deviation, is there a way to quantify ""large"" effect sizes in this way vs. ""smaller"" - or is there some other way to quantify effect size?",True
@chauphamminh1121,2019-01-02T03:47:27Z,1,"great video ! Thanks so much for the tutorial. So understandable. But I still have a problem with the last part. I wonder if increasing the sample size helps to distinguish the 2 distributions, so how big the sample size should be ? Let's re-use your example with diet mice and non-diet ones. First, I draw 2 sample sets (weight some mice from each set), calculate the SD and write down the size N for each the sample. With the formula SE = SD/ sqrt(N),  I can estimate SE for each the sample. So what's next ? I do not sure how we can choose an appropriate N, because intuitively, just rise the size to get better result, is it right ? Hoping you can help me out. Thanks in advance ! Btw, keep up the good work.",True
@coolsonic8982,2018-11-28T23:22:24Z,2,Great video,True
@LiquidLithe,2018-10-01T04:08:33Z,5,"Excellent video! I’m attempting to replicate the first part (with the increased false positives) and am having a bit of trouble. Basically, I’m taking n number of samples of an x and a y from the same random Gaussian (with mean of 0 and variance of 1), and storing those in lists to access later. I’m then t-testing those n number of samples, storing all indices with p-values of 0.05-.1 and then using those indices to go back and add one random value (sampled from the same Gaussian) to both the original x and the original y. Is that the right process? I’m finding the proportion of false positives increases, but not by nearly as much as 30%!",True
@himals1277,2018-06-22T17:51:58Z,1,"what do you mean that you added ""bogus data""?where did you get it from??",True
@beckwilde,2018-04-23T23:54:59Z,1,great explanation,True
@safecomiguel,2018-04-02T00:20:05Z,0,"Josh, these are amazing! Thank you!  Idea for future Stat Quest: How to build a predictive model for a rare event.  I have seem different things such as over-sampling the minority class, under-sampling the majority class, incorporating the prior probabilities to adjust y intercept due to the over or under sampling, and others such as SMOTE, gradient boosting, and other modeling techniques.    Heck, just an explanation on why the y intercept needs to be adjusted when over or under sampling would be great video.",True
@lilmoesk899,2017-06-24T06:31:30Z,4,"Thanks so much for your videos. I'm not sure why, but ""power"" has been one of those concepts in statistics that I haven't ever really been able to get my head around. This video definitely helps!",True
@mariadelujancalcagno,2017-04-25T02:13:15Z,1,You are a genius to explain the things! Thank you!,True
