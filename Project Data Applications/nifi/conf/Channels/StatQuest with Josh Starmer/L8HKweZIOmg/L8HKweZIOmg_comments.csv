author,updated_at,like_count,text,public
@statquest,2023-05-06T19:02:20Z,8,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@yasharzargari4360,2024-05-29T22:58:14Z,1,This channel is awesome. Thank you,True
@WeightsByDev,2024-04-19T19:16:33Z,1,This video is very helpful... BAM!,True
@user-te7tu7tk8f,2024-04-19T15:24:55Z,1,"Thank you, so I now can have intuition of why the name is encoder and decoder, that I've curious for full 1 years.",True
@user-se8ld5nn7o,2024-04-10T01:03:16Z,0,"Another amazing video and I cannot thank you enough to help us understand neural network in a such friendly way! At 4:48, you mentioned ""because the vocabulary contains a mix of words and symbols, we refer to the individual elements in a vocabulary as tokens"" . I wonder if this applies to models like GPT when it's about ""limits of the context length (e.g., GPT3.5, 4096 tokens) or control the output token size.",True
@bobuilder4444,2024-04-01T16:26:20Z,0,Do you need the same number of lstm cells as there are embedding values?,True
@ramielkady938,2024-03-29T18:37:07Z,0,"Is it that challenging to explain how ChatGPT ""Answers Question?"" - not how it translates ? Someone advertised they will explain how vehicles work, but they cannot stop talking about bicycles. and they never ever talk a single word about vehicles ... I mean why ?  Put in your title ""How computers translate"" so you don't mislead us. Last questions: Do you think ChatGPT revolutionized translation ? who cares about translation man ?  Google has been doing this since before you were born.",True
@rollingstone1784,2024-03-27T08:59:32Z,0,"@statquest: 14:05: should be ""phrase"" instead of ""phase""‚Ä¶",True
@omarmohamed-hc5uf,2024-03-18T06:52:37Z,0,can someone explain to me more thoroughly what is the purpose of the multiple layers with multiple LSTM cells of the encoder-decoder model for seq2seq problems because i didn't understand it too well from the video as the explanation was too vague. but still it's a great video üëç,True
@user-il8vc4pc5f,2024-03-16T16:06:41Z,0,"Didn't understand the part that when we are using 2 LSTM cells per layer, Since the input to these states is the same and we are training it the same way why would the weight parameters be any different. Pls correct me if I'm wrong.",True
@antimagicray,2024-03-09T17:32:03Z,0,what is unrolling? I understand everything except for what you mean by 'unrolled'?,True
@GenesisChat,2024-03-08T05:56:16Z,1,"14:34 seems like a painful training, but one that, added to great compassion for other students, led you to produce those marvels of good education materials!",True
@GenesisChat,2024-03-08T05:33:09Z,0,"As other people say, these lessons are gold. Le'ts say SOTA. There's a very little detail I don't understand though. Why using the words let's to go in the example, when what we want to translate let's go? It kinds of make things somewhat confusing to me...",True
@Foba_Bett,2024-03-04T11:22:14Z,1,These videos are doing god's work. Nothing even comes close.,True
@harshmittal63,2024-03-01T02:24:13Z,0,"Hi Josh, I have a question at time stamp 11:54.   Why are we feeding the <EOS> token to the decoder, shouldn't we feed the <SOS> (start of sequence) token to initiate the translation? Thank you for sharing these world-class tutorials for free :)  Cheers!",True
@ankushpandit7708,2024-02-29T09:48:15Z,0,What is the logic behind using multiple layers and multiple cells in each layers?,True
@timk570,2024-02-27T09:50:26Z,0,Only one Embedding value is give to one LSTM Unit right? Not multiple embeding values to each LSTM Unit.,True
@cyberpunkdarren,2024-02-27T03:02:38Z,0,I actually learned faster after muting the audio. The smarmy kiddy speak was distracting.,True
@slolom001,2024-02-17T21:21:41Z,0,"Awesome videos! I was wondering how do people training larger models, know ""im ready to press train"" on the big version? Because if some of their assumptions were wrong they wasted all that time training. Is there some smaller version they can create to verify theyre getting good results, and theyre ready to train the big one?",True
@BooleanDisorder,2024-02-10T20:59:43Z,1,300 million bams! ‚ù§,True
@amrutumrankar4609,2024-02-01T06:41:41Z,0,In this full network where does we are telling to convert English word to Spanish word? for example in LSTM OR in Neural network before SoftMax function?,True
@Jai-tl3iq,2024-01-27T07:16:00Z,0,"Sir, So in encoder-decoder architecture, will the number of LSTM units be the same as the number of words in a sequence? I mean, I've seen in many drawings, illustrations, they have three words and the same number of three LSTM cells?",True
@utkarshujwal3286,2024-01-19T09:47:25Z,0,"Dr. Starmer thanks for the video and I had a doubt about this one. While I could understand the training cycle of the model I ain't quite sure about how inference testing is done, because during inference there wont be any tokens to be fed into the decoder side of the model, then how would it come up with a response? If I have to keep it crisp I couldnt understand how the architecture distinguishes training from inference? Is there some signal passed into the decoder side of the model.",True
@harshilsajan4397,2024-01-06T15:36:45Z,0,"Hi great video! Just a question, to give the input to lstm, the input length will be constrained by lstm length right? For example 'let's' in first one and 'go' in second one.",True
@user-if6ny5dk9z,2024-01-04T07:24:06Z,1,Thank You Sir...................,True
@benetramioicomas3785,2024-01-02T11:27:23Z,1,"Hello! Awesome video as everything from this channel, but I have a question: how do you calculate the amount of weights and biases of both your network and the original one? If you could break down how you did it, it would be very useful! Thanks!",True
@101alexmartin,2023-12-31T18:18:35Z,1,"Thanks for the video Josh, it‚Äôs very clearly explained.   I have a technical question about the Decoder, that I might have missed during the video. How can you dynamically change the sequence lenght fed to the Decoder? In other words, how can you unroll the decoder‚Äôs lstms? For instance, when you feed the <EOS> token to the (let‚Äôs say, already trained) Decoder, and then you get <vamos> and feed it  together with the <EOS> token, the length of the input sequence to the decoder dynamically grows from 1 (<EOS>) to 2 (<EOS>+<vamos>). The architecture of the NN cannot change, so I‚Äôm unsure on how to implement this.  Cheers! üëçüèªüëçüèª",True
@ilirhajrullahu4083,2023-12-25T16:18:40Z,0,"This channel is great. I have loved the series so far, thank you very much!  I have a question:  Why do we need a second layer for the encoder and decoder? Could I have achieved the same result using only 1 layer?",True
@avishkaravishkar1451,2023-12-19T14:58:09Z,0,Hi Josh. Are the 2 embeddings added up before it goes as an input to lstm?,True
@hey_steven,2023-12-10T13:02:43Z,0,Why there is ‚Äúto‚Äù in let‚Äôs go word embedding?,True
@amortalbeing,2023-12-07T19:00:26Z,1,I liked it a lot. thanks ‚ù§,True
@anupmandal5396,2023-12-03T20:35:37Z,1,Awesome Video. Please make a video on GAN and BPTT. Request.....,True
@user-dk3mk4il3g,2023-11-29T19:07:11Z,0,"Hi sir, one question can there be a case where number of layers in decoder could be different than the encoder. Or it can never happen due to size of context vector? will adding a new layer in decoder give any advantage?",True
@juliali3081,2023-11-26T09:55:59Z,4,"It took me more than 16 minutes (the length of the video) to get what happens since I have to pause the video to think, but I should say it is very clearly explained! Love your video!!",True
@dslkgjsdlkfjd,2023-11-25T13:51:21Z,0,Do the LSTMS  in the second layers have the same weights and biases as the LSTMS in the first layer? Sorry if I missed that part.,True
@alecrodrigue,2023-11-21T00:38:03Z,1,awesome vid as always Josh :),True
@pranaymandadapu9666,2023-11-14T16:10:12Z,0,"First of all, thank you so much for the clear explanation!  I was confused when you said in the decoder during training that the next word we will give to the LSTM is not the predicted word, but we will use the word in training data. How will you let the network know whether the predicted token is correct?",True
@grb321,2023-11-14T00:29:23Z,0,"Very clear, but I'm confused by the statement that the target sentence is reversed because that is how it's done in the original paper.  In 'Sequence to Sequence Learning with Neural Networks' by Suskever et al, it is the source sentence that's reversed, not the target sentence.",True
@MariaHendrikx,2023-11-05T10:41:32Z,1,Really well explained! Thnx! :D,True
@chrischauhan1649,2023-10-30T12:13:52Z,0,"I have one question, in encoder you say we have 2 layers of LSTMs with 2 LSTM cells in each layer, why we didn't count stacked LSTM cells (if we do, we would have 4 LSTM cells per layer). Can you explain that? Also considering Pytorch in torch.nn.LSTM(), here we would have input_size = 2, num_layer= 2 what would be the hidden_size =2 or 4?",True
@chrischauhan1649,2023-10-30T11:19:46Z,1,"This is what the internet is made for, world class education at home for free.",True
@xxxiu13,2023-10-23T04:07:51Z,1,Great explanation!,True
@harshvardhankhanna7030,2023-10-21T17:14:42Z,0,I did not understand the use of multiple LSTM cells . Its like training two separate neural networks on same problem with networks having no connection. How does two separate cells help each other learn better? Thanks in advance for the reply.,True
@bibhutibaibhavbora8770,2023-10-13T08:11:24Z,1,See this is the kind of explanation I was waiting for‚ù§,True
@user-km8ou2ml2d,2023-10-12T18:46:17Z,0,"Is the matching of number of embeddings to number of LSTM cells per layer a coincidence or does each LSTM cell read/receive one of the embedding dimensions? (simple example had 2 -> 2, Seq2Seq paper had 1000 -> 1000)",True
@dsagman,2023-10-10T02:26:04Z,1,this is my homework assignment today. how did youtube know to put this in my feed? maybe the next statquest will explain. üòÇ,True
@luiscarlospallaresascanio2374,2023-09-17T20:09:08Z,1,Ahora tiene audio en espa√±ol :0,True
@zhangeluo3947,2023-09-13T07:53:42Z,0,"Thank you so much sir for your clear explanation! But I have a question is that if you do word embedding for all tokens in d (let's say >2) dimensions, is that mean we can use the number of LSTM cells as d rather than just 2 cells for each layer? Or even more deep layers not just 2? Thank you!",True
@roczhang2009,2023-09-02T14:57:17Z,1,"Hey, thanks for your awesome work in explaining these complex concepts concisely and clearly! However, I did have some confusion after watching this video for the first time (I cleared them by watching it several times) and wanted to share these notes with you since I think they could potentially make the video even better:  1. The ""ir vamos y <EOS>"" tokens in the decoding layer are a bit misleading in two ways:  	a. I thought ""ir"" and ""y"" stood for the ""¬°"" and ""!"" in ""¬°Vamos!"" Thus, I was expecting the first output from the decoding layer to be ""ir"" instead of ""vamos.""  	b. The position of the ""<EOS>"" token is also a bit misleading because I thought it was the end-of-sentence token for ""¬°Vamos!"" and wondered why we would start from the end of the sentence. I think ""<SOS> ir vamos y"" would have been easier to follow and would cause less confusion.  2. [6:20] One silly question I had at this part was, ""Is each value of the 2-D embedding used as an input for each LSTM cell, or are the two values used twice as inputs for two cells?"" Since 2 and 2 are such a great match, lol.  3. One important aspect that is missing, IMO, in several videos is how the training stage is done. Based on my understanding, what's explained in this video is the inference stage. I think training is also very worth explaining (basically how the networks learn the weights and biases in a certain model structure design).  4. Another tip is that I felt as the topic gets more complicated, it's worth making the video longer too. 16 minutes for this topic felt a little short for me.  Anyways, this is still one of the best tutorial videos I've watched. Thank you for your effort!!",True
@willw4096,2023-08-27T00:28:35Z,1,1:57 5:25 5:48 5:56‚ùó 6:07 6:12 6:41 8:08 8:20 8:30,True
@HAAH999,2023-08-24T03:29:26Z,0,"When we connect the outputs from layer 1 to layer 2, do we connect both long/short memories or only the short term memory?",True
@wellingtonereh3423,2023-08-23T11:45:48Z,0,"Thank you for the content. I have three questions:  1) I've studied bentrevett github implementation and I've noticed that the size of LSTM hidden layers are 512. But the input for LSTM is 256(size of embeddings). The hidden layer output from LSTM shouldn't be 256? I understood the layers, for example, when I printed the shapes:  hidden shape: torch.Size([2, 1, 512]) cell shape: torch.Size([2, 1, 512]) , I know I have size 2 because the LSTM have 2 layers. But the number 512 crash my head. 2) Cells are long short memory and hidden layers are short memory? 3) How batch size affects the model? If my batch size is 1, my sentence will be encoded in context vector and decoded in second LSTM. But if I pass 2 or more sentences, my encoder will handle it?",True
@coolrohitjha2008,2023-08-20T20:44:09Z,1,Great lecture Josh!!! What is the significance of using multiple LSTM cells since we already have multiple embeddings for each word?  TIA,True
@juaneshberger9567,2023-08-08T02:27:45Z,1,"Best ML vids out there, thanks!",True
@ligezhang4735,2023-08-06T01:44:34Z,1,"Wonderful tutorial! Studying on Statquest is really like a recursive process. I first search for transformers, then follow the links below all the way to RNN, and finally study backward all the way to the top! That is a really good learning experience thanks!",True
@tupaiadhikari,2023-07-27T18:12:31Z,1,"Thank you Professor Josh, now I understand the working of Se2Seq models completely. If possible can you make a python based coding video either in Keras or Pytorch so that we can follow it completely through code? Thanks once again Professor Josh !",True
@Nono-de3zi,2023-07-27T17:54:27Z,0,"What is the activation function used in the output fully connected layer (between the final short-term memories and the inputs to the Softmax)? Is it an identity activation gate? I see in various documentations ""linear"", ""affine"", etc.",True
@yangminqi839,2023-07-17T12:07:29Z,0,"Hi Josh! Your video is amazing! But I have one question: When building the Encoder, you mentioned that 2 LSTM cells and 2 LSTM layer are used, I think one LSTM layer has only 1 LSTM cell (in terms of Pytorch's nn.LSTM) if we don't unroll, isn't it? So is there two different LSTM neural networks (nn.LSTM) are used, each one has two layers, and each layer has 1 LSTM cell? Or there is just one LSTM neural network with 2 layers, and 2 LSTM cells in one layer (this means nn.LSTM can have multiple LSTM cells) ? Which one is correct? I think is the former, please correct me if I'm wrong! Many Thanks!!",True
@shashankagarwal4047,2023-07-15T04:00:02Z,1,Thanks!,True
@mateuszsmendowski2677,2023-07-10T11:34:33Z,1,"Coming from video about LSTMs. Again, the explanation is so smooth. Everything is perfectly discussed. I find it immersively useful to refresh my knowledge base. Respect!",True
@cat-a-lyst,2023-06-29T16:40:28Z,11,I literally searched everywhere and finally came across your channel. seems like gradient descent worked fine .,True
@cat-a-lyst,2023-06-29T16:39:24Z,1,you are an excellent teacher,True
@siddharthadevanv8256,2023-06-27T19:53:55Z,0,You're videos are really amazing... ‚ù§ Can you make a video on boltzmann machines?,True
@ririnch7408,2023-06-26T01:39:18Z,2,"Hello, thank you for the wonderful tutorial once again. Just a question about word2vec output of embedding values, I'm a bit confused as to how we can input multiple embedding values from one word input into LSTM input. Unrolling  it doesn't seem to make sense since its based on one word, if so, do we sum up all these embedding values into another layer of y=x and with weights associated them in order to get a single value for a single word input?",True
@user-qd1sb6ho8l,2023-06-23T13:47:50Z,0,"Thank you, Josh. You are amazing.   Would you please teach Graph Neural Networks?",True
@Luxcium,2023-06-20T20:40:27Z,0,Oups üôä What is ¬´¬†*Seq2Seq*¬†¬ª I must go watch *Long Short Term-Memory* I think I will have to check out the quest also *Word Embedding and Word2Vec‚Ä¶* and then I will be happy to come back to learn with Josh üòÖ I am impatient to learn *Attention for Neural Networks* _Clearly Explained_,True
@mitchynz,2023-06-20T19:53:31Z,0,"Hi Josh - this one didn't really click for me. There's no 'aha' moment that I get with almost all your videos. I think we need to walk through the maths - or have a a follow up - even if it takes an hour. Perhaps a guest lecturer or willing student (happy to offer my time) ... alas I guess as the algorithms become more complex the less reasonable this becomes, however you did a masterful job simplifying CNN's that I've never seen elsewhere so I'm sure if anyone can do it, you can! Thanks regardless - there's a lot of joy in this community thanks to your teaching.",True
@HaiderAli-nm1oh,2023-06-19T17:54:28Z,0,awsome ! is their video on transformers ?,True
@baocaohoang3444,2023-06-15T18:14:31Z,1,Best channel ever ‚ù§,True
@soukainafatimi7414,2023-06-15T17:08:19Z,0,"thank you for this video and for all the efforts. i have one question , what is the diffrence between the lstm cells and the lstm units? what is the dimention of the hidden state  and cell state of the example in the video ?  i am really confused",True
@gabip265,2023-06-15T10:12:55Z,9,"I can't thank you enough for these tutorials on NLP. From the first tutorial related to RNNs to this tutorial, you explained so concisely and clearly notions that I have struggled and was scared to tackle for couple of weeks, due to the amount of papers/tutorials someone should read/watch in order to be up to date with the most recent advancement in NLP/ASR. You jump-started my journey and made it much more pleasant! Thank you so much!",True
@theneumann7,2023-06-07T22:46:36Z,1,perfect as usualü¶æ,True
@enestemel9490,2023-06-06T19:35:10Z,1,Thank you Joshhh !!! I really love the way you teach everything <3,True
@scorinth,2023-06-06T13:53:22Z,0,"So, if I understand correctly, the context vector in this example has 8 dimensions?  2 dimensions to the word embedding, times 2 since each layer outputs long and short term states, times two because there are two layers.  So the context vector can be represented by 8 scalars...?",True
@uebyCyka,2023-06-05T17:34:27Z,0,"So, how do we train the encoder part? Or is it already pretrained? Like word2vec?",True
@szymonkaczmarski8477,2023-06-04T08:04:53Z,0,"Great video! Finally some good explanation! I have a question regarding SOS and EOS tokens, sometimes it is mentioned that the decoder start the process of decoding by taking the SOS token, how does the whole picture differ then, for the both input sentences we always have then SOS and EOS tokens?",True
@CarlJohnson-jj9ic,2023-06-03T05:35:31Z,0,"I like most of the videos but this one seems a bit redundant, what is the key theme in this video? I can't afford to keep watching these without production value...",True
@vishnuthanki8966,2023-06-01T10:34:16Z,0,Hey Josh....I have a request to make and this is little bit off-track but can you also provide help in understanding Cox Regression and Survival Analysis thing ??,True
@advaithsahasranamam6170,2023-05-30T23:53:29Z,1,"Great explanation, love it! PS do you have a suggestion for where I can learn to work with seq2seq with tensorflow?",True
@chenzeping9603,2023-05-29T10:20:09Z,0,How do the two lstms in the same layer know to learn different things? Wouldn‚Äôt they learn the exact same thing?,True
@CelinePhan,2023-05-27T04:38:26Z,1,love your songs so much,True
@Xayuap,2023-05-26T20:01:35Z,0,"hi, 9:00 does the deco connects to the encoder 1 on 1? or do we have to connect each deco output to each encoder input all to all fully connected fashion?",True
@fancytoadette,2023-05-26T01:34:51Z,2,Omg I‚Äôm sooooooo happy that you are making videos on this!!! Have been heard it a lot but never figured it out until today üòÇ cannot wait for the ones on attention and transformers ‚ù§ Again thank you for making these awesome videos they really helped me A LOT,True
@ashkraze,2023-05-25T19:06:25Z,0,what is an unrolled lstm?,True
@BHAVYAJAIN-lw1fo,2023-05-24T11:54:37Z,2,cant wait for the tranformers video,True
@paulk6900,2023-05-23T20:35:57Z,4,"I just wanted to mention that I really love and appreciate you as well as your content. You have been an incredible inspiration for me and my friends to found our own start up im the realm of AI without any prior knowledge. Through your videos I was capable to get a basic overview about most of the important topics and to do my own research according to those outlines. So without taking into consideration if the start up fails or not, I am still great full for you and I guess the implications that I got out of your videos led to a path that will forever change my life. So thanks‚ù§",True
@TheApgreyd,2023-05-23T15:25:03Z,0,Is it possible to see more vids about vanilla pytorch?,True
@kmc1741,2023-05-22T02:35:34Z,1,I'm a student who studies in Korea. I love your video and I appreciate that you made these videos. Can I ask you when does the video about 'Transformers' upload? It'll be big help for me to study NLP. Thank you.,True
@lucianozaffaina9853,2023-05-20T08:49:52Z,1,Can you make a video where  you explain how Least Angle Regression works?,True
@kadirkaandurmaz4391,2023-05-17T17:27:52Z,1,Wow. Splendid!..,True
@spp626,2023-05-17T08:41:02Z,0,Hello Josh! I really like your videos and explanation. Here I m with a doubt. Can we use the data of last 150 years for stock price prediction like crude oil etc in time series using garch? I have done the analysis by garch model  but does it seem an over large data? Or should I use data of last 50 or 60 years only? Could you please help me out? Thank you in advance.,True
@eboubaker3722,2023-05-16T19:45:43Z,0,"hi love your videos can make a video about ""Association Rules""",True
@rachit7185,2023-05-16T14:18:31Z,14,"An awesome video as always! Super excited for videos on attention, transformers and LLM. In the era of AI and ChatGPT, these are going to go viral, making this knowledge accessible to more people, explained in a much simpler manner.",True
@harshyadav1190,2023-05-15T20:18:51Z,0,"I am confused what does LSTM cell means, And how 2 different lstm cell will have different weights and biases?",True
@AI_Financier,2023-05-14T06:11:13Z,1,"Great video! thanks for producing such a high quality, clear and yet simple tutorial",True
@jakemitchell6552,2023-05-13T17:24:08Z,2,Please do a series on time series forecasting with fourier components (short-time fourier transform) and how to combine multiple frame-length stft outputs into a single inversion call (wavelets?),True
@snowmountain6352,2023-05-12T23:15:51Z,0,need transformers!,True
@TonnyPodiyan,2023-05-12T16:44:01Z,1,"Hello Sir, I was going through your stats videos (qq plot, distribution  etc)and loved your content. I would be really grateful, if you can make something regarding a worm plot. Nothing comes up on youtube when I search it.",True
@selinatian6607,2023-05-11T11:07:47Z,1,Like this! And want transformers!,True
@timmygilbert4102,2023-05-10T23:38:55Z,0,Can't wait to see the stanford parser head structure explained as a step towards attention!,True
@tornadospin9,2023-05-10T15:46:42Z,110,"This channel is like the Khan Academy of neural networks, machine learning, and statistics. Truly remarkable explanations",True
@Er1kth3b00s,2023-05-10T12:28:33Z,1,Amazing! Can't wait to check out the Self-Attention and Transformers 'Quests!,True
@khaikit1232,2023-05-10T10:13:01Z,1,"Hi Josh,   Thanks for the much-needed content on encoder-decoder! :)  However, I had a few questions/clarifications in mind: 1) Do the number of cells between each layer within the Encoder or Decoder be the same? 2) From the illustration of the model, the information from the second layer of the encoder will only flow to the second layer of the decoder. Is this understanding correct? 3) Building off from 2), does the number of cells from each layer of the Encoder have to be equal to the number of cells from each corresponding layer of the Decoder? 4) Do the number of layers between the decoder & encoder have to be the same?  I think my main problem is trying to visualise the model architecture and how the information flows if there are different numbers of cells/layers. Like how would an encoder with 3 layers and 2 cells per layer connect to the decoder that perhaps have only 1 layer but 3 cells.",True
@falconer8518,2023-05-09T23:32:21Z,0,"To train the model, it is enough to make a backprop for the decoder Or you need to update the weights for the encoder ?",True
@ZinzinsIA,2023-05-09T19:14:23Z,1,"Absolutely amazing as always, thank you so much. Can't wait for attention and transformers lessons, it will again help me so much for my current internship !",True
@magtazeum4071,2023-05-09T18:14:27Z,0,"Hi Josh, is there a bundle of pdf books on statictics to purchase in your store ? I already bought the studyguide on linear regression .",True
@marswang7111,2023-05-09T05:25:19Z,2,üòÄüòÄüòÄLove it,True
@marswang7111,2023-05-09T05:24:46Z,1,Love it,True
@hawawaa1168,2023-05-09T02:49:10Z,1,"yoooo Lets goooooo , Josh posted !",True
@Sarifmen,2023-05-08T20:06:13Z,3,We are getting to Transformers. LEETS GOOO,True
@shafiullm,2023-05-08T19:53:19Z,2,I got my finals of my final course in my final day tomorrow of my undergraduate journey and you posted this exactly few hours ago.. thats a triple final bam for me,True
@kevalan1042,2023-05-08T19:32:25Z,2,BAM-to-BAM!,True
@ygbr2997,2023-05-08T17:49:17Z,0,using <EOS> as the first input in the decoder to start the whole translation does appear to be magical,True
@prashlovessamosa,2023-05-08T17:25:33Z,1,Damm again awesome stuff.,True
@The-Martian73,2023-05-08T14:24:12Z,1,Hello Josh !!üòä,True
@sheldonsebastian7232,2023-05-08T14:13:05Z,1,Yaas more on Transformers! Waiting for statquest illustrated book on those topics!,True
@m.taufiqaffandi,2023-05-08T13:06:08Z,8,This is amazing. Can't wait for the Transormers tutorial to be released.,True
@reinerheiner1148,2023-05-08T08:09:44Z,43,"This channel is gold. I remember how, for my first coding job, where I had no programming knowledge (lol) but had no choice than to take it anyways, I quickly had to learn php and mysql. To get myself started, I searched for the simplest php coding books and then got myself two books from the php & mysql for kids series, even though I was already in my mid twenties. Long story short, I quickly learned the basics, and did code for a living. Complex topics don't have to be complex, in fact they are always built on building blocks of simple concepts and can be explained and taught as such IMHO. Thank you so much for explaining it KISS style. Because once again, I have to learn machine learning more or less from scratch, but this time for my own personal projects.",True
@MCMelonslice,2023-05-08T07:55:39Z,2,"Incredible, Josh. This is exactly what I needed right now!",True
@Rumit_Pathare,2023-05-08T07:02:01Z,1,you posted this video when I needed the most Thanks man and really awesome üëçüèª ,True
@lequanghai2k4,2023-05-08T06:31:40Z,1,wanna see more !,True
@diamondstep3957,2023-05-08T06:16:30Z,2,Love your videos Josh! Thanks for sharing all your knowledge in such a concise way.,True
@datasciencepassions4522,2023-05-08T05:57:29Z,1,Awesome!,True
@bennybenbenw,2023-05-08T05:26:17Z,0,"hope we can have ELMo, self attention, transformer, bert, GPT soon",True
@rrrprogram8667,2023-05-08T04:41:41Z,2,Hey... Hope u r doing good..... So u are about to reach MEGA BAMMMMM,True
@sheiphanshaijan1249,2023-05-08T04:19:52Z,3,Been waiting for this for so long. ‚ù§. Thank you Josh.,True
@KR-fy3ls,2023-05-08T04:18:04Z,2,Been waiting for this from you. Love it.,True
@statquest,2023-05-06T19:02:20Z,8,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
