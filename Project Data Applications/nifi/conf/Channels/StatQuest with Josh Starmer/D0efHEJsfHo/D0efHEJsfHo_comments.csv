author,updated_at,like_count,text,public
@statquest,2021-05-03T13:49:38Z,74,"NOTE: To apply this method to a classification, replace SSR with Gini Impurity (or Information Gain or Entropy or whatever metric you are using).  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@RM-bs2td,2024-05-22T13:47:42Z,0,How to get alpha candidates?,True
@yigithangediz2769,2024-05-14T00:33:25Z,1,excellent explanation. thank you,True
@jiaxuzhang3527,2024-05-13T12:13:31Z,1,Absolutely brillian videos!!! I watched everything from the 1st one to this one in the list and understood so many things that I never understood in schools. I love your videos so much!,True
@user-bz8nm6eb6g,2024-04-22T15:34:23Z,1,Great explanation!,True
@fiammettastrazzerapernicia3287,2024-04-21T17:42:52Z,1,you had me at the first 5 seconds,True
@sadranezam3367,2024-04-20T18:54:50Z,1,"came for the pruning technique , stayed for the ""dididodidodido""",True
@yashyadav5605,2024-04-19T10:12:20Z,1,Smellyyy cat Smellyyy caaatttt What are they feeding youuuüòÇüòÇüòÇüòÇüòÇüòÇ,True
@heerbrahmbhatt6917,2024-04-12T23:07:23Z,1,I'm 50% here for stats and 50% here for the sound effects!,True
@user-bl6zo3xl8x,2024-02-29T12:41:29Z,0,Are we going to keep SSR minimum in the equation {Tree score = SSR + alpha*T} when choosing alpha for cross-validation? so that the New Tree score after removing the leave should be less than the SSR (minimum) +alpha(chosen)*T?,True
@dingusagar,2024-02-28T15:15:49Z,0,Thanks a lot for this.  I came here after getting confused reading this concept from a book. I am inspired by your teaching style. Your style of teaching by examples is the best way to transfer knowledge without losing the audience at any point.  May I ask how much time do you spend to create a tutorial like this? Also what kind of tools do you use to make these videos.,True
@gayathrigirishnair7405,2024-02-05T11:42:56Z,4,This is the best explanation of regression trees that I could find online. Professors are always too mathematical and programmers are too practical. You're explanation is juusssst right. Thanks a bunch for this!,True
@annawilson3824,2024-01-04T18:21:56Z,0,"Why need step 1: The full tree serves as a starting point or a ""maximum complexity model"" from which pruning can systematically reduce complexity. Pruning methods work by simplifying this full tree step by step, assessing the impact of each simplification on model performance.",True
@gunupurugirija7201,2023-12-12T15:09:16Z,2,"lol, that intro one who watches friends and Stat Quest would get it!! love your content its the best machine learning tutorials available",True
@user-jo2zb1rp5w,2023-12-04T02:26:40Z,0,This part is not included in the book and pdf file that I bought. Can you please provide materials,True
@aeronaj7843,2023-11-19T14:35:21Z,0,my data analytics marks is indeed smelly josh,True
@janethe9862,2023-11-17T02:41:10Z,0,need the ame thing on classification tree!,True
@shridharvaidya-fv6bz,2023-11-13T14:26:27Z,0,Can u also make a video about pruning classification tress,True
@hopelesssuprem1867,2023-11-03T11:19:47Z,0,"Hi Josh! There is a mistake in your explanation. In the comments, you said that in order to find alpha, you need to do a complete search of them, adding 1 to the alpha value until it becomes large enough to make sense to prune, but this is a fundamentally incorrect statement.  Alphas from cost_complexity_pruning_path are found using weakest link pruning and this is done as follows: for all subtrees in the tree, alpha is calculated as the difference between the weighted impurity of the root node of the subtree and its total weighted impurity in the leaves. The node/subtree with the smallest alpha is deleted and falls into the cost_complexity_pruning_path array, and then this procedure is repeated from the bottom to the root node in the tree. After the selection of the best alpha from cost_complexit _pruning_path is carried out on a test set or using cross-validation.",True
@user-jj3we9jv9i,2023-10-28T21:09:41Z,2,Liked and Commented to help you with the YouTube algorithm.,True
@subashp7925,2023-10-28T15:47:55Z,1,"Excellent explanation, you are the master in teaching, thank you so much much for your valuable effort",True
@edwin290999,2023-10-24T01:00:22Z,0,Does this work for classification trees as well?,True
@robertpollock8617,2023-10-21T11:04:42Z,0,"Also once you find the alpha values why do you have to go back and run it on the training data each time when you just end up looking at the residuals for the training data? Also for each training test set iteration will you get the same tree structures each time? Or do you have to use the training data for each iteration to get the base tree which could be different from the previous iteration's base tree structure. Even though the base trees and pruned tree outcomes may be different for each iteration as long as you use the same alpha values and the subsequent pruned tree has less RSS you can compile the RSS values for each alpha value no matter the tress structure and can still take the average RSS value for each alpha for comparison? For me, these steps weren't 100% clear or am I totally off base? Obviously the cross-validation iteration I guess is not clear to me. Thanks Rob",True
@robertpollock8617,2023-10-21T10:49:44Z,0,Josp. Great job as usual however you just pulled the starting alpha and subsequent values out of the air. How did you come up with these values?,True
@grys9245,2023-10-09T09:17:01Z,1,"Not FRIENDS üòÇ I was juuuust on a Friends binge a few hours ago before swapping it out for a stats binge. Clearly, everything is‚Ä¶(coughs) correlated üéµ Jokes aside, if I keep going, will there be a quest that starts with üéµstats be there for youuuuüéµ",True
@user-fi2vi9lo2c,2023-09-30T08:27:47Z,0,"Dear Josh, thanks a lot for this video! It's awesome! You told us how to prune regression trees and your explanation was very clear. I've got a question, how can I prune  classification trees? What is the biggest difference between regression trees and classification trees when pruning? I guess that the Tree Score is calculated in a different way when we prune classification trees. Can we simply add tree complexity penalty (alpha*number of leaves) to Gini Impurity to get a Tree Score in case of classification problem?",True
@ankitc011,2023-09-14T07:06:35Z,0,"with alpha as 10000 we get a tree with 3 leaves ( say tree 2 ). When we increase alpha we start pruning from tree 2 or the original tree ? From the video it looks like we start runing from tree 2, but i dont quite get the intuition why we start pruning from tree 2 and not the original tree.",True
@marahakermi-nt7lc,2023-08-02T18:11:46Z,0,thanks joshh for this beautiful video i  have a question in the statquest classification tree using python you extract the alphas only  from  train data however in this statquest you extract.alphas from all data(train and test data) ?,True
@pratyanshvaibhav,2023-07-10T10:43:54Z,3,sir i am learning ML from your videos and everyday i am forced to comment expressing the beauty with which the concept is explained..and the best part is you still clear our doubts even after 3 years..for those who don't know sir has also written a book which is too good,True
@trupologhelper7020,2023-07-09T16:54:21Z,1,"Hey, Josh! Is it ok that we are using train+test to find alpha values? I mean that we are peeping into the future. do we know good thresholds using a test sample(not only train), or am I wrong? Thank you",True
@christopherkyte1477,2023-06-29T05:33:24Z,0,Still super confused on how we picked the best alpha to begin with. Great video nonetheless,True
@soniasu2744,2023-06-28T13:37:40Z,1,you are literally doing god's work,True
@user-dv3xs9br5u,2023-06-19T16:56:43Z,0,"Thank you very much Josh. Could I as a question: what if we throw away the alpha, but just use the three sub-trees and the full tree to run the cross validation and find out the optimal tree structure with the lowest SSR? I mean deciding the best alpha is essentially the same as finding out the best tree structure, for example, in the video 15:16, the alpha=10000 gives the lowest SSR, i can also say the three-leaf-node tree gives the lowest SSR, and then I just choose the three-leaf-node tree as my final decision tree.  So it seems meaningless to find out alpha.",True
@abdelghafourelgharbaoui3886,2023-05-23T18:25:14Z,0,"The best way I can thank you with, is advising you to learn and search about ""ISLAM"", and be a Muslim (muslim mean one who submits to ALLAH). If you want there is channel called Boby's perspective of a person called boby that have a long journey through many believes before he reached the TRUTH, this journey is documented is his channel (Boby's perspective).",True
@amnont8724,2023-05-12T19:16:56Z,1,"Hey Josh, how can we choose our alpha wisely? So that the tree with the minimum tree score will really work well for testing data too. Is there a specific rule of thumb?",True
@mirroring_2035,2023-05-05T16:24:07Z,0,"So a question. We learned previously that cross validation is used to test the model on different ""blocks"" of the test set. But in this case you are advocating for the cross validation to be used for hyper parameter tuning. Does that mean the test sets remain constant?",True
@lolikpof,2023-04-30T13:27:01Z,1,"Cost Complexity Pruning is otherwise known as post-pruning, while limiting the tree depth, enforcing a minimum amount of samples per leaf/split, a minimum impurity decrease is known as pre-pruning, correct? My question is, can you apply pre-pruning first, and then apply post-pruning to the pre-pruned tree? If yes, then i assume that the alpha parameters will be found from the pre-pruned tree, right? Not the initial full tree? And then cross validation will also be performed with the pre-pruned tree, not the initial full tree, to determine the final optimal alpha score?  And on a separate note, should only those alpha obtained from the tree trained on all the data be used when cross validating, and why? Is there no chance that some other, random alpha, might result in better performance? Considering that cross validation is done on several different test/train splits, and there will be those that do better with one alpha, and those that do better with other alphas, doesn't it make sense to try all possible alphas (from 0 to infinity) in the cross validation, not only those that give the best tree scores for the full tree? Isn't there a chance that some other alpha will give, on average, a lower sum of squared residuals than those obtained from the full tree?",True
@hopelesssuprem1867,2023-04-01T01:50:35Z,0,"Josh, that's good that u support connection with us. There's one question I still can't understand: how do we choose alpha i.e. ok in the 1-st time this is always zero but how to get the next alpha? Are we doing this randomly or how? I really can't understand it).",True
@moiseiloo3809,2023-03-24T09:32:18Z,0,"Great explanation. I really enjoyed the video, but I'm a bit confused. Why use all of the data to build the initial tree in step 1? If we do that there is no test data left for testing the tree.",True
@guohaotan7617,2023-03-22T10:09:06Z,0,"is alpha = 0, 10000, 15000, 22000 arbitrary when you reduce the number of leaves?",True
@marieblanchemanche9217,2023-03-04T14:05:08Z,1,"love the reference to Phoebe ! also thank you, all your videos are very helpful",True
@JoRoCaRa,2023-02-17T01:47:56Z,1,you are awesome! clear! to the point!,True
@knightedpanther,2023-02-05T02:29:00Z,1,Thank you so much for this amazing video. Very Amazing!,True
@tamaskiss3237,2023-01-30T18:26:35Z,1,"Thank you for these videos Josh, I really love learning from them. Just one question, when we do the cross validation, should not the alphas be different compared to those in the full sized training data and also on the different cross validation set? If yes, how should we decide which alpha should get the most vote as they are basically different on every training data?",True
@swethanandyala,2023-01-03T05:40:27Z,0,hi Josh. There is small typo error in the video @ 9.45 . The tree score for the tree with 2 leaves is 19243.7+(10000 * 2) but it was written as 19243.7+(10000 * 3),True
@auzaluis,2022-12-23T23:34:00Z,4,I don't know why I spend a lot of time googling if I always end up watching statquest haahahha,True
@sebastiandahnert7692,2022-12-12T16:57:44Z,1,smelly stat :D :D :D :D I love it,True
@ijjasahamed9938,2022-12-07T14:14:03Z,0,i think in the last instead of taking average of lowest value of tree score you said lowest average of  sum of square residuals. can you help me with this,True
@onyman8837,2022-11-18T12:56:27Z,0,"Thanks for the great video, Josh. Quick question. At 15:06 what is the exact meaning of ""on average""? Do you mean simply the most frequently selected alpha during the CV is the final value? Or, should I have to calculate the average of SSR for test set, something like that?",True
@Serenity_Whisper_Music,2022-11-10T23:50:01Z,1,"Thanks so much for your video.  I've watched 8-times and have still one question.  It is about 13:17~14:08  Could you possibly explain more details about the sentence 13:17 ‚ÄúUse the alpha values we found before to build trees(full and sub) that minimize the tree score‚Äù?   My questions about this sentence are 1. How can we use alpha in building trees process? - I thought the way we build trees(full and sub) is the same as how we did in your video ‚Äòregression tree‚Äô  *I understand tree score, alpha, and how alpha plays role in changing the tree scores of different size trees(full, sub trees)  my question about the role of alpha(from whole data) in creating trees..  2. If we can build trees with the same way as we did in ‚Äòregression tree video‚Äô, why do we need this process of 13:17~14:08?",True
@ldk5007,2022-11-02T16:05:56Z,1,"What a beautiful content! I'm not an English speaker, but His video is more helpful than the Korean lecture provided by the college I attending.",True
@inwonderland9842,2022-08-24T16:34:48Z,1,üå≤,True
@juliankoch5704,2022-08-17T13:47:17Z,1,"i'm curious as to how we find sensible alphas. there seems to be an explanatory gap here since in the section ""comparing pruned trees with alpha"" it says (in the NOTE at 8:23) that we find it during cross validation, but in the cross validation section at 13:18 we are supposed to ""use the a values we found before"". sensible alpha values would probably vary widely depending on the SSRs of the trees (and ultimately the variable ranges) and even more so once we do classification since gini and entropy give small values for which alphas of 10k would not work usefully. surely simply guessing various alpha levels and finding which of the guessed ones work best in cross validation is not the best method or am i misunderstanding something here?",True
@abdulsami5843,2022-08-06T01:18:14Z,0,the  average alpha value since is averaged across 10 folds won't be the exact same as any number of discrete values so in the end we will pick the discrete value that is closest to our averaged value right ? or is there like a distinct counter for each value and a score kept as to which one kept least SSR most times and on that note are these alpha values continuous or discrete across which we are making the pruned trees ?  apologies lots of question thrown there,True
@beshosamir8978,2022-07-26T16:23:30Z,1,"Hi Josh , I hope u answer my question, I was searching for 3 days till now and i got nothing  I have 2 problem which is : 1_ How to determine alpha where there is more one leaf in the bottom of tree (i.e : u said increase alpha till pruning this leaf get lower score) , so if i have more than one leaf in the last level of tree, which one should i cut or should i look for all subtrees every time increasing the alpha it seems like it will get high complexity?   2_ in implementation when i will give the model the ideal alpha to implement the decision tree, how the model will know when building it in every step he take  is that will lead to the subtree related to this alpha finally , u r such amazing i really enjoyed every lesson i took from this channel",True
@stedev2256,2022-07-19T11:35:42Z,0,"Hello, first of all thanks for the great material you produced and shared, certainly among the clearest and effective I've come across.  My questions are about the cross-validation trees to determine the right alpha values.  As a premise, if I understood correctly, we first determine candidate alpha values by : a) create a ""full"" tree from the full training+testing datasets b) produce the corresponding family of ""pruned"" versions (and I guess asses their SSRs in preparation for the next step) based on the morphology of the ""full"" tree (meaning, all possible pruned trees are considered - is that correct?) c) identify the candidate alpha values as those by which the ""full"" tree's score becomes higher than one of the pruned versions.  Assuming the above is correct, when we move on to cross-validate in order to ultimately determine the right alpha, I understand that we resample a training set (and a corresponding test set) for a number of times. Each time, we build a new tree from the training set, and its associated set of pruned versions (let me call these tress a ""cross-validation family of trees"" (CVFTs)), and assess their SSRs based on the test set for the current round in order to contribute to ultimately calculate the actual alpha to use.  First question: how come every CVFTs in your slides has a number of members that equals the number of candidate values for alpha? couldn't  a resampled training set might give rise to trees with more or even fewer leaves - and corresponding pruned versions - than the tree that was used to identify the candidate alpha values? And in that case,  the candidate alpha values might be in larger or smaller number than the possible number of trees in the CVFTs at hand. I imagine that a possible answer is that the number of members in a CVFTs can actually be different than the number of candidate alphas, and that the pruned tress in a CVFTs are actually identified through their Tree Scores when each of the alpha candidate values is applied -- and if so I guess the issue is that perhaps this mechanism does not stand out 100% from the presentation...   Second question: if we assess the trees in each CVFTs only by their SSRs, wouldn't always the tree with more leaves (therefore alpha=0) win?  Thanks much",True
@alexpowell-perry2233,2022-06-21T14:45:59Z,0,"HI Josh, do you have a copy of the dataset used in this example?",True
@sdsachin24,2022-06-12T18:04:33Z,0,"@StatQuest with Josh Starmer, I have purchased your book but I didn't find these concepts (pruning, random forest, adaboost, gradient boosting) in that. Is there a way to access these presentation slides?",True
@kerimbasbug,2022-06-10T19:06:07Z,1,Perfect!,True
@corrinechou5271,2022-04-16T16:57:04Z,1,"Well explained, thank you so much",True
@melisakamaci5248,2022-04-13T11:31:17Z,1,"The song cracked me up, perfect reference",True
@juanete69,2022-04-01T17:13:39Z,0,"Why do you need to create the tree with the full dataset (training+test, at 11:25) ? Is it not enough with the trees created with the training data during the k-fold?",True
@cuonghoba681,2022-02-19T16:14:41Z,1,Very clear. Thank you very much,True
@arashvahabpour45,2022-02-18T18:47:26Z,1,"I like his calculation sound, biboo-biboo-boobi :D",True
@rkotcher,2022-02-13T18:05:43Z,1,"Great video! Trying to implement now, from scratch, and I have a few questions: at 12:12 ‚Äúnow we will increase alpha until pruning leaves gives us a lower tree score‚Äù. So, do we precompute all of the trees first? (I have L trees if I start with L leaves?), and then continue to increase k by some increment until the tree in question has the lowest score? If I‚Äôm wrong, maybe somebody could clarify? Thanks so much, and thanks for the video!!",True
@graceqin5024,2022-01-28T02:43:31Z,0,"Hi Josh, It is my understanding that we 1) build a full tree will full data; calculate SSR for the full tree and all tiers of pruned trees as 7:19 shows; 2) define different alpha gates and the correspondent pruned trees as shown 12:53; 3) Use 10-fold cross validation to decide alpha and the tree leaves (T) comes with alpha.   I would like to confirm if I understand it correctly. Thank you and also thank you for wonder videos!",True
@thepresistence5935,2022-01-23T06:29:04Z,1,"Teachers all over the world, must learn from josh bro!",True
@mohammadidreesbhatresearch6505,2022-01-17T04:39:53Z,0,At the end this video gets hard to understand . I mean the presentation is not understandable....If we have to use ultimately 10 Fold cv. Than for what previous steps used for,True
@andersk,2022-01-12T17:40:23Z,1,"At 12:20, let's say the right-hand side of the tree had node instead of just a leaf, and that node led to two leaves. In this situation, what would be pruned first to created the pruned comparison tree: prune the two leaves at the bottom of the left side only first because it's deeper? or the two leaves at the bottom of the right only first? or prune all ends with two leaves at the same time?",True
@ahmedrejeb8575,2021-12-11T10:48:05Z,1,this guy is a living legend ‚ù§,True
@r_793,2021-11-29T21:18:31Z,0,"Regarding choosing Œ± (starting at 11:19) when we fit a new regression to the FULL data, does this not cause us to 'overfit' Œ± to some extent? I was wondering what would happen if we did the following:  i) Split the data k different ways into a set that we find Œ± for and a set that we ignore. ii) On the set that we find alpha for, we get [Œ±11, Œ±12, Œ±13] (the first set of Œ±'s such that we get better tree scores cutting the tree by 1, 2, and 3 levels respectively.) up to [Œ±k1, Œ±k2, Œ±k3]. iii) We then take the average Œ± for each cut so [(Œ±11+Œ±21+...+Œ±k1) / k, (Œ±12+Œ±22+...+Œ±k2) / k, (Œ±13+Œ±23+...+Œ±k3) / k] as our set of final Œ±'s.  iv) Perform K-Fold Cross Validation using the above to see what Œ± gives the lowest SSR for it's optimal tree.   Would my method make little to no difference? Or is my method overfitting more in some sense? Let me know what you think!",True
@ariellavanialynn8922,2021-11-06T00:47:05Z,0,"Sorry, I still have no idea, when we want to replace 52.8% and 100% leaves with 73.8% leaf, does it from taking the average of data that included in dosage <= 29?",True
@diegodiaz3341,2021-10-22T01:39:59Z,1,"LMAO at the awesome song, and, as always, thank you for the video, I must say though, I do not dread terminology alert, quite the opposite, I actually get pumped when they show up, cuz Im about to learn something I can use for SWAG later.",True
@yasserothman4023,2021-10-19T11:30:23Z,0,@3:35 when you remove the leaves how do you change the decision rule of the parent node ?  @11:55 when you build the tree from the full data set how do you get the SSR ? i mean what is the input to get it ?,True
@ishikajohari1508,2021-10-11T08:43:10Z,1,You got me at Smelly Stat!,True
@ltoco4415,2021-10-03T13:28:52Z,4,"Thank you for the amazing content Josh!   I had a doubt, do we change the alpha and the number of nodes simultaneously cause I thought that we change just alpha and then check which tree performs better (keeping one thing constant and changing the other gives us a better way to compare). Also at 11:32, you used the entire data (train+test) to build the model and find different values of alpha and then used those values on train split to build the model, compare test scores and finally find the best value of alpha. If we use the train+test data to find alpha, isn't that causing data leakage?  Also, why do we compute the alpha values beforehand? why don't we do it the usual way(the way we do in ridge regression) where we find the optimal value of regularization parameter using GridSearchCV because anyhow alpha is analogous to regularization parameter (I think).",True
@haosmark,2021-09-09T17:31:10Z,0,"The last section about alpha is a bit confusing, and I couldn't reproduce it. With these same RSS values of 543.8, 5494.8, 19243.7, 28897.2, and their respective node counts of 4, 3, 2, 1; setting the alpha to 15000 means we get a descending set of tree scores, so why did we increase alpha to 22000? These are the scores that you get with alpha=15000: (4) 60543.8, (3) 50494.8, (2) 49243.7, (1) 43897.20.  Other than that, really useful video. Thank you!",True
@LQNam,2021-08-29T11:56:14Z,0,"Thanks, Josh Starmer. The way of using train + test data to find a list of alpha, then use K-fold CV on train data to find out the optimal alpha leads to the data leakage.",True
@mdsultanahemad,2021-08-26T18:59:49Z,1,"so the intro is smelly cat smelly cat what are they feeding you, great <3",True
@hemlatasharma5288,2021-08-20T10:39:08Z,1,"Hello Josh, Thanks for this amazing video, I am implementing cost complexity pruning on the basis of this video. Although I have one question: How do you build a decision tree using a particular value of alpha using the training data (during cross fold validation)??  How does alpha help?   I am working on classification decision tree, here's what I do: 1. Use all data to build full tree, get all subtrees and for every subtree get a value of alpha.  Missclassification error of one subtree = sum of gini impurities of all leaf nodes  2. Divide data into 10 folds, for each fold: - build decision using each value of alpha and training set. How? What role does alpha play here? I can grow a tree and then get subtrees without alpha - calculate test error (1-accuracy) for each subtree.  - select subtree, represented by alpha having the lowest test error.  3. selected alpha = avg alpha across all folds   4. Pruned tree = tree that has the alpha = selected alpha  I apologise if this is a stupid question :)",True
@Aman-uk6fw,2021-07-09T16:01:17Z,1,I visited this video just 2 months before but I didn't notice that ke you are f.r.i.e.n.d.s fan or may be phobe's fan üòÅ,True
@shashanksundi5669,2021-06-28T16:07:50Z,1,Thank You !! Just perfect :),True
@karannchew2534,2021-06-28T05:54:37Z,1,"Hi Josh,  I don't get this part: 12:10 ""increase alpha until pruning leave give a lower tree score""  What does ""lower tree score mean""? Lower than the the score from the previous round?  The first (full) tree use alpha of zero. The second tree, pruned, will have a higher SSE, hence higher Tree Score. A non-zero alpha will only increase the Tree Score further.  So, how is it getting a ""lower tree score""?  Thanks, Kar Ann  ---  Notes for my own future reference:   *WHY PRUNING* To avoid over fitting, a less full tree is needed. Pruning procedure uses tree score that balance between classification error and number of leaves.    *OVERALL STEPS* 1. Create a set of Prune Trees.  Use Tree Score to decide how much to prune. The trees should have comparable ""quality"" in terms of error and number of node.   Tree Score = SSR + Œ±T 2. Then, apply different subset of data (cross-validationally) to all Prune Trees.The Pruned Tree that give lowest SSR (with testing data set) is the winner.    *DETAILED STEPS*  Tree Score, TS = SSR + alpha* T   (T=no. of leaves)   TS is used to decide how much to prune, and to create different trees.  *1. Create a set of Prune Trees, which have comparable quality, in terms of error and number of leaves*  Create the 1st trial tree.  The fullest tree.  Tree1.   Œ±1=0.  Get TS1.    Create the 2nd trial tree.  A new tree.  A pruned version of Tree1.  Tree2.  Œ±2=a new value bigger than 0.     Compare the Tree Score between the old (full) and new trees (of smaller size i.e. pruned).      Even though the newer (pruned) tree has a bigger SSR, it has a lower T.      At Œ±2 = 0, the newer tree has a higher TS than the old tree.      But the new tree TS increases slower than the old TS, because the old TS has a higher T.    Eventually, with a certain Œ±2 value, the newer tree has a higher TS. Get TS2, using the new Œ±2.  Create the 3rd tree.  A pruned version of Tree2.   Get TS3, using the new Œ±3.  Repeat and create more prune trees.  Each with a Œ± values.  Now, there are many set of tree. First tree is a fill tree.  Last tree is only one leaf. All tree comparable quality, based on fine balance between classification errors and number of leaves.   *2. Apply different set of data, cross-validationally, to all trees.  The Prune Tree that give lowest average SSR  is the winner*  .Create different values of alpha from the above eg. Œ±1 - Œ±5 .Use different set of training and test data i.e. 10 folds cross validation .From the 10 folds training and testing data, get SSR, with Œ±1 - Œ±4 .Get the SSR of each Œ±, averaged over 10 sets of training and testing data .Pick the tree, and its associated Œ±, that gives the lowest SSR   The fullest tree, though gives the lowest SSR with the training data, it doesn't give the lowest SSR with test data due to over fitting.  Cross validation also helps to minimise impact of over fitting.",True
@gilyardeni9125,2021-06-26T10:33:38Z,0,is rss and ssr the same?,True
@Rohith.,2021-06-20T17:17:21Z,0,"at 14:19 I think it should be lowest tree score, instead of lowest sum of squared residuals. Correct me if I am wrong.",True
@ayushmishra9415,2021-06-16T19:42:18Z,0,prof kindly accept my linkedin request,True
@rappa753,2021-06-14T12:10:45Z,0,Thanks for the great video. One question though: Why is the full-sized tree build from all data (see 11:25) and not just the testing data? Couldn't this potentially give problems w.r.t. leakage?,True
@flaviofreire9323,2021-06-14T01:29:58Z,1,Love the song in the beginning!!,True
@AstroT2340,2021-06-11T09:52:45Z,0,How do you decide how many folds to use in cross-validation?,True
@bibiworm,2021-06-10T23:59:16Z,0,"14:11, in cross-validation, we are calculating ssr using testing data, not tree score????",True
@bibiworm,2021-06-10T23:35:20Z,0,Why do you choose alpha levels using all the data before cross-validation? Does it lead to data leakage? I can't seem to wrap my head around it. thanks.,True
@bibiworm,2021-06-10T23:03:20Z,2,"At 12:33, it reads on the video that ""we increase alpha again until pruning leaves will give us a lower tree score"". My question is lower than what? My understanding is lower than tree score of the full sized tree at that specific alpha value, in this example, at alpha=10000, because we have already established the tree score of the full sized tree at alpha = 0 has the smallest tree score. Similarly at 12:43, the third tree at alpha = 15,000 is chosen because it has lower tree score than the second tree at alpha = 15000. Please let me know if this is correct. Thanks.",True
@bibiworm,2021-06-10T22:56:25Z,0,"A quick question please. In this example, the tree is not balanced, in the sense that right subtree is a lot deeper. What if the left subtree is as deep as the right subtree, then how do we choose which side of the internal node to collapse, or in other words, which side of the leaf nodes to delete? Based on what is shown at 12:33, we should go with whichever that gives us lower tree score, right?",True
@arda8206,2021-06-10T09:32:26Z,2,"One good idea for cross-validation maybe is we can split data first to train and test and again split train to train and validation sets. Therefore, we can guarantee that our test set is totally new to environment. This will result in more realistic scores.",True
@dengzhonghan5125,2021-05-31T02:28:28Z,1,"Thanks, Josh, every time I watch your video, I feel like the concept is very easy to understand! lol",True
@preranadas4037,2021-05-29T06:01:14Z,1,Best video on pruning and tree selection till date!!!!!,True
@allistermi6014,2021-05-27T22:47:05Z,0,what is tuning parameter?,True
@rashigupta1813,2021-05-26T16:10:30Z,1,"BEST CHANNEL! no i m not just saying, i m shouting!",True
@wabsy845,2021-05-23T21:34:54Z,3,It is such a great explanation. Super helpful! Clear and fun! Really appreciate your time in making the video. Thank you!,True
@Aman-uk6fw,2021-05-07T07:28:18Z,1,"how to decide the alpha value ? from which value should i start the testing of tree score as in this video you took alpha=0,10000,15000,22000 if i have a different data set then how should i take that alpha value?",True
@assafv1,2021-05-04T10:21:13Z,0,Question. Is it always the case that if for some value of alpha say alpha_0 a tree trained on all the training data has some number of terminal nodes say N. Then for the same value of alpha a tree trained on only some of the training data (some of the folds from cross validation) will also have the same number of nodes N?,True
@statquest,2021-05-03T13:49:38Z,74,"NOTE: To apply this method to a classification, replace SSR with Gini Impurity (or Information Gain or Entropy or whatever metric you are using).  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@doyelmukherjee2769,2021-05-03T13:29:43Z,0,How do we do pruning for classification  problem?,True
@gundamdhinesh5379,2021-04-23T05:08:41Z,1,Really love your content. Must watch content for any learner,True
@saurabhkumarapp06,2021-04-13T19:33:15Z,0,"It's actually: Smelly stat, smel-ly stat.",True
@eramitjangra4660,2021-03-21T17:03:00Z,1,I searched lot of thing for my project on ML to start from scratch. Then i landed here  You nailed it. üî•üî•üôè Now i am on edge of completing my project  thank a lot,True
@karannchew2534,2021-03-17T09:40:14Z,0,"(Making notes for my own future reference)  Tree Score, SSR + Œ±T, is used to create a set of Prune Trees.  Then, apply data (cross-validationally), to all Prune Trees.  The Prune Tree (and its Œ± value) that give lowest SSR (with testing data set) is the winner.",True
@tymothylim6550,2021-03-14T09:08:42Z,6,Thank you very much for this video! I really enjoyed the full step-by-step process of building the various trees using different alpha values and the use of cross validation to select the best alpha!,True
@TheBustra,2021-03-12T17:46:39Z,0,"Hello, great video! What it is not fully clear to me is if the largest tree we build in each fold must have the same size of the tree fitted to the whole set of data, i.e. the tree that generated the subtrees from which the sequence of penalty parameters is derived. Thanks",True
@anzhemeng8833,2021-03-10T16:31:53Z,0,How to prune a tree before it fully grows up (or down)?,True
@sharmaaj,2021-03-09T23:50:38Z,0,can i try the CV just on SSE rather than the Tree score? i assume the essence is to build models on training data set and run it on validation data set to select the one with lower SSE. Cross validation does it even better.  But why need Tree score when i can just do it on SSE only?  thanks,True
@junbinlin6764,2021-02-24T12:59:02Z,0,Can I apply this kind of pruning to classification tree?,True
@cherisykonstanz2807,2021-02-24T09:19:35Z,0,Was there ever a quadruple BAAAM?,True
@definitelynosebreather,2021-02-21T03:30:59Z,1,"Damn. I was finding only scientific articles and I was having trouble understanding the CCP, now you made it very clear! Thanks.",True
@apurvagupta6217,2021-02-11T10:04:10Z,2,The way you explain..and the amount of effort you put in the videos is great. I have learnt a lot from you sir. I always feel so positive and motivated while learning from you. Thank a lotüéà,True
@jiayiwu4101,2021-02-01T14:26:20Z,0,"Would we use same trees we build in Step 1 to calculate tree score in Step 2? Or when we use CV, we will build K new sequence of trees with K training data? Thank you!",True
@bibiworm,2021-01-11T17:45:21Z,0,Classification trees are pruned the same way? Thanks.,True
@Theviswanath57,2020-12-21T11:31:40Z,0,"In ""Use cross validation to compare alphas"" section I think it's better to compute cross validation metrics for each alpha and then decides best alpha basis cross validation metrics;",True
@bharathis9295,2020-11-28T11:00:27Z,2,Phobe buffay,True
@grandthruadversity,2020-11-26T08:40:21Z,0,"Such great explanation! Way better than those fkn dumb ass university professors who cant explain for shit. Except every time you say ""bam"" it sounds awkward af! homie please!",True
@janschmidt1218,2020-11-12T11:37:42Z,2,"Great video, thanks a lot! I just have one question to the cross-validation procedure: When creating the original sequence of subtrees from the whole big dataset we get certain discrete alpha values for each subtree (in the video it is 10 000, 15 000 and 22 000). You said we obtain these values as soon as pruning leaves gives us a lower tree score. So the pruning is done first and then the alpha is increased till the tree score is lower than for the unpruned tree. Are we then using these discrete alpha values in the cross-validation procedure? Because then there must be something like a guarantee that these discrete alpha values also lead to a pruning in the training subset we using in the current step of cross-validation, or not?",True
@bobbyfischer1672,2020-10-29T03:07:49Z,3,"At 12:05 you said that full tree has lowest tree score when alpha = 0. By default, pruned trees have higher SSR. Won't increasing alpha increase their tree scores even further, instead of lowering them?",True
@marisagonzalez679,2020-10-25T19:05:18Z,0,YOU DID NOT INCLUDE SMELLY CAT!!! hahahaha how cool!!!!!,True
@ankurmazumder5590,2020-10-15T12:45:00Z,0,"when varying alpha and checking for which alpha pruned tree performes better, how to find which pair of leaves to prune considering more than one pair of pruned tree?",True
@ruxiz2007,2020-10-14T03:15:31Z,1,Double BAMM! beebeeboobeeboo! üòÇ,True
@leozhang4450,2020-10-06T19:29:13Z,2,"Hi Josh, Thanks for this GREAT video!!! Just wanted to ask what's the principal to choose several alpha as the starting values? ie. in the video, you choose these 3 alpha values as the candidates to determine the final alpha: 10,000, 15,000, 22,000, how did you come up with these three alpha?",True
@josephmhango1502,2020-09-16T22:04:41Z,2,really josh? smelly stat? lol... made my day,True
@zhengwu7240,2020-09-10T07:36:46Z,5,"Hey Josh, Great video ! Quick questions: for classification tree, do we simply replace SSR  with gini impurity and follow similar steps?",True
@donkkey245,2020-08-24T01:16:06Z,1,encore„ÄÇ„ÄÇ„ÄÇ„ÄÇencore„ÄÇ„ÄÇ„ÄÇ„ÄÇ,True
@piotrmazgaj,2020-08-22T11:17:20Z,0,"the scikit learn reference, if someone was interested: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html",True
@paglahawa1,2020-08-11T08:07:39Z,0,"Thanks a lot for the video. However i have a small query. The original big tree gives me the best tree score when alpha is 0. Now when i prune the tree in the next step by deleting a leaf ( keeping alpha = 0 ), it gives me worse tree score than the before. And then adding alpha*T gives me more worse value of tree score. But you mentioned that it gives me better tree score. Am a missing something ? Thanks in advance.",True
@minhto4186,2020-08-06T10:56:05Z,0,please help me ! how do i use cost complexity pruning for classification tree ?,True
@mikestev8539,2020-07-21T18:49:12Z,0,"Good explanation in general, especially that this topic is difficult. But can you suggest where I could learn more about making post-pruning decision trees in R.",True
@lokeshvarshney3921,2020-07-21T10:35:05Z,0,"How did you initially choose alpha as 0, 10,000, 15000, 22000. that's a big assumption",True
@munawersheikh00,2020-07-16T03:50:12Z,5,"I love the way you say ""BAMM""!!! Gives great relief during the video :) I want to say your style of teaching is great. The way you are explaining is making very easy for us to understand. In my opinion I can say ""A difficult subject with easy to understand using your video lectures!"" Thank you very much.",True
@visheshkushwaha9446,2020-07-06T13:42:29Z,1,Explanation is TRIPLE BAM!!!,True
@amirakhlaghi8143,2020-07-05T15:52:22Z,0,"But, how do we compute different sub-tress correspond to each alpha value?",True
@AdityaSingh-lf7oe,2020-07-04T08:48:59Z,0,"So if I am correct give me a BAM.. otherwise OH NO!  To find alpha, first we use all of the training data and build a tree  We come up with a tree with 4 leaf nodes... Until SSR(of tree with 4 leaf nodes) +alpha(4) < SSR(tree with 3) + alpha(3), we increase alpha... We come up with alpha 10,000.. And then next alphas 15,000 and 22,000 Now we create new training and testing data FROM THE COMPLETE TRAINING DATASET... and create a new tree which will be different from tree created before... (Note: This tree will be different from above tree, so alphas found using same above process will be different and not 10k, 15k and 22k.. Yet we don't calculate those new alphas.. instead alpha of tree with one pruned node is assigned 10k and alpha of tree with two pruned nodes is assigned 15k)  Now in this new tree that we created, we prune a node and calculate score using TRAINING DATA which we created for THIS tree = SSR +10kT... And similarly find tree scores...  Now keeping note of above scores,  We create new traing and testing data from the training set and repeat ... If say, tree with one pruned node gives lowest scores on avg, we choose alpha corresponding to tree with one pruned node, i.e 10,000",True
@varaddingankar3794,2020-06-12T19:19:21Z,1,@Josh Starmer Will you please consider creating a quest for implementing the above explanation in R? Will be really helpful !!! P.S Great Quest :),True
@SuperNexus14,2020-06-05T15:59:41Z,3,"Thank you for the video, I just have one question. We use pruning to figure out the optimal number of leaves for the tree we build using the original training data. The purpose of the original validation/ testing data was to validate the performance of this tree, as we did not use that data in the creation of said tree. Now in the pruning process, the original testing/ validation data is used in conjunction with the original training data to determine alpha. This means, that the original validation data had a large impact on the tree we created from the orinal training data and is therefore no longer independant from it. How is this procedure justified and would it be necessary to get a third independant data set for a true validation of the trees predictive performance?",True
@zwarasevski123,2020-05-26T11:01:15Z,1,BAAAM,True
@michael_d2,2020-05-26T10:26:25Z,0,"Is it better to use a validation dataset for alpha value tuning? If we select alpha based on the test set, may it cause overfitting?",True
@amarakbar2374,2020-05-21T12:39:30Z,1,Wonderful explanation. Thank you very much,True
@triplefruition,2020-05-19T11:15:20Z,7,"Oh my Buddha! I'm falling in love with funny of your voice when you're explaining. Before I met your channel, my head is spinning round and round. I don't know what to do with my learning, but you came in and took me by big surprise. You made the abstract concept to be simple! Thank you!!!!! :)))))))",True
@NIHIT555,2020-05-18T09:09:01Z,89,Josh: you have truly cracked  how to use technology(slides/basic animation) to change the way we are teaching for decades. I wish all universities take a note from you and revise the way they are teaching,True
@xinranwen4849,2020-05-15T09:06:09Z,2,"This video is awesome! Why didn't I know StatQuest earlier, it really helps, THX!!! btw: I'm confused about the built of trees in 13:19, how could we know that it is the bottom 2 leaves should be cut for a new training set when Œ±=10000(pre-calculated), is that just a coincidence? Or the cut depends on which leaves give the lowest Tree Score?",True
@mainakray6452,2020-05-14T13:25:13Z,0,"another gr8 experience....but can u plz explain how we can choose alpha in more detail.....when alpha=0 ,its gives full tree.....but after that how can we choose 10000....plz explain.....",True
@miriza2,2020-05-13T02:17:42Z,0,"Are the trees at 11:25 (based on all the data), 13:36 (based on 1st set of training data), and at 14:40 (based on 2nd set of training data), identical on the dosage threshold values in the blue boxes or not? That is unclear to me...",True
@pushkarparanjpe187,2020-05-12T06:22:06Z,1,So good! Consistently high quality across across videos and time! Keep going. Many thanks!,True
@aggelosdidachos3073,2020-05-11T11:19:02Z,0,"I'm a bit confounded. Isn't it the case that the more the nodes are, the smaller the SSR (as in the example in 9:30)**? If so, how is it possible that a pruned tree with an alpha of 10.000 (**14:22**) has a smaller SSR than a full tree with an alpha of 0? Or, could my second sentence** possibly be true only in terms of constructing the regression tree (based on the training data) and not necessarily when using it to the test data? I hope I'm formulating the question clear enough :P",True
@a_sun5941,2020-04-25T06:24:38Z,0,"if the full tree size is really big, lets say 1000 leafs, and the tree is not balanced, do we need to exhaustively list out all the possible way to prune this tree? how many leaves each time we prune? 2 leaves like in the video or it depends?",True
@a_sun5941,2020-04-25T06:16:57Z,0,"what should be the range of alpha to choose from during cross validation, 5000 to 20,000 ? 5000 each increment ?  or it depends on the SSR value for the full tree we customize alpha range according to the SSR value of full tree?",True
@mujeebrahman5282,2020-04-24T10:23:22Z,3,The good thing about his videos is you just have to watch any video once and the concept will not leave you for a long time.,True
@mcapro,2020-04-19T11:42:39Z,1,You are the best.,True
@thanhtungnguyen7500,2020-04-16T14:41:56Z,1,thank you Josh for your very easy understanding explanation & lovely rhythm,True
@hemantdas9546,2020-04-16T05:11:09Z,0,Hi Josh ! Can we use this in classification decision tree?,True
@enlighteninginformation7647,2020-04-14T21:45:00Z,3,This video really helped me to clearly understand the concept. Thank you for this good work,True
@chirags9774,2020-04-14T01:28:55Z,2,Best intro of all the statquest videos which I have seen üòç,True
@surbhijain2123,2020-04-07T14:56:57Z,4,Is there any formula to calculate alpha for cost complexity pruning?,True
@shrikantdeshmukh7951,2020-03-26T14:36:42Z,0,how to decide min number of sample for node/ leaf and other parameters available  in sklearn,True
@sushilchauhan2586,2020-03-25T13:34:41Z,1,black kitty white kitty     STATQUEST is best!,True
@alecvan7143,2020-03-24T13:22:34Z,10,"Re watching after practising I can even further appreciate the quality of your explanations, thanks Josh :)",True
@rajarajeshwaripremkumar3078,2020-03-19T20:27:23Z,0,"Thank you for such a good explanation. But I am really confused as to how it is different from doing a CV at every split, if for every split we just keep adjusting the MSE of the K-fold cv, we will ultimately refrain from splitting tree with 3 leaves to a tree with 4 leaves.",True
@divyagupta432,2020-03-10T10:03:52Z,0,Please publish a session on reduced error pruning also.,True
@theblindcritic5876,2020-02-28T16:13:33Z,1,This is brilliant work! Thanks a ton!,True
@charliehenith,2020-02-27T13:29:05Z,0,Hey love the videos however Im still bit unsure about how you work out the alpha value?,True
@longma7042,2020-02-24T08:55:33Z,1,"greate , thank you for your work . very clear",True
@sammydolgin,2020-02-22T19:29:53Z,0,"Is SSR the same as what I commonly hear to be ""RSS"" -- residual sum of squares? Or are these 2 different things?",True
@dongli7157,2020-02-11T15:38:54Z,2,"Hi Josh, those explanatory vidoes are incredible. Thanks for the great work!",True
@hussainkamran3789,2020-02-11T13:25:19Z,0,I didn't get the idea that why increasing alpha for a particular tree/sub tree will decrease its SSR???,True
@fabianoprado4066,2020-02-04T16:00:15Z,0,Hi Josh!! Can you guys say some R package's functions that use this pruning method ?? Thanks a lot!!,True
@itisakash,2020-01-30T00:52:26Z,3,"Hello, This is a clearly explained video and thanks for this entire series. Can you please let us know how to implement the pruning using sklearn in python?",True
@bharatbajoria,2020-01-22T17:35:56Z,1,Can you please make another video on alpha ?,True
@carabidus,2020-01-21T02:11:49Z,0,"Josh, the videos on this channel are nothing short of superb. I have only one suggestion: how about a dark theme for these presentations? That white background is like a supernova, especially on my 55"" TV.",True
@ribeli_boni3091,2020-01-09T08:27:41Z,0,"Little bit confusing with the full data and the training data. Because in a previous step, we applied recursive binary splitting in order to find a tree for a training data. Thus, we aplly cost complexity pruning to prevent overfitting. Therefore, the training data we used in the previous step becomes our full data where we again do k-folds cross validation with training sets withing the training set. Is that correct?",True
@user-xn7qt3zl4m,2020-01-08T05:14:13Z,2,The Best! The Best! The Best! video I've ever seen about Tree Pruning.  Thanks a lot. Now I got the concepts. BAMM!,True
@toja240,2020-01-07T20:17:01Z,0,"Hey, I have a question: if I have leaves in different branches on the same level, should I prune all leaves on this level or just in one branch I'm currently looking at?",True
@Bennilenny,2020-01-03T09:52:56Z,13,Your intros make me smile :),True
@dfragoulis,2020-01-02T18:56:34Z,1,"Potentially stupid question: Will we still need an additional (outside of the ""full data"") testing set to test the final (pruned) tree?",True
@Monia77777,2020-01-01T17:01:35Z,2,Friends reference - cherry on top! :),True
@Rectalium,2019-12-31T10:12:40Z,2,Your channel is amazing man. Great job,True
@monishaathikesavanpremalat7587,2019-12-17T13:20:06Z,0,Can you plz explain how to prune classification tree @Josh Starmer,True
@dhananjaysawai5087,2019-12-14T04:39:23Z,1,Best  Explanation  Dam !!!,True
@skumarr53,2019-12-11T12:59:21Z,1,"Just a random thought, what if we prune the tree directly based on ssr computed on validation set instead of adding penalty. Anyway the tree that works well on validation set is selected. Why are adding penalty?. Does it helps controlling fluctuations in the number of leafs selected across cross validation folds during Hypertuning .",True
@egorepishin1134,2019-12-04T20:00:12Z,0,But where is much anticipated xgboost???,True
@abhishekkaranath2119,2019-11-28T13:00:26Z,2,Subbed only for the intros,True
@aop2182,2019-11-27T04:37:02Z,11,This is awesome. I remember I was a bit confused when I was reading tree based methods in An Introduction to Statistical Learning. This really helps me understand it much easier when I can visualize it other than read some formulas.  Thank you!,True
@andyjiang8988,2019-11-26T20:45:36Z,192,The channel with the lowest Gini score of likes vs. dislikes,True
@rrrprogram4704,2019-11-26T17:48:22Z,1,we love you DOSS.. hope me too will surely one day be a patreon member,True
@mathematicalninja2756,2019-11-26T17:29:55Z,3,This channel is gold mine i am telling ya :D   can you cover box cox theorem (power transformations),True
@InsightsWithAkshay,2019-11-26T04:06:20Z,2,"In the video, you selected 4 values of alpha i.e. 0, 10k, 15k and 22k. How did you come up with exact alpha values? In the real-world, the dimensions of data will be huge so how to decide value alpha?",True
@mohamedhanifansari9224,2019-11-26T02:33:33Z,1,"Thank Josh, for your lucid explanation. We are longing for the XGBoost Videos. Any updates on that ?",True
@yulinliu850,2019-11-25T23:50:40Z,1,Thanks Josh!,True
@charlottel9534,2019-11-25T22:46:56Z,1,Thank you so much for posting this!,True
@jcatlantis,2019-11-25T20:10:49Z,0,"What about order of removing leaves? If we have a huge tree, do we need to generate all possible combinations of subtrees and alpha values?",True
@jcatlantis,2019-11-25T19:14:38Z,2,How new trees are build by imposing previous alpha values? Maybe it is not possible to find new continuously smaller trees of reduced Tree Scores for fixed alphas. Before alpha was a parameter and during cross validation is a constraint :(,True
@jerzysomkowski827,2019-11-25T17:09:25Z,1,I thought it would be  XGBoost video :(,True
@hellochii1675,2019-11-25T16:24:13Z,2,As always the clearest explanation! Thank you so much üòä and BAM BAM BAM,True
@hellochii1675,2019-11-25T16:13:34Z,2,Happy early Thanksgiving ü¶É üí• üí• üí•,True
@dhananjaykansal8097,2019-11-25T13:55:22Z,50,Ahhh Phobeee from Friends aka Smelly Cat. Haha good one Josh.,True
@rrrprogram8667,2019-11-25T11:50:15Z,30,There are some notifications... Right when it shows on ur phone... ur feelings says.. I going to learn something today...  MEGAAA BAMMMMM,True
@EngRiadAlmadani,2019-11-25T11:19:55Z,1,Hero,True
@BalistikJumbo,2019-11-25T11:18:35Z,1,As always awesome! Thank you Josh!!! Horraaaayyyy,True
