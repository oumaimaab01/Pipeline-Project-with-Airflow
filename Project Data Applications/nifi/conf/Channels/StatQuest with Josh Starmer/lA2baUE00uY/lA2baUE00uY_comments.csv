author,updated_at,like_count,text,public
@statquest,2024-02-19T15:37:17Z,2,To learn more about Lightning: https://lightning.ai/?utm_medium=social&utm_source=youtube&utm_campaign=podcast Or just get started with a cool Lightning Studio tutorial: https://lightning.ai/lightning-ai/studios/the-easiest-way-to-use-gpus-in-the-cloud?utm_medium=social&utm_source=youtube&utm_campaign=podcast  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@faridsaud6567,2024-03-12T23:34:06Z,1,Nice story :D,True
@abuboimofo6605,2024-03-11T19:35:24Z,4,"""People are more obsessed with the methodology/outcome rather than the impact it makes"" -Bam josh!",True
@Hitheigy,2024-03-10T02:12:01Z,1,"hi, i love your video so much,!they explain neural networks clearly, they help me a lot in my study. so can i translate your video to chinese and publish it on chinese site. is there any way i can contact with you so we can discuss this. thank you and your videos, they did help me in understanding !",True
@mukeshkumar-tr4bu,2024-03-09T09:18:24Z,1,Awesome content...keep it up... we need more podcasts like this.,True
@kartikchaturvedi7868,2024-03-09T06:48:07Z,0,Superrrb Awesome Fantastic video,True
@ananthakrishnank3208,2024-03-07T11:55:36Z,0,"14:40 ""Metric makes us visualize data in a linear scale. But those people do not lie on a linear scale"". I don't get the point.",True
@user-qm4eb5ro4y,2024-03-07T08:07:16Z,0,"I have recently started refreshing my Machine Learning skills and I have been following your StatQuest videos on YouTube. I also purchased your book. I found both the videos and the book extremely helpful and easy to understand. Thank you so much for providing these invaluable resources. You have made Machine Learning fun.  I have a question regarding Backpropagation for Neural Networks when we use the sigmoid activation function. I have trouble calculating the derivative of the cross-entropy loss function with respect to W1. The derivative I get following your video on the chain rule is dE/dW = -(y - sigma(W1x1 + W2X2 +b))x, but I have also seen people doing it as -(y - y_out) *sigma(W1x1 + W2X2 +b) * (1 -sigma(W1x1 + W2X2 +b)) * x. I'm confused as to which one is correct.  I know you are busy, so my apologies for taking your time. Any help from you will be greatly appreciated.  Thank you so much in advance.",True
@sabaokangan,2024-03-04T19:56:57Z,2,Thank you so much for sharing this with us on YouTube ‚ù§Ô∏è‚Äçüî• from SeoulNatU,True
@Conqueror25,2024-03-04T17:56:24Z,0,"Indians have been summoned. Maharashtrians, even more so üö©",True
@PISATA4362,2024-03-04T17:40:23Z,5,"Very nice Achal, kee it up",True
@anilawasthi7963,2024-03-04T17:08:44Z,3,Very nice Achal .keep it up.,True
@tonywillingham8109,2024-03-04T14:17:17Z,0,what the heck?,True
@vigneshvicky6720,2024-03-04T14:06:48Z,1,We want yolov8,True
@jks234,2024-03-04T12:35:35Z,2,Lol.  16:30 The trademark lines come out quite naturally.  Turns out ‚Äúbam‚Äù and quest aren‚Äôt Josh‚Äôs lines. They are statistical terms. :p,True
@hktk3626,2024-03-04T10:14:00Z,1,üéâüéâüéâüéâüéâ,True
@user-bg7ue8uy5t,2024-03-04T10:12:45Z,1,"I've been diving deep into the world of self-attention from scratch, but I've hit a bit of a rough patch, particularly with the tricky aspects of the loss function and weight updates.  I've been a big fan of your educational content for a while now, and I was wondering if you'd be up for creating a video tutorial that delves into these specific challenges. Your knack for breaking down complex topics in a clear and engaging way could really help me and others who are struggling with similar issues.  I think collaborating on this could not only help us tackle these hurdles but also provide valuable insights to your audience. Plus, I'm excited about the opportunity to learn from your expertise firsthand. The I write: import  numpy as np  x=np.array([[2,1,5]]) x1=np.array([[2,1,5]]) y=np.array([   [11.2,4.8,8]   ]) test=np.array([[1,0,0],[0.2,0.8,0],[0.1,0.2,0.7]]) def mask(data,tellar=False):   row,columen=data.shape   trangle=np.tri(row,columen)   if tellar:     trangle[trangle==0]=0     trangle[trangle==1]=1     return trangle*data   else:     trangle[trangle==0]=-np.inf      trangle[trangle==1]=0     return trangle+data w=np.random.random((3,3)) w1=np.random.random((3,3)) w3=np.ones((3,3))#np.random.random((3,3)) def softmax(x):   value=np.copy(x)   higher=np.max(value,axis=1,keepdims=True)   amount=value-higher   expontial=np.exp(amount)   sum_expontial=np.sum(expontial,axis=1,keepdims=True)   result=expontial/sum_expontial   return result def dsoftmax(x):   first=np.sum(x,axis=1,keepdims=True)   second=x/first   return second def forward(f,s):   q=f@w    k=s@w1   v=f@w3   data=mask(k.T@q/np.sqrt(q.shape[-1]))   result=softmax(data)   final=v@np.copy(result)   return k,q,v,result,final learn=0.0005 for i in range(1000):   key,quary,value,table,prd=forward(x,x1)   high_loss=y-prd   loss=(softmax(mask(value.T@high_loss))-table)   loss1=high_loss@table.T   rank2=x.T@loss1   rank=np.sum(quary*loss,axis=0,keepdims=True)   rank1=np.sum(key.T*loss,axis=1,keepdims=True).T   err=x.T@rank   err1=x1.T@rank1   w+=err*learn   w1+=err1*learn   w3+=rank2*learn   if i%20==0:     print(""loss"",np.sum(np.abs(y-prd))) print(forward(x,x1)[-1])",True
@Kevin-xs1ft,2024-03-04T07:01:13Z,1,first,True
