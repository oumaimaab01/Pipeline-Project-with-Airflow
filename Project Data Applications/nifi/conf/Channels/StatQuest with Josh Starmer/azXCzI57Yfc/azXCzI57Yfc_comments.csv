author,updated_at,like_count,text,public
@statquest,2020-05-21T00:12:10Z,28,NOTE: The StatQuest LDA Study Guide is available! https://statquest.gumroad.com Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@ebadulislam123,2024-05-17T12:51:22Z,2,"Whats the most frequent phrase said by Josh ? a) ""Bam"" b) ""Waa waa"" c) ""Oh No"" d) ""I am a geneticist""",True
@letitiaab,2024-04-26T21:12:25Z,1,The song at the beginning 🤣🤣,True
@mrunalwaghmare,2024-04-12T12:40:55Z,1,love ur vids they are simple to understand <3 thank you for your efforts!,True
@mokshithreddy4849,2024-04-11T13:16:00Z,2,:),True
@Abhinavkumar-og3xd,2024-04-09T11:08:57Z,0,Please speak in hindi.,True
@aswinbudhathoki6300,2024-04-03T17:23:08Z,1,BAM!!,True
@zoc2,2024-04-03T16:48:11Z,1,"I love the ""womp womp""",True
@alphabetadministrator,2024-03-06T20:20:17Z,1,"Hello Josh. As always, thank you for your super intuitive videos. I won't survive college without you.   I do have an unanswered conundrum about this video, however. For Linear Discriminant Analysis, shouldn't there be at least as many predictors as the number of clusters? Here's why. Say p=1 and I have 2 clusters. In this case, there is nothing I can do to further optimize the class separations. The points as they are on the line already maximizes the Fisher Criterion(between-class scatter/in-class scatter). While I do not have the second predictor axis to begin with, even if I were to apply a linear transformation on the line to find a new line to re-project the data on, it will only make the means closer together. Extending this reasoning to the 2D case where you used gene x and gene y as predictors and 3 classes, if the 3 classes exist on a 2D plane, there is nothing we can do to further optimize the separation of the means of the 3 classes because re-projecting the points on a new tilted 2D plane will most likely reduce the distances between the means. Now, if each scatter lied perfectly vertically such that as Gene Y goes up the classes are separated distinctly, then we could re-project the points on a new line(that would be parallel to the invisible vertical class separation line) to further minimize each class's scatter, but this kind of case is very rare.  Given my reasoning, my intuition is that an implicit assumption for LDA is that there needs to be at least as many predictors as the number of classes to separate. Is my intuition valid?",True
@nidakhan4758,2024-02-19T06:41:05Z,0,You could have used a simpler example than genes. :p,True
@willbutplural,2024-02-12T21:13:49Z,0,6:30,True
@bhargavnits,2024-01-05T03:01:02Z,0,"we get LDA directions by maximizing (m1-m2)**2 / (s1**2 + s2**2). ... For a test point, how do we get the class label ...",True
@wtfJonKnowNothing,2023-12-27T05:43:27Z,1,I'm more aligned to hear and love the song than the lecture these days :),True
@user-yt1dz2sl2z,2023-12-01T15:36:50Z,1,"Very useful and intuitive, also sick intro music right there as usual! xD",True
@KayYesYouTuber,2023-11-27T17:46:26Z,0,Thanks. This is a nice video. Can you add a video on this topic with a numerical example on how LDA works.,True
@priyantynurulfatimah6408,2023-11-26T16:34:17Z,0,Hi! I am still confused on part 9:10 - 10:00 where you mention about maximize distance between means and optimize the distance between means and scatter. I still don't get it how the new line can change direction from upper right oblique to lower right oblique. And I still don't understand what is maximizing distance and minimizing variance Y^Y please help,True
@andreisandu2575,2023-11-14T13:27:52Z,0,"Hello - just have a question with regards to an app that I am trying to build - is it possible to use this LDA model in order to perform topic analysis from a webpage that is provided as input? If so, how will this work? Would I need to also input a large dataset for reference?  Thank you!",True
@namanjha4964,2023-10-09T17:12:19Z,1,"Thank You very very very much, You bring joy to me",True
@aramkarakhanyan4224,2023-09-08T09:54:26Z,0,14:30 please explain this part . LD1 (the first new axis that LDA creates) accounts for the most variation between the categories.  what are  LD1 and LD2?,True
@Zumerjud,2023-09-04T21:35:17Z,1,Beautiful explanation!!,True
@woodworkingaspirations1720,2023-08-11T06:57:52Z,1,"This lecture has no instance of ""bam""  or ""double Bam""",True
@ashishdayal172,2023-07-28T14:34:05Z,0,hii josh what is the meaning of loading score?,True
@gourabbiswas6961,2023-06-10T18:19:55Z,1,Thanks,True
@r12-ux7us,2023-06-04T11:27:13Z,1,Linear Discriminant Analysis sounds function of  minority report system(i.e movie).,True
@sheelstera,2023-05-13T15:42:19Z,0,@StatQuest  for LDA with 3 categories do the 2 new axes have to be orthogonal?,True
@KaptainLuis,2023-05-06T21:34:46Z,1,thx!!,True
@alexlee3511,2023-04-04T02:44:12Z,0,"Very nice video! About the equation here 8:11 , I wanna know if it should be equal 1? So more maximizing the distance between means, higher the scattering we can measured.",True
@sarahlou1746,2023-03-27T19:37:19Z,0,"OK, but how do you interpret the output??",True
@odeliabellaiche,2023-03-19T11:57:48Z,0,Hi! Can you explain me please the similarities and differences between LDA and QDA? Thanks a lot for what you do!!!,True
@zahrahadavand2290,2023-03-14T20:51:15Z,0,Does LDA makes sense for regression (not classification)?,True
@jasonbourn29,2023-02-15T04:22:35Z,0,Great vedios Please don't use these biology transcript gene ..sort of things can't connect for some people like me,True
@johnbarryyallagher1128,2023-02-15T02:31:33Z,0,for three axis the best way to visualise is to draw contours like a landscape,True
@nintishia,2023-01-29T08:15:38Z,1,"Once again, a fantastic job. Thanks, StatQuest.",True
@akshayrameshwar4869,2023-01-17T13:53:20Z,0,"Question ??? At 12:50 Q1) how did you plot the data points on the ld1 and ld2 axes. Did you use projection of data points (at 11:51) on the axes (ld1 and ld2) like used in the PCA?   Q2) In ML, we say that LDA is a classifier, if so? a) how can we classify new data points (test data)? b) or do we use logistic regression or other classifying techniques after LDA?",True
@scottsun9413,2023-01-09T14:24:59Z,1,"Really great videos, saved me from my data science classes. I'm applying for graduate program at UNC, hope I can have the opportunity to meet the content creators sometime in the future.",True
@yuvrajagarkar8942,2023-01-04T07:08:50Z,0,Nice video .!!  Any good books to learn these ?  Please recommend,True
@JacobGravesen,2023-01-01T11:22:40Z,1,This man is a legend,True
@andyn6053,2022-12-18T04:51:24Z,0,Can LDA and PCA be used to determine which features to keep when training a supervised classification model? I dont want to create new axis but I want to understand which features are best in separating my different output classes. Or is there a better method to use for this kind of feature selection analysis?,True
@sureshmakwana8709,2022-12-05T10:51:49Z,0,The best explanation on whole internet 💯,True
@chieftainsupreme9387,2022-12-05T02:44:53Z,1,You're an excellent teacher. Thank you so much.,True
@meiguzhimei,2022-11-26T16:19:51Z,0,so many motherfucker smart people in the world. Dam!,True
@sridharyamijala4739,2022-11-23T11:09:57Z,1,Another excellent video just as great as the one on PCA. I read a Professor's view on most of the models and algorithms stuff in ML where he recommended understanding the concepts well so that we know where to apply and not worry too much about the actual computation at that stage. The thing that is great in your videos is that you explain the concept very well.,True
@jorgefioranelli,2022-10-29T03:09:52Z,1,Thanks!,True
@rockfordlines3547,2022-10-28T19:11:40Z,1,love statquest,True
@hamishmatthews21,2022-10-24T11:40:29Z,1,WOMP WOMP!,True
@jinyog5276,2022-10-19T07:30:24Z,1,"I didn't  understand what the professor talked about in the lecture until I watched your videos. Thanks Josh, you save me! ",True
@ritendubhattacharyya313,2022-09-20T18:19:52Z,0,You love phoebe in friends?,True
@ZnanoG,2022-09-08T18:34:45Z,0,How the lines are done in minute 9:37 is not quite clear,True
@anjalidutta8686,2022-09-04T07:00:45Z,0,Are the LDA axis always orthogonal like pca?,True
@merida3975,2022-08-19T15:44:11Z,2,"The song at the beginning made my day, even though I took wrong tutorial of Linear discriminant analysis in data science. Just awesome. Love it a lot. We need more and more funny teachers like you.",True
@beshosamir8978,2022-08-04T04:44:37Z,0,what if we have more than  3 categories did we still get 2d plan ?,True
@saharafox2360,2022-07-28T21:32:28Z,1,That helped me a lot! Thank you sooo much! Now I'm ready for my exam tomorrow :),True
@rmiliming,2022-07-16T05:36:08Z,1,very clearly explained. the video is very enjoyable to watch too! Statquest has all that is needed to learn machine learning algos and stats well,True
@happylearning-gp,2022-06-22T13:13:43Z,1,"Excellent Tutorial, Thank you very much",True
@arpitagupta4474,2022-06-17T05:51:01Z,2,I am able to grasp on this topic without being scared. Kudos to this channel,True
@amitjaiswal7912,2022-06-13T02:54:43Z,0,Very very enough,True
@jahanvi9429,2022-05-26T17:37:09Z,1,the song in the introduction is always awesome. thanks lol! and very useful video,True
@adejumobiidris2892,2022-05-21T16:40:27Z,1,Thank you so much for helping me provide a faster solution for the confusion that has taken control of my head for 72h.,True
@shivkrishnajaiswal8394,2022-05-14T18:38:24Z,0,seems like LDA is a method which utilizes data label (y) information while reducing the dimension of input data(X). This information is missing when using PCA. We only have X in PCA,True
@ivankuseta2174,2022-05-07T14:08:24Z,0,Are the new axes created in LDA for 3 categories necessarily perpendicular?,True
@Jef-ur7zv,2022-05-05T11:40:04Z,0,"Great video once again, one question though. Is linear discriminant analysis (LDA) more likely to overfit than support vector machine (SVM)? And what is the role of an objective function for a lineair discriminant?",True
@jkim9931,2022-05-05T03:36:05Z,0,5:20 LDA key point,True
@EGlobalKnowledge,2022-04-28T14:22:28Z,2,A wonderful explanation. Thank you,True
@stearin1978,2022-04-27T07:28:44Z,0,Nice pErt about sepErability:-),True
@user-it5ty4ry3o,2022-04-24T14:21:53Z,1,"hey, thanks for an amazing explanation!  I have a question: In the video you mentioned the mean difference is squared in order to prevent negativity, you can just as well use the absolute value, is there a reason the prefer squaring?",True
@kiacaster1507,2022-04-22T10:06:10Z,1,卧槽牛逼,True
@sharqjanbuchh5635,2022-04-15T14:05:31Z,0,Plz upload QDA,True
@hanaibrahim1563,2022-04-14T04:14:00Z,1,Amazing. Thank you for this excellent video. Explained everything super clearly to me in a super concise manner without all the academic jargon getting in the way.,True
@sohambasu660,2022-04-12T15:35:31Z,0,"isn't this Fisher transform ? Also, how do you calculate the scatter ? any formula for that ?",True
@AI_Financier,2022-04-11T05:16:33Z,0,"Great video again, maybe it should be added that the big difference between PCA and LDA is the fact that for LDA, data must be labeled (groups id) or it is kind of supervised learning, while PCA is unsupervised, am i right?",True
@naiduvinay4911,2022-04-06T04:36:56Z,0,whauun whaaun is the worst part of the lecture,True
@ZOBAER496,2022-04-02T01:53:03Z,0,"Hi, I have a query on using LDA as feature extraction method before creating a classification model and testing it on unseen test data. When we fit the model we use both training data and training labels. Then we transform the training data using the fitted model and get reduced number dimensions. Now, if we use this model to transform the test data, keeping the test labels unseen (which is necessary to be kept unseen for testing), then what does it actually do? It doesn't have the labels of test data, so it cannot follow the rule of maximizing the class separability here.  To be more specific, I am talking about lda_model.transform() function of scikit-learn library.  It has only one parameter X, it doesn't take any label or y parameter. Then what does it internally do to a set of unseen X that was not used while fitting the model? If it projects the unseen data on the already fitted dimensions, then will it really be able to maintain the class separability for test data too in the same way it did for training data?",True
@tatidutra,2022-03-30T15:32:18Z,1,"Really helpul video, thank you! ;)",True
@pranjalpatil9659,2022-03-21T14:17:22Z,1,Now it all makes sense!!,True
@ruihanli7241,2022-03-09T02:00:44Z,0,"With n-dim data, i think LDA find the best axis one by one, for example data projected on LD1 have larger sum(d^2)/sum(s^2) than LD2 than ... than LDn-1. Once we have found new n-1 axis for these data, we do things like PCA to just set the n-th axis LDn as the perpendicular line to the rest n-1 axis.",True
@abuzarpatel3295,2022-03-08T19:41:00Z,0,What does the blue dot represent?,True
@daisy-fb5jc,2022-03-01T14:26:34Z,1,"I wish I can throw this video to my professor, and teach her how to give understandable lectures. Just a wish.",True
@case540,2022-02-27T23:14:02Z,0,"My textbook (Intro to Statistical Learning with applications in R) describes LDA very, very differently. Goes into assuming classes are Gaussian distributed, and finds class boundaries where the Gaussian's have the same probabilities. Is this the same thing?   Thanks a ton for your videos! Ive watched basically all of them! Everything is sooo well explained",True
@DaveGogerly,2022-02-23T17:01:50Z,1,"I love your stuff, you have the knack to explain things better than most!",True
@bhanureddy2087,2022-02-17T14:32:45Z,0,I'm just so dum I don't understand anything,True
@BillHaug,2022-02-16T11:24:25Z,1,"I would agree that ""awesome song..."" is an appropriate label.",True
@kwamenani7775,2022-01-21T16:19:59Z,1,"Hi, Josh is it possible to make your account accept gifts? I feel like I owe you a lot, I failed a couple of interviews in the past in ML theory but binge-watched videos on your channel within 2 months and have landed multiple offers.",True
@hamzaghandi4807,2022-01-17T21:04:55Z,2,"Besides this wonderful explanation, Your music is very good !",True
@ouumijin,2022-01-06T19:50:12Z,1,your videos helped me a lot. Thank you so much for such a clear explanation.,True
@ZOBAER496,2022-01-03T19:50:33Z,0,I have a question. Is there any assumption related stationarity of data or feature vectors for LDA algorithm to be applicable? Will LDA work if the data is non-stationary?,True
@woohoobaby8352,2021-12-24T15:03:42Z,1,"Thank You, this really helps a lot.",True
@eniisy,2021-12-21T00:37:15Z,1,This lesson is just so beautiful dude!!!!,True
@ZOBAER496,2021-12-16T22:22:53Z,0,Two questions which were not covered in the video: 1) Is feature scaling (like Z-score standardization) needed for LDA? Is it needed for PCA? 2) Will LDA work fine if the original feature vectors are not normally distributed?,True
@mahdimantash313,2021-12-13T11:07:44Z,2,I really can't thank you enough for that...you did in 16 mins what I couldn't do in 4 hours. keep on the good work!! and thank you again !!!,True
@MiizSexii58,2021-12-11T17:10:12Z,1,Thank you really enjoyed this video!,True
@iwwyl847,2021-12-06T13:32:34Z,0,"The concept is explained well, but im still not sure how to calculate the scatter and distance between means, that is, how to actually calculate the LDA line.. I would have loved to have a real example on how it is calculated.",True
@lifeislarge,2021-11-28T05:22:20Z,1,Never thought anyone could explain things this easily. I appreciate the effort. Thank You,True
@SPeeDKiLL45,2021-11-17T18:39:07Z,0,Thank you for the video mate!   So are Gen 1 and 2 two seperate Classes ? Because you used the word Groups  I always confuse this,True
@beccalynch4407,2021-11-12T18:04:43Z,71,"Just spent hours so confused, watching my lectures where the professor used only lin alg and not a single picture. Watched this video and understood it right away. Thank you so much for what you do!",True
@wei-tingko7871,2021-11-09T08:48:08Z,1,"I really like your channel, the explanation of concepts was clear and precise!!",True
@rachelrex8368,2021-11-02T01:52:55Z,0,Why would you choose to use LDA vs. Logistic Regression?,True
@gert-janschaap3194,2021-10-30T11:47:23Z,0,I was searching for Linear Discriminant Analysis and I found this video. I pressed it and I heard a song. Not wat I expected XD,True
@vitorhugoRH,2021-10-13T13:36:47Z,0,"Getting into machine learning and your videos are a blessing! Is there a rule to know how many samples x variables x classes you should have for trustworthy results with LDA? For instance, let's say i have 6 classes and 10 variables. What's the minimum amount of samples I should use for my training data?",True
@raultorresaragon7012,2021-10-07T16:37:29Z,0,Great job simplifying and demystifying these topics. Any chance you could go over the math (ala step-by-step) of how LDA creates the new subspace to project the data to?,True
@hannav7125,2021-10-04T06:30:44Z,1,fact: none of you skipped the intro,True
@amalnasir9940,2021-09-22T19:55:52Z,0,Does LDA use Fisher Discriminant equation to create the axes? Are they the same? I am a little confused of the difference between LDA and Fisher Discriminant Analysis.,True
@cnbmonster1042,2021-09-14T05:40:23Z,1,Amazing! I subscribed after watching your video only twice!,True
@jerry11136,2021-09-13T21:11:13Z,1,nice intro,True
@danielche2349,2021-09-08T07:22:30Z,0,how did you use the central point in the 3 category example in finding the two axis? you didn't explain how that related to generating axis,True
@vincentlin9926,2021-09-06T15:22:40Z,0,Do we have any application example by getting information after LDA? I wonder if we can fit some regression into each separated groups by LDA? Is that one of the possible application?,True
@alkrab764,2021-09-03T18:12:21Z,0,"I know I'm commenting 5 years after the video's been published but Stat Quest has never steered me wrong. How important is the distribution of the two classes (in this example, groups of people) when deciding  whether you use LDA to create a decision boundary?",True
@ksaghir10,2021-09-02T01:38:30Z,0,"I was wondering, when performing LDA LOOCV and QDA LOOCV on the same data set, and the accuracy of both model is similar. how do we decide which model is better?",True
@adatse51849,2021-08-11T11:21:44Z,1,I like this song most! 😍,True
@XShollaj,2021-08-06T21:00:08Z,1,"BAM BAM  Josh, you're the man",True
@elizabeths3989,2021-08-04T23:39:10Z,2,You are about to be the reason I pass my qualifying exam in bioinformatics 🙏🙏,True
@felixlucien7375,2021-07-25T03:54:24Z,1,Double BAM!,True
@advaitathreya5558,2021-07-17T11:48:25Z,0,@12:57 How are 10000 genes being plotted on 2-axes down from 10000 axes is unclear to me.  Can someone explain this please?,True
@h_2577,2021-06-28T14:22:08Z,1,"So this is the first video I am watching and it starts with the song ""Statqueeeest"". 😂❤",True
@michaelgeorgoulopoulos8678,2021-06-25T08:07:44Z,1,Fav statQuest intro!,True
@galan8115,2021-06-23T14:37:07Z,0,"Sorry to bother you @StatQuest with Josh Starmer, but you are my last hope! I have made an LDA multinomial classifier, but i dont know what to include in the article appart from the accuracy//confusion matrix. I have not found any examples in publications, what appart from those should i include, the LDA-Coefficients of linear discriminants, the proportion of trace too?",True
@yohannistelila8879,2021-06-21T11:31:58Z,1,BAM!,True
@alis5893,2021-06-13T23:10:18Z,3,"Josh.  you are an amazing teacher.  i have learned so much from you , a big thank you from the bottom ofmy heart.  god bless you",True
@sharan9993,2021-05-30T14:06:07Z,0,So when to use pca and when to lda?,True
@harinisudha8664,2021-05-29T03:44:14Z,1,AHHAAA moment!,True
@ChaminduWeerasinghe,2021-05-16T19:00:35Z,1,Best explanation iv ever seen on ML. This is the first time iv watch ML youtube video without rewind :| ..  Keep Up bro..,True
@gaboceron100,2021-05-16T05:33:27Z,1,"Very illustrative, thanks for the video!",True
@gauranggarg549,2021-05-12T09:30:34Z,2,Cant understand a topic and then u find a statquest video on it  TRIPLE BAMM!!,True
@bitsajmer,2021-05-07T05:11:37Z,0,Hi Josh!!! Hope you are staying safe!!!  I have 2 questions. 1. How to calculate the central point for each scatter when we have 3 or more dimensions?  The formula to optimize in 2 dimensions was d^2/(S1^2+S2^2) 2. Then how come in 3 dimensional formula we have 3 d^2 terms?  Regards Yash,True
@BroaderBasicsBuddy,2021-05-06T21:53:26Z,1,Hooray from India!!,True
@ashisparida668,2021-05-05T09:41:44Z,1,aap bombay aa sakte hai,True
@anujlahoty8022,2021-05-03T14:24:37Z,1,Superb.,True
@sergeya.419,2021-04-28T20:42:06Z,1,Thank you for your explanations! Love it,True
@Jello1963,2021-04-23T17:54:31Z,0,of course you can draw 4D graphs: simply use colors as the 4th dimension.,True
@vishwajeetohal9137,2021-04-21T16:18:27Z,0,Are there scree plots for LDA too?,True
@kenanmorani9204,2021-04-19T15:56:31Z,0,Can we find lda code in python?,True
@dpcarlyle,2021-04-19T11:37:41Z,1,Thank you for the amazing explanation :) you make it so much fun....,True
@karannchew2534,2021-04-05T14:11:43Z,0,"Like PCA, LDA ""compress"" the data into lower dimensions. But unlike PCA, it do so while keeping/maximising the classificability (separability) of the data according to the given classification, as much as possible.  Data must already be classified to use LDA.   Find the line (the new lower dimension) such that the difference between the means of two classes of data are maximised. At the same time, the variance among the data  of the same class is minimised.",True
@0Edm0,2021-03-16T10:41:40Z,1,"Thanks, very clear explanation!",True
@snehalgaikwad6436,2021-03-15T09:50:49Z,0,"can I treat number of features as dimensions? i.e, if am having 10 features of any image that means 10 dimensions??",True
@snehalgaikwad6436,2021-03-15T09:48:50Z,0,"for 39 classes, will it create 38 axes??",True
@amrit20061994,2021-03-11T06:28:22Z,2,"""But what if we used data from 10k genes?"" ""Suddenly, being able to create 2 axes that maximize the separation of three categories is 'super cool'."" Well played, StatQuest, well played!",True
@farahbkz.8014,2021-03-01T23:20:35Z,1,So helpful!!! Super grateful I found you,True
@goksuntuncayengin7104,2021-02-27T14:13:17Z,1,"Hi Josh, thanks for the video! I want to ask that whether LDA always determines a (dimensions-1)D space or not. (Line for 2 pts, plane for 3 pts etc.)",True
@ThangPham-dx9ic,2021-02-26T06:55:19Z,0,"Super interesting video! Your videos are much better than those boring equations on the books BTW, may I ask you a question? What you mean in ""LDA for 3 categories"" is number of LDs = number of categories - 1, right?",True
@oklu_,2021-02-21T10:21:05Z,2,"thank you for your kind, slow, and detailed explanation😭",True
@evon4441,2021-02-15T03:29:29Z,0,"I love your channel and have studied a lot form it, but Just to annoy you the spelling of maximizing is wrong :D :P",True
@jifi9866,2021-02-13T15:52:57Z,0,old but gold,True
@jennysspiceoflife8581,2021-02-04T00:41:22Z,1,"Thank you for the illustration, it's very clear!",True
@xinfangmin3665,2021-02-02T12:02:08Z,1,It is very vivid！,True
@jansowa9035,2021-02-02T09:03:29Z,1,No BAMs? ;(,True
@MrRynRules,2021-02-01T22:43:45Z,1,Thank you!,True
@shikhapareek7000,2021-01-26T15:51:54Z,1,Thankyou you explained very well 🙏,True
@raviyadav2552,2021-01-18T13:50:28Z,1,statqueeeeeeeeeeeeeeeeeeeeeeest  watchout,True
@kattavia92dva,2021-01-10T15:54:59Z,0,Is this similar to PLS-DA? Thanks for your awesome super helpful videos!,True
@salmankhan-cu9hn,2021-01-08T15:16:03Z,1,you are the best. Thanks for such a good explanation :),True
@namoan1216,2021-01-06T11:00:30Z,0,"my friend say you giảng dễ hiểu, i will dislike if khó hiểu",True
@Pmarmagne,2021-01-03T15:58:52Z,1,Another clearly explained video by StatQuest!,True
@DJayDiamond,2020-12-22T17:52:22Z,0,"Doesn't LDA reduce to k-1 dimensions? So the 10,000 genes example is wrong...",True
@jialingzhang1341,2020-12-11T22:29:36Z,0,Thanks for this brilliant video! One thing I think is worth mentioning or emphasizing is LDA is supervised and PCA is unsupervised.,True
@melodyh5993,2020-12-05T14:56:58Z,1,Excellent like ever,True
@vaibhav_uk,2020-11-24T05:34:14Z,0,"At 8:30, I think the numerator will almost always be smaller than the numerator. E.g. Series 1: 1-10, Mean = 5.5, Spread = 9. Series 2: 11-20, Mean= 15.5, Spread =9. Mean Diff=15.5-5.5=10, Square= 100, While Dr square will  be 162",True
@yuniprastika7022,2020-11-19T09:19:48Z,440,"the funny thing is, so many materials from this channel are for those university students (like me) but he keeps treating us like kindergarten children. Haha feels like i'll never be growing up, by watching your videos sir! QUADRO BAAM SIR, THIS WORLD HAS BEEN GONE TOO SERIOUS, THANK YOU FOR BRINGING BACK THE JOY",True
@HoneyofKnowledge,2020-11-14T11:07:48Z,0,Essence of Video is at 6:36,True
@arungandhi5612,2020-11-11T15:38:20Z,1,you are very cool bro.  I aced my work at my research institute because of youuuuuuuu,True
@Peekabostreet,2020-10-28T15:49:36Z,0,Stats people love parodied songs for some reason,True
@wouterdecoster4020,2020-10-28T06:52:13Z,0,"Now the drugs don't work, they just make you worse",True
@foramjoshi3699,2020-10-26T04:39:39Z,1,The 96 people who disliked this our the patients for whom the medicines did not work :P,True
@beautyisinmind2163,2020-10-19T04:19:40Z,0,"what is the difference between Fisher discriminant analysis, LDA, Distance discriminant analysis???",True
@saiakhil1997,2020-10-06T18:22:46Z,1,I  really liked how you compared the processes of PCA and LDA analysis. I got to know a different way to view LDA due to this video,True
@panditrishabh9813,2020-10-06T16:49:08Z,0,NO BAM in whole video. Its shocking!!!!,True
@nihadtp539,2020-09-16T18:39:35Z,0,"Hi I have a question, if someone were to work in your line of work from a strictly engineering(Datascience) backgroungd, Is it necessary to have a degree related to Biology? Like with genes and stuff. By the way I love your videos. Thanks",True
@herberthsieh66,2020-09-05T22:17:10Z,0,Could you also make a video about Generalized Discriminant Analysis?,True
@123samanthastar,2020-08-27T15:40:48Z,1,Thanks for the video!,True
@seanliu54,2020-08-26T14:59:11Z,0,What are some advantages and disadvantages of LDA?,True
@LifeofTF,2020-08-22T01:39:26Z,1,Loved the explanation. Your channel has been a truly invaluable source for studying ML. I was wondering whether you could make a video on the differences/similarities along with use cases for KNN/LDA/PCA.,True
@stephanieballonromero7959,2020-08-17T09:45:45Z,0,"Excellent video, can I use LDA with categorical independent variables and categorical dependent variables. I have a questionnaire with several items trying to classify pedestrians into 5 categories all measure on a Likert scale (7-points). If I can't use LDA, what method would you recommend me to classify my pedestrians?",True
@neillunavat,2020-08-15T16:03:30Z,1,I am so glad this channel has grown to around 316k subscribers. Very well explained. The best of bests. <3,True
@ammarkhan2611,2020-08-13T12:27:03Z,0,"Hi Josh. Is LDA a supervised learning method, whereas PCA an unsupervised learning method  ?",True
@MrSoumyabrata,2020-07-30T04:45:39Z,1,Thank you for sharing it out.,True
@veraschrenkova2287,2020-07-29T17:40:55Z,0,"Hi, maybe mean should be represented by  ̅x since it represents average of selection and you further use s^2 for variation =) otherwise great video guys",True
@art.ventures,2020-07-23T22:57:08Z,1,Who did finish the video and immediately restart it just for listening to the song?,True
@andresfelipecanobotero8814,2020-07-18T23:50:59Z,1,"wow, amazingly explained! Thank you",True
@rodrigolivianu9531,2020-07-11T22:04:02Z,1,"Great video! Just wanted to point out that LDA is a classifier, which involves a few more steps than the procedure described here, such as assumption that the data is gaussian. The procedure here described is only the feature extraction/dimensionality reduction phase of the LDA. G",True
@AbzerKelminalPM,2020-07-11T07:43:50Z,0,"Hi Professor, is it possible to perform LDA on a certain no.of  PCs each taken from a set of samples [for ex: 5 PCs from each sample. Lets say total 10 samples ]? Instead of LDA on few PCs obtained using data from all samples altogether?",True
@laurading7012,2020-07-07T16:29:21Z,1,"I just graduated from high school, but your videos helped me understand many research papers. Thank you very much!!!!!",True
@aayushchopra4035,2020-06-27T17:43:49Z,0,A small doubt.Is scatter the varience or just the distance b/w smallest and largest point?,True
@zeynabmousavi1736,2020-06-24T21:43:07Z,0,Can I use this LDA to rank the features that based on their ability to discriminate between two classes? say I have several genes and I want to classify cancer vs non-cancer based on the genes. My aim is not to transform the features or dimension reduction. I just want to rank features (genes) from the highest power of classification (cancer vs non-cancer) to lowest.,True
@YuzuruA,2020-06-12T14:04:44Z,1,"Just saw the support vector machines and got surprised as the goal is almost the same, method is 90 degrees different!",True
@Andy-il7kf,2020-06-10T17:05:51Z,0,Thank you for this! Am I right in thinking LDA is the same as Canonical Variate Analysis (CVA)?,True
@lalabary347,2020-06-02T21:05:26Z,1,Thank u so much you could not imagine how you much you helped  me with this thank you,True
@Ayush-gp9tx,2020-06-02T16:54:12Z,0,So the waun waun was really horrible .... at the end of it makes people feel worse ...lol,True
@ArghyadeepPal,2020-06-02T12:13:58Z,2,Waiting for your new album lol..,True
@hugo3796,2020-06-01T13:10:24Z,1,I love the intro song. Which song did you get inspiration from ?,True
@hemaswaroop7970,2020-06-01T10:58:26Z,0,"Hey Josh, your explanation for LDA is good. Just like there is a step-by-step analysis for PCA, do you have a step-by-step for LDA as well?",True
@rahulrawatism,2020-05-27T10:46:01Z,1,superb explanation,True
@seifeldineslam,2020-05-27T06:46:20Z,25,"This was honestly helpful, i am an aspiring behavioral geneticist (Aspiring because I am still an undergraduate of biotechnology) with really disrupted fundamentals of math especially statics. Your existence as a youtube channel is a treasure discovery to me !",True
@RaviShankar-jm1qw,2020-05-24T07:24:29Z,0,"Hi Josh! Wanted to clarify a simple doubt -> Would it be true to say that Linear Discriminant Functions are linear functions of x which are calculated for each class (if we have 3 classes of classification, each class will have its own Discriminant Function) and any random variable (X=x) is attributed to the class for which the Discriminant Function gives the highest value? (Value of x is plugged into each of the Discriminant Functions and the class for which the value is highest after plugging x, x is attributed to that class). Thanks for your help in advance :)",True
@rohil1993,2020-05-23T20:52:29Z,3,This explains the beauty of LDA so well! Thank you so much!,True
@gangarajagopal7701,2020-05-21T11:36:48Z,0,Please don't sing,True
@statquest,2020-05-21T00:12:10Z,28,NOTE: The StatQuest LDA Study Guide is available! https://statquest.gumroad.com Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@halehdamirchi146,2020-05-14T15:35:03Z,0,can you please share the slides for your videos as well?,True
@gauravganguly5255,2020-05-14T00:29:23Z,0,How Covariance comes into play in LDA? And what is Quadratic Discriminant Analysis? Thanks in Advance.  Your videos are unmatchable.,True
@MohammedNoureldin,2020-05-08T13:33:57Z,1,"Man I like you! Thanks a lot you helped me to understand PCA and LDA without even making ""Owa Owa"" once! :D",True
@vivekmankar9643,2020-05-07T03:21:48Z,1,This channel deserves millions of subscribers !!!!,True
@Darkos134,2020-05-06T06:54:15Z,1,thank you for saving me. my professor doesnt really explain anything well,True
@datoubi,2020-05-05T22:35:16Z,1,i recommended all your videos to my fellow students in the data analysis course,True
@yashlakdawala7232,2020-04-30T17:29:38Z,1,You didn't BAM! back then?,True
@1337lalaland,2020-04-30T09:20:42Z,0,"Hi, according to which criteria do you choose Gene X, Gene Y etc?",True
@shaileshsankaran3021,2020-04-28T18:02:39Z,0,"how exactly the 10000 genes got mapped to 2 axis is still not clear, please help",True
@Azuremastery,2020-04-28T06:19:14Z,4,"Hi Josh, Helpful to understand the differences between PCA and LDA and how LDA actually works internally. You're indeed making life easier with visual demonstrations for students like me :) God bless and Thank you!",True
@pierre-alexandrequittet6461,2020-04-26T10:53:28Z,1,"This is brilliant, thank you so much",True
@RaviShankar-jm1qw,2020-04-25T13:33:18Z,2,Simply superb! Awesome Josh!!!!,True
@HelloWorld-ji9fp,2020-04-25T11:50:32Z,1,Thanks Josh...Very informative video....,True
@dontricera8774,2020-04-22T13:44:30Z,1,Thank you so much for this video!!,True
@tracyyi4478,2020-04-20T20:13:54Z,0,"How do we calculate the ""minimize the variation (which LDA calls scatter, and is represented by S squared) within each category""? Is it variance or covariance?",True
@juanvelasquez7243,2020-04-16T17:27:48Z,1,"thanks for your videos dude, you make it way more simple to understand",True
@arnobchowdhury1804,2020-04-16T17:22:17Z,0,isn't that s is standard deviation?,True
@MIbra96,2020-04-16T13:36:24Z,0,So if the goal was to do some classification/regression one would apply some dimension reduction technique like LDA or PCA (if that is useful in the given context) and then use the new features for whatever classification/regression technique one wants to use?,True
@liranzaidman1610,2020-04-13T09:26:51Z,0,"Crazy!!, this is amazing stuff, I didn't know there are so many ways to define groups.  Question please - is there a sort of ""Venn diagram"" method to group data? I mean, the methods you teach here define a stand-alone group, but many times there are points which are located near other points from a different group, so they must have something in common which should be a sub-category.  Thanks",True
@Illinoise888,2020-04-11T22:41:32Z,1,"Thanks for the video! I have an exam next week and even though its open book, I still didn't feel comfortable going into it. This video definitely helped!",True
@niazkhan926,2020-04-07T22:58:52Z,0,"Excellent video and video series, Josh! This has been an excellent learning experience while staying at home during the coronavirus pandemic. I was wondering if this type of LDA is also called partial least squares discriminant analysis (PLSDA)? Stay healthy and safe",True
@chi-yunchen1932,2020-04-04T16:42:16Z,1,Clear and wonderful explanation. Thank you very much :),True
@newmohak,2020-04-04T16:14:20Z,1,Fabulous work. Very helpful. Keep it up.,True
@darrenlukas,2020-04-03T05:40:35Z,0,"Hi, thanks for the great explanation on LDA! I'm just wondering since this is a dimensionality reduction technique and LDA is able to use those statistical knowledge of of classes according to the new axes to create a decision boundary, is there any way to get the interpretation on the impact of a single feature? For instance, in regression, we are able to use the coefficient to interpret the impact of change in variable A to the prediction, how can I do similar thing in LDA? Or plotting the data into the new axes is the only way to visualize/interpret this? Thanks a lot!",True
@AbdulQayyum-kd3gf,2020-04-01T13:42:13Z,1,"wonderful explanation, How can I get the slides?",True
@Dr.CandanEsin,2020-03-24T14:28:53Z,1,"Too much time and effort spent, but they worth it. Best explanation I watched after six weeks of search. Cordially thank you.",True
@damoonrobatian9371,2020-03-20T00:16:32Z,0,"Cool video! However, there seems to be a wrong statement in it: In section ""LDA for 3 categories"", it is claimed that LDA provides a plane because we have 3 centers and etc ...! The dimension of the reduced space is independent of the number of classes. To understand this just imagine one-dimensional data with 3 or more classes. According to what is explained here, LDA will give 2 or more dimensions for the reduced space!!! All in all, do not forget that LDA reduces dimensionality and not vice versa. Anyway, thank you for the inspiring video.",True
@N0RAH11,2020-03-17T07:28:28Z,1,thank you for your fun way 😂,True
@sampathkumarmanchala2237,2020-03-16T14:58:41Z,0,can you please give code in python,True
@mikboy018,2020-03-14T20:59:09Z,0,Great video. I have a question regarding the diagonal line at 6:48. Shouldn't it come through the origin point?,True
@gayathrikurada3315,2020-03-09T18:55:02Z,0,"Hi Josh, thanks for the clear explanation but a small doubt regarding the number of features to consider in LDA like in PCA we have scree plots to identify how many variables are max contributing but how do we identify the number of discriminants that are max contributing to classify ?",True
@arunramji,2020-03-08T19:52:37Z,1,Thanks !!,True
@stella_here,2020-03-05T11:58:49Z,1,best video. very clear. Thanks.,True
@michaelyang7657,2020-03-03T01:50:25Z,2,"Dude you're an Alpha, better than most of my professors",True
@Ashley-jx7zk,2020-02-29T11:50:44Z,1,"Thanks for the video! It really helps! May I check is that PCA and LDA are similar in the sense that they both reduce dimension, but  PCA is unsupervised learning while LDA is supervised learning?",True
@sayantanmitra7897,2020-02-24T00:19:01Z,0,"@Josh Starmer Big fan! However here I am confused how do you solve the problem of 10k dimension when you have 10k gene expression data. for PCA, you explained eigenvectors, I think that part is lacking in this video. Or most likely i am missing something, knowing that how thorough your videos are.",True
@mikhaeldito,2020-02-23T19:42:47Z,0,I get an idea after revisiting several of your older videos. Would it be possible to do explain the nitty-gritty mathematical details along with these fantastic illustrations? So it would be like the famous Introduction to Statistical Learning with R but also accessible to people who are not math savvy. I want to have an in-depth understanding but I think it would be great if someone could guide us in learning how to read all those formulas.,True
@nishisaxena4831,2020-02-21T13:06:39Z,1,"much better than my university lecture that I listened to twice but couldn't understand ... this was awesome, thanks!",True
@kermitskumkakedkorpse1434,2020-02-20T05:32:05Z,0,Is that an OC theme song? You bastard I'm in.,True
@paulhamacher773,2020-02-05T16:54:13Z,2,This channel is pure gold!,True
@gabrielelombardi7430,2020-01-29T15:14:51Z,0,@StatQuest What is the difference between LDA and Partial Least Squares Discriminant Analysis?,True
@ahmedabbas2595,2020-01-29T12:58:00Z,1,"You are awesome josh!, no, really, you are TRIPLE-BAM AWESOME!",True
@seant7907,2020-01-29T03:51:03Z,1,subscribed just because the way you described this topic is so simple and understandable. nice job!,True
@ericklestrange6255,2020-01-26T16:24:52Z,0,"dear josh, i cant understand the intuition behind the matrix 𝑺−1*𝑺𝐵. why inverse of within-class scatter matrix? Why multiplied by Between-class scatter matrix? And what would applying this matrix mean? Let me quote the lines “steps of the LDA are similar to the steps of the PCA. However, instead of performing the eigendecomposition on the covariance matrix, we solve the generalized eigenvalue problem of the matrix, 𝑺−1*𝑺𝐵”",True
@ozozan7895,2020-01-23T09:33:30Z,0,What is the difference between LDA and PLS/PLS-DA? do you have any video related to PLS/PLS-DA?,True
@21bagong,2020-01-18T13:27:49Z,0,"Hi Josh, your explanation is very helpful. But, does it the same concept with Partial Least Square (PLS) ?",True
@princezard3506,2020-01-11T06:47:30Z,0,Can you explain the differences between variation and separability?,True
@AndresEOA,2020-01-11T02:04:03Z,0,"Awesome!   I do have a question for you Josh!! If I had 500 variables and three factors A-organ(A1, A2) and factor B-specie (B1, B2) C-treatment (C1,C2,C3). I want to know which variables contribute to difference: first (organ) during treatment C, secondly (specie) during treatment C.   1. Would you approach this doing PCA or LDA??   2. For example, In case of organ separation, would you do PCA/LDA of A1 (in all C cases), then PCA/LDA of A2 (in all C cases) and then compare visually if for example if the 20 variables that contribute more to treatment-C separation appear in both A1 and A2 situations???   3. What would be your approach???   Many thanks",True
@AdityaDodda,2020-01-04T16:58:17Z,2,@StatQuest: Thank you for the video. It is very helpful. Would it be fair to say that PCA is unsupervised but LDA is supervised?,True
@neelkhare6934,2019-12-27T11:43:08Z,4,"Wow , that is one of the best explanations of LDA  it helped me get an intuitive idea about LDA and what it actually does in classification  Thank You!",True
@sakhawat3003,2019-12-23T09:33:28Z,0,"Hey man! That was a nice clear cut explanation . I have been doing machine learning using LDA but I never knew what this LDA actually does . I only had a vague idea . By the way , you wrote ""seperatibility"" instead of ""separability "" at  5:26 ....",True
@zn4q3oi18zx,2019-12-21T14:16:53Z,2,"I really enjoyed your video! But it seems ""Linear Discriminant Analysis"" in this video actually means Fisher's linear discriminant?",True
@kaif7312,2019-12-17T05:18:04Z,0,Can you do a video about Bayesian Method? Appreciated!,True
@whenmathsmeetcoding1836,2019-12-16T03:13:36Z,1,Your video are as like always awesome..,True
@sultanmansour639,2019-12-14T02:17:40Z,1,Thanks for your great video!,True
@ankk98,2019-12-13T18:59:40Z,1,subscribed :),True
@shinigamiryuk786,2019-12-11T09:06:26Z,1,Very well explained,True
@leanneswhitmore4672,2019-11-23T16:01:43Z,0,"Hi! great video! question though when I try and reduce a data (with many features, in this case genes ) set using LDA with two classes it says it can only reduce it to one dimension which I am not totally understanding why based on your video!  thanks for any explanation anyone can provide!   Leanne",True
@lydiachong1274,2019-11-20T12:30:21Z,0,You and Dr  Andy Fields should do a collab.,True
@armansh7978,2019-11-19T21:22:52Z,4,"Awesome, just I can say bravo man, bravo, thank you very much.",True
@saifkarigar5844,2019-11-13T16:29:08Z,1,"ayy didnt know you were UNCCH, UNCC saying hi! Thanks a lot for these tutorials.",True
@amanzholdaribay9871,2019-11-10T17:26:38Z,1,Thanks as every time! The best explanations of complicated things!,True
@Muzik2hruRain,2019-11-09T07:25:00Z,2,"You, sir, you are a life saver. Now in every complicated machine learning topics I look for your explanation, or at least wonder how you would have approached this. Thank you, really.",True
@arkarnyanhein,2019-11-09T04:30:59Z,0,"I really like this video because you used to say BAMMMM and I got distracted hahhahahah. In this video, you don't. Your explanation is pretty great by the way.",True
@alexhoneycutt8352,2019-10-28T04:42:38Z,4,"This helped me understand LDA before my midterm! I could not wrap my head around how the functions worked and what they did, but I got an ""ah-hah!"" moment at 6:49 and I totally understand it now. Thank you for explaining this!",True
@Aryaman37,2019-10-26T20:47:18Z,1,thankyou so much,True
@onurdemir353,2019-10-26T09:27:33Z,1,"You are awesome.Eventually,I was able to reach understanding point of machine learning staffs thanks to you.",True
@gptty,2019-10-26T04:27:05Z,1,I get it! You sir is the best lecturer in statistics,True
@mihikataneja145,2019-10-23T10:02:03Z,0,please make video of ltsa algorithm,True
@tomaszberent801,2019-10-18T15:40:03Z,1,"The video shows how LDA reduces dimensions and we can clearly see a newly constructed axis (like with PCA) which - in LDA analysis - maximizes the separation. That was very clear!. How does this line relates to a line that actually separates the two categories on an original XY plain you refer to on 2:48 minute of your video?. After all it is this line (do we call it a discriminant function?) which is usually used to show the separation?. The latter is intuitively understood as a separation border, the former explains how we reduced dimension. What is the link between the two?",True
@tomaszberent801,2019-10-18T09:24:57Z,0,"Josh, BRILLIANT!!!! Some quations tough.  1) In what sense LDA generates the same separation as linear regresssion (however understood) as often claimed? Doesn't LDA use its own ""optimazation"" criterion, somewhat different from e.g. OLS? 2) Can we do ROC having identified a ""separation line"" between two categories using LDA? I am a bit confused here. As far as logistic regression is concerned, it is all clear: we can do ROC as different cutoffs are available to us after we estimate the curve. But with LDA, it looks to me that the ""optimization"" criterion defines (kind of) one cutoff point and we should not mess with it anyore, i.e. should not use other cutoffs for the line eatimated.. If I am correct, why are there so many research papers which estimate the z-scores (using LDA) and subsequently change the cutoff point to calculate specificity and sensitivity (or any other classification or misclassification metric), or ROC/AUC itself.   Hopefully my inquiry is clear. Still I wish I was so clear as you are on those funny videos.",True
@Dekike2,2019-10-17T10:45:55Z,0,Hi!! I didn't find the sample code for R in the website!! Is it still there?,True
@maverickstclare3756,2019-10-15T05:54:25Z,7,"""Dr, those cancer pills just make me feel worse"" presses red button ""wohp waaaaaaaa"" ""next patient please"" :)",True
@joshwu9187,2019-10-14T05:31:55Z,0,What if we need to separate the 100D data into 4 categories? Can we still use 2D plot to represent it? since the three point plate is not enough to represent it.,True
@rspvsanjaykumargupta,2019-10-12T19:57:09Z,0,"may you explain diagonal discrmination analysis, https://in.mathworks.com/help/stats/classify.html",True
@daciameneguzzo5016,2019-10-04T15:39:01Z,1,"great video, very helpful, thank you!",True
@TheRandyrands,2019-10-03T10:44:47Z,1,Thanks- very useful,True
@jathiswarbhaskar2479,2019-09-19T14:42:25Z,1,Wonderfully explained.. thanks !,True
@bokai5829,2019-08-31T00:21:00Z,317,Every time I heard the intro music. I know my assignment is due in 2 days.,True
@mirzashafi,2019-08-23T17:54:51Z,1,Wow really helpful. Thanks a lot :),True
@chemicalbiomedengine,2019-08-20T18:22:59Z,2,always excited when i look for a topic and its available on statquest,True
@ramarajugadiraju6886,2019-08-16T04:43:01Z,1,Very nice video . Admiring your passion and contribution !,True
@shawn-jung,2019-08-12T04:12:55Z,0,"I watch Statquest video to listen to awesome songs, not for LDA or regression analysis",True
@mohaktrivedi9591,2019-07-30T03:45:21Z,1,Sick intro!,True
@anantgupta331,2019-07-09T01:20:38Z,1,Brilliant! Thanks,True
@sanketbadhe3572,2019-07-08T19:45:37Z,2,I just watched all your videos for intro track :P ......awesome tracks and nicely explained videos,True
@basiqquddusi9601,2019-07-08T09:15:35Z,1,Oh my God  your video is so good.,True
@assortedtea902,2019-06-26T00:45:16Z,2,What the heck is a gene transcript. i really hate it when these things are mentioned casually and the listener is assumed to know them already. NO i dont know what is a gene transcript. now i have to pause the video and google about gene transcripts. ugghhh,True
@kharzakyt,2019-06-20T18:53:58Z,0,*separability,True
@junecnol79,2019-06-09T15:18:32Z,1,crystal clear,True
@whasuklee,2019-05-29T14:53:46Z,5,"Came for my midterm tomorrow, stayed for the intro track.",True
@PaawanS,2019-05-15T00:21:04Z,2,I really like the systematic way you approach each topic and anticipate all the questions a student might have.,True
@PaawanS,2019-05-15T00:17:54Z,0,All your videos are extremely helpful!,True
@MrDeepak8866,2019-05-14T17:40:49Z,0,why 43 dislikes ?,True
@niteshnijhawan3856,2019-05-13T16:57:58Z,0,Simply Brilliant,True
@priyankanagpal2207,2019-05-11T15:18:58Z,0,Very good explanation,True
@user-ic1bb3tv9d,2019-05-06T04:25:27Z,0,"Can i say that the most brilliant thing about statquest is the silly song??? love it, super fan",True
@004307ec,2019-05-03T04:11:37Z,0,Is it basically a modified version of linear regression?,True
@loyodea5147,2019-04-30T20:44:39Z,0,Great video! Thank you!,True
@cowabungaken,2019-04-27T03:26:16Z,0,"If an LDA plot separates the data by category very well, what can you learn about the original dataset? If LDA doesn't separate the data that well? What can you learn about the original dataset?",True
@MartinUToob,2019-04-20T20:05:04Z,6,"When's the StatQuest album coming out? (Here come the Grammies!) 🎸👑 Actually, the only reason I watch your videos is for the music. 😍🎶🎵",True
@raghavgaur8901,2019-03-29T15:26:39Z,0,Sir I had one doubt that by using lda we can't convert a 3-densional data into one dimension right,True
@simongoisse9271,2019-03-26T22:33:42Z,1,"Very well explained, good job. Thx",True
@gimmemoreborisbrejcha9794,2019-03-26T13:56:52Z,0,mığağağağağağağağağağa,True
@severrnijKGU,2019-03-24T02:15:08Z,0,"so, here we are. we are scholars in some field and suddenly have the need to find patterns in data. So we all go out shopping out of desparation and we end up in places like this.",True
,2019-03-20T16:29:28Z,1,I think I love you! thanks for these amazing videos! It's helping me to understand a lot of things for my PhD!,True
@ouahidalalga9437,2019-03-03T10:54:34Z,1,"Very instructive, Thank you.",True
@Adam_0464,2019-02-22T14:50:55Z,2,Very informative! Thank you so much!,True
@jacobmoore8734,2019-02-19T17:13:08Z,0,"2D gets reduced to 1D (new axis) and 3D gets reduced to 2D (new plane).  So why doesn't 10,00D get reduced to 9999D? As I understood the video, even 10,000 dimensional data will be reduced to 2D plane via LDA. Can someone explain? I'm lost.",True
@Yzhang250,2019-02-15T03:29:09Z,0,Quit a song!!!,True
@thenkprajapati,2019-02-12T14:41:27Z,1,Indeed clearly explained. Please also make videos on Independent Component Analysis and Singular Value Decomposition.,True
@alexanderlewzey1102,2019-02-02T10:31:31Z,1,top draw as always,True
@ricardolee451,2019-01-24T09:12:48Z,1,"Excellent Video! But I have a question: now we only have a measurement to know which line is better, but how to find it? We cannot go through all the possible lines and calculate each of them's  d_square/s_square.",True
@alialsaady5,2018-12-29T12:52:47Z,1,"Im sorry but I have another question. At 12:00 you say: two points define a line and three points define a plane. Isn't it possible to have three categories on a 2D graph? In this example, we want to predict if someone gets healed by the drug. Suppose we're predicting it on two variables Gene X and Gene Y. How does the dataset look like in that case?  So suppose you have tested it on 20 people, so you have 20 oberservations for Gene X and Gene Y.  You should have another variable that indicates to which category each observation belongs right? So suppose we're having the variables: Gene X, Gene Y and Class. The datapoints in the graph represent the class. But on a 2D graph you can still visualize the third class with another color? Does the number of dimensions not depend on the number of variables instead of the number of classes?   And also another question: can you please explain why LDA creates K-1 axes where K is the number of classes?",True
@alialsaady5,2018-12-27T11:28:24Z,0,"Hi Josh,  In this article LDA is explained in 5 steps:  https://sebastianraschka.com/Articles/2014_python_lda.html  Can you please explain how the matrices in this article relates to your explanation in this video? The explanation in the article looks different. Thanks in advance",True
@tonycardinal413,2018-12-24T14:10:02Z,1,I am speechless . So clear,True
@alialsaady5,2018-12-20T12:36:50Z,2,"Thank you for the explanation, it's pretty clear. But there is something I dont understand. When you have 3 categories, LDA creates 2 axes to seperate the data. But what if you have 4 categories or more? How many axes will LDA create to seperate the data?",True
@sirin5861,2018-12-19T09:29:45Z,1,Thanks really helpful and i love your intro !,True
@rishiprakash763,2018-12-11T11:58:02Z,1,Can we use PCA in supervised learning to reduce dimensions by only using the x variebles and after reducing join it with supervised model and execute LDA for better separability,True
@noobshady,2018-12-02T19:28:16Z,1,that intro...,True
@tarunbirgambhir3627,2018-11-28T20:03:43Z,2,Can you provide an example where high variance in the data from PCA is more important than high ‘separability’ of the data from LDA for a classification problem?,True
@bonleofen6722,2018-11-24T19:41:47Z,7,"4:14 was waiting for the ""sound""",True
@worldofbrahmatej2023,2018-11-17T23:08:34Z,2,Excellent!  You are a better teacher than many overrated professors out there :),True
@user-ht7gw9ww1c,2018-11-12T05:20:00Z,3,Why is he always singing at the beginning of the video?? Lolol,True
@yuhaooo8143,2018-11-03T06:40:37Z,2,"is it for n categories, we construct n-1 axis? thanks for reply:)",True
@snehashishpaul2740,2018-10-30T08:06:55Z,1,@11:39 you have mentioned for 3 categories we need to increase the mean and decrease the scatter. How can we measure scatter from a point?,True
@aaranyabarman3316,2018-10-28T05:00:10Z,0,Waun Waun :(,True
@aaranyabarman3316,2018-10-28T04:57:14Z,0,The fuck is the unnecessary sound effects?,True
@republic2033,2018-10-23T00:11:25Z,3,"Thank you, very educative and entertaining!",True
@afsangujarati9427,2018-10-19T20:19:52Z,2,"Great video, but I am still unclear about (11:53) having 2 axes to separate the data.",True
@a_sn_hh7027,2018-09-25T20:46:00Z,1,stattt questt..,True
@chaoSefat,2018-09-20T14:59:10Z,1,I love you man,True
@TiagoPereira-hm1nq,2018-09-17T00:09:27Z,1,Terrific!,True
@tariqkhasawneh4536,2018-09-15T14:40:11Z,1,"i can see some similarity between LDA and clustering, are they the same thing? Or is the LDA an offshoot of clustering?",True
@ravihammond,2018-09-01T13:54:04Z,4,This guy is amazing.,True
@manonanthinithangam9941,2018-08-16T19:37:16Z,1,Videos are awesome. Please post a video on eQTL and GWAS,True
@alexbeatson,2018-07-26T05:53:47Z,1,"Stop intro singing please, that is really weird. And the rest is awesome.",True
@blownspeakersss,2018-07-22T22:49:14Z,2,"Why does LDA here seem totally different from how LDA is presented in the ISLR textbook? In ISLR, we simply assume P(X | Y = k) is Gaussian for all k classes. Then we literally just plug in estimates into bayes theorem. So now we have an estimate for P(Y = k | X), which is the desired probability for classifying a feature vector X into a class k. Then, we take the log of bayes theorem, with our estimates, and we get a linear discriminate function that is used for classifying.  The way it's presented in ISLR is similar to how it's presented in this lecture: https://www.youtube.com/watch?v=_m7TMkzZzus  I just don't see why the same topic of LDA seems vastly different in this video?   Edit: Apparently there are different ""discriminate rules"". The one I'm referring to is called Bayes Discriminant Rule. The type presented here is called Fisher’s linear discriminant rule.",True
@theultimatereductionist7592,2018-07-22T06:25:28Z,1,"I know in any dimension this is an NP complete problem because it has the same cardinality of solution space as the subset sum problem: which is 2^N where N= number of data points, therefore 2^N is the number of all subsets one must check: i.e. project all possible subsets down to the new axis.  Of course, once on a single ordered 1D axis, since the data points will now have a fixed order relative to each other, one has at most N places to make the separation.",True
@AdhityaMohan,2018-06-13T05:27:46Z,1,Thanks for such a crisp video mate :),True
@ProfessionalTycoons,2018-06-12T21:08:52Z,1,This is really really good video.,True
@ddw802,2018-06-12T13:36:29Z,1,the best statistics video,True
@AnirudhJas,2018-06-06T17:53:42Z,1,Very nicely explained! Thank you very much Josh!,True
@muskanjhunjhunwalla8505,2018-06-03T10:55:08Z,1,It was a very helpful video. I get to understand it in the first attempt only. Thanks a lot for this video sir.,True
@ravipandey3097,2018-06-03T01:37:26Z,2,"For a two-class problem, we can go with a single axis. a three-class problem has to go with at least 2 axes (three mean points) and similarly four-class has to have at-least three axes. Is my understanding right ?",True
@davidatencia10,2018-06-01T14:49:59Z,2,Excelente video :D Lo pongo en español para que sepan el alcance que tienen. Muchas gracias!,True
@jane-ll3bs,2018-06-01T10:08:14Z,1,"best ever,thanks!",True
@uditarpit,2018-05-31T06:45:08Z,1,So LDA only for categorical data( supervised reduction)  and PCA for unsupervised reduction.,True
@DonHora,2018-05-26T10:52:40Z,1,Very good concept explanation,True
@mrdoerp,2018-05-23T19:07:19Z,2,"this videos are incredible, i would pay for it if i had money",True
@rrrprogram8667,2018-05-13T09:00:57Z,0,Great channel... i love spending time here,True
@gretawilliams8799,2018-05-05T17:45:17Z,1,"Hey, what the fukkkk  I came better for discriminant of Math",True
@gretawilliams8799,2018-05-05T17:40:03Z,1,Fuck your into,True
@Anmolmovies,2018-05-04T05:17:55Z,5,Absolutely brilliant. Kudo's to you for making seem it so simple. Thanks!,True
@JatinNagpal1,2018-04-24T06:54:59Z,0,"Thanks for the video.   I have a little doubt --> We used LDA for clustering/ finding whether the drug would be a good suit to a new patient or not?  How is it different from decision tree (or random forest) or K-means clustering or Support vector machines? Not in conceptual sense about how these different algorithms does it differently, but in practical sense, i.e. to say if I have a problem in hand like the above, how do I conclude that whether should I use random forest, K-means clustering, SVM or LDA?",True
@achillesarmstrong9639,2018-04-20T11:16:29Z,0,nice video,True
@liliafisk2746,2018-04-09T08:32:34Z,0,"hi, i really enjoyed videos about tSNE , PCA and LDA. Are you planning to make one for GTM (generative topographic mapping?) Thanks.",True
@robinduan1985,2018-03-29T11:55:20Z,76,This is amazing! 15 mins video does way better than my lecturer in an 2 hours class,True
@rodrigo7089,2018-03-29T08:47:36Z,0,"It really helped me, thanks!!!",True
@sjwang3892,2018-03-21T17:45:52Z,0,Nice OP. Saved my day drowning in the stats,True
@domenicodifraia7338,2018-03-16T14:31:27Z,0,Best YouTube channel ever!,True
@paulojose7568,2018-03-02T04:54:47Z,1,won won :(,True
@jianzhang2218,2018-02-27T02:44:44Z,0,This is a very good video! Thanks for your brilliant work!,True
@bht9871,2018-02-20T16:17:51Z,0,"this is very useful, but by talking about LDA, we can see the weakness of PCA, so what is the weakness of LDA? how do you think? I think you should mention both strength and weakness in the summary.",True
@prakashr7139,2018-02-05T18:02:55Z,0,Best ... !!! First class Explanation ... !!!,True
@LaKtJ,2018-01-28T16:18:29Z,0,"Regarding LDA for 3 categories, how do you maximize the distance between the central point and each category's central point? These points are always the same, aren't they? So how do you maximize something that does not change?",True
@MyMpc1,2018-01-20T18:10:38Z,0,What do we want to separate the data for?  We already know who the drug works for and who it doesn't. And we know about what features separate the two groups - the cell characteristics.  So what value does separating the data add?,True
@akhileshjoshi8484,2018-01-15T15:03:27Z,0,thanks :) understood like pro :),True
@livetolearn477,2018-01-13T11:50:33Z,0,Really great explanation. Thanks.,True
@JalerSekarMaji,2018-01-13T01:09:17Z,0,"Wow! At first ""wt.f is Statquest"" then At the end of video, STATQUEST! and I checked on the description. Its a great website !  Thanks",True
@vishwanathg8083,2018-01-03T04:31:00Z,0,"Thankyou , Explanation of  LDA & PCA is very clear....",True
@literallyshane4306,2018-01-02T21:45:35Z,0,awesome! thank you :),True
@mohammadadnan8248,2017-12-25T19:39:54Z,0,"Tomorrow is my exam, that might be helpful Thanks a lot from India",True
@EbraM96,2017-12-11T23:22:52Z,0,That musical intro is much fun :D,True
@sabaali6628,2017-12-05T00:15:50Z,0,How exactly would you do an LDA analysis on PCA data?,True
@ivanzhovannik5419,2017-11-21T14:44:20Z,0,"Thank you for the great explanation, Joshua! Great job!",True
@yiha0her0,2017-11-15T19:23:38Z,0,This is amazing. Thank you so much.,True
@Quaquaquaqua,2017-11-09T16:40:57Z,0,thanks! could you explain multiple correspondence analysis?,True
@deeptivirgo2009,2017-11-08T14:39:41Z,0,superb!! you have explained this concept amazingly well!!,True
@eluchiemeluchiem480,2017-11-06T12:04:32Z,0,Love d song,True
@haydo8373,2017-10-14T03:40:20Z,156,Hey what is the intro track called? I couldn't find it on Spotify. . .  :D,True
@xujerry7891,2017-09-23T03:57:40Z,2,"Hi, Joshua. Thank you for your videos, it’s really helpful. I have a question: so when you have a LDA to categorize n categories, does it mean that you need (n-1) axis to separate the points? In that case, how can I visualize them?",True
@mastermike890,2017-09-17T20:21:14Z,2,This is an AWESOME vid. Thanks for making this idea so simple,True
@vijivennelakanti9430,2017-09-08T07:20:53Z,0,Thank you! Very intuitively explained!,True
@weixu553,2017-08-08T03:01:04Z,0,"great video, you make all the academic terms very understandable, cheers from China",True
@surajna3751,2017-08-05T05:10:12Z,0,"Wonderful video, Great explanation!!! Could you please do a video on multi dimensional scaling and expectation maximization algorithm. Thank you",True
@lizzzi1,2017-07-25T06:14:21Z,0,"Great video, thank you a lot :)",True
@user-qt5mw5xd2e,2017-07-14T09:34:47Z,0,awesome!,True
@preeyank5,2017-07-03T10:05:05Z,0,"Good explanation,nice work...Thank you!",True
@ducvu2109,2017-06-11T08:58:37Z,0,"Thank you for your helpful video. Could you please provide me the code to run this analysis in R please. Although I tried with massive search on the internet. I am struggle with lda function. My code: data1.lda <- lda( Group ~ B1 + B2 + B3+ D2-11 + D2-12 + D2-21 + D2-22 + D6-11 + D6-12 + D6-21 + D6-22 + D6-31 + D6-32, data = data1) in which B1-3, D...are the samples) I have 13 samples with 39,130 reads. Best regards Amber",True
@adelutzaification,2017-06-07T12:13:42Z,0,"People doing genotyping or transcript profiling use ""PCA"" to differentiate between different categories (homozygotes/hets or tissues/similar expression profiles). It seems that LDA would make more sense to be used instead of PCA. Is it actual PCA that is used for the analysis of the above examples or is it LDA that is actually used and PCA is just a misnomer? I enjoy your videos btw. Thanks a lot.",True
@mrjuly5,2017-05-27T03:14:36Z,2,Love the intro,True
@yuanyuan3056,2017-05-25T20:23:35Z,0,"Thank you very much for the video, very intuitive and very logical. In the video you mentioned separation s, is this standard deviation within each group, or range(max-min)? Thanks!",True
@hlatse98,2017-05-23T18:22:04Z,47,Brilliant video! Very helpful. Thank you.,True
@duchuyho7027,2017-05-12T05:25:09Z,0,Great video Joshua ! Looking forward to learning more from you ! Cheers from Japan !,True
@lujingchen4400,2017-05-03T20:35:15Z,0,Awesome!,True
@JuneSiyu,2017-04-15T15:01:47Z,5,Nice singing,True
@sassmos008,2017-04-10T21:36:30Z,0,wow... my professor has been trying to teach me the concepts for weeks. and now I finally understand. Thank you so much. I will refer this to my mates.,True
@balexander28,2017-03-31T09:57:57Z,1,All of your StatQuest videos are awesome!  Thanks for using your time to help others!  Much appreciated!,True
@RaghuMittal,2017-03-30T11:09:15Z,1,"Great video! I initially couldn't understand LDA looking at the math equations elsewhere, but when I came across this video, I was able to understand LDA very well. Thanks for the effort.",True
@nishantjain2975,2017-03-29T14:06:38Z,0,Thanks a lot. It really helped in understanding better. Awesome :),True
@greina6945,2017-03-10T17:15:55Z,0,"Very nice explanation. The only issue I have is that the first and second axes for both PCA and LDA are not Gene 1 and Gene 2. They are instead some linear combination of Gene 1 and Gene 2. So in a 10,000 gene space, you will get some combination of some of the 10,000 genes that clearly separate the two groups. For example, LD1 could be one third of Gene 12 plus one third of gene 45 plus one sixth of gene 456 plus one sixth of gene 1,234.",True
@anurag8725,2017-03-01T19:51:12Z,0,"Viedo was really good, it helped me a lot. Can you please explain how you projected 10,000 genes on 2D space. What is LD1 and LD2",True
@dannyhsiao8017,2017-02-20T16:37:06Z,0,Great video! thank you!,True
@austinmw89,2017-02-18T04:06:37Z,0,So this is Fisher LDA rather than generative Gaussian LDA right,True
@lowbias,2017-02-13T19:44:19Z,0,"Great explanation, thank you!",True
@Purplepimple,2017-02-13T08:13:44Z,0,Nice intro song.  i really enjoyed your video. Please make more!,True
@Brayn410,2017-02-10T11:23:06Z,0,"Hello Joshua Starmer, thanks for your great Video. I like your slow calm voice. :) But I've got a question. Is it correct, that the projection P projects a Dataset from the d-dimensional Space into a (d-1)-dimensional space?  Sry for my bad english, I hope you understand my problem. :)  Best regards.",True
@NanaSuperTramp,2017-01-23T20:02:20Z,0,Excellent very usefull thank you very much it makes things less messy in my head,True
@SeqBioMusic,2017-01-21T23:50:51Z,1,"Awesome! It'll be good to give some differences of PCA and LDA. For example, PCA is studying the X. LDA is studying the X->Y.",True
@viktorlofving3204,2017-01-09T23:58:00Z,0,Fantastic video! Thank you :),True
@tetlleyplus,2017-01-05T18:21:19Z,0,awesome! thanks mate!,True
@alexdamado,2017-01-04T02:39:00Z,0,"very clearly explained. Great video, thanks for posting. As a blue devil it pains me to admit as much : )",True
@peterantley,2017-01-03T03:42:02Z,0,You are my hero. I am a senior hoping to get into data science and your videos are great and very helpful. Keep up the good work.,True
@swordchen7385,2016-11-17T12:08:25Z,0,"On the one hand ,we try to find the maximum distance between the two groups mean ,On the other hand ,we need to make sure the variance of the single group to be the minimum.This method can be as much as possible to separate the two sets of data.I understand right?",True
@user-dx1lm2td7l,2016-11-14T17:06:10Z,0,"Great video! But i think using square in (u-u)^2 is just to make sure that the numerator and the denominator(which is variance) have the same unit. And again, great video",True
@probeymick,2016-11-05T17:42:35Z,2,I just had to watch this because of the intro song... XD,True
@cr74life96,2016-11-05T07:49:03Z,0,what the fuck was that at beggining?good video overall though,True
@RievenRast,2016-10-31T14:09:12Z,0,Again thank you! Your tutorials are super easy to understand!,True
@karthickvadivel1664,2016-10-09T08:10:00Z,0,awesome video... :),True
@abhiramshastri6109,2016-10-02T16:33:21Z,0,Please give a link to the PCA video which was refered to,True
@Bruuuuucie,2016-09-23T04:02:26Z,0,Great video! You saved a struggling stats major for sure!,True
@rachelstarmer9835,2016-07-13T14:24:54Z,86,"Awesome!  Even I get it and love it!  I'm going to share one of your stat-quest posts as an example of why simple explanations in everyday language is far superior to using academic jargon in complex ways to argue a point. Also, it's a great example of how to develop an argument. You've created something here that's useful beyond statistics!  Three cheers for the liberal arts education!!!! Three cheers for Stat-Quest!!",True
@aneeshmenon12,2016-07-12T04:52:25Z,3,woww........toooo goodddddddddddd.....dear Starmer...nothing to say..you are incredible...I am eagerly waiting for your next video...,True
