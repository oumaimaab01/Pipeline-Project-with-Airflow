author,updated_at,like_count,text,public
@statquest,2023-02-03T15:50:50Z,3,"To learn more about one common way to create histograms of continuous variables, see: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357 To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@user-rf8jf1ot3t,2024-05-27T14:36:07Z,1,I love this video. Simple and clear.,True
@harishankarkarthik3570,2024-05-04T08:52:35Z,0,The calculation at 8:27 seems incorrect. I plugged it into a calculator and got 0.32. The log is base 2 right?,True
@liam_42,2024-05-01T10:33:54Z,0,"Hello, that's a great video and it has helped me understand a lot about Mutual Information as well as your other video about entropy. I do have a question. At 11:13 the answer you get after calculation is 0.5004 and it is explained that it is close to 0.5. However when I do the math (( 4   √∑   5 )   √ó   log ( 5   √∑   4 )   +   ( 1   √∑   5 )   √ó   log( 5 ) ) the answer I get is 0.217322... Am I missing something? Because from what I understood, the closer you get to 0.5, the better it is but it is not confirmed by my other examples. Is there a maximum to mutual information? Thank you for your video.",True
@smilefaxxe2557,2024-04-20T10:32:15Z,1,"Great explanation, thank you! ‚ù§üî•",True
@ian-haggerty,2024-04-20T09:16:36Z,0,"Seriously though, I think the KL divergence is worth a mention here.  Mutual information appears to be the KL divergence between the actual (empirically derived) joint probability mass function, and the (empirically derived) probability mass function assuming independence.  I know that's a lot of words, but my brain can't help seeing these relationships.",True
@ian-haggerty,2024-04-20T09:12:57Z,1,Entropy === The expectation of the surprise!!! I'll never look at this concept the same again ,True
@ruiqili1818,2024-04-19T05:02:12Z,0,Your explanations are alway awesome! I wonder how to explain Normalized Mutual Information?,True
@PunmasterSTP,2024-03-25T17:11:44Z,1,"Mutual information, clearly explained?  More like ""Magnificent demonstration, you deserve more fame!"" üëç",True
@sasha297603ha,2024-03-18T07:46:57Z,1,"Love it, thanks!",True
@Tufelkind,2024-03-15T21:50:07Z,1,It's like FoodWishes for stats,True
@Geneu97,2024-02-16T04:35:47Z,2,Thank you for being a content creator,True
@buckithed,2024-02-11T03:53:06Z,1,Fireüî•üî•üî•,True
@dhanrajm6537,2024-02-10T04:46:50Z,0,"hi, what will be the base of the logarithm when calculating entropy. I believe it was mentioned in the entropy video that for 2 outputs(yes/no or heads/tails) the base of the logarithm will be two. Is there any generalization to this statement?",True
@user-sn4ni3np8h,2024-02-07T11:10:01Z,2,"Two sigmas are like two for loops, such that, for every index of outer Sigma, the inner sigmaales a complete iteration.",True
@AI_Financier,2024-01-12T08:06:33Z,0,"3 more things: 1- it would have been great if you could make a comparison with correlation too here, 2- discuss the minimum and maximum value of the MI, 3- the intuition of this specific formula",True
@AI_Financier,2024-01-10T00:30:23Z,1,maybe next video on this: KL divergence,True
@BorisNVM,2024-01-03T06:38:11Z,1,this is cool,True
@noazamstein5795,2023-12-26T08:38:31Z,0,"is there a good and stable way to calculate mutual information for numeric variables *where the binning is not good*, e.g. highly skewed distributions where the middle bins are very different from the edge bins?",True
@adityaagrawal2397,2023-11-11T19:46:23Z,1,"Just started Learning ML, am assured now that the journey would be smooth with this channel",True
@Lynxdom,2023-10-17T22:29:50Z,1,You got a like just for the musical numbers!,True
@faizalrafi,2023-10-12T14:28:02Z,13,I am binge-watching this series. Very clear and concise explanations for every topics given in the most interesting way!,True
@9erik1,2023-09-19T15:21:09Z,1,"6:18 not small bam, big bam... thank you very much...",True
@user-yx5rj2jv2d,2023-09-11T15:53:30Z,1,awesome,True
@viranchivedpathak4231,2023-09-07T06:00:08Z,1,DOUBLE BAM!!,True
@archithiwrekar4021,2023-08-09T06:22:42Z,0,"Hey, so what if our dependent variable ( here, loves troll 2) is continuous? Can we use Mutual information in that case? by binning aren't we just converting it into a categorical variable?",True
@marahakermi-nt7lc,2023-07-25T13:54:50Z,0,thankss joshh üòçüòç in 1:30 since the response variable is not continuous and takes on 0 or 1(yes/no)  can we model it with logistic regression?,True
@raizen74,2023-07-15T13:33:22Z,1,Superb explanation! Your channel is great!,True
@isaacfernandez2243,2023-07-10T23:27:42Z,1,"Dude, you don't even know me, and I don't really know you either, but oh boyy, I fucking love you. Thank you. One day I will teach people just like you do.",True
@TommyMN,2023-06-14T15:52:07Z,0,"If I could I'd kiss you on the mouth, wish you did a whole playlist about data compression",True
@bernardtiongingsheng85,2023-06-14T05:28:13Z,1,Thank you so mcuh! It is really helpful. I really hope you can explain KL divergence in the next video.,True
@6nodder6,2023-06-12T00:04:50Z,0,"Is it weird that my prof. gave me the mutual information equation as one that uses entropy? We were given ""I(A; B) = H(B) - sum_b P(B = b) * H(A | B = b)"" with no mention of the equation you showed in this video",True
@yourfutureself4327,2023-06-03T02:07:37Z,1,i'm more of a 'Goblin 3: the frolicking' man myself,True
@VaibhaviDeo,2023-05-27T15:08:25Z,1,you are the best god sent really stay blessed,True
@KatanyaTrader,2023-03-13T02:28:49Z,1,"OMG i never see this channel, how many hours would be saveeddd.. new subs here, thanks alottt for ur vids",True
@arash2229,2023-03-03T22:20:18Z,1,Thank youuuu. you explain everything clearly,True
@murilopalomosebilla2999,2023-03-01T22:12:15Z,1,Excellent content as always!,True
@joshuasirusstara2044,2023-02-25T09:37:00Z,2,that small bam ,True
@AxDhan,2023-02-24T22:08:03Z,1,"small bam = ""bamsito""",True
@RaviPrakash-dz9fm,2023-02-20T13:29:19Z,0,Can we have videos about all the gazillion hypothesis tests available!!,True
@andrewdouglas9559,2023-02-16T23:00:18Z,0,It seems information gain (defined via entropy) and mutual information are the same thing?,True
@pranabsarmaiitm2487,2023-02-14T13:46:01Z,0,awesome!!! Now waiting for a video on Chi2 Test of Independence.,True
@stepavancouver,2023-02-13T04:32:23Z,1,An interesting explanation and nice sence of humor üëç,True
@romeo72899,2023-02-12T14:52:47Z,0,Can you please make a video on Latent Dirichlet Allocation,True
@wowZhenek,2023-02-11T09:11:42Z,0,"Josh, thank you for the awesome easily digestible video. One question. Is there any specific guideline about binning the continuous variable? I'm fairly certain that depending on how you split it (how many bins you choose and how spread they are) the result might be different.",True
@Fan-vk9gx,2023-02-10T14:46:55Z,7,"Super! I have been struggled between copula, mutual information, etc. for a while, that is exactly what I am looking for! Thank you, Josh! This video is really helpful!",True
@eltonsantos4724,2023-02-10T00:59:30Z,3,Que Top. Dublado em portugu√™s,True
@marcoventura9451,2023-02-09T22:03:35Z,2,"I have had bad times in the last two year, I abandoned StatQuest  and my stats / programming (R and Python) lessons and I missed  them very much. They call this depression. Josh Starmer is not only an amazing teacher but he can give us the will to be a part of a worldwide community. We owe so much to this clever and intelligent guy. Thank You.",True
@dragoncurveenthusiast,2023-02-09T13:18:25Z,2,Your explanations are awesome!,True
@zachchairez4568,2023-02-09T02:19:33Z,2,Great job!  Love it!,True
@Chuckmeister3,2023-02-08T15:51:14Z,0,What does it mean if mutual information is above 0.5? If 0.5 is perfectly shared information...,True
@aleksandartta,2023-02-07T21:06:49Z,1,1) based on what to choose the number of bins? Does larger number of bins gives lesser mutual information?  2) what if the label (output value) is numerical? Thank in advance,True
@MegaNightdude,2023-02-07T10:55:40Z,3,Great stuff. As always.,True
@666shemhamforash93,2023-02-06T18:47:27Z,1,Amazing as always! Any update on the transformer video?,True
@SelinDrawz,2023-02-06T16:32:56Z,38,Thank u daddy stat quest for carrying me through my university course <3,True
@kenmayer9334,2023-02-06T15:39:04Z,4,"Awesome stuff, Josh.  Thank you!",True
@rogerc23,2023-02-06T15:11:32Z,0,Ummm I know I have a cold right now but did anyone only hear an Italian girl speaking ?,True
@Maciek17PL,2023-02-06T14:58:18Z,1,Amazing as always!!!,True
@FREELEARNING,2023-02-06T14:52:55Z,0,"Great content. But just don't sing, you're not up to that.",True
@AlexanderYap,2023-02-06T10:59:17Z,0,"If I want to calculate the correlation between Likes Popcorn and Likes Troll 2, can I use something like Chi2? Similarly between Height bins and Likes Troll 2. What's the advantage of calculating the Mutual Information?",True
@felipevaldes7679,2023-02-06T07:08:05Z,1,I love this channel,True
@igorg4129,2023-02-06T06:03:46Z,1,"I was always interested how should we think if we want to invent such a technique. Imean ok, lets say I ""suspect""  that the probabilities here should do the job, and say my goal is to get at the end of a day some ""flag"" from 0 to 1 which indicates the strenght of a relationship,  but how should I think on,  to deside like what comes to denominator  vs nominator, when use log etc. There should be something like an ""thinking algorithm"" P.s  Understanding this will be very helpfull in understanding the existing fancy formulas",True
@GMD023,2023-02-06T05:15:14Z,1,Off topic question...but will chatgpt replace us as data scientists/analysts/ statisticians. I just discovered it tonight and it blew me away. I basically learned html and css in a day with it. Im worried it will massively reduce jobs in our field. I did a project that would normally take all day in a few minutes...scary stuff.,True
@pablovivas5234,2023-02-06T05:04:48Z,1,Keep it up. Great content,True
