author,updated_at,like_count,text,public
@statquest,2022-05-07T18:25:06Z,37,Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@short_math_talks,2024-05-24T07:14:14Z,1,BAM!,True
@JorgeDiaz-ej3bn,2024-05-23T19:54:47Z,0,We always get headüòè,True
@ekeswaritupakula2419,2024-05-19T13:52:58Z,0,if you are calculating for 3 inputs would the base value be changed to 3?,True
@miromanooov,2024-05-10T09:29:03Z,0,Why can't we just take 1-p as a surprise?,True
@subomiyusuf1917,2024-05-05T17:38:29Z,0,"Thanks for this, now I know exactly when I will be getting head",True
@inovariligabirshvil2724,2024-05-05T11:38:58Z,1,triple bam,True
@abdulazizalhaidari7665,2024-04-28T20:48:30Z,1,1 Trillion BAM! for this  Video,True
@schmetterling4477,2024-04-23T08:18:42Z,0,"That's all  cool, but it's not what physicists call ""entropy"". Entropy depends on temperature. What's the temperature of your data? ;-)",True
@viethoalam9958,2024-04-17T11:26:06Z,1,this is so smooth and easy to understand in connecting between ‚Äúsurprise‚Äù ‚Äî an emotion with a mathmatic theory - number.,True
@proggenius2024,2024-04-14T22:49:38Z,1,coool,True
@enjoyingcanvas8364,2024-04-13T07:00:49Z,1,Thank you for so far best explanation of Entropy,True
@danielbrockerttravel,2024-04-08T20:31:37Z,1,This is excellent. I absolutely love this.,True
@bushraw66,2024-04-02T16:10:24Z,1,I can't believe this guy made entropy fun and understandable. The intro song really lowered my anxiety about passing my exam thank you so much for your content,True
@eusha54,2024-03-30T18:12:53Z,1,You are really great man! You explained this so easily. Loved it!,True
@loandy6841,2024-03-28T09:46:58Z,0,I still quit not understand why we use log base 2 instead of log base 10....,True
@PunmasterSTP,2024-03-22T14:39:35Z,1,"I always found entropy to be quite a confusing concept, but this video handled it expertly!",True
@letsbimtogether,2024-03-21T02:06:27Z,1,i need all my math taught like this üòÖ,True
@akivaSk,2024-03-21T00:25:58Z,1,That was surprisingly easy to understand (thus high entropy I guess),True
@duffel87,2024-03-19T22:45:51Z,1,you are incredible,True
@r0cketRacoon,2024-03-14T14:27:55Z,0,does the entropy formula have the maximum value?,True
@user-es4xm2uh7u,2024-03-13T19:55:54Z,0,Please make a playlist on information theory.,True
@CHRISTIANCOLTRANE,2024-03-11T17:13:12Z,1,Wow this was so good! üéâ,True
@tejaronic,2024-03-10T19:26:29Z,0,"no singing please, you're doing great explanation which doesn't have to be at kids level, little be serious would be tremendous!",True
@mahmoudesmailfouad7736,2024-03-03T13:00:28Z,1,Extremely Awesome!! Thanks for the explanation,True
@dabestpilot4157,2024-03-02T13:29:01Z,1,"Wonderful video, sending this to my group thats supposed to be making a presentation on this next week.",True
@sammitiyadav6914,2024-02-29T07:12:44Z,1,"I follow most of your videos, not sure how I missed this gold! This is just the best entropy video I‚Äôve ever seen.",True
@SethuIyer95,2024-02-26T09:40:14Z,0,"Does this apply to multi class? Expected value of surprise of A when A,B,C are involved.",True
@ian-haggerty,2024-02-24T07:48:23Z,1,Genius explanation! üëè,True
@avivjan3402,2024-02-22T23:50:41Z,1,Great explanation,True
@dandavis5832,2024-02-21T22:00:51Z,0,"Just because I don't know what the hell network entropy is doesn't mean you have to take that condescending tone with me, mister.",True
@fiinanccewallah3535,2024-02-17T21:51:32Z,1,@statquest i am grateful to you thank you for teching,True
@bhargavapothakamuri4218,2024-02-15T14:41:51Z,1,Awesome !!,True
@raunak5344,2024-02-14T03:47:21Z,0,6:55 shouldn't the probability be multiplied by 3C1 = 3 ?,True
@taijingshen4503,2024-02-10T01:47:28Z,0,"Amazing derivation of Entropy with all the analogies used! Definitely no need for memorizing this formula anymore as it becomes an intuition right now! Wanna add what I‚Äôve read from countless of online materials elsewhere which they quoted that ->  Entropy is like the measure of uncertainty and disorderness in the dataset. The higher the disorderness (or surprise as stated by statsquest) , then the lower the energy available for doing useful ‚Äúwork‚Äù!",True
@mobinzarreh4788,2024-02-09T02:57:36Z,1,"just, wow...",True
@rozhindoroudi3514,2024-02-06T02:39:24Z,1,"Thanks a lot for the clear explanation (as always). I have a question, if we have multiple outcomes would the base of log change from 2 to number of outcomes? For example when calculating Entropy in multi-class classification, how would we determine the base of log?",True
@matthias2447,2024-02-02T13:16:45Z,1,Exceptional. Thanks!,True
@gregsmindsketches,2024-02-01T00:27:15Z,1,Omg I've been confused by this for months and it finally clicked for me!!!! This made it so obvious that i feel dumb for not getting it prior. üòÖ,True
@abdeenmohialdeen4898,2024-01-31T19:54:19Z,1,Very  Very  helpful üëè,True
@GregSom,2024-01-30T21:00:51Z,1,"Simple, elegant and funny. Just perfect. Thanks!",True
@elaprendiz0000,2024-01-29T06:24:04Z,0,Is Suprise = Perplexity?,True
@zavadoyb1596,2024-01-27T02:49:30Z,1,Best explanation I have ever seen.,True
@HDmoviemakerEz,2024-01-25T22:00:59Z,1,"""The next time you want to suprise someone, just whisper: The log of inverse of the probability"". I tried that with my girldfriend....  it doesn't work :(. instead of it, I had to explain it to her. Well, at least I learned it :D   Thank you!",True
@guliyevshahriyar,2024-01-25T12:30:58Z,1,This is phenomenal work for ENTIRE DATA SCIENCE! Thank you a loot.,True
@guliyevshahriyar,2024-01-25T12:29:59Z,1,Thanks,True
@chenmarkson7413,2024-01-21T02:38:06Z,1,You might like to know that I am sharing this video with my whole class of CSC311 Introduction to Machine Learning at the University of Toronto.  You are doing a phenomenal work in explaining concepts in such an intuitively understandable way! Hugest thanks!,True
@soumikbhattacharyya7443,2024-01-20T15:52:58Z,0,"Hi Josh, you took the base of the log as the number of outputs (also, you said it is just customary to use that). In a machine learning problem, let's say a simple yes/no classification problem, it should also be 2, right? Next question comes from the customary part, if someone loves to take 10 (or e, or anything) the maximum value of entropy will surely not be 1, right? I just wanted to ensure if I got these correct. Thanks for yet another precise,  entertaining explanation!! BAM!!",True
@felixlaw8377,2024-01-19T06:47:29Z,1,Being able to derive entropy and show it simply to us in a funny way is just mindblowing... Hats off to you sir!!,True
@shouryagoel6481,2024-01-10T18:38:18Z,1,If only StatSquatch had eyes .....,True
@michaelrockinger,2024-01-09T17:17:25Z,1,excellent video!! great explanation of a difficult concept.,True
@rzxxx,2024-01-05T15:33:57Z,1,best entropy explanation ever,True
@snehadutta2351,2024-01-02T13:07:09Z,1,amazing explanation,True
@TheWSPepe,2023-12-28T23:06:21Z,1,"great content, thank you",True
@davidmaslo7765,2023-12-28T10:46:50Z,0,"5:37 this is incorrect, since the logarithm function for the base larger than 1 looks different. Otherwise, great explanation.",True
@aaryan9058,2023-12-28T04:36:02Z,0,"Josh, Great video as always. But could you help me understand, how the entropy (which normally understood as a measure orderedness), is the expected value for surprise?",True
@1243576891,2023-12-27T01:09:52Z,1,I like the idea of surprise.,True
@thers9297,2023-12-26T10:49:53Z,1,You might be the greatest teacher on youtube,True
@New_Hercules,2023-12-23T16:32:45Z,0,"Regarding the case where getting tails has a probability of zero. The log formula says that the probability is indeterminate. However, probability describes our interpretation of the world, and not the actual state of the wolrd (like the difference between the earth and a map of the earth). Considering this, it may be that our interpretation of the world is incorrect and we don't know it. This implies that although we may think there is zero chance of flipping a coin to get tails, it is still possible. And if this happened, it would be really surprising! So our function shouldn't be indeterminate, but rather shoot to a very large number! To be more precise, the more confidence we have in our model, the more surprised we should be if it turns out to be wrong!",True
@user-km5oy5sl3d,2023-12-16T00:11:38Z,0,I can not see the man who discovered entropy when he hears you call it surprise.,True
@Stilzel,2023-12-15T17:33:00Z,1,"Josh, thank you so much for your videos, you are a GREAT teacher. I wish you all the best, and thank you again!",True
@vaibhavnakrani2983,2023-12-10T05:00:40Z,1,I bow down to you sir. It was truly amazing in a simple way.,True
@twishanuaichroy1938,2023-12-05T07:35:27Z,1,I cant believe a fool like me understood this topic so easily because of the way you taught. Thanks a lot man.,True
@kuchuksary,2023-11-24T14:57:15Z,1,This is the beeeeeeeeeest explanation of entropy! THANK YOU!!,True
@MightyGrom3611,2023-11-23T21:07:45Z,1,"Great explanation, Super BAM!",True
@user-xj1pr3pn1p,2023-11-17T09:33:16Z,0,"thanks very much, it's useful a lot!",True
@AdrianBoyko,2023-11-10T19:14:06Z,1,Does this work for other kinds of things or only for chickens? ü§î,True
@amirevil666hasani6,2023-11-09T18:15:15Z,1,God bless you and your family,True
@amirparsi4165,2023-11-02T10:42:08Z,1,"wow, now i can make small talk in surprise parties.  Amazing explanation üëè",True
@Melki,2023-11-01T02:22:56Z,1,Thank you :),True
@tomasalmeida5306,2023-10-31T09:45:26Z,1,"My surprise after getting one heads is always 100%       Great video, very helpful, thanks!",True
@gustavorm5686,2023-10-29T15:15:39Z,3,"the best explanation for entropy i saw, after browsing for tens  of videos. well done prof!!",True
@redoktopus3047,2023-10-23T19:24:30Z,1,Statistician A: Our team ran the number and what we got really surprised us! Statistician B: So your initial model was wrong? A: no! It was spot on! We never get our predicted surprise so we usually predict that we'll be surprised and are never surprised when it happens. B: But this time? A: This time we got exactly what we expected! B: I'm shocked!,True
@wat_wut,2023-10-21T15:36:46Z,0,"Thanks for the video! I would also like to ask, is there an explanation for cross entropy in terms of probability and surprise, like what you showed in the video? I could only find explanations talking about information and transmitting messages between probability distributions :(",True
@bashiransari6258,2023-10-21T11:33:58Z,1,just awesome,True
@nathanisong9393,2023-10-20T00:33:31Z,0,"Cool. And please if i may ask, what's the significance explanation of getting higher average surprise (entropy = 1) than in a case of where we have bigger surprise of picking one of 7 chickens... I hope I didn't mix up",True
@ahmadkhalil94,2023-10-19T11:43:58Z,1,"Sir, you are calculating the probabilities incorrectly. Please make sure that you don't publish false information. Thanks...",True
@shikharmishra1,2023-10-18T05:23:12Z,0,Hi!!! Your videos are a savior in my master's level class in neural networks. Do you have a video on Relative Entropy i.e. the Kullback Lieblar Divergence ? I tried looking for it and couldn't find it. Could you make one please !,True
@itsmalay,2023-10-14T17:11:23Z,0,"Is it okey if i explain your ideas into a different language, so that others can learn also ? I am not gonna copy your video but the lesson and idea will be relavent",True
@khanhhuyen8630,2023-10-14T07:47:26Z,1,log(1/probability),True
@aryanafraz,2023-10-10T15:10:03Z,1,I barely comment on a video on YouTube BUT SERIOUSLY your whole channel is the best channel I have ever seen on Youtube.,True
@noradlark167,2023-10-08T14:47:47Z,0,The log of the inverse of the probability motherf--er!!!,True
@Kerem-Ertem,2023-10-04T21:23:47Z,1,Entropy is kind of complicated especially in data science. And that explanation was pure. Thanks!,True
@saidisha6199,2023-10-04T13:33:58Z,5,"One of the best explanations of entropy. I had been struggling for a while with this concept and there was no intuitive way I could understand and remember the formula so far, your video made it possible. Great video!",True
@NesmaAlmoazamy,2023-10-02T04:51:56Z,1,"Excellent, thanks for the straightforward and extraordinary explanations.",True
@muhammetaliaykanat608,2023-09-29T09:50:32Z,1,I love it man. You are the only guy that I make comment on his videos. It was amazing explanation.  Have best log(1/probability) for your all birthdaysüòÇ,True
@alinazem6662,2023-09-28T05:04:51Z,1,Your videos are as straightforward as Y=mX. thanks Josh.,True
@user-cc6ro9kv1k,2023-09-18T20:18:32Z,3,"This is such a fascinating video, learning the theory of ML and I can certainly say you are a gifted person. Your perfect understanding field of Probability, Math and ML gives ability to explain it in the best way in the entire world. I'm amazed with your explanation skills",True
@RaRdEvA,2023-09-17T02:46:23Z,1,you should have a monument to how cool you explain it. I love you.,True
@benjaminkleppe,2023-09-13T12:18:10Z,1,"Amazing explanation, thank you!",True
@sidd1454,2023-09-10T04:50:55Z,1,This has got to be the best video made in the history of YouTube for me. I dont care about others.,True
@BeginnerVille,2023-09-08T16:25:33Z,1,"I have search for hours, the only one I ""feel"" I get it.",True
@anupbarua6151,2023-09-06T18:22:20Z,1,i love entrophy,True
@questwithshahzad3058,2023-09-06T17:42:26Z,1,i wish i had internet and online school along with youtube videos like these,True
@viranchivedpathak4231,2023-09-06T16:51:09Z,2,Bouble Dam!!,True
@whimsicalvibes,2023-09-04T13:15:30Z,1,Fantastic Video. Very helpful.,True
@dylansatow3315,2023-09-03T17:35:56Z,3,Wow this was amazing. I've never seen entropy explained this clearly before,True
@andrewnguyen3312,2023-08-26T22:08:14Z,1,Wow‚Ä¶.,True
@nirmithrjain6265,2023-08-20T07:44:35Z,1,"Seriously, you are the best teacher I have ever had",True
@hassantamer4385,2023-08-15T21:55:57Z,1,BAAAAAAAAM,True
@user-su7id4xe2d,2023-08-14T04:23:16Z,1,The most complete video I've ever seen on this platform : ) I subscribedüòé,True
@-Spirit,2023-08-13T22:04:07Z,1,"""Look at all those chickens"" -that one meme",True
@user-fw5zr7uc2c,2023-08-10T19:59:37Z,1,"perfection, thank you !",True
@ankitchakraborty4552,2023-08-10T15:25:26Z,1,Arguably the best mathematics youtuber in our generation,True
@r.i.p.volodya,2023-08-07T18:18:12Z,0,"01:00 If the spelling is correct then ""Leibler"" is pronounced LYE-blur not LEE-blur.",True
@DanishArchive,2023-08-05T13:28:45Z,2,"I was awestruck when I finally understood what on earth Entropy is. In most algorithms, I hear entropy must be less, and I felt that it is some weird value which the model gives, and we have to tune it to reduce it. But now, sitting here, watching this video felt like an eye-opener. What a simplistic and beautiful way to explain complicated concepts. You truly are amazing, Josh!!! Super BAMMM!!",True
@prateekyadav7679,2023-08-05T07:34:14Z,0,But why do we study entropy at all? How can it be interpreted? What does it mean to have high or low entropy?,True
@hemanthhariharan5105,2023-08-03T21:38:24Z,3,I'm truly amazed by the power of simplicity and intuition. Hats off Josh!,True
@AdityaVerma-eo5xi,2023-08-02T17:46:41Z,0,"So in simple terms, entropy just tells how well balanced our data is.",True
@storiesbydeeba7364,2023-07-28T00:41:27Z,1,Amazing Explanation !!!!!,True
@benpeyton,2023-07-25T19:00:34Z,1,Incredibly well done! BAM!,True
@thegt,2023-07-25T00:45:23Z,1,Simply amazing... I have been using CrossEntropy for months and only now I understood where the word Entropy came from in CrossEntropy,True
@anjaliranolia4066,2023-07-24T15:05:09Z,1,best video i saw ... woaahhhh hats off,True
@michalstrnad7685,2023-07-22T16:43:18Z,0,"Probably the best explanation I've seen. But there still is one little thing that I find unintuitive. Why do we use a logarithm?   To me, the intuitive thing to do is just (1/probability) - 1. Values closer to 0 become large, values closer to 1 become close to 0. Assuming that the probability can only be between 0 and 1, then when you graph the two separate functions (log_2(1/probability) and (1/probability) - 1)) you get similar-looking graphs for these values. Yes, they are not the same, but they are similar. So what makes the log a better solution?",True
@sushilkhadka8069,2023-07-20T18:16:41Z,1,"If only university taught us this way, every kid would be willing to learn STEM.  Instead of idolizing actors and singers, scientists would have been our celebrities. Always a fan of your work.  BAAM !!!!",True
@minhuc-08tran90,2023-07-19T19:09:53Z,1,"What a useful video, I really like the layout and content of the video. It's easy to understand. This channel is really cool ‚ù§‚ù§",True
@NickMirro,2023-07-18T04:24:11Z,0,"Great and very educational video, however, I was blindsided by that make America white again ad üòµ‚Äçüí´",True
@tlijaniraed1599,2023-07-18T00:30:43Z,0,"very informative video thx josh so much ,  what is the difference between entropy and KL divergence ?",True
@doctorwilly,2023-07-17T17:37:55Z,1,BAM! I am starting to get addicted to ur videos,True
@_BhagavadGita,2023-07-07T19:10:51Z,1,superb,True
@lkd982,2023-07-05T09:45:21Z,0,Very annoying vocal style,True
@NeoZondix,2023-07-04T19:29:35Z,0,Stat quest is the only place where I get heads.  This joke was brought to you by heads.,True
@lucasliu2116,2023-07-04T08:48:35Z,1,i love ur video but the last whisper thing is so silly hahaha,True
@varuntejkasula748,2023-07-01T16:29:17Z,1,absolutely clear. Can't expect a more clearer explanation than this,True
@amnont8724,2023-07-01T12:48:54Z,0,"6:01 Hey Josh, if I want to calculate the entropy given that there are 3 possible outcomes (In your example, you assumed there are 2 possible outcomes), so I should use the same formula to calculate the entropy, but with the base 3 instead of base 2?",True
@ayomideoyekanmi3563,2023-06-27T13:04:34Z,0,"Hi Josh, thanks for this video!  I have a question from 9:01:  Why is the surprise for 100 flips of head(x), not 0,15 x 100 only? If surprise for one flip is gotten by log(1 / p(x)), then doesn't that means to get the surprise for 100 flips, you just multiply log(1 / p(x)) by 100?",True
@areebahmad7012,2023-06-19T13:37:30Z,1,"This is absolutely amazing, I studied my whole course of Probability and statistics at my University but there was so much chaos. Now As I am learning machine learning this makes me a lot clear.",True
@Bjorn_R,2023-06-18T11:08:35Z,0,"Hello Josh I am currently reading on entropy for my exam in datamining for you guessed it.. Data science. I discovered that there are multiple versions of entropy  . My teacher covered entropy, binary entropym n-ary entropy. and quite frankly I am confused. Seems like you cover them all here... with just one equation..  I really like your explanation. Is it just the same approach with 3 bit entropy? Do I just add a third probability to the mix and follow the recipy?",True
@ycombinator765,2023-06-11T13:17:36Z,2,What the actual damn hell man. This is jaw dropping,True
@ab-zg6hn,2023-05-23T10:18:56Z,0,Why can't we use just 1-px?,True
@arpitanand4693,2023-05-20T16:01:14Z,0,"Does this mean that any experiment with unbiased outcomes where the probability of all occurrences of outcomes are the same (eg-picking a card of a particular suit from a regular deck) , should always have a surprise of 1?",True
@bachxuanquang2837,2023-05-20T12:37:09Z,0,"Sorry everyone I don't get the part 0.9 x 100 x 0.15 + 0.1 x 100 x 3.32 ...If we want to know the level of surprise we get for flipping the coin 100 times, why don't we just take log base 2 of 90, which is only 6.492, and similarly log base 2 of 10, which is 3.322, then add them up to get 9.8 ..., which is much lower than 46.7?",True
@thuytrantruong1996,2023-05-19T07:41:23Z,1,The best explaination I've ever seen,True
@shivam13juna,2023-05-18T11:51:06Z,2,this was a surprise!!,True
@orhan4876,2023-05-18T10:03:11Z,1,best,True
@gelamegeneishvili7240,2023-05-13T15:03:13Z,0,"To be fair, getting two heads in a row is a surprise",True
@maddy2u,2023-05-08T09:17:59Z,2,Amazing explanation ‚ù§,True
@jsebdev1539,2023-05-06T04:08:13Z,1,I'm so happy these channel exists! hurray!!!,True
@patbentolilarhythmking,2023-05-05T07:07:11Z,1,"Enjoyable, interesting!",True
@trsshowstopper9518,2023-04-28T20:53:43Z,0,Still don't understand why the log function was chose and no other.,True
@asdfafafdasfasdfs,2023-04-28T07:16:14Z,0,Why does the formula use base 2 for the log?,True
@legendr7103,2023-04-22T12:54:49Z,1,üêêüêêüêêüêê,True
@adirozeri7162,2023-04-21T06:24:31Z,0,"Wow I just love your videos!!! Gotta say tho there a small jump when you move from calculation a specific surprise and when calculating expected surprise on 8:10 approx. thanks!!!  Also, aren‚Äôt you doing entropy there for 200 coin tosses? ü§î  When you calculate the chickens entropy you do it for each of the samples but for the coin it was for all probable samples. Is it the same? What am I missing?",True
@makwelewishbert555,2023-04-14T11:03:56Z,0,"@6:11 why use log base 2, what is the reasoning behind?",True
@yasameenmohammed4366,2023-03-31T08:22:47Z,1,infinity BAM üòµüíØ,True
@jaskaransingh0304,2023-03-29T15:01:51Z,1,Great Explanation!,True
@wenkang5185,2023-03-29T12:34:05Z,0,"thanks for your explanation, actually  I am not understanding why is using log, why doesn‚Äôt using other functions like ln()üòÇ",True
@junoverena2897,2023-03-19T22:42:43Z,1,this is SO CLEAR,True
@davidfield5295,2023-03-14T19:27:11Z,1,Awesome explanation!,True
@adzictanja,2023-03-14T16:52:24Z,1,"Next time someone asks me how's it going, I'll just say ""low log of the inverse of probability""",True
@loay1844,2023-03-08T21:41:59Z,4,"Wow im sooo impressed. Frrr! It‚Äôs been a week trying to understand entropy and I rly thought I was never going to understand this bs. This video is arguably the best video on YouTube! Not only about entropy, but absolutely!! Thank you soo much",True
@yakubumshelia1668,2023-03-05T15:47:00Z,1,simple and smart. 2 hrs lecture in 15mins BAMMM!!!!,True
@pingmelody8609,2023-03-01T18:05:20Z,1,"You are so talented! I'm so thankful Youtube recommendation system guided me into your videos, it's a whole new world!Every data scientists should watch your videos!!!! bam!!",True
@narekaghekyan2707,2023-03-01T08:22:17Z,0,"When we talk about the surprise, we say how surprise we will be to see a Head if we throw a coin with 0.9 of Head and 0.1 of Tail probabilities. That means in order to talk about the surprise we need to know the probabilities and also we need to have an exception (how much we expect to see a Head), because if we are asking how surprised we are to get a Tail the number will be different.   Now I don't understand what does it mean the average surprise? https://youtu.be/YtebGVx-Fxw?t=579 There is no expectation here? Average surprise of getting what - Head, Tail or what?",True
@nivethanyogarajah1493,2023-02-26T12:15:09Z,1,Incredible!,True
@user-lw8sq1gb7e,2023-02-25T10:04:31Z,1,Amazing! Thanks for putting this together!,True
@imad_uddin,2023-02-20T20:14:14Z,1,Thanks!,True
@tigerspidey123,2023-02-20T13:03:16Z,0,may I ask why we are taking log base 2? is it because it is binomial ?,True
@aljj6140,2023-02-19T09:48:29Z,1,"bammmm! great job, I enjoyed the video!",True
@user-co6pu8zv3v,2023-02-15T16:58:58Z,1,Thanl you for your explanation! It is very intuitive,True
@erickgomez7775,2023-02-13T15:29:51Z,0,Hey! Statsquatch! Leave the chickens alone. Go vegan or sm.,True
@shamshersingh9680,2023-02-10T02:22:40Z,3,How can it be!! How can you simplify such complex topics into such simple explanations. Hats Off Man. I seriously wish if I could have had a Maths teacher like you back in school. I have become fan of your videos. Your videos are the first and last stop solution for all my doubts. Thanks Josh. You are a boon to learners like us. Impressed.,True
@0x90meansnop8,2023-02-05T21:58:49Z,1,My suprise for getting heads is currently over 9000.,True
@Raven-bi3xn,2023-02-05T15:51:22Z,2,"As always, amazing video, Josh. I was hoping to the end that it‚Äôll be mentioned that ‚ÄúSurprise = Information‚Äù and the unit of it is bits. As in, if the probability of getting 0 and 1 in a binary message is equally 0.5, each binary has exactly 1 bit surprise/information in it, and that‚Äôs where ‚Äúbits‚Äù originated from. Maybe something to possibly fit in if you ever wanted to make a video about cross entropy or KL divergence as cross entropy = entropy + KL divergence.",True
@mohammedabdullahifteqar7838,2023-02-05T12:01:15Z,1,I would have liked it more than once If I could. Thank you very much for making these videos,True
@leesweets4110,2023-02-04T17:38:19Z,0,"5:30 The probability of something can be zero but it can still happen.  Take for example the probability of an infinitesimally thin dart hitting a specific point on a circular dart board. It is zero. But there is still a 100% chance of hitting some point in the circle, so no specific point is impossible.  If you want surprise to be zero at x=1 then why not just downshift?  S = (1/x) - 1. This achieves the same goal without the logarithm.  Im sure there are more complicated reasons for using the logarithm you did/nt go into (besides computational convenience, I mean), but surely an argument can be made that multiple conceivable definitions of entropy can be put forward, yes?  6:05 In general, what is the appropriate base of the logarithm?  Suppose I were computing the entropy of the probability distribution associated with a set of sample data that was collected empirically. Why would I use base 2 over the natural? Does it matter, is it relevant at all?  Its often implicit, but the logarithm does introduce another variable, the base, which many gloss over.",True
@vinodraman2423,2023-01-30T15:02:11Z,1,When you understand the concept just by looking at the 1/3rd of the video!,True
@luisbielmillan8467,2023-01-28T11:29:56Z,1,WOW,True
@zhelin927,2023-01-17T07:42:45Z,1,This is really really really sooooooo amazing!!!!!! THANK YOU SOOOO MUCH!,True
@yalcinbulut8948,2023-01-14T20:24:16Z,0,"Thanks for the great intuitive explanation. As far as I understand, surprise can be thought of as instantaneous velocity, while entropy is equivalent to the average velocity.",True
@dohasan5053,2023-01-12T21:14:33Z,0,can you please do a video on the Maximum entropy bootstrapping method please üôè,True
@dohasan5053,2023-01-12T11:14:39Z,1,Hello! your videos are amazing and you do a great job in simplifying such difficult concepts and making them easier to understand. Thank you so much.,True
@data_kai,2023-01-06T16:39:01Z,1,"Dear Josh! Unfortunately, the following video is blocked in Germany :( Could you please have a look into it? Thanks so much for your extremely hard work! <3 ""The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.)""",True
@ryanperkins1525,2022-12-26T04:03:29Z,0,is Surprise log2 or log?  at 11:58 you seem to change them?,True
@TheParkitny,2022-12-25T08:53:00Z,2,If only text books explained things this way. Life would've been easier as an undergrad.,True
@edip_c,2022-12-22T13:09:55Z,0,üôá‚Äç‚ôÇÔ∏è,True
@mohammadidreesbhat1109,2022-12-22T10:47:41Z,0,Probability of tossing a coin three times and getting HHT is 1/8 = 0.125 How come it is 0.9 x 0.9 x 0.1,True
@alphatradingvol,2022-12-20T17:14:37Z,0,Entropy is one of the greatest piece of information when used properly for certain things.,True
@TShrimp,2022-12-19T13:06:23Z,1,That explanation was so freaking awsome.,True
@pietronickl8779,2022-12-19T09:50:13Z,1,"Thanks for these super clear explanations ‚Äì you really manage to break down complex concepts till they seem simple and (almost) intuitive. Also really appreciate the pace, ie the patience of going step-by-step and not making any crazy leaps 2 mins to the end üëèüëèüëè",True
@TheArrogan,2022-12-18T23:24:27Z,0,This only explains the formula on the paper. It helps you memorize the formula. Unfortunately it gives no intuition entropy reveals.  I appreciate you can talk more about ‚Äúsurprise‚Äù.,True
@itugamayotaner4036,2022-12-18T00:00:19Z,1,"""Calculate the Entropy of the chicken"" üòÅ Who knew you could have fun and learn at the same time as an adult?  I love this channel.",True
@qZoneful,2022-12-17T16:50:13Z,1,"I am a biology student who is trying to understand the entropy concept for my species distribution models, and I choose to believe that this video uploaded from heaven with the consensus of all the passed statisticians",True
@johnlu7655,2022-12-17T02:50:08Z,0,I'm a bit confused. I think 0.9*0.9*0.1 is not the possibility of getting two heads and one tails: it's the possibility of getting just the last one tail. Isn't the following deduction built on this?,True
@leassis91,2022-12-12T11:52:57Z,1,"this is gold, bro. one of your best videos",True
@herojedrus2567,2022-12-10T13:16:44Z,1,we need to understand what surprise is.... so lets talk about chickens xd that made me laugh so hard,True
@veeek8,2022-12-08T15:25:57Z,1,Thanks,True
@conan-thelastbarbender,2022-12-04T19:15:17Z,1,Triple Bam indeed Josh!,True
@shaporovanatalia6805,2022-12-01T21:20:23Z,1,amazing ! thank you!,True
@brokenmachine9207,2022-11-29T05:56:30Z,1,BAM üî•üî•,True
@mikeshinoda2806,2022-11-23T06:12:48Z,1,slow video but good content!,True
@Arm-if2yz,2022-11-21T16:35:37Z,1,Less than 20 minutes to well-understand a topic than i took hours to vaguely understand,True
@whatitmeans,2022-11-20T07:25:34Z,1,"Amazing video... but the reason of the use of logarithms is much deeper, related with the meaning of what information really is. I highly recomend you to read the paper ""Transmission of Information"" (1927) by R. V. L. Hartley",True
@SowrabhAdiga,2022-11-12T09:53:17Z,1,You sir are an ideal math teacher !!,True
@sade922,2022-11-11T16:39:07Z,52,9 minutes of your video explained everything better than 2 hours of my professor giving a lecture... Thank you!!!,True
@danielpaul65,2022-11-09T07:09:08Z,2,Starting the video with a message declaring that we can understand Entropy is the best starting line I have ever seen from any teacher in my life. Great work!!!,True
@veenapanini2253,2022-11-08T10:16:07Z,1,Thanks a ton!,True
@sofilove...20,2022-11-06T10:04:22Z,1,Thanks a lot..I mostly fall in lovE with log(s):)(:,True
@jnico7,2022-11-01T13:22:15Z,1,Very easy to understand thanks you !!!,True
@amourzombie,2022-10-31T19:45:05Z,1,Triple BAM!!!,True
@sandeepmishra3275,2022-10-31T10:37:01Z,0,Do we have to take the modulus of surprise as I think you did not take the negative value for consideration and just wanted to ask what does entropy actually say.,True
@randomyoutubeaccount6906,2022-10-26T17:23:50Z,0,How do you do this with a calculator?,True
@randomyoutubeaccount6906,2022-10-26T17:01:02Z,0,when/how is Log used? like the calculator button? I got kind of lost after calculating for S,True
@_________________404,2022-10-25T22:58:10Z,1,Finally something that makes sense. The theoretical CS university course only showed the equations without trying to explain what they mean.,True
@korkutkaynardag9147,2022-10-25T18:35:03Z,1,"after my parents, I love you the most in the world.",True
@johnyf.q.8043,2022-10-22T14:29:13Z,0,"I've got to say, this is a good video (upvoted) but I definitely feel the need to add a note. This example of n=3 coin tosses must be taken for the specific outcome of one head, followed by one head, followed by one Tail, P(HHT) = .9*.9*.1, because the probability of having two tails and one head is of 0.243 (or 0.081*3) which can be confirmed by a binomial distribution or a tree diagram. Maybe it was just me, but I had some issues watching the video the first time.  If I got the point made in the video correclty, the P(2H, 1T) would have an entropy equal to log_{2}(1 / 0.243) ‚âà 2.041.",True
@duongphanai7094,2022-10-22T10:35:43Z,1,15:47 LOL,True
@nagainu,2022-10-21T15:15:52Z,1,First time i see a medium bam and it was amazing,True
@bagavathypriya4628,2022-10-20T14:57:37Z,3,You are the BEST teacher !! Thanking God that you exists.,True
@yakubsadlilseyam5166,2022-10-19T17:23:48Z,1,It's amazing 100% clear,True
@_justinxu,2022-10-18T08:26:43Z,0,I don't expect this ,True
@LaranjaAcid,2022-10-16T14:08:44Z,1,Valeu!,True
@hosseineyvazi3122,2022-10-16T10:14:00Z,1,"great , bro",True
@okohmiracle1002,2022-10-15T19:23:14Z,0,"Hello Josh, this is an amazing video. Thank you very much. please, how did you arrive at 0.9? Do you have a video on probability? I would like to learn how you split the 0.9, 0.9, and 0.1 on the head, head, and tail.",True
@rafirafi5919,2022-10-11T13:56:06Z,1,Very Well Explained,True
@amisadaisanchez1897,2022-10-10T01:58:33Z,1,"Best video ever, never saw entropy that way.",True
@master_zenrade,2022-10-08T14:19:52Z,1,Aware! So much surprises can give you heart attack.,True
@jaysonl3685,2022-10-07T02:07:10Z,2,"Absolutely amazing and intuitive explanation Josh! I couldn't have understood it without you, huge thanks :D",True
@_bohemian2778,2022-09-29T00:55:14Z,1,Nice ‚ù§,True
@sjungbae,2022-09-28T04:22:33Z,1,ÏòÅÏñ¥ ÏÑ§Î™ÖÏù¥ÏßÄÎßå Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÎÑ§Ïöî.. Í≥†ÎßôÏäµÎãàÎã§.,True
@baharehbehrooziasl9517,2022-09-26T19:42:30Z,0,Thank you for the very clear explanation as always. What is the application of this concept in machine learning?,True
@devrus265,2022-09-26T14:55:13Z,1,This is by far the best explanation I heard on entropy.,True
@sanjaykrish8719,2022-09-25T13:59:12Z,1,Thanks!,True
@docteurlambda,2022-09-23T17:46:43Z,1,BAM,True
@phucnguyenthanh9223,2022-09-23T03:53:52Z,1,Better than my master course. Thank you!,True
@qingfangliu4687,2022-09-21T21:11:04Z,0,Do you have a recommendation for understanding mutual information and KL-divergence along this way?,True
@chetankumarnaik9293,2022-09-18T03:31:34Z,0,Why can't we just use 1-p to calculate surprise why take the inverse,True
@salahamani9619,2022-09-12T14:27:15Z,1,Thank you  for the amazing explanation !,True
@ryanyu512,2022-09-10T02:57:54Z,1,"Really thanks for your crystal clear explanation! When I need to learn something about statistics/machine learning, I search ""xyz, statquest"" in the first place!",True
@Ali-wf9ef,2022-09-05T15:20:25Z,0,"Such a good explanation. So entropy shows how balanced a distribution of data is, if we have same amount of data from each value the entropy is 1 and it decreases when there is imbalance between amount of each value correct?",True
@user-pz9bg4sb7n,2022-09-05T10:47:37Z,1,"really thank you , that was too helpful‚ù§‚ù§‚ú®‚ú®",True
@teodorskerimovs4300,2022-09-04T20:27:55Z,1,Insanely good,True
@crampedspacebrew,2022-09-02T20:10:58Z,1,Your videos rule. That is all.,True
@nicolasparis7518,2022-09-02T13:01:14Z,1,"What a freaking genius man. If we'd all had such explanations from school, Elon Musk would be just a normal guy ;)üòâ",True
@lowerlowerhk,2022-08-28T14:07:25Z,1,Excellent explanation ane the jokes are so lame that it's funny!,True
@jacobmoore8734,2022-08-14T20:34:12Z,0,"For continuous variables, if entropy is still an appropriate idea, I assume we use likelihood (because P(X=x) => 0). I imagine that a monte carlo simulation is easier than analytically evaluating the integral.",True
@this-is-bioman,2022-08-14T12:10:51Z,0,"Let's say I get it, but why do we need it in machine learning and how does entropy helps us with anything?",True
@jonascoelho,2022-08-13T03:28:11Z,1,It would take me years to understand this if it were not for this video,True
@ninjackk,2022-08-12T08:19:33Z,1,Super great ! thanks a lot for your video !,True
@achyuthnandikotkur5647,2022-08-11T09:28:38Z,1,BAM!,True
@vunguyenthai4366,2022-08-11T06:18:41Z,1,"Nice video, thank you for your contribution.",True
@ashutoshpanigrahy7326,2022-08-09T06:19:55Z,1,WoW!,True
@lauro199471,2022-08-08T03:24:34Z,0,I know this sounds silly. But why do we multiply expected value and surprise instead of adding them together? What is the intuition of when to add or multiply values?,True
@ralphchien184,2022-08-03T09:07:39Z,1,This is the most excellent explaination that I have ever seen. Impressive! Impressive! Impressive! Three times I must give. Thanks a lot!,True
@rangjungyeshe,2022-08-02T20:05:38Z,1,"Fantastically clear explanation of a notoriously tricky subject. Apparently Johnny Von Neumann told Shannon to call his measure of information entropy, since ‚Äúno one really knows what entropy is, so in a debate you will always have the advantage.‚Äù I suspect J V N wouldn't have said that if he's known about your video...",True
@namesurname1040,2022-08-01T23:35:51Z,1,It was just amazing .Thank you for that video!It really helped me.,True
@tamirrozenfeld3572,2022-08-01T06:13:22Z,1,thank you !,True
@bimoindracahya1124,2022-07-31T08:46:16Z,1,love the intro,True
@alialthiab7527,2022-07-27T22:19:50Z,1,You are awesome. I finally understood the entropy concept without any equations. Big loveüòç,True
@Akshay-vq1uv,2022-07-27T16:52:07Z,1,Thanks!,True
@Akshay-vq1uv,2022-07-27T16:49:04Z,1,please turn on donations from google pay! You have helped me more than my teacher ever tried,True
@ignacio-urbina,2022-07-24T21:49:54Z,1,This is great. Thank you!,True
@user-pk6pd8oy2o,2022-07-22T17:57:25Z,1,Best explanation ever,True
@nossonweissman,2022-07-22T02:46:33Z,0,"Correction: 6:43, I think you mean to say ""P(HHT)"" and not ""two heads and a tail"", which is equivalent to ""P(HHT) + P(HTH) + P(THH)""",True
@litttlemooncream5049,2022-07-20T09:00:54Z,1,chickens yyds,True
@fichardreynman4944,2022-07-19T16:46:44Z,1,"Just, SO GOOD.",True
@saichandsharma4162,2022-07-16T18:38:54Z,0,"What is the use of entropy? If entropy is used to check the imbalance of classes in dataset, then why can't we just calculate the count of examples in each class?",True
@arieljiang8198,2022-07-15T19:54:54Z,1,THE BEST video about entropy,True
@arieljiang8198,2022-07-15T19:53:31Z,1,StatSquash is sooo cute,True
@trashantrathore4995,2022-07-14T18:40:47Z,1,"This Content is GEM to any learner studying these topics, Thank u Whole Team of ""STATQUEST with Josh Starmer"" for making such a Great channel and awesome videos that have explanation of what even a 10th Class student can easily Understand. üôåüëè",True
@Ra-pt6tx,2022-07-14T07:44:41Z,1,So intresting,True
@uchungnguyen1474,2022-07-10T17:46:24Z,1,Bam!!! this guy know how stupid i am to teach me that clear!!!!,True
@dimpap9659,2022-07-10T14:59:29Z,0,Now this also makes perfect sense https://www.youtube.com/watch?v=NsLKQTh-Bqo&ab_channel=rawuncutvines,True
@adirozeri7162,2022-07-03T06:23:28Z,0,Thank you for this amazing explanation! I‚Äôve been coming back to statsquest for the past five years now and it is amazing! thank you so much but I do have one question tho.  At 09:00 -‚ÄúNow we can add the two terms together to find out the total surprise.‚Äù  I‚Äôm trying to understand if you are trying to calculate the surprise for getting tales 100 times out of 200 times of coin flip. In this case shouldn‚Äôt you divide by 200?,True
@mackr1581,2022-07-03T05:55:35Z,1,Your contents are awesome. Hope to finish all your contents in ML playlist. Will comment once again when I finish this playlist.,True
@marioadiez,2022-07-03T00:36:21Z,1,"I understand, so will buy my own guitar, and compose a beautiful Hooray song!",True
@gianlucalepiscopia3123,2022-06-30T16:41:11Z,1,"The name is Josh, Saint Josh",True
@mohammedlabeeb,2022-06-29T08:06:27Z,1,Really Great video. Right to the point. I met with one of my coworker who is very seasoned in Data science to help me work on a project and use entropy for the first time. After one hour I was as confused as I could be. But this video really helped. I wish if I saw this video before I had my meeting.,True
@AndreiSAA,2022-06-26T13:37:15Z,1,Amazing explanation! Bam! THANK YOU SO MUCH!,True
@Regular.Biceps,2022-06-25T11:10:34Z,0,1. How to determine the probability of an event before punching that into the formula?  2. What if the probability that we are using is a continuous variable? It changes with the change in dynamics of the system ?,True
@nikhilgupta4859,2022-06-24T18:13:06Z,1,"Heyy Sir, I am your subscriber from past 1.5 year and I feel honoured to tell you, after following you I finally got a job transition as a senior data scientist at an MNC 6 month back. Now I have understood the datascience project ecosystem in my company. You are one of the contributors for my success. Thanks a Ton!!!!!  Also I would like to open my hands for helping learners. So learners you can tag me asking any doubts. I would be more than happy helping you.",True
@tomashernandezacosta9715,2022-06-22T09:50:16Z,1,This is THEE single BEST explanation for Entropy that I have ever heard. After this video I bought your book instantly. TRIPLE BAM!!,True
@tianyiluo0105,2022-06-19T15:44:44Z,2,This is a great explanation! Thank you Josh and StatQuest!,True
@iannemo1736,2022-06-18T14:17:36Z,0,15:50,True
@lorenzologiudice3147,2022-06-15T07:37:18Z,0,6:31 log(1/0.9) is 0.105 not 0.15 and log(1/0.1) is 2.30 not 3.32 (btw great video :D),True
@luisalejandrohernandezmaya254,2022-06-13T04:23:18Z,1,I feel surprised as I expected.,True
@torosalvajebcn,2022-06-12T16:53:57Z,0,Great video. Will you make one about the link between entropy and information theory?,True
@zavar8667,2022-06-10T15:54:31Z,0,"Okay, but why the log function?  It is not a constructive argument to show that the log function has all the desired attributes (as my math notes introduced the formula). I'm looking for an explanation where someone sets out the goal to measure some well-defined quantity like in physics (I believe in physics there is a similar concept with the same name) and show the thought process that concludes in using the log of the probabilities. Could you help me out?   PS.: I love your videos, good job, thank you!",True
@KleineInii,2022-06-09T12:47:52Z,1,"Thank you so much for sharing this great explanation with us! I stopped the video after you derived the formula and then derived it again on my own. Makes so complete sense!!!  I am giving a talk at a conference in 2 weeks, and in my presentation there is a formula using mutual information. I was asked to explain this in my pratice presentation and was not able to. Now, after seeing your video, I am so clear about the concept of entropy and feel much more confident when I need to explain it :)",True
@SushilKumar-dr9rj,2022-06-08T19:19:43Z,2,"I have been following your channel for a very long time now. I know that the probability that you will create kickass content is 1. Yet every time I see new content on this channel, I am hugely surprised. Guess there are some things that even Maths can't explain.",True
@himeshkoli8607,2022-06-08T10:54:40Z,0,"Just wanted to ask what will be surprise in ""ML terms"" in general in decision trees? Is it uncertainty?",True
@yawkool596,2022-06-08T03:21:41Z,1,Thank you so much. The feeling of discovering a new world. Bam.,True
@paungchandal2788,2022-06-07T18:41:50Z,1,"very good explanation, appreciate much for your enlightment",True
@joaovitordeon3245,2022-06-07T12:56:10Z,1,"Nice, nice, nice!",True
@nuranichandra2177,2022-06-05T00:00:17Z,1,Amazing explanation of the most mysterious term - entropy!,True
@marioadiez,2022-06-03T15:21:30Z,0,"I've started here for the songs, and now I see a lot of people coming and making me bang questions...",True
@paulkosmala2730,2022-06-02T20:04:13Z,0,so at 5:32 i see this picture and I'm staring at it...  given a flipping of a coin - with the mere existence of the tails side - how can we assume log(1/1) is even valid...  in other words - is it proven that bits can be 'stuck' and never flip? I think that very premise is suspect. can certainty exist - can a bit always be 1 - and if so - is it (by definition) a bit?  given a coin and 100 attempts - if it lands heads every time you can say for certain it lands on heads every time... but if you flip it: that process (flipping) is some amount of certainty removed from the system.,True
@kevalan1042,2022-06-02T11:18:36Z,2,"You had me at ""let's talk about chickens""",True
@MLDawn,2022-05-29T20:37:57Z,0,"By the way, at 12:05, I am not sure p(x) should be defined as probability of surprise! It is the probability of a given outcome x, which will then be multiplied by the surprise of having such an outcome.",True
@sharatsidis3425,2022-05-28T16:35:47Z,1,"it was so easy? thanks for the video, best explanation",True
@muskanmahajan04,2022-05-24T05:12:55Z,1,By far the best explanation I've seen. You are a true saviour.,True
@MLDawn,2022-05-21T21:42:23Z,1,Great video! I had posted a question earlier but by the end of the video you explained it :-). Thanks a lot!,True
@ArmanAli-ww7ml,2022-05-14T01:47:01Z,1,I can‚Äôt thank you enough for your efforts and the way of teaching,True
@hansenmarc,2022-05-13T00:39:51Z,0,"Expectation of surprisal is a fascinating concept.  So if the probability of being surprised is 1, then the entropy for twist endings in M. Night Shyamalan movies is 0.   Bam?",True
@jongcheulkim7284,2022-05-12T10:12:32Z,1,Thank you so much,True
@abdilive3023,2022-05-11T19:30:52Z,2,"Ever best explanation, thank you!",True
@GiasoneP,2022-05-10T02:45:53Z,1,Orange chicken‚Ä¶ I just ordered some Chinese.,True
@user-nt6rm2cd6k,2022-05-10T02:39:57Z,0,ÊáâË©≤Ë¶ÅÂÜçËàâ‰æã ‰∏âÁ®ÆÈ°èËâ≤ÁöÑÈõû,True
@sukursukur3617,2022-05-07T23:12:37Z,0,"Imagine you have a raw set. You want to build a histogram. You dont know bin range, bin start and end locations and number of bins. Can ideal histogram be built by using max entropy law?",True
@ahmednagi7074,2022-05-07T22:59:29Z,1,u just killed it,True
@statquest,2022-05-07T18:25:06Z,37,Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@DubZenStep,2022-05-06T21:15:39Z,2,The world needs an army of people like you man. This explanation is outstanding. A triple bam.,True
@krushnakantbabhale6161,2022-05-06T04:11:30Z,0,The natural tendency of a system to be get disordered(surprised) is entropy,True
@heliosobsidian,2022-05-03T19:00:23Z,0,Thanks for this such a 100% understandable explanation!  May I know what is the log base in other number of output?  Let say the something occur in a whole week.,True
@pbawa2003,2022-05-03T18:03:18Z,5,"this is most simple way to explain Entropy, way to go Josh, love your videos !!!",True
@forresthu6204,2022-05-03T01:38:16Z,1,Thanks,True
@forresthu6204,2022-05-03T01:37:36Z,1,This is the BEST version of the explanation about entropy.,True
@Xcalator35,2022-04-30T23:10:38Z,1,Wow!! You're amazing!!! You are capable of explaning complex stuff like this as if it was concieved to a 8 year old...and that's PRECISELY how I want things explained to me!!!,True
@tooljerk666,2022-04-28T19:52:20Z,0,"Thanks for the video. In Data Science/ML, is the goal to lower entropy to we can make accurate predictions, or is entropy potentially a good thing, indicating we found something completely surprising in our data? Or does it just depend on the problem at hand?",True
@jsmdnq,2022-04-27T18:09:49Z,0,sum(plogp) = log(prod(p^p)),True
@skbanerjee6909,2022-04-27T15:25:42Z,1,Excellent!,True
@rmcgraw7943,2022-04-27T08:32:26Z,1,Found it.,True
@danieltrivino6736,2022-04-26T14:35:12Z,1,Best explanation ever!,True
@MrCracou,2022-04-25T13:11:44Z,0,Hello. I have a problem at 13.36 as we move from log (base e or base 10 depending on the language) to ln with base 2 in the second line.... and it's not (exactly) the same thing. Who can explain?,True
@gustavderkits8433,2022-04-25T00:38:15Z,0,The statement ‚Äúit is customary‚Äù is a much worse argument for log base 2 than the fact that you are flipping coins.,True
@Regular.Biceps,2022-04-22T20:36:12Z,0,"If there are more than two outputs, say three, shall we use log3 ?",True
@Regular.Biceps,2022-04-22T06:07:58Z,1,Man you're the best,True
@wong4359,2022-04-18T16:59:04Z,0,It is really log(1/probability)!,True
@michaelzavin969,2022-04-05T06:23:18Z,1,Just wow !  i've watched my prof's lecture (1.5 h long) 3 times and did not understand anything  and here you come with 15 minutes long video and BAM and medium BAM !! and I finally got it  THANK YOU!!!,True
@markoconnell804,2022-04-04T20:09:39Z,1,This is cool!,True
@markoconnell804,2022-04-04T20:02:11Z,0,So now I need to learn what log is.,True
@pianisteddy,2022-04-04T17:12:10Z,0,"The bams sound pretty awkward, but the content is real nice. Thanks for the video!",True
@TheOnlySaneAmerican,2022-04-04T05:34:16Z,0,"at 4:32, Isn't the probability of getting another heads .5? If so then the inverse would be 2?",True
@jimmylander2089,2022-04-03T14:03:27Z,1,This video is a HUGE BAM!!!,True
@CC-px1xv,2022-03-31T01:03:50Z,1,The best video on entropy!!!,True
@wanmei_xiatian4977,2022-03-27T19:37:48Z,2,Thank you and BAMBAMBAM!!!,True
@cdvs100,2022-03-27T19:05:03Z,1,Excelente!!!!!,True
@StephenHsiang,2022-03-23T16:06:13Z,1,This is a sort of ephiphany to me.,True
@ais3153,2022-03-21T06:53:57Z,1,You are GREAT! Thanks a million,True
@jorgelinasassera1884,2022-03-19T16:06:23Z,1,Thank you so much! I know nothing about stats and I undertood!,True
@ahmrh,2022-03-19T01:02:18Z,1,"better than my professor lmao, thank you so much",True
@machater1001,2022-03-17T13:55:53Z,1,Thanks for another great video. Can you look into getting some StatSquatch merch for your store? I'd love to be able to purchase some.,True
@snapfinger1,2022-03-17T12:30:23Z,0,Things fall apart.,True
@vahidnikoofard2939,2022-03-15T02:07:22Z,1,The best ever explanation of entropy!!!,True
@hkhademian,2022-03-12T10:14:25Z,1,I 1/log(prob)ed when I saw this high quality content,True
@joyageuse,2022-03-11T20:13:38Z,0,"Thank you for the easily understandable video!  Just a quick question: at 12:14, is the label for P(x), ""the probability of each outcome"" instead?",True
@mishmohd,2022-03-11T11:27:30Z,1,UUUUUGH üò©,True
@tiago9617,2022-03-11T08:40:06Z,1,I can't understand how it's possible to be so good at teaching something,True
@code_tuner,2022-03-09T08:26:04Z,1,ohmygod this is so clear!!!!!,True
@AtiqurRahman-uk6vj,2022-03-05T09:56:02Z,33,"Your self promotion is not shameless, it a gift to humanity. Free content that explains way better than paid content on Coursera. Thanks for helping out poor guys like us Josh",True
@SonSon-rq5dj,2022-03-03T09:41:24Z,1,"Solid video, solid explanation. Best channel out there for your data mining needs",True
@mmaroti,2022-03-02T20:04:25Z,1,"You have just explained ""entropy"" in terms of ""surprise"". The entropy is the average number of bits you need to encode an element from a distribution. Fractional bits are not really a problem, but you can encode 1000 elements with Huffman encoding, and you get an approximation.",True
@ostanin_vadym,2022-03-01T20:30:03Z,1,Thanks for the content,True
@lucavisconti1872,2022-03-01T20:17:41Z,0,That's indeed a very clear explanation.Can entropy be used to retrieve missing information from a set of data? how?,True
@azamatbagatov4933,2022-02-27T22:34:37Z,1,I am surprised how easily understandable entropy is! Thanks!,True
@delzosmith9240,2022-02-27T12:30:03Z,1,I really like this thanks. Short and simple and easy to follow. New subscriber and a like :),True
@shilashm5691,2022-02-26T08:01:58Z,0,"At 12.11 is it a mistake ?, you used the word probability of suprise,  it is probability of outcome ryt?",True
@ronaldjohnson7449,2022-02-25T14:23:43Z,0,enthalpy vs entropy ... order vs chaos,True
@eddiechen6389,2022-02-24T22:31:06Z,1,Your video is million üí• bam,True
@davids3477,2022-02-22T19:37:58Z,1,"great video, thanks!",True
@kaushaljani6769,2022-02-22T18:15:05Z,1,Bam!!! Hats off man that was the easiest explaination i ever come across... Thanks for making such kinds of tutorials.,True
@kebman,2022-02-22T17:23:22Z,0,How would you translate this to a Markov chain?,True
@pushpakkothekar9271,2022-02-21T05:03:59Z,1,"Awesome explanation, liked + subscribed done",True
@anibaldk,2022-02-21T02:18:28Z,1,Priceless channel for anyone interested in statistics. Just BRILLIANT.,True
@AbCd-fo6ys,2022-02-20T22:08:47Z,1,"you are amazing, thank you a lot",True
@CellRus,2022-02-20T17:56:21Z,9,"Absolutely amazing. I always come back to your videos from time to time for simple (but absolutely useful) explanation of complicated concepts that I found in papers. They all have helped me a lot, and I feel I'm better at communicating these concepts to other researchers too.",True
@lezaroo,2022-02-20T05:18:42Z,1,Awesomeee!,True
@arkabose89,2022-02-20T02:09:50Z,0,"Thank you for this video. However, is there any video that explains the use case for this quantification of surprise? Would be great if you can cover that",True
@sheng-peichen3767,2022-02-19T07:05:53Z,1,"Hat off to you, sir.",True
@3g1991,2022-02-18T16:34:10Z,1,Beautiful.,True
@franky07724,2022-02-18T01:24:37Z,1,"Genius ... concept, equation, and explanation.",True
@angryworm80,2022-02-17T23:12:13Z,1,Awsome!!!! üëçüèª,True
@reidflemingworldstoughestm1394,2022-02-17T06:52:21Z,0,"Hey stat guy, I have an interesting problem you might be able to help me with. We have a game with an S sided die, no numbers, all smooth white faces. We color one face black and roll. If we get a black, we stop rolling. If we get a white, we color one more face black, and roll again. This pattern continues until the S-th roll when every face is black and rolling a ""stop"" is guaranteed.  What I want to find is an equation I can plug the number of sides into that will give the expected number of rolls, on average, it will take to roll a black side.  I have an equation for the probability of rolling a stop given which roll I'm on, and a summation of that expression from 1 to the kth roll. I made a noodler for it on that calculator web space, name of (visual cues here against the possibility that youtube will axe the outright spelling of the name) DESert MOSquito, with the word sdgp1yrrg8 to be pasted in the calc addr bar.",True
@wcottee,2022-02-15T21:07:44Z,0,What if we had a single die with 6 possible outcomes?...do we use log base 6 instead of log base 2?,True
@sdsa007,2022-02-15T07:38:39Z,1,"OMG! finally, Entropy! CLEARLY EXPLAINED! The Nobel BAM! prize is to be rewarded!",True
@zhafranp8639,2022-02-13T15:51:10Z,0,"i have 45 output, does the log base stay the same or change to 45? thank you",True
@ajokaefi,2022-02-13T13:41:58Z,1,"Josh, voce e o cara! ... (meaning: you are ""THE MAN"")",True
@dystopiaseven,2022-02-13T07:59:10Z,1,oh,True
@mattizzle81,2022-02-13T01:27:40Z,0,"As not so much of an information theory wiz I got a first-hand taste of this. I wanted to try creating a machine learning model that can predict input colors and mixing percentages for a given paint color. It does not work with a machine learning model like xgboost. xgboost could model the mixing algorithm just fine, but the inverse, unmixing I found is impossible without a brute force search. Entropy. lol hence why I searched this video",True
@becayebalde3820,2022-02-12T19:18:38Z,2,"If you were my mathematics teacher, man I would have loved maths so much",True
@monazaizan947,2022-02-12T06:04:03Z,1,This explanation is amazing. Thank you so much!!,True
@adancastro2220,2022-02-11T17:56:36Z,1,"Excellent video!  You made it again fellows! it just log(1/P(x)) me that i understand it after a couple of years trying, Thank you sincerely!   üòÑ",True
@eduardovenegas782,2022-02-11T09:47:17Z,0,"Sorry but that ""bam"" thing its really hard to accept... it annoying",True
@grahamhenry9368,2022-02-08T07:07:16Z,0,Why not just use 1-Probability as the surprise? The surprise seems like an arbitrary value as it doesn‚Äôt have an upper limit,True
@emmydistortion3997,2022-02-08T07:01:44Z,36,Awesome world-level teaching... Thank you!,True
@oldcowbb,2022-02-08T02:32:03Z,0,took me too long to finally understand what this magical entropy is,True
@markbanash921,2022-02-07T05:00:23Z,1,This should be included in every class on statistical mechanics.  It makes the equation on Boltzmann's grave obvious,True
@ChocolateMilkCultLeader,2022-02-07T04:09:35Z,1,Baaammmm,True
@chyldstudios,2022-02-06T10:53:58Z,1,This video surprised me ... about 0.7.,True
@pomfrittbroccoli,2022-02-05T17:37:53Z,1,Very entertaining! Thank you!,True
@jonathanrockhill6039,2022-02-05T16:01:59Z,1,Major bam!,True
@mayurmulik1647,2022-02-05T14:09:30Z,1,"First time in my 25 years of life so far, I understood entropy.............",True
@pramodkhatiwada6189,2022-02-02T12:34:19Z,0,"After understanding the math behind the entropy, it makes sense to know who derived it and when!  Thanks for the explanation.",True
@mail2valan,2022-02-01T17:59:22Z,2,"Hey, thanks for a great explanation. Quick question: my dataset has a target that has 3 different states. So as per the explanation you gave at around 6 min mark, should I be using 3 as the base of log function while calculating entropy?",True
@nashaeshire6534,2022-01-31T09:52:11Z,0,"Help me a lot for my Data Science Master.  When I'm stuck in some lecture, I alway take a look at your chanel and hope for the subject to be here. Thanks a lot!  I'm actually stuck about calculating Intrinsic Dimension, like: Suppose A={0,1,2,3,‚Ä¶,25} What is the asymptotic intrinsic dimension of A?  I really appreciate some help or some lecture (I'm pretty sure this problem concern so few people than you'd probably won't do a video about it).  Best,",True
@hunortotbagi,2022-01-30T09:39:23Z,1,Brilliant explanation! I'm blown away :),True
@chaitu2037,2022-01-28T18:58:27Z,1,"""This is by far the best explanation for entropy that I have ever come across"", thanks so much!",True
@shrankhlapandey263,2022-01-24T17:50:53Z,1,BAM! got the entropy! üòé,True
@gianlucalepiscopia3123,2022-01-22T14:56:56Z,1,Bam is the sound of the hit I give to my head for not having known you before,True
@ichimatsu8422,2022-01-18T21:04:40Z,1,The absolute GOAT when it comes to stats on youtube,True
@boyemarc-antoine7027,2022-01-18T16:55:48Z,1,last joke is just so FUNNY ! great job i love ur serie,True
@sandraviknander7898,2022-01-12T10:24:42Z,1,"Next ‚Äúsurprise‚Äù  party I‚Äôm going to instruct everyone to yell log of the inverse probability instead of the traditional surprise.  Great explanation as always, thank you Josh!",True
@monoarul_islam_3,2022-01-11T08:07:37Z,1,I haven't finished the video yet. But seems like even a 5-years old kid will understand this thing at ease. I'm sure your dad's influence has a bigger role in making you this much cool.  Time for me to learn Machine Learning :3,True
@olegkazanskyi9752,2022-01-05T00:00:52Z,1,This is a purely amazing explanation. Bravo!,True
@libertyrosevengeance7471,2022-01-04T21:59:02Z,1,"Lovely intro, the content is great too!",True
@victornyabutiongera709,2022-01-02T21:22:14Z,0,Thanks ü§©,True
@pradyumnagupta3989,2021-12-29T18:31:42Z,1,Awesome awesome video. The best explaination of entropy I have seen so far!,True
@jinyunghong,2021-12-28T19:19:14Z,2,The best explanation of entropy! Thank you so much as always :),True
@Asdun77,2021-12-26T18:52:27Z,1,"God bless you, it's really great video .",True
@tommymerelte4399,2021-12-16T03:06:19Z,1,Jesus you don't know how much this helped me,True
@patturnweaver,2021-12-14T22:02:21Z,1,awesome explanation,True
@cristianofroes4681,2021-12-14T14:52:09Z,1,"Hello from Brazil, thank you very much for this (the best so far) explanation.... I really impressed how simply you do the hole thing. Keep going.... Good Bless you.",True
@GokulAchin,2021-12-11T05:34:13Z,9,"Please give Josh a nobel prize for not getting a single dislikes in many of his videos and for his contribution to the ML , Stats community. I have to forgive myself for not finding this channel way before when i started my interest in data science. You are definitely inspiring me to teach many people the same content you taught us.",True
@hajarhajar8906,2021-12-08T15:10:47Z,1,This guy is a life savior!!!!!!!!!!,True
@doot2359,2021-12-05T15:22:50Z,1,*BAM!* You explained much better then Shannon him self. Hir is a like and a sub from my heart *DOUBLE BAM!*,True
@bacnguyenkhac154,2021-11-29T16:13:38Z,1,"Oh BAM, the way you explain is so interesting and easy to understand ! Thank you, Mr. BAM !!!!",True
@rwamux6882,2021-11-29T08:22:33Z,1,You make learning fun. Subbed immediately,True
@emsdy6741,2021-11-23T23:40:36Z,1,"DOUBLE BAM! Thanks for the video. I liked how you derived the formula of entropy, and now it is easier to understand.",True
@mothehs,2021-11-12T11:32:09Z,1,"Hey Josh,  You are literally the man, Can I get a whoop..whooop. Or a Bam!!!!!! hehe.",True
@antibioticsOfWorld,2021-11-12T00:18:32Z,0,Superb explanation  Just one query . While doing that coin thing the surprise for 100 coin flips and getting head is just 0.15*100 right ? Y did u multiply probability as well ? ...  Bcoz while doing for 2head and 1tail example u just added their surprise values thats üòÖ ..may be i am missing something,True
@lourencopintodasilva4321,2021-11-09T14:49:57Z,1,Best explanation about something I've ever seen. Just subscribed the channel,True
@K17LL,2021-11-07T16:06:11Z,1,OMG - Now I get it! :D  Huge thank from Germany :),True
@neuodev,2021-11-07T08:54:01Z,1,"Thanks, Josh for this clear explanation!!. Also you have a migical ability to simplify math!!",True
@BhanudaySharma506,2021-11-06T05:30:17Z,0,"So, Basically, Entropy of some event is ""How uncertain we are about the results"".  More certainty of the expected results, lesser the entropy of the system.",True
@user-ui5rx9hg9r,2021-11-03T03:48:15Z,0,mark,True
@danielmcleod6535,2021-11-01T21:34:39Z,2,"Hi Josh, you're always my go to for stats videos. Thank you, they're amazing. Would love to hear you talk through chi squared? I couldn't see anything on your channel thus far on it.",True
@haya4895,2021-10-31T18:24:15Z,1,phenomenal explanation,True
@wanderingseeker2932,2021-10-30T13:29:14Z,1,hoooraayyyy!!!!!!!,True
@pacco2012,2021-10-30T00:39:24Z,1,wow! your explanation saved my butt big time! probably the best one on this platform,True
@ojithmagamage1809,2021-10-29T16:40:50Z,1,Thanks sir!,True
@XuankangLin,2021-10-28T05:05:10Z,1,"Thanks soooooo much, for my life I have never understood entropy so clearly!",True
@alecvan7143,2021-10-26T19:18:22Z,1,amazing.. as always!,True
@marcoventura9451,2021-10-23T18:16:09Z,1,I am surprised! :-),True
@shoumiknandi6912,2021-10-21T17:23:04Z,0,Could you please make a video on KL divergence and cross entropy ?,True
@pilupz232,2021-10-20T10:28:14Z,1,"Thanks for this awesome video Josh !! I have 1 question, do you have any idea on when we should use entropy / gini for classification trees ? BAMM !",True
@isaiaskaung8856,2021-10-18T17:54:25Z,1,BAM!!!!,True
@drdlecture6137,2021-10-15T10:03:51Z,1,The best explanation ever. Thank you and a quadruple bam!,True
@haipengli4769,2021-10-14T12:08:39Z,1,Big bam!,True
@shahedmahbub9013,2021-10-13T15:34:52Z,8,"Thanks for all your efforts in creating a smart, funny and most importantly CLEAR explanation. This was awesome.",True
@Ken-S,2021-10-12T10:25:23Z,1,I am surprised that entropy has another meaning with surprise! Thank you!,True
@auzaluis,2021-10-11T16:37:26Z,1,"Mr statquest: I beg your pardon for leaving you for a few weeks, I was busy being a mere mortal",True
@sumitmishra8449,2021-10-07T15:38:56Z,2,"Thank you, Josh, you literally are the best teacher out there. I got a job as a Data Analyst and I Only watched your videos for all the explanations and understanding. Made a lot of notes as well. Sincerely Thank you.   PS: First thing I'm gonna do with my salary is buy a membership!! Infinite Bam!!",True
@nonlinearfit6785,2021-10-06T20:41:47Z,1,Was quite usefull. Thanks.,True
@eduardoh.m2072,2021-10-06T19:04:26Z,14,"You, sir, you are the very first person to actually explain this subject and not just repeat some random definition without giving any thought to it. I'm amazed by the amount of people who confuse rambling on about the topic with actually explaining it. Thank you!",True
@debarunkumer2019,2021-10-06T11:23:08Z,1,Beautiful,True
@secondago1929,2021-10-06T09:28:08Z,1,Bam!,True
@vitorribeirosa,2021-10-04T14:05:15Z,1,Amazingly explained!!! Thank you very much...,True
@jiaweizhang6189,2021-10-04T05:37:11Z,1,I have studied ml for a lot of time. i dont clearly know what is cross-entropy or entropy until now. This is the best explanation for entropy!,True
@ZaidAlhusainy,2021-10-04T02:38:29Z,1,"Your videos is global treasure. I feel sorry for other scientific fields, because they don't have Josh Starmer ‚ô•",True
@wolfgangi,2021-10-03T04:28:45Z,9,"I freaking love these video, Josh has a gift to explain things so vividly",True
@adityajnhs3816,2021-10-02T06:11:52Z,0,"The description is good, but it doesn't explain why the units of entropy and information is 'bits', And why is it 'customary' to use log base 2 , for coin flip, and not e, or 10 etc. I understand that there are 2 alternatives in coin flip, so 2 might be natural. But is it necessary? There's no logical explanation for that ? Also, why only log,..any other function with 'similar' properties can be chosen ?",True
@TheBoSSmiCK14,2021-09-28T16:07:59Z,1,super clear ! thanks,True
@shaahinfazeli9095,2021-09-27T17:51:59Z,1,You are truly amazing in simply explain the complicated things!,True
@mvikyk,2021-09-26T01:53:09Z,1,"Even though I dont like advertisements, I watched through 'Shameless self promotion'. That much respect I have for your awesome video on Entropy!",True
@yacinerouizi844,2021-09-25T13:27:28Z,1,thank you,True
@TuNguyen-ox5lt,2021-09-23T09:39:02Z,1,this is definitely the most intuitive way to really gasp the ideal of entropy. you're just wonderful.  Thank you soo  much,True
@cmacompilation4649,2021-09-23T08:54:35Z,1,MAAAAAANNNNNNN   ‚ù§‚ù§‚ù§  Triple Looove,True
@googol-boy-data,2021-09-23T03:47:05Z,1,what the f***... how can you make this concept looks like so easy! Thanks for that <3,True
@cv21a,2021-09-22T19:33:02Z,1,Really good.,True
@ihbarddx,2021-09-21T18:16:35Z,1,"Entropy, as you described it, is the log likelihood of a data sample drawn from a binomial distribution.",True
@DanielTompkinsGuitar,2021-09-21T17:52:38Z,1,"Much to my log(1/probability), I found myself eating orange chicken whilst watching this.",True
@lullabyX86,2021-09-21T07:10:58Z,0,"Physics Teacher: When we mix two kinds of gas molecules, the entropy increases in the system Stat Teacher: Well, the same but with chickens",True
@0305jhurley,2021-09-20T17:26:10Z,0,"I wish people wouldn‚Äôt  call this the entropy. It is not the same as the Boltzmann entropy, which you may find inscribed on his tombstone, S is equal to k log W, where W stands for the German word for probability, wahscheinlichkeit.",True
@igorkuivjogifernandes3012,2021-09-20T00:13:49Z,1,Very nice video! Never though entropy was that simple!,True
@rishikaushik8307,2021-09-18T14:04:30Z,0,"is the definition of surprise fundamental (like a definition), or can it be derived from another starting point? I always thought the definition of a ""bit"" was defined as the number of yes or no questions required to have full information and all information theory was developed on it.  Can we derive ""surprise"" from ""bit"" or they are independently defined quantities that happen to work together?",True
@Miguel_Noether,2021-09-18T13:19:47Z,1,"15:42 Quadruple bam if you would have ended saying ""and thus I have proven to you the second law of thermodynamics""  :')",True
@prajaktakadam2010,2021-09-18T09:23:18Z,1,Best!,True
@hemanbassi12,2021-09-16T15:29:32Z,1,"Sweet and perfect explanation, this was so insightful. thank you.",True
@dagoninfinite,2021-09-15T17:42:20Z,1,It sounds almost like hal,True
@alexmanners1540,2021-09-15T13:41:29Z,1,Very  well explained its remains me when it was presented by mi Statistical Physics Professor.,True
@Zonno5,2021-09-15T13:06:17Z,0,So if I want to win a lottery I should pick one with a high entropy so I have a better chance of getting the winning ticket?,True
@SanderKonijnenberg,2021-09-14T12:16:11Z,0,"Thanks for this clear explanation! I think it's very interesting and well-presented. However, it does raise one question for me, and I'm curious whether you have any thoughts about it.  In physics, systems tend to a state of highest entropy, because such a state is statistically most likely. I.e. such a state would be 'least surprising', whereas if a system would spontaneously evolve towards a state with low entropy, that would be 'extremely surprising'.  These observations appear to contradict the idea that you can intuitively think of entropy as 'the degree of surprise'. The problem of course lies in the precise, technical definition of 'surprise', but do you happen to have a suggestion for how this paradox can be resolved in simple layman's terms?",True
@18meiy13,2021-09-14T08:12:30Z,1,Nice explaination. I finally get it.,True
@serathicsides4837,2021-09-13T15:54:15Z,1,Just want to say BAM. Also entropy is denoted by S. I knew why it is today.ü§ó,True
@vitauscas1972,2021-09-13T14:03:48Z,0,"Hi, Why it is called Entropy. (thermodynamics)",True
@reiitenshi,2021-09-13T13:10:52Z,1,you don't know how much you've saved my ass with this explanation of entropy,True
@sumitpaliwal1540,2021-09-13T12:36:42Z,1,StatQuest: super easy explanations to complex problems. BAM!!!,True
@christiansiemes9902,2021-09-11T19:12:19Z,1,"Great video! For the first time, I understood what entropy means. Thanks!",True
@victorcoulon9375,2021-09-09T17:03:46Z,2,Outstanding explanation made of simplicity. Huge thanks!,True
@lukafarkas420,2021-09-07T21:17:29Z,1,Nice video,True
@cexploreful,2021-09-07T14:08:53Z,0,"*Kullback Leibler DIVERGENCE. It's not symetric so it's not a distance. d(A,B) neq d(B,A) try Thomas & Cover book for a better approach based on the 20 question guess game... ^_^ edit: ""Elements of Information Theory""¬¨Thomas  & Cover (Ch. 1, p. 5-6)",True
@Anujkumar-my1wi,2021-09-07T12:15:10Z,0,"Can you me what is the meaning of weights parameter used in logistic and linear regression etc ,is it rate of change of function with respect to inputs or is it just some multiplicative factor and if it is the rate of change then shouldn't the value e multiply with our weights be change in inputs rather than inputs itself so that when multiplied we can get change in y",True
@user-gx3eg5sz9n,2021-09-06T16:06:13Z,1,"Do you have a WeChat or Alipay Josh, that would be easier for Chinese fans to donateü§™",True
@maurocruz1824,2021-09-06T12:59:36Z,1,Good explanation!,True
@woddenhorse,2021-09-06T06:43:06Z,1,Simply Amazing. The best explanation out there üî•üî•,True
@michaelgeorgoulopoulos8678,2021-09-05T13:21:46Z,3,The inverse probability is a much better way of putting it than the minus sign. It was all this time in front of me and I didn't notice. Thank you!,True
@md.adnannabib2066,2021-09-05T04:54:59Z,1,"Damn,man.you are a jem.man a crystal clear and intuitive explanation of what entropy is.best of luck.go on.BAM BAM BAM!!!!!!!!!!!!!!!!!",True
@BenitoAndito,2021-09-04T16:06:11Z,2,The total surpise of me picking out an orange chicken every time I go to Panda Express: 0,True
@ebeb9156,2021-09-04T11:02:09Z,1,Bim ... para bim bum BAAAAM !,True
@pyakpepyak,2021-09-04T06:08:30Z,1,"In the second half of the video you start using given probability of heads and tails 0.9 (T) and 0.1 (H) as a ""probability of a surprise"" (10:20 in the video). Did you mean just the probability of a given event and not the probability of a surprise, because surprise itself is already calculated? I think the chicken example uses the right wording.",True
@dafeiwang7797,2021-09-04T02:33:51Z,1,ÊàëÊâÄËßÅËøáÊúÄÊòì‰∫éÁêÜËß£ÁöÑËÄÅÂ∏àüë®‚Äçüè´ Greate,True
@PedroRibeiro-zs5go,2021-09-04T00:33:44Z,0,"Hi Josh, you're the BEST!!! Would love a subsequent video on the KL-Divergence!",True
@souravdey1227,2021-09-03T07:48:10Z,0,"Brilliant as always. Sir, request you to please do a series on Causal Analysis. I am going through the seminal works of Judea Pearl and would love to learn from you about it. With actual comparative examples, showing how it leads to insightful results as compared to normal statistical anlaysis.",True
@sumailsumailov1572,2021-09-03T06:31:22Z,1,Small bam :( Your interpretation is much better than I've ever met in student books. Thank you for job!,True
@laiwang4820,2021-09-03T04:35:09Z,0,fun but sometimes the too annoying(the voice),True
@deschryverjan,2021-09-01T20:47:56Z,1,No surprise here: yet another amazing video! Great explanation!,True
@amalnasir9940,2021-09-01T17:13:00Z,1,Wow! I don‚Äôt know what to say! Genius! Triple thank you!!!!,True
@bobsalita3417,2021-09-01T10:24:58Z,0,I don't have an intuition about the magnitude of surprise. Wish that was given a couple sentences.,True
@ufooex,2021-08-31T21:32:32Z,1,Your explanation is really clear. Thank you.,True
@chaitanyasharma8212,2021-08-31T20:04:09Z,0,Decision Trees ID3 algorithm gang wya ?,True
@pittypat2,2021-08-31T16:51:15Z,0,Can you explain the motivation for using log of inverse? Surely there must be a more fundamental reason for it's use other than it working out nicely,True
@ChrisadaSookdhis,2021-08-31T15:36:15Z,0,I'd really like to see your explanation of cross entropy now!,True
@BillHaug,2021-08-31T11:36:41Z,1,This is really helpful thank you... even though we didn't do a lot of maths step- by-step by-step by step... statquest,True
@lakminiwijesekara8662,2021-08-31T02:09:09Z,1,The best explanation I've ever come across.,True
@zhuxuanfu3126,2021-08-30T14:24:19Z,1,,True
@karthiklogan9384,2021-08-30T10:46:16Z,1,Amazing video.Pls keep doing this,True
@LouisChiaki,2021-08-29T19:56:26Z,0,Great video! Though we should have a video in the future to note why we pick log(1/p) rather than any other form.,True
@ilyboc,2021-08-29T19:02:41Z,1,log(1/p(x)) ... üëÅÔ∏èüëÑüëÅÔ∏è,True
@adipurnomo5683,2021-08-28T11:39:05Z,1,Please explain about Information gain a.k.a mutual information sir. You SUCH great man!,True
@erv993,2021-08-28T04:02:00Z,2,This is a brilliant explanation! It all makes sense now!,True
@marwolaeth111,2021-08-27T18:38:10Z,2,"Well, I guess how angry some of the starchy mathematicians might become, but ¬°this is simply damn brilliant! ",True
@algorithminc.8850,2021-08-27T17:48:21Z,2,"""Hello .... I'm Josh Starmer, and welcome to StatQuest.""  ---  hehehe ... cracks me up.  You've got a great channel ... solid explanations ... thanks and cheers.",True
@felixvanderspek1293,2021-08-27T16:50:33Z,315,"""Simplicity is the ultimate sophistication."" - Leonardo da Vinci  Thanks for explaining in such simple terms!",True
@kusy,2021-08-27T09:34:09Z,0,What is the reason to believe that the number of two chickens will gradually become equal with time as if guided by some universal law?,True
@djaym7,2021-08-26T21:56:08Z,1,This is GOLD,True
@user-vi3pi9rf7w,2021-08-26T20:50:21Z,1,"How crazy is that, I was studying entropy yesterday  But it was for thermodynamics.",True
@SierraDN,2021-08-26T18:46:30Z,1,"Very, very good!",True
@nataliatenoriomaia1635,2021-08-26T18:07:49Z,6,"Awesome as always, Josh! Thank you for continuing to share high quality content with us. You‚Äôre a very talented teacher. I wish you all the best!",True
@nicolasmora9331,2021-08-26T17:08:56Z,1,Excellent video! Thank you. I never thought of entropy as sur p r i s e.,True
@rafaelcorreiadasilva2301,2021-08-26T14:19:47Z,1,"it is indeed, clearly explained.",True
@crabson1864,2021-08-26T12:49:57Z,0,"Thank you for the video. It made me wonder why the term is called entropy and not uncertainty? You are very uncertain when there is 50/50 chance of getting something, and a bit less uncertain when it is 30/70.",True
@khursani8,2021-08-26T11:52:43Z,1,This video really helpful for me to understand entropy I have been using it for a while but don't know how it exist and why I need it Now I understand thanks :),True
@arunvaibhav5059,2021-08-26T07:51:34Z,1,"BEST, BEST and The BEST!!! TRIPLE BAMMM!",True
@Endocrin-PatientCom,2021-08-26T07:44:03Z,1,"Great video, as usually. In most textbooks, you see only the final equation of entropy, but no such a clear explanation.",True
@SanjeevKumar-hj1fb,2021-08-26T07:07:56Z,0,"Really a fantastic explanation  !!. Some of the resources on the internet explain Entropy in terms of Information Gain/No of bits needed. Could you please prepare a video explaining that as well? Thanks in advance, Sanjeev",True
@namratachavan6024,2021-08-26T06:39:17Z,2,It is very well explained about Entropy with simple example. Thank you so much. And most important you are not doing shamelessly promotion its actually worth it for everyone like me to understand the concept in very easiest manner.,True
@johnathancorgan3994,2021-08-26T03:02:49Z,1,How can you small bam anything published by Claude Shannon üòè,True
@paulkoh6887,2021-08-26T01:31:27Z,1,I love you StatQuest,True
@verticlelimits,2021-08-25T22:59:26Z,0,"thanks Josh for explaining so very nicely. as we are using univariant we are using log2, but say if we have more than 2 colors of chicken? then would we use log instead and add p(x).log(1/px) for all the diff colors?",True
@AhmedGamal-mm4sp,2021-08-25T21:15:50Z,1,Thank you so much for the clear explanation! Could you please make a video on DESeq negative binomial modeling clearly explained?,True
@rabbitazteca23,2021-08-25T19:31:02Z,0,"Hi! I wonder if you might be able to create or curate topics related to stats in Data Science and Machine Learning ranging from beginner to advance? For example, what kind of stats should we use for Data Exploratory Analysis and basically what kinds of stats we may need to start of with Data Science.  EDIT: Your playlist called #66daysofData seem to be a very good start and it certainly contains some data science topics that I applied on datasets before when I got started with Data Science!",True
@SaffaGains,2021-08-25T19:04:41Z,1,My surprise for this result occuring is undefined.,True
@leopapuda,2021-08-25T18:41:56Z,1,"So this is entropy? Finally understood it, What a surprise! DOUBLE BAM! üòØ",True
@calebterrelorellana2478,2021-08-25T18:00:39Z,1,"great way to explain ML algorithms, thank you very much!",True
@Bigheadbear666,2021-08-25T17:11:02Z,1,BEST STATS CHANNEL EVER!,True
@nick_g,2021-08-25T15:50:30Z,1,Omg if there‚Äôs a statsquach 5-panel hat then just take my $40 now,True
@MrDaanjanssen,2021-08-25T15:22:15Z,1,superb explanation my man,True
@chrisvause5693,2021-08-25T14:23:03Z,1,"StatQuest! C'mon grab your friends.  You'll really start to under-stand. With Josh Starmer and StatSquatch the StatSquatch, The learning never ends it's StatQuest Time!",True
@OwenMcKinley,2021-08-25T14:22:14Z,22,"üòäüòä15:45 hahaha ‚Äúpsst‚Ä¶ the log of the inverse of the probability...‚Äù Josh this was a fantastic tutorial. Love how I can just wake up and see content like this fresh in my YT recommendations We all appreciate it",True
@nakul___,2021-08-25T14:09:53Z,1,Been looking forward to this one for a while and was not disappointed at all ‚Äî thanks!,True
@chaoli932,2021-08-25T13:20:03Z,1,"Thank you, sir. Thank to your videos, I begin to do some research using ML technology.",True
@TotorasLife,2021-08-25T13:18:51Z,1,Thank you for the great explanation. It‚Äôs very inspirational.,True
@ceseb23,2021-08-25T12:39:25Z,3,"Hello, Thanks for this video, its really helpfull as always :D. Quick question : why not use Surprise = 1- P(x), as it scale inverse to the probability and the surprise of a sure event is 0 as requested ?",True
@DrJohnnyStalker,2021-08-25T12:27:40Z,1,even though i knew xgboost and entropy. statsquest explanation is always worth watching and adds value to your intuition and understanding. best intuition videos for xgboost and entropy and many other topics. 10‚Ç¨ Donation is out ... keep going.,True
@ahmedabuali6768,2021-08-25T11:52:43Z,0,"very very nice, great work, I love it too much,  could you please teach us how to compute the confidence interval for kruskal.test() by R, please.",True
@SPLICY,2021-08-25T11:19:15Z,1,"You down with en-tro-py?  (yeah, you know me)",True
@alexmiller3260,2021-08-25T10:35:16Z,0,"Why don't we use Bernoulli's formula to calc the probability of getting Head, Head, Tail?",True
@pseudounknow5559,2021-08-25T09:53:41Z,2,Best explanation ever,True
@algorithmo134,2021-08-25T09:14:40Z,0,Good video. Please make videos on math behind all machine learning concepts. It would be so helpful to us learners! Quadra Bam!,True
@louisszeto31620000,2021-08-25T09:08:26Z,1,Josh can we have explanation on the fancy dimension reduction algorithm you mentioned in the video related to entropy up next.,True
@surendrabarsode8959,2021-08-25T08:24:37Z,1,This is the best ever explanation of entropy I have seen!! The real surprise is the totally innovative idea of 'surprise! Thanks with entropy of zero!!!.,True
@redcat7467,2021-08-25T08:07:28Z,1,"You know what is common between a gun and StatQuest? The both do BAM!  Basically, the concept I have reviewed maximum times. Still, watched the Quest.",True
@user-or7ji5hv8y,2021-08-25T07:57:49Z,0,"If we had to come up with another measure for surprise that still relates to probability, would it still be expressed in this formulation? Just wondering how much of it is design.",True
@mewdlez,2021-08-25T07:04:20Z,1,i am also surprised when i get heads,True
@shahf13,2021-08-25T07:00:48Z,1,YAY I have been waiting for a good explanation of entropy for a long time,True
@bjornnorenjobb,2021-08-25T06:46:44Z,2,"I love your videos, you make a wonderful effort for explaining difficult topics on a easy way. Huge thanks!",True
@work4ai,2021-08-25T06:32:50Z,1,Wow!,True
@shaz-z506,2021-08-25T06:26:32Z,1,"Wow! this is quit a surprise to see such a clear explanation on entropy, could you please create a future video on KL divergence also.",True
@wangsherpa2801,2021-08-25T06:26:13Z,1,World's best explanation. Thanks!,True
@notnilc2107,2021-08-25T06:08:00Z,1,Bam!,True
@eliotbehr5340,2021-08-25T05:56:02Z,1,You should make a shirt with statsquatch :),True
@RoRight,2021-08-25T05:54:09Z,1,I was NOT surprised by the high quality of this video given StatQuest's high probability of producing awesome videos.,True
@cls8895,2021-08-25T05:51:27Z,1,"WOW its SUPEERRR EASY and well explained!! I only had known about the entropy in physics, but now I can see the calculation way of the entropy. THANK YOU for your hard work for easy understanding from S.Korea!",True
@dvo66,2021-08-25T05:39:37Z,37,"best entropy explanation. I took a 500 level ML class last spring in my masters and this is better explanation than my prof(no disrespect to him, he is amazing too)",True
@eliyahubasa9401,2021-08-25T05:23:21Z,3,"Thanks, I‚Äôd been waiting for good explanation about entropy for a long time. Thanks :)",True
@Stop-and-listen,2021-08-25T05:17:52Z,1,The best of the best!,True
@EngRiadAlmadani,2021-08-25T04:53:26Z,0,Great work Just one question if we say entropy  =1 at this case thay are similar and when we decreasing the entropy the similarity decrease as well is that right??,True
@esorminihaz8269,2021-08-25T04:52:49Z,1,I was just reading about information theory on Wikipedia. What a surprise!,True
@thechhavibansal,2021-08-25T04:33:04Z,1,Thank you for explaining this concept so easily :),True
@lan30198,2021-08-25T04:25:42Z,3,"Fuck, I never understand about the entropy before watching this video, you are amazing",True
@tranquil123r,2021-08-25T04:23:20Z,7,Loved it. The best explanation I came across on Entropy. Thanks Josh!,True
@cambridgebreaths3581,2021-08-25T04:21:42Z,0,"Thanks Josh. Are surprise and surprisal ( Free Energy Principle) the same thing, statistically speaking. Thank you kindly",True
@luischinchilla-garcia4840,2021-08-25T04:18:57Z,766,This is quite possibly the best explanation of entropy I've ever seen. This is even better than Shannon's own paper!,True
@Erosis,2021-08-25T04:17:58Z,0,KL divergence would be awesome to learn about! They are used in VAEs.,True
@vaishnavi4354,2021-08-25T04:14:37Z,2,Bam! !! This is awesome  Kudos to you üôáüôáüôáüôáüôáüôá,True
@VadexVx,2021-08-25T04:03:16Z,0,"Hey josh, can i request an explanation video about feature hasher? especially library from sklearn",True
@cat-.-,2021-08-21T07:29:03Z,1,I love the surprise explaination!!,True
@Shionita,2021-08-19T02:44:18Z,22,I feel so happy because I just learned something new thanks as always!! üòÅ,True
@deepakmehta1813,2021-08-18T19:18:29Z,76,"Amazing video on Entropy Josh. Thank you. I am certainly more addicted to statquest than Netflix.  I really liked the way you have introduced the notion of surprise, how you used it to pedagogically explain entropy.  It is certainly now easy to think and remember the definition of entropy.",True
