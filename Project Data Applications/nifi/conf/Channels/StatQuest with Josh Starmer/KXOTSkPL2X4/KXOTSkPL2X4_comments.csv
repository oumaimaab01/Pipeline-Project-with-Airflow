author,updated_at,like_count,text,public
@statquest,2023-02-22T14:56:26Z,3,"Corrections: 4:09 It is also worth noting that if there were more than 2 target values, for example, if Loves Troll 2 could be 0, 1 and 2, then, when calculating the OptionCount for a sample with Loves Troll 2 = 1, we would include rows that had Loves Troll 2 = 1 and 2. To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@ravi122133,2024-02-11T17:32:17Z,0,"@statquest , I think in the paper they take the case when each sample has a unique category to show that it leads to leakage. and not the case that all samples have the same category. Section 3.2 Greedy TS of the CatBoost paper.",True
@Joaopedro_,2023-11-30T01:06:58Z,0,Manda um salve para o Caio Ducati,True
@nitinsiwach1989,2023-11-11T08:52:28Z,0,Not only is the motivation unjustifiable. The way Target encoding is done by catboost also makes no sense. Even in your toy example the different categories are numerically exactly the same when encoded and there is absolutely no reason it should be the case,True
@hopelesssuprem1867,2023-09-29T12:40:57Z,0,"Hi Josh, I hope you will read this comment. First of all, thanks for the explanation, but due to the fact that the CatBoost developers were too lazy to make normal documentation, you slightly misunderstood the essence of ordered target coding in CatBoost and there are some mistakes in your video. The value for each categorical feature is calculated using the following formula: CTR = (TargetSum + Prior) / (TotalCount) + 1, where TargetSum is the sum of the target values for a specific categorical feature up to the current moment, prior is a constant defined as the arithmetic mean of the target values, and TotalCount is the number of all observations for a specific categorical feature, met up to the current moment. To verify my words, you can run the following code:  ! pip install category_encoders from category_encoders.cat_boost import CatBoostEncoder  train = pd.DataFrame({""color"": [""red"", ""blue"", ""black"", ""black"", ""blue"", ""blue"", ""red"", ""blue""]}) target = pd.Series([2, 1, 2, 1, 0, 3, 0, 1])  cbe_encoder = CatBoostEncoder() train_cbe = cbe_encoder.fit_transform(train, target) prior = target.mean() print(prior) print(train_cbe)  If to use cbe_encoder.transform(train, target) it will calculate the same values for all specific categorical features. There is a paper Categorical Encoding with CatBoost Encoder on geeksforgeeks.   If it won't be difficult for you, add a video with the corrected version.",True
@EvanZamir,2023-09-08T19:48:36Z,0,My guess is the ordered target encoding acts like a form of regularization.,True
@EvanZamir,2023-09-08T18:34:59Z,0,Lightning can be used with CatBoost?,True
@johndavid5907,2023-08-15T11:16:03Z,0,"Hi there sir, can you tell me that the value prior variable is holding is that the value of significance level value?",True
@beautyisinmind2163,2023-08-08T02:18:11Z,0,Catgorial boosting is only suitable for data with categorial features or we can use it even if our data has no categorical features? While using on continuous features does it require any conversion?,True
@BlueRS123,2023-06-20T13:28:36Z,0,Will you cover LightGBM?,True
@heteromodal,2023-06-17T11:44:32Z,0,hey Josh - is there a mathematical justification to the prior in the numerator being defined as 0.05? regardless of a justification existing :) - is it always the case or just what you saw in their examples but it's not certain that that's a fixed value?  thank you as always for a great video!,True
@daniellaicheukpan,2023-06-09T07:51:31Z,0,"hi Josh. thanks for your videos. I have one question. in your example, color blue can be encoded to several numerical values. Assume that I trained and deployed this model, when a new data comes with color = blue, which have no ""loves troll 2"" column How can the model know how to encode the color into which value? thanks so much",True
@matteomorellini5974,2023-04-22T15:07:26Z,0,"Hi Josh first of all thanks for your amazing work and passion. I'd like to suggest you a video about Optuna which, at least in my case, would be extremely helpful",True
@shubhamgupta6551,2023-04-08T18:52:41Z,0,How was the ordered target encoding applied at the time of scoring?  There will not be any target variable and we don't have a single value for a category i.e Blue color encoded multiple time with different values,True
@frischidn3869,2023-03-09T09:08:36Z,1,"Hello, thanks for the video. I wanna ask, what if the target variable (Loves Troll 2) is in multiclass (Like, Dislike, So-so). How will the encoding work then for the Favorite Color?  And should we encode the target variable first such as 0= Dislike 1= So-so 2= Like Before we then proceed to CatBoost encoding the feature (Favorite Color)?",True
@dl569,2023-03-05T14:35:12Z,1,"can't wait to see Transformer, PLEASE!!!!!!",True
@AllNightNightwish,2023-03-05T04:33:27Z,2,"Hi Josh, I agree with your point here about it being unnecessary (also having seen the previous longer explanation you posted a while back). However, I think their main point and contribution was not the mitigation in a single tree, but throughout the ensemble. If i understand it correctly, by using ordered boosting and randomization over each tree they guarantee that there is no leakage between the separate trees, because none of the samples have ever seen the original value. They use multiple models trained on different fractions of the dataset for each tree, just so they can make predictions that don't have any leakage at all. I'm still not sure that it wouldn't just work with leave one out encoding but given that context it seems to be more useful at least.",True
@davidguo1267,2023-03-03T01:44:35Z,0,"Thanks for the explanation. By the way, have you talked about backpropagation through time in recurrent neural networks? If not, are you planning to talk about it?",True
@murilopalomosebilla2999,2023-03-02T20:02:18Z,2,"It may be silly, but having a boosting method with cat in its name is really cool haha",True
@tapiotanskanen3494,2023-03-02T10:38:59Z,1,"1:57 - Is this correct? On the chapter 3.2 - *Greedy TS* - they talk about a problem _""This estimate is noisy for low-frequency categories..."",_ but your example has (maximally) high-frequency category. Later they stipulate _""Assume i-th feature is categorical, _*_all its values_*_ are unique, ...""._ To me this means that there are only single row for each category. In other words, each category (label) is unique, i.e. we have exactly one example per category (label).",True
@ericchang927,2023-03-01T16:53:59Z,1,Greate video!!! could you pls also introduce lightgbm? Thanks!,True
@mrcoet,2023-03-01T12:19:01Z,9,Thank you! I'm doing my master thesis and I'm checking your channel every day waiting for Transformers. Thank you again!,True
@c.nbhaskar4718,2023-02-28T16:31:08Z,0,Great tutorial but i am Eagerly waiting for  statquest on Transformers,True
@tessa10001,2023-02-27T19:22:37Z,2,Where was this when i made my master thesis with catboost :(,True
@TJ-hs1qm,2023-02-27T18:40:08Z,2,"Hey Josh, I was wondering if you could do a series on graph theory and NLP? exploring this stuff would be really helpful. Thanks!",True
@junaidbutt3000,2023-02-27T11:45:29Z,1,"Clear and concise as always Josh! I was wondering if there was a natural way to extend the OptionCount metric for multiclass problems? It makes sense in the binary classification, we count the observations where a category class c co-occurs with the positive class of the target variable (1 in this case). If this was adapted for multiclass problems, how would we adapt this encoding equation?",True
@guimaraesalysson,2023-02-27T10:42:03Z,0,"In this simple example of people who liked the colors whether or not they liked the movie, wouldn't ""leakage"" make sense? After all, if for example 90% of people who like blue liked the movie, wouldn't knowing that the color the next person likes is blue already provide information? Why is the leak a leak in this case?",True
@jameslucas5590,2023-02-27T08:02:12Z,1,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!   https://youtu.be/589nCGeWG1w",True
@joy5636,2023-02-27T07:55:47Z,1,"Wow, I am so excited to see the Catboost topic! thank u !",True
@xaviernogueira,2023-02-27T05:49:17Z,7,Glad to see CatBoost! Would love to hear more about data leakage mitigation.,True
@luiscarlospallaresascanio2374,2023-02-27T05:23:04Z,0,"Que usaste para traducir el texto, en español? :0 ya había visto otros videos de traducción pero no pensé que pasarían a hacer el cambio tan rápido",True
@firstkaransingh,2023-02-27T05:21:12Z,5,Finally a video on cat boost. I was waiting for a proper explanation.,True
@aghazi94,2023-02-22T01:54:52Z,2,I have been waiting for this for so long. Thanks alot,True
