author,updated_at,like_count,text,public
@statquest,2020-06-30T17:44:21Z,21,NOTE: At 31:25 we should use the mean and standard deviation from the training dataset to center and scale the testing data. The updated jupyter notebook reflects this change. ALSO NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: http://statquest.gumroad.com/l/iulnea  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@amanuel2135,2024-05-23T00:32:55Z,0,Is there a reason why you're not using LDA(multi-class) rather than PCA?,True
@footballkingarabic346,2024-05-15T11:45:20Z,0,Anyone watching this video on 2024 use ConfusionMatrixDisplay.from_estimator() instead of using plot confusing matrix() with the same parameters,True
@user-kc1hr5ug6d,2024-05-14T16:11:13Z,0,why do you use 1:1 resampling instead of stratified resampling? The dataset contains 3.5 no_default:1 default. Does this affect SVM results?,True
@Jannerparejagutierrez,2024-04-15T21:24:29Z,1,"Thank you very much for the video! I have a question, in SVM should the variables only be numeric or does it also support text?  Thank you!",True
@engrmuhammadumar,2024-04-15T08:24:26Z,0,"Those who are facing error can update the code as follow.   clf = SVC(random_state=0) clf.fit(X_train_scaled, y_train)  predictions = clf.predict(X_test_scaled) cm = confusion_matrix(y_test, predictions, labels=clf.classes_) disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_) disp.plot()",True
@user-ib6yl4bu1u,2024-03-10T06:35:37Z,0,"Hi i have a question, aren't we supposed to split the data even more, and then use the validation dataset for hyperparameter tuning, we can pass it to grid_search, e.g. grid_search(x_validation,y_validation) instead of using the training dataset again?",True
@hangchen,2024-02-14T00:41:58Z,1,See the real Josh Starmer!!!!! SUPER TRIPLE LARGE BAM!!!!,True
@ndbweurt34485,2023-12-21T11:45:01Z,1,that tabla behind you tho!😵,True
@causticmonster,2023-11-06T14:44:46Z,0,Are you supposed to scale the one-hot encoded variables as well?,True
@yourfutureself4327,2023-08-26T22:22:15Z,1,💚💚💚,True
@konstantinlevin8651,2023-08-07T17:03:10Z,1,"I've reread the ""hitchhikers guide to galaxy"" again (first time I read I was 12) and now it makes a lot more sense why the random state is 42 :)))",True
@teelee3543,2023-06-20T23:08:36Z,0,"I bought your machine learning book but I can not find the pdf version right now , what should I do ? Would you like to help me to find a copy?",True
@forrest404,2023-06-20T17:01:08Z,2,I love this kind of webinar where you teach in real time and go through concrete examples. Just purchased the material package and can't wait to go through them with you. I hope you'll make more content like this in the future 😊(I love the short and sweet vids too but I learn by doing so this helps solidify all the theory stuff!),True
@stardust857,2023-06-09T13:12:01Z,1,"Thank you so much, it was a wonderful video!!!",True
@KetakiGadgil_15,2023-06-01T11:55:13Z,0,"Why don't we use as.type(""Category"") for marriage,pay_ variables to convert them to categorical variable  instead of using get_dummies ?",True
@NaumRusomarov,2023-04-12T22:05:01Z,0,svm are kinda my favourite thing in ML. very simple and mathematically concise yet highly usable.,True
@murifedontrun3363,2023-03-26T15:12:06Z,1,Ultra BAM !!!!,True
@ramanyazdi1191,2023-03-03T02:55:13Z,0,pd.read_excel() has no parameter 'sep',True
@abdirasakfarah9878,2023-03-01T11:26:50Z,0,how  i can get this dataset,True
@vegaarcturus509,2023-02-06T06:39:30Z,0,"Correct me if im wrong but when people think of machine learning, they think of ai self improvement but SVM is just finding correlations between data sets?",True
@berkaktas5470,2022-12-30T20:43:19Z,1,"Thank you Josh, this taught me a good lesson on both PCA and SVM. Great work!",True
@NicolasValderrama-pv6qt,2022-12-30T02:08:44Z,1,Very helpful! thanks :),True
@savagesage786,2022-12-06T05:11:54Z,1,Handsome man<3,True
@md.nazrulislamsiddique7492,2022-11-14T02:58:32Z,0,"Your video is so awesome. Everything related to SVM in one video, BAM.",True
@caialyu2833,2022-11-05T05:02:41Z,1,THANK YOU!!!!!!!!!!,True
@sebastioncornejo4440,2022-10-04T00:27:10Z,1,Haha the double bam at 31:22had me dying lol. Great content! And love your channel!,True
@geo.yaflman,2022-08-05T14:11:47Z,1,BAAAAAAAAAMMMMM,True
@umeshk0697,2022-07-23T22:41:43Z,0,"Hey Actually I want to make time series prediction for carbon emission dataset which contain car models & there engine, cc, fuel consumption, carbon emission information so what you think which machine learning algorithm will be best for this? Thanks!",True
@cmpunk3367,2022-07-01T03:44:08Z,0,"Thanks for the brilliant tutorial Josh! You are truly an inspiration.   I just had two questions here :-  1) You applied a regularization technique here by finding the right value for C. What kind of regularization is this? L1, L2 or L1&L2? 2) Is it possible to apply L1, L2, and elastic net regularization on SVMs? If yes, how should I do it?",True
@Norainjoe,2022-06-12T21:43:03Z,0,"I was working in a jupityr notebook, used the pd.getdummies(), and it will not go backwards- i am stuck with what it output for variables. I have restarted the kernal, my machine, everything.  I have no idea how to troubleshoot this, but I have spent 20 hours on this homework assignment and this totally messed things up.",True
@toxic_roy,2022-06-09T14:14:38Z,0,"To all guys like me who tried using the data from uci respository through link, and its not working, its probably becoz  pd.read_csv cannot read  .xls files. You need the xlrd package( python -m pip install xlrd)[type in command prompt]. then use read_excel instead of read_csv. Enjoy",True
@Mustistics,2022-06-07T12:11:12Z,1,"One final question (I swear!): At the final code segment, you type  X_test_pca = pca.transform(X_train_scaled) Isn't that supposed to be X_test_scaled?",True
@Mustistics,2022-06-07T11:23:49Z,0,"One more question: when you're defining the param_grid, you have a comma after the last curly brackets. It actually works with or without that comma. I don't get why it isn't throwing an ""error"" in there, since that comma isn't supposed to be there. 🤔",True
@Mustistics,2022-06-07T07:10:10Z,0,"Hey Josh, thanks for the video.  One question: you drop the ID column right from the start. In real life, once you made sure your model is valid and accurate, you would actually need to match those IDs to the probabilities of default. How would you do that? Put the ID in a list before dropping and then adding the list as a column to the predict proba?",True
@lollmao249,2022-06-01T19:24:18Z,0,can SVM be used for forecasting timeseries ?,True
@dhirachatchayaporn8769,2022-05-24T05:36:46Z,1,Thank you for great tutorial!!!,True
@ilducedimas,2022-05-17T21:20:35Z,1,"what a lovable smart man, thanks for the great work!",True
@leebradbury8879,2022-05-10T18:33:39Z,0,"Another great video, I wish I had found this channel years ago!  I am assuming the way you have coded for the optimising of Parameters could be used as the basis code for other models like Random Forest and it will just be the parameters changing dependent on the model that is being optimised?",True
@tolga1292,2022-05-08T18:19:38Z,11,You Sir are an outstanding educator.,True
@jackjakie6076,2022-05-01T03:26:40Z,0,"Hello, I am a student from China. When can I support payment by Alipay or wechat?",True
@felipenogueira2462,2022-04-24T18:10:45Z,1,I instantly liked the video just for the Ukulele,True
@joxa6119,2022-03-25T04:43:24Z,0,"The most perfect guide for SVM in Youtube. Will donate after I get my first job! Thank you so much.   Btw, I have question, why don't you use PCA before doing the modelling part? Are PCA only been use for visualization?",True
@shwetaredkar734,2022-03-23T08:28:35Z,1,Triple BAM!! Guess What?? You are the best teacher I've ever come across. My life is saved. Good to know you play Tabla too.,True
@moodrammer8205,2022-02-21T16:56:40Z,1,Very useful ! Thank you very much  !,True
@beautyisinmind2163,2022-02-16T08:45:07Z,0,"why different split has different accuracy like 66:33, 70:30, 80:20?",True
@maishamahboob7423,2022-01-30T18:14:34Z,1,Are those Tablas behind him?,True
@martinparidon9056,2022-01-14T10:30:32Z,0,I have a request. You explain brilliantly (also with your background  info in other videos) how to create and optimize your SVM. Could you also make a video about how to actually use your svm in a target system? That would make sense I think. Because I think that this would necessitate saving the scaler during creation of the SVM and loading it at runtime. Regards.,True
@hareezvizard9233,2022-01-12T14:11:26Z,0,"If my df is 101, what value should i set on n_samples? is there a specific number? like in the video, you use n_samples=1000. one more thing is downsampling the same as splitting? i mean both are the same but different ways/methods or what?",True
@hareezvizard9233,2022-01-11T07:00:27Z,0,"33:46 how is your svc show many details like the value of C, degree and etc? when i run my code like yours, it only showed SVC(random_state=42)....",True
@martinparidon9056,2022-01-07T13:01:43Z,1,Thanks a bunc. Helping me a lot getting started with my SVM. Regards,True
@iunknown563,2022-01-06T15:23:05Z,1,Very approachable!,True
@JoaoVictor-sw9go,2022-01-04T22:45:26Z,0,"Josh, this video has helped me out a lot in my studies, but I have a question. When we scale the data, we should also include the categorical variables? Shouldn't we just scale all the data excluding the categorical ones?",True
@nick9198,2021-12-12T09:35:45Z,1,"Your dedication is unreal, you replied to all the comments. Wow!  p.s. thanks for the video",True
@heko502,2021-12-06T08:47:30Z,0,Can you explain SVM but with a regression problem (continuous variable for the dependant variable) ? (Thank you so much for your awesome videos),True
@ruesaintdenis,2021-11-12T22:31:02Z,1,"wow, he's handsome.. who knew!",True
@will6403,2021-10-25T17:38:08Z,0,Why are you fitting your GridSearchCV on only the training data? Shouldn't you pass your entire X dataset when doing GridSearchCV with cross validation?,True
@GaMiNGYT-dc2cf,2021-09-29T18:52:31Z,1,This guy doesn't deserve the dislike button to be in his videos...what a clear explanation!!!,True
@cool_sword,2021-09-23T00:05:31Z,1,"You already get a lot of love, but I have to add to it and tell you how great these are. No joke, I've had nights when I plan on watching some TV or some movies and I decide to check out some 'Quests instead!",True
@sahilpandita2964,2021-09-21T20:55:37Z,1,"When Josh said 'OH NO!!', I was waiting for the line 'Terminology Alert!!!'.",True
@LiquidMasti,2021-09-07T09:36:57Z,0,23:32 - mah man! lose temper like every programmer,True
@RaviRajput-mq2ew,2021-09-03T14:29:50Z,1,This is really great. Thank You Sir for this great effort!!,True
@kenricktan5271,2021-08-31T12:20:15Z,11,"I'm so happy to find out that saying BAM + DOUBLE BAM comes naturally to you (and was not just for the videos). Amazing walkthrough as usual, Josh!",True
@andreabvtt,2021-08-15T14:50:29Z,1,Amazing content! How do I know when you have a webinar planned? and where do you stream it? Thanks!!,True
@harsharajendran41088,2021-07-13T00:42:03Z,0,where are the predicted y_values? Only the graph is shown in the end. where are the numerical results? What's the code for obtaining that?,True
@KirillBezzubkine,2021-07-09T21:00:35Z,0,8:22 - 'feel free to play with yourself',True
@jovanagluhovic3139,2021-07-07T11:25:36Z,1,It helped a lot! Thank You on shared time and knowladge.,True
@lprashanthi7298,2021-06-27T10:49:15Z,0,How do we set values for C and gamma especially the penalty Parameter C .. is it only by Hit and trial?,True
@anuragsharma-os3vj,2021-06-19T05:22:44Z,1,Your videos are so informative as always. The way you explain the topics are on another level. But I see a Tabla(twin hand drums) behind you. Do you play that?  I also loves to play Tabla. Double BAM!!!! :D,True
@LLFRA,2021-06-14T14:44:26Z,0,what if y is categorical too and its also already encoded? how to feed the SVM with it? thanks in advance,True
@mahendranandi5622,2021-06-07T14:53:55Z,0,sorry. but thanks. from where can i get these two jupytor notebooks ? i need to practice once.,True
@baomiTV,2021-05-27T04:06:34Z,57,"After eight years of employment after graduation, I got laid off in 2020. I went back to school to pursue my second master in Data Science. I was still confused after machine learning classes, but after I watched your videos which were the same topics as the ones in my classes, you led me into a totally different world. Same concepts were taught by you in much easier way. BAM!!!",True
@pakistanproud8123,2021-05-11T23:27:06Z,0,"How can we generate simple scatter plot with just one line after we apply svm.fit(x_train , y_train)  ??",True
@omidforoqi4163,2021-05-08T10:55:19Z,1,I love StatQuest. please continue to make video with python =),True
@AntonioRodriguez-bg2mb,2021-05-06T11:35:26Z,0,me sacaste un susto con la musica de inicio XD,True
@vram11,2021-05-06T08:01:07Z,1,Precise and to the point. Luv this and I am def going to extend my support to you,True
@rogertea1857,2021-04-13T08:55:38Z,1,Triple BAM!!!,True
@chethan6311,2021-04-10T14:40:55Z,0,"In  Build and Draw Final SVM section, I was thrown by an error for the line ""optimal_params.fit(pca_train_scaled, y_train)"" mentioning  ""Invalid parameter c for estimator SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',     max_iter=-1, probability=False, random_state=None, shrinking=True,     tol=0.001, verbose=False). Check the list of available parameters with `estimator.get_params().keys()`.""  Iam not able to over come it Can you help with this",True
@ionut5316,2021-04-09T16:55:01Z,1,I purchased the notebook and I also watched the whole ad so you can make more money.,True
@liangke4276,2021-04-06T16:42:01Z,1,you video deserves to be translated into more languages so people don't speak English can also learn from your amazing content,True
@mucahitdemirc,2021-04-05T07:11:21Z,48,I will definitely donate to this channel as soon as I got a job! Thanks.,True
@samanvafadar7719,2021-04-04T19:34:43Z,0,"Again great video , Thanks. just 1 question , hope you answer.. is there any thing like ""model importance"" in Rstudio ? i need those independent variable influence ..",True
@imdadood5705,2021-03-24T07:49:38Z,1,How it started: df  How it is going: df_23_without_missingdata_scaled_with_magic_powers,True
@junobullish,2021-03-21T05:46:02Z,1,i love you!,True
@abir95571,2021-03-18T12:49:56Z,5,929 likes and 0 dislikes ... that's a triple BAM !!,True
@ivnesapple479,2021-03-18T09:41:12Z,0,"Hi,Josh.  Is SVM sensitive to the correlation  between the features?  I think Marriage_1 and Marriage_2 and Marriage_3 is correlated.(the sum is 1)",True
@ivnesapple479,2021-03-16T08:13:01Z,4,"Really appreciate for your slow speaking speed ,which makes it possible for not a English speaker ,like me ,a Chinese,to  learn.",True
@swalehomar3753,2021-03-06T05:51:59Z,3,This is amazing! Am in love with your approach of handling these stuff. Very clear and concise.,True
@piyushnchauhan,2021-02-22T15:34:24Z,1,Face behind lovely❤️ hello,True
@pielang5524,2021-02-17T07:07:16Z,1,807 likes 0 dislikes. That's super bammm!,True
@jack.1.,2021-02-16T19:12:15Z,3,"Really amazing video, I've been in and around data science and ML for a while but this is the first time I feel like I've gone the full way from mathematical concept -> working program (using medium complexity ML methods) -> insight/ question answered.",True
@whispers191,2021-02-16T16:16:27Z,1,Great tutorial! Thank you!,True
@ashishgoyal4958,2021-02-15T06:55:59Z,8,Thank you so much for making this amazing code-walkthrough for  SVM. Looking forward for more code walkthroughs like this.,True
@khaganieynullazada2794,2021-01-27T23:51:58Z,4,"Again great work Josh,  thanks so much. I actually worked at UNC-Chapel Hill, but I discovered you after moving to another University. Hope will meet you one day to thank you in person for the amazing content you are creating.",True
@tanphan3970,2021-01-25T05:20:41Z,0,"Dear Josh,  I fell not logic when you set the C hyperparameter in 2-time(when apply to X_train_scaled and pca_train_scaled) you define the param_grid. The first,  C= 1000 is not in your list, the second C = 1000 is adding and it is becoming the best parameter in grid-search.  Any ideal in this step?  Thanks and have a nice week!",True
@tanphan3970,2021-01-25T05:09:38Z,0,"Dear Josh,  My understanding, n_components hyperparameter in PCA() is the number of dimensions that we want to reduce down to.  Therefore, I make some confusion. 1. If we use PCA() with no reference any n_components, so what exactly is the number of components in this case? 2. In other tutorials, n_components can set in floating (0.0 to 1.0), it is not make sense if we understand as a dimension number. Thanks, have a nice week!",True
@tanphan3970,2021-01-22T06:47:20Z,0,"Dear Josh,  I do not understanding your decision in Confusion Matrix.  Do not default : 79% and defaulted: 61% --> not awesome.  Why?  This number is small, isnt it?  or do you have a threshold in your decision?",True
@tanphan3970,2021-01-21T08:06:34Z,0,"Hello Josh Starmer,  Can you explain more about some hyperparameter in resample?  replace=False -->  we will not change any data in original data (df_default) and if True mean original df_default will be changed? random_state --> help others can get the same result with you? So how many people can get same result to you? 42??? Thanks",True
@anikar1302,2021-01-21T03:58:30Z,3,i always love those musical intros,True
@yeyuan4235,2020-12-27T07:05:22Z,1,"Josh - Thanks for the video and it is super helpful!! A couple of questions though: 1. Under ""Transform the test dataset with the PCA..."", should we use the code that you commented out - i.e. X_test_pca=pca.transform(X_test_scaled), instead of X_test_pca=pca.transform(X_train_scaled)? didn't get why we applied the PCA transformation on train dataset to derive testing data. 2. Noticed that 1,000 defaults and 1,000 non-defaults were selected to construct the training sample. Do the numbers of two classes have to be equal for SVM? If not, would this cause any bias as the ratio seems a lot different from the original data? Thank you!",True
@michal.tomczyk,2020-12-18T11:04:10Z,0,"Say, in the original data set, we had a ratio of 30:70 of defaulted to non-defaulted credit accounts. Is it obligatory to have a balanced down-sampled data frame before we proceed with the analysis?",True
@stunning-computer-99,2020-12-18T01:27:10Z,1,643 likes and 0 dislike- triple bam!,True
@xinwenhe8665,2020-12-07T20:09:59Z,1,"Thank you for sharing - this is an amazing video! I have one quick question tho: I can understand why we are scaling the continuous variables, but why are we also standardizing the dummy variables?",True
@rizwanmuhammad6468,2020-12-07T06:16:24Z,0,"Thanks.  Wondering if there are more than 3 classes as target, do we yes one hot encoding for that ?  And if so how would the code work.. does y become a 3 column  dataset.  And do models take that as is.",True
@harvey2242,2020-12-06T03:05:18Z,1,Awesome as always!!! :),True
@kappa7072,2020-12-06T00:19:11Z,2,"Josh, you are wonderful! Thanks a million form Italy!",True
@TD-in5qe,2020-12-03T03:03:24Z,1,"This is amazing. Thank you, Josh!",True
@user-tz9sr4fy1z,2020-11-30T10:25:15Z,1,"Your videos are amazing !!!! I am soo happy u clearly explain many of the the topics I need!! :)   (p.s. do u receive requests? I would really love a StatQuest on AR,MA,ARIMA,SARIMA models)",True
@DarkLemon4321,2020-11-23T19:03:53Z,0,"Great lecture ;) but anyway I have one question - is it correct to standardize X_train and X_test separately? I mean, shouldn't the standardization parameters be the same for both datasets? In the current approach, the data are not comparable, as if they were from a completely different world. Am I correct?",True
@leonisaacs7231,2020-11-16T02:53:08Z,0,"Hi Josh, really great content, learning a lot.  Out of curiosity when doing One Hot Encoding, is there a reason why you did not say drop-first=True to avoid Multi-collinearity?",True
@marcmedawar8073,2020-11-06T12:59:49Z,0,"for the DEFAULTED column, we have much more column for people who defaulted that for people who did not, why do  you treat this imbalance classification problem as balance?",True
@georgenedelev6059,2020-11-03T09:10:49Z,1,You are a god.,True
@midhileshmomidi2434,2020-10-15T08:06:01Z,1,The man behind the voice,True
@irmaktekin3287,2020-10-14T11:53:05Z,2,Thanks! I really like the way you explain things: calm and simple :),True
@t.t.cooperphd5389,2020-10-07T15:10:51Z,57,455 likes and 0 dislikes.... that's a double BAM!,True
@zahrasoltani8630,2020-10-07T12:13:08Z,2,Can you explain why you used 'x_test_pca =pca.transform( x_train_scaled) when you wanted to transform test data with PCA?,True
@annapeng88,2020-09-23T07:23:35Z,6,I feel a bit starstruck finally seeing your face... :p Love your videos as always!,True
@kris12326,2020-09-21T00:43:11Z,1,Thanks a lot Josh!!,True
@zahrasoltani8630,2020-09-16T06:47:24Z,2,"Hello Josh, Do you have any lecture about support vector data description (SVDD) as well. Actually, your way of describing problems is amazing.",True
@liuchen6870,2020-09-12T15:03:16Z,0,"Hi Josh!  What if our dataset has 【continuous columns】 & 【""categorical number"" columns】 at the same time, should we start with getting dummies first to convert our categorical columns to continuous columns AND Standardscaler the rest continuous columns in order to give the data 0 mean?    Is there any correlation between ""get_dummies"" & ""encoder"" ?  I really appreciate any answers you would share with US, cheers!",True
@pareshnavalakha7127,2020-09-01T16:23:30Z,5,Hope to listen to the Tabla's behind you at the start of your training one day.,True
@mufasaaluma6387,2020-08-30T19:42:54Z,0,"hello sir please am having issues virtualizing my SVM model......can u go through this script and confirm the fault......am using python 3.8 IDEL import pandas as pd from sklearn.datasets import load_iris iris=load_iris() dir(iris) print(iris) print(iris.feature_names) df=pd.DataFrame(iris.data,columns=iris.feature_names) print(df.head()) print('\n') df['target']=iris.target print(df.head()) print(iris.target_names) print('\n') print(df[df.target==2].head()) print('\n') df['flower_name']=df.target.apply(lambda x: iris.target_names[x]) print(df.head()) print('\n') from matplotlib import pyplot as plt df0=df[df.target==0] df1=df[df.target==1] df2=df[df.target==2] print(df2.head()) print('\n') plt.xlabel('sepal length(cm)') plt.ylabel('sepal width(cm)') plt.show()  #please am getting an error when trying to virtualize this function below.....   plt.scatter(df0['sepal length(cm)'],df0['sepal width(cm)'], color='green',marker='+') plt.scatter(df1['sepal length(cm)'],df1['sepal width(cm)'], color='blue' ,marker='-') plt.show()",True
@EvandroSegundo,2020-08-16T19:10:34Z,0,"Great tutorial! In fact, all your videos are great. I have just on question: When looking for the best value for C, the algorithm went for the upper limit. Shouldn't we try again with higher values as suggestions? I haven't tried myself so I really don't know what would happen.",True
@lucaslai6782,2020-08-12T22:15:41Z,0,"Hello Josh,  from a statistical perspective, how do you deal with ""weird data""?   As an illustration, for this dataset,  EDUCATION, Category 1 = graduate school 2 = university 3 = high school 4 = others   However,  df['EDUCATION'].unique() array([2, 1, 3, 5, 4, 6, 0], dtype=int64)  How do you deal with ""5 and 6""?  They are not in the category. Do you treat them as ""missing values'? Also, how about some data values which are out of range? They are definitely wrong.",True
@pawanpant9707,2020-08-12T05:58:14Z,1,you are amazing.. :),True
@bytesizebiotech,2020-08-05T13:56:35Z,1,"So, although the publishing company is elsevier, they are not the ones who did the research. If you ever want to read a paper, you can send an email to the primary investigator (the last author of the paper) or any of the first authors really, and they will freely give you the article to read",True
@Bilal-sz8pk,2020-07-28T15:39:48Z,0,"Hi Josh,  I have a question. in 32:30 ,we scale the X_test and X_train, but i think that they didnt scaled same way. Bc They are not in same sample and their standard dev and means are different from eachother.   I tried with this tiny sets to check if i think correct, and looks like scaling process little  wrong?  xxx = [1, 4, 400, 10000, 100000] yyy = [1,4,400,10000,11] scale(xxx) scale(yyy)  Can u check and write me, did i think wrong?",True
@alexandremondaini,2020-07-21T09:45:01Z,1,"Hi Josh, Thank you very much for your lessons ! you explain very well unlike many teachers. I just have one doubt, when you scale(X_train) and scale(X_test) you're actually scaling the encoded 'categorical' variables. Thus the sparse encoded matrix of 0 and 1 encoded by the features ['SEX','MARRIAGE',....] will be scaled as well, is that correct ? Shouldn't be only the numerical features to get scaled ? Thanks a lot for your lessons",True
@RD19899,2020-07-12T15:36:12Z,0,"Josh Starmer is more like Jimmy Mcgill from Better call saul, a whole package playfully he is making us understand it's easy no rocket science. Though your hair style looks like him",True
@lucianotarsia9985,2020-07-07T06:44:31Z,4,"Hi from Argentina.  Great video! It really was from start to finish, it covers every step with dedication.  Thanks for sharing your knowledge!",True
@finderlandrs7965,2020-07-05T16:19:59Z,0,"Hey Josh, could you make a video explaining the softmax function? Thanks!",True
@anushkadwivedi9833,2020-07-05T07:16:45Z,0,"Sir, Your work is amazing and if you could help me with this as I am working on classification problem and I want the probability of all the target categorical output. So, how to do it?",True
@steelcitysi,2020-07-05T04:03:12Z,1,"You are awesome. I hope you do something on NLP (tf idf, word2vec, etc.),  for some reason your style was made for my brain",True
@hrdyam865,2020-07-04T13:13:45Z,0,"Thank you very much.. In the radial basis function video, only hyperparameter gamma was involved.. regularization parameter C was not there in the radial kernel function.. Are we using different radial kernel function here or the same one which was shown in radial kernel video? Thanks again.. your videos are great help ..",True
@KomangWahyuTrisna,2020-07-04T11:41:59Z,1,I learned a lot from your channel. I am a big fan of you. Looking forward for your Deep learning and NLP tutorial with python,True
@bjornlarsson1037,2020-07-04T08:44:57Z,1,"Great tutorial Josh! You must truly have one of the highest thumbs up to thumbs down ratios on youtube. Just two questions. 1) Right now you are using standarscaler on all of your variables, including the ones you have encoded. What is your reasoning for this instead of just scaling the continous variables, or maybe it doesn't affect the result? 2) What are your thoughts on onehotencoding before vs after splitting the data? Obviously right now, when your doing get_dummies your are doing it before splitting the data. From what I have understood, whether to do it before or after splitting is a pretty heated topic and I have found several questions on stack exchange where half the people say do it before and the other half say that doing it before is absolutely wrong and that it instead should be done after. In this dataset it will have an effect, because using your random states will produce a train test that on some variables have fewer categories than the test data does, which would mean that those observations should be dropped if onehotenconding is done after splitting. If I instead used onehotencoding before splitting, they would not be dropped. Would love to hear your thoughts on that topic, because I have found no real consenus on what is the right approach. Thanks again Josh!",True
@deepanjan1234,2020-07-04T06:24:48Z,1,You are just awesome. I just love your videos as they are really amazing.  Stay safe .,True
@caseyhannan1858,2020-07-02T19:55:13Z,1,is that a pump organ?,True
@mainakray6452,2020-07-02T12:35:08Z,1,"gr8 experience, looking for ANN.",True
@roni123467,2020-07-02T07:17:02Z,5,"Really detailed and nice lesson! I liked how detailed the explanations were, It is definitely DOUBLE BAM worthy!   Thank you.",True
@Mayday4u,2020-07-01T10:38:29Z,0,"@StatQuest with Josh Starmer First before all, I really like _all_ your videos especially this one!   I have a comment on your comment on your scaling of the X data. You are scaling the train and test data with two sperate scalers. However this is not optimal since the mean/std of the train and test data can differ. For example, let's say the mean of your train data is 5 and the mean of your test data is 6 and that both sets have a std of 1. Then a observation x with a value of 4 is normalized differently depending on when it is observed. If it is observed in the train set then it is normalized to -2 and if it is observed in the test set then it is normalized to -1. This should not be happening since the information of the observation should not depend on the scaling. Further since the model is trained on the train set a value of -2 could have a different meaning to the model and ultimately for the prediction than a value of -1 (if the unscaled trainset contains 3 for example).  So you should scale the train and test data using a scaler which is trained on the train data.",True
@bosepukur,2020-07-01T09:08:41Z,1,Josh u r an inspiration in teaching...Plz keep it up,True
@yashlakdawala7232,2020-07-01T08:02:45Z,2,Dare someone dislike the video.,True
@sachof,2020-07-01T07:12:26Z,0,Is it your first video with Python ?,True
@satyajitrajbanshi3620,2020-07-01T05:25:30Z,0,Very helpful video for students like me ... would be more helpful if I would get the file for free.,True
@feravladimirovna1044,2020-07-01T03:58:18Z,0,"thanks! I just did not understand how to download the dataset , on the site I just found (default of credit card clients.xls) and not (default of credit card clients.tsv), maybe I should follow the second method in your slides",True
@albertog2196,2020-07-01T02:45:26Z,0,What does 'gamma' represent?,True
@deojeetsarkar2006,2020-06-30T23:34:04Z,1,Good to see no haters for the saintly man.,True
@moona5454,2020-06-30T22:59:38Z,1,"I am not an expert but a small help for everyone here ^_^ , if you want to find the missing values very easily, you can type dataframe.isnull().sum() ; dataframe is the name of the object containing the data. And thank you Josh for the amazing webinar ♥",True
@randommcranderson5155,2020-06-30T22:51:30Z,29,"You're a pretty amazing nerd, I love it. This is an amazing tutorial.",True
@DatascienceConcepts,2020-06-30T20:52:05Z,1,Awesome teaching! Very interesting lectures.,True
@elvsrbad2,2020-06-30T19:35:48Z,1,This video came out the same week I decided to learn this.  Get out of my head!,True
@alkapandey1008,2020-06-30T18:44:16Z,2,You are amazing. Keep posting. Best wishes from India.,True
@ramakdixit8648,2020-06-30T18:22:15Z,1,Wow. Thanks Josh . Your videos are always a go to resource,True
@jamesvalencia3298,2020-06-30T17:54:49Z,1,I always ser your videos! Please continue this series of videos and surely I will purchase a notebook soon.,True
@pkmath12345,2020-06-30T17:53:34Z,4,Love python! Been using R much lately! Would love to have some of R videos,True
@gspb4,2020-06-30T17:52:47Z,1,Hey Josh. I've been noticing you've done your last few practical videos in python. Are you planning on making similar videos in R? What dictates whether you code in R or python?,True
@statquest,2020-06-30T17:44:21Z,21,NOTE: At 31:25 we should use the mean and standard deviation from the training dataset to center and scale the testing data. The updated jupyter notebook reflects this change. ALSO NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: http://statquest.gumroad.com/l/iulnea  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@adityay525125,2020-06-30T17:42:58Z,0,Hey Josh can we get this ipnyb file please?,True
@schris3587,2020-06-30T17:41:03Z,1,Thank you Josh!!!,True
@NoOffenseAnimation,2020-06-30T17:40:29Z,1,I'm early! Bam!!!,True
@RahulEdvin,2020-06-30T17:36:20Z,6,"Josh, you are Phenomenal! Love and Respect from Madras !",True
@arjunmallick4901,2020-06-30T17:36:04Z,1,Aahhhh....Something that I was stuck with...thanks a lot❣,True
@RealSlimShady7,2020-06-30T17:35:36Z,1,FIRST!!! HOOORAY!!!! BAMMM!!!!!,True
