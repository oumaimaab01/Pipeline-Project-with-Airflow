author,updated_at,like_count,text,public
@statquest,2023-07-23T10:17:02Z,46,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@sauravchandra10,2024-05-30T20:28:05Z,1,"It is so complex, ill have to watch this 2 3 times. But you did much better than anyone else. Thanks.",True
@balintnk,2024-05-30T19:13:22Z,0,"Awesome video!! A question. If the key - query resolution always passes through a soft max function, won't it always result in the center world being the highest probability?",True
@mehdinickzamir6778,2024-05-30T15:42:49Z,1,Wonderful! There's so much detail packed into such a short video. It took me three hours to understand it all :))),True
@adityasuryan37,2024-05-30T14:50:51Z,0,"Sir I have a question. Do we only apply different weights for position encoded values or do we apply different weights for query, key and value for each multi head attention cell as well?",True
@esmeluo6574,2024-05-27T23:11:10Z,0,"This is such a great video! I do have one question, it looks like encoder calculate self attention for all words in the input (regardless of ordering) but decoder only compute self attention related to the words that appeared previously, it this the correct understanding? It also looked like the work for encoder can be parallelized but decoder is sequential (since the second token use first token as an input. Is this the correct understanding?",True
@dy8576,2024-05-27T14:11:09Z,2,"I keep coming back to this video, every time i forget about the inner workings, and its always as easy to regather everything. What content!",True
@paratracker,2024-05-24T15:56:36Z,1,"Love the detail Josh, but you keep saying 'we' do <xyz> when, I suspect you often mean a/the training process, not an action taken by the developer at the keyboard. I THRIVE on courtroom level specifics, otherwise I end up with foggy memory. Could you please be unambiguously precise when referring to subjects and objects?",True
@afro_rush3882,2024-05-23T06:05:07Z,0,Is the section in the Encoder-Decoder Attention also known as the Cross-Attention or Multi-Head Cross-Attention??,True
@chocky_1874,2024-05-21T17:57:25Z,1,"Best Explanation, thanks :)",True
@vladimirbosinceanu5778,2024-05-20T09:52:30Z,1,"Amazing as always. The internet is a better place because of humans like you. Thank you, Josh!",True
@ziku8910,2024-05-20T05:34:52Z,1,"üé∂üé∂ Welcome to the Stat Quest, it's time to learn and groove, With Josh Starmer's rhythm, we‚Äôre in a stats-approved move. We're diving into data, with R and Python too, Let's break it down together, 'cause that‚Äôs what we do!  (Chorus) Stat Quest, yeah, we're breaking it down, With histograms and p-values, no need to frown. Regression, correlation, we're making it fun, With every single lesson, we‚Äôre getting it done!  (Verse 2) From basic distributions to the mean and the mode, To more complex analyses, we‚Äôll lighten the load. We‚Äôve got box plots, scatter plots, and violin, too, Visualization magic, it‚Äôs all here for you!  (Bridge) And when it gets tricky, don‚Äôt you fear, Stat Quest is here to make it all clear. We‚Äôll tackle those concepts, big and small, Step-by-step learning, we‚Äôll conquer it all!  (Chorus) Stat Quest, yeah, we're breaking it down, With histograms and p-values, no need to frown. Regression, correlation, we're making it fun, With every single lesson, we‚Äôre getting it done!  (Outro) So let's get statistical, with confidence high, In the world of data, we'll reach for the sky. With Josh as our guide, we'll learn and improve, Stat Quest forever, let‚Äôs keep in the groove! üé∂üé∂",True
@abdullahbaig7517,2024-05-18T11:58:40Z,1,It's interesting how many different topics one has to contextualize to understand something like transformers! It's great to see all the math happening in detail and stop and ponder time-to-time while learning something as complex as transformers for the first time. It really helped build my intuition of a lot of building blocks for a transformer-based neural network. Thank you!,True
@ClarenceWijaya,2024-05-17T12:13:14Z,0,"Josh I want to ask you a question. So in your explanation the word embedding operation includes a multiplier and adder operation, but when I check other explanation about transformers, including the one they put in the tensorflow tf.keras.layers.embedding, the operation performed is more like a lookup table where the weights is directly considered as the vector representation of each token. Did I understand this wrong, or this is just a different way to do word embedding?",True
@jamemamjame,2024-05-16T20:02:11Z,1,"you are the unique ML teacher guy in the world, and I don't think anyone can explain this thing like you. Thank you myself for knowing your channel!",True
@guilhermecorrea3604,2024-05-16T11:48:55Z,1,Very well explained! thanks for making this,True
@empatikokumalar8202,2024-05-08T08:11:58Z,0,How is the input and weight value formed? Where do we find those values?,True
@christinecalapati6673,2024-05-07T16:42:44Z,1,Hi! I am really learning a lot here. May I know where to access the manuscript mentioned here? Thanks a lot! üòä,True
@Luxcium,2024-05-07T09:50:42Z,1,One day I will know how to use Lightning üòÖ hopefully it will be by JS on SQ üéâüéâüéâüéâ,True
@ankushkothiyal5372,2024-05-06T16:05:11Z,0,Why does Josh sounds like a voice assistant in this video?,True
@n.h.son1902,2024-05-04T04:23:39Z,1,"Thanks a lot. This is an amazing video explaining how Transformers work in detail. The visualization is a great point. I've learned and revised some properties of each main component in the network during the video. I like the way you lead us to do the maths bit by bit to the end result. It'd be much more intuitive, however, if you could show us the formula for positional encoding. I know for the weights for Q, V, and K we can train neural networks to get them but for the positional encoding, they are just explicit sine and cosine functions. So I was kind of curious about how you actually computed the numbers for the position embeddings. Anyway, it's a great video. Thank you again!",True
@ydafuss,2024-05-03T14:26:10Z,0,Stop with the BAM! tho,True
@sangdilbiswal30,2024-05-01T18:58:40Z,0,totally above me,True
@agentmama8338,2024-05-01T14:30:54Z,1,Guruüôè,True
@tomasjavurek1030,2024-04-29T13:04:08Z,1,"Oh, statistics, you're the key, Unlocking secrets for all to see. With histograms, box plots, and scattergrams bold, We'll navigate the data, as the story's told.     (chatgpt 3.5 answer to StatSquatch's question to this date, isn't that bad...)",True
@pranav7471,2024-04-27T20:15:38Z,1,"A great explanation of Transformer, the one thing I found missing was the decoder has a masked self attention, to prevent future embedding from ""leaking"" into current output",True
@jordanmuniz6167,2024-04-27T01:26:29Z,1,Your videos have to be the best instance of teaching I have ever seen! Thank you for the amazing work!,True
@VulpisInFundo,2024-04-24T20:25:15Z,0,The encoder-decoder part isn't clear to me at all. Where do the Spanish words in the video come from? How does this system account for different verb conjugation and grammar?,True
@jerrytang9514,2024-04-22T00:54:20Z,0,"Great tutorial! Thank you but I don't understand why using the same sets of weight for query, key and value for each input word. Is it just for illustration purposes or does that also apply in practice?",True
@AkashSiddabattula,2024-04-21T15:34:12Z,0,"Sir i have a doubt in the vedio for the encoder decoder attention of eos we got output vamos(lets go) is it right translation? Or is it like position wise because we are starting with eos in decoder and lets go is in starting in encoder we get vamos after translation and so on, if this is the case then relation between input and output(encoder decoder attention) doesnt make any sense.please explain",True
@shamshersingh9680,2024-04-21T05:39:28Z,0,"How positional encodings help keep track of word order when the the words are interchanged in the sentence. For example for sentence ', the positional encodings for  and  are  and  respectively. Whereas for sentence '', the positional encodings for  and  are  and  respectively. So in this case, how the model knows whether it is squatch eats pizza or pizza eats squatch? I am bit of confused here.",True
@shamshersingh9680,2024-04-21T02:12:24Z,0,"Hi Josh, thanks for another brilliant explanation. Your videos are such a saviour. I have a doubt here. Can you explain a bit more about Positional Encoding. How the sine and cosine curve appear and what values of input words are used to encode positional values for each embedding. What guides the relative spread of each sine and cosine curve for each position. Thanks in advance.",True
@3r1kz,2024-04-19T13:12:13Z,0,"I think I'm missing something here. How is the self-attention actually calculated, what is the process? Are random sentences send into this layer to figure out how often the word ""it"" and ""pizza"" are in a given sentence and are the weights changed during this training to become more similar when they are actually found in the same sentence? Ah found it, I see the training of finding similar words is a separated proces done through the cosine similarity routine with a lookup table.",True
@user-et8es9vg5z,2024-04-13T17:39:08Z,1,"I finally decided to buy your book thinking there'd be transformers in the ""Neural Network"" section. But even if they're not, I'm glad it supports you. Your content is the best in popularisation that I've seen. It mainly helps me a lot to refresh and understand better than before to start my internship in AI after 1 year of gap year.",True
@hanhtrannguyen7791,2024-04-11T07:37:08Z,1,impressive content!!,True
@fatemehghanadi3046,2024-04-09T11:00:58Z,1,U explained it really clear. It was the best transformer video i've watched. <3,True
@response2u,2024-04-08T19:11:39Z,0,"It was all great until it hit the decoder section. I think that section was not as elaborately discussed as the encoder section, making it slightly harder to digest. All in all, great presentation!",True
@sgttomas,2024-04-07T16:29:23Z,1,"How do you have the same voice as the Chemical Process Safety Board, Incident Investigation Report Video from 1993?  Love it!!!",True
@KarevAS99,2024-04-07T04:19:07Z,0,Thank you so much for your videos! They greatly help me with my research. One thing I've noticed both about this video and a video about Decoder-only Transformers is that it looks like you have skipped the last Feed Forward Network part of the original Transformer model. Was that intentional? Does adding FFN result in any significant performance improvements?,True
@alihaghighat1244,2024-04-04T17:30:37Z,0,"Very good as always! Just a quick question. When we want to translate let's go from English to Spanish, the input we have is only 'Let's go'. How do we pass <EOS> to get one of the words among (ir, vamos and y)? I mean how do the models know to candidate ir, vamos and y?",True
@colossuselka-zc7hb,2024-04-04T11:33:54Z,1,such a gem!!!,True
@REV_Pika,2024-04-03T06:59:08Z,1,"its amazing how you make a 2 hours lecture in just 30 mins and explain it way better, after finishing this video and realizing what I just grasped, its mind blowing how you can make such complicated subject easy to understand. thank you very much!",True
@JavierSanchez-yc8qo,2024-04-03T05:44:58Z,1,@statquest  you are a true professional and a master of your craft.  The field of ML is getting a little stronger each day bc of content like this!,True
@akarimsiddiqui7572,2024-04-03T02:22:37Z,1,I finally found you! Thank you for this detailed yet super simple break down.,True
@abhinavsb9228,2024-04-02T19:53:22Z,0,WOOOW!!...So basically every AI models are just bunch of random mathematical functions and equations nothing more than. Now I'm realized even human intelligence is just a complex arrangement of random mathematical equations.,True
@zhenliu3335,2024-04-02T09:42:16Z,0,"5:47 you say the weights in the network for ‚Äúlet‚Äôs ‚Äùare the exact same as the weights in the network for ‚Äúgo‚Äù,I am so confused üòÖthe one is 1.87,the other is-0.78,why they are exact same?can you explain?thank you ‚ù§",True
@CruelCrusader90,2024-04-01T19:49:18Z,0,"as a first time viewer its hard to take this seriously when it sounds like a dora the explorer parody XD im not even 2 minutes in and already wanna leave. but its hard to ignore the following and after reading some of the comments ill tolerate the childish overtone.  out of curiosity, is the childish execution a play on how memory athletes use humor to remember things? or how ""only when you full understand something can you successfully explain it to a child"" kinda thing?",True
@thecaveytchannel,2024-04-01T18:56:19Z,1,BAM!,True
@umang9997,2024-04-01T18:45:02Z,0,"I am sorry, This might be a really stupid question. But in the diagram at 34:14, which block is actually called the transformer? Is transformer the whole encoder-decoder block?",True
@premy,2024-04-01T00:50:01Z,0,"Did I understand correctly that the similarity calculated during self-attention only depends on each individual pair of words and not on all of the words in the sentence? So that in both sentences ""The pizza came out of the oven and it tasted good!"" and ""I was baking pizza in the oven and it stopped working!"" the ""it"" will be more closely associated with ""pizza"" given the same training corpus as assumed in the video, despite being actually associated with ""oven"" in the second case? I couldn't find a connection to the remaining words in the calculation and what you say at 14:20 seems to imply this. Anyway thank you for the video, I was looking forward to it but only got to watch it now!",True
@eclaircielyu,2024-03-31T01:21:24Z,1,Thanks! Really helpful!,True
@theunconventionalenglishman,2024-03-30T19:02:39Z,1,I've recently discovered your channell and I love it - the songs rule.   Cheers mate,True
@tusharchoudhary547,2024-03-30T15:56:25Z,0,"How is <EOS> more related to ""let's"" and not ""go""?",True
@sreerampa3904,2024-03-30T06:55:45Z,0,"At 19:44, I'm a bit confused. The self-attention value for ""go"" with respect to ""let's"" is higher than with ""go"" itself. Can you explain this. Thanks.",True
@yahyazakariaalimohammed4391,2024-03-30T01:24:59Z,1,perfect perfect perfect,True
@somyaagarwal2942,2024-03-29T19:13:31Z,1,God I am dizzy with all this info floating around in my head. Super awesome stuff Josh!! Thank you !! I'm gonna lie down now lol,True
@CharlesPayne,2024-03-29T16:29:16Z,2,"Not to be a buzz kill, but I suffered a bad Traumatic brain injury in my late 40's  after being hit by an SUV while stopped on a motorcycle. I'm blessed i survived . At the time  my job dealt with engineering and architecting IT solutions and I was looking  forward to advancing my career into AI and Machine Learning.  I was in a coma for a while and I lost lots of what i used to know. I know have Learning disabilities and memory issues.  I have improved some over the last years, but If I'm being honest with myself, I wouldn't want me as an engineer, so I'm trying to move into management.  I'm glad I ran across these videos .  I purchased the .pdf books and notebooks  today and I can honestly  say they are well worth it. Josh,  I'm so glad You created this material.  Your books and notebooks etc.. are helping me slowly understand complex topics in hopes that I can stay relevant and continue to advance my career.  Thanks again!",True
@limitlesslife7536,2024-03-29T13:31:37Z,2,you are a blessing for anyone who is a visual learner. You have the gift to be able to explain complex topics in easy way.,True
@erikkiessig,2024-03-29T12:18:01Z,0,Well so this means ChatGPT actually should spit out the result all at once instead of slowly typing it out?,True
@royhuang2042,2024-03-28T21:54:48Z,1,Ë∞¢Ë∞¢ÔºÅYou saved my life man!,True
@chaitanyasannidhi9982,2024-03-28T16:03:20Z,0,"Hello, I have a question regarding the implementation of positional encoding. What's the reasoning behind using alternative sine and cosine curves to get the positional values? I do not exactly understand how they encode position.  Please excuse me if I'm wrong, but is this similar to a cosine transformation?",True
@joko3968,2024-03-28T09:09:10Z,1,Danke!,True
@robinzhang8322,2024-03-27T16:02:14Z,1,"thanks a loooooot !  so amazing teaching video about ML, BAM! BAM! BAM!",True
@user-xp2gc7tm8h,2024-03-27T08:23:50Z,1,the best and simplest video to learn transformer ever!,True
@michaelcharlesthearchangel,2024-03-27T00:59:26Z,1,"Bravo! Excellent teaching skills! Teaching weights and biases is not easy but, by God, man, you've done it!",True
@vuhuynh8740,2024-03-26T07:39:34Z,1,StatQuest is awesome!!,True
@askhataubakirov9370,2024-03-25T18:35:11Z,0,"hey! you're doing a great job with these videos! quick question from a beginner: ""where do the weights usually come from? are they guessed at some point and if so, how?""",True
@parulthakur3440,2024-03-25T18:19:46Z,1,What is quest,True
@Sandeep_Kotha,2024-03-24T07:31:34Z,0,"Hi, I see that at 18:56 mins, Before multiplying the scale values by softmax output values , i think we should divide by sqrt(dimension) right?As I can see the formulae for Attention score formuale is Softmax(Q*KTranspose/Sqrt(Dimension)) * V. I am little confused. Could you please help me undestanding that or pls let me know if i miss anything? thank you.",True
@harryliu1005,2024-03-22T09:22:18Z,0,"Another Great, inspiring video! I have a question: What tools you used to draw  structures of neural networks?",True
@Primes357,2024-03-22T02:54:52Z,1,"Josh, thanks for this class. I had a question: Were the Self-Attention values for ""Let's"" and for ""go"" (at 19:40) identical by pure chance or is there an explanation for why they gave the same value and I didn't understand it?",True
@jordantran3102,2024-03-20T16:25:44Z,0,"While I think some of your videos are great, I think by trying to simplify the explanation you made it less intuitive to understand, or apply. Since the self-attention mechanism is already very clean, using more inputs than just two, and using matrices to represent transformations would have been a lot more intuitive to understand as opposed to having to keep track of all the directions and arrows and numbers in different boxes all over the screen.   Just my opinion though! Lots of people here think this explanation was great, and fair enough, we all learn differently.",True
@henrydeutsch5130,2024-03-20T13:52:57Z,0,Personally I think the childish graphics and catchphrases like ‚Äúbam‚Äù take away from the video. You can explain things simply without including coloring book graphics I think,True
@MrKB_SSJ2,2024-03-19T20:30:51Z,0,15:44,True
@sk3ffingtonai,2024-03-17T16:52:39Z,1,üëèüëè,True
@ashutoshgupta3942,2024-03-17T05:30:23Z,1,Using Backpropagation we are training these query key and value. so that transformer can learn.,True
@afrasiabkhan6425,2024-03-16T19:29:56Z,0,rupai kaat overactinga kaaaa,True
@SofianMW,2024-03-15T08:50:43Z,1,thanks much!,True
@luckusters8568,2024-03-14T10:15:33Z,1,"Amazing explanantion. It really helped me understand the paper.  For anyone still confused about the meaning of/idea behind query, key, and value I also recommend the video ""Key Query Value Attention Explained"" by user ""Alex-AI"".",True
@malinyamato2291,2024-03-14T03:49:31Z,1,"I am a Master of CS Sc with speciality in AI stuent who has watched most of your videos. You saved me as you can explain AI and ML stuff much better than  anyone at my univ. After watching this I got the concept... however, as your squach monster pointed out: How and at what pints are the weights in the transformer calculated assuminglyh through some sort of backpropagation through the entire encoder decoder setup. That must be super complicated !!!",True
@tudor6210,2024-03-12T15:44:03Z,1,Thank you!! One of the best explanations of transformers out there.,True
@NewsLetter-sq1eh,2024-03-11T16:37:13Z,2,This is great!,True
@purpleblue0101,2024-03-11T07:27:30Z,1,I LOVE YOU.,True
@parthdodiya9428,2024-03-09T20:40:52Z,1,"I haven't yet heard what you are going to say in self promotion. But the phrase ""shameless self promotion"" made me subscribe channel and like the video üòÑ Too good video, Keep it up!!!",True
@irishchannel120,2024-03-09T14:16:55Z,1,"These videos on machine learning (and statistics!) are incredible and empowering! Plus they make me laugh! You are the reason that I was able to keep my head above water in my Machine Learning courses as a pregnant grad student with limited time, money and energy. Thank you and I will definitely be checking out some merch!",True
@CheniGivani-fh1kc,2024-03-07T07:11:55Z,2,Thanks!,True
@cs-60_kirtantank77,2024-03-06T06:18:14Z,0,"32:21 Also sir,  1) we had 4 output nodes in this dense layer but in real life scenario, we will have all the known spanish words in vocab, right? if no, please elaborate.  2) and what will be the first generated output token in the case we have these softmax values  ir (0.1), vamos (0.4), y (0.4), <EOS> (0.1) ?  Thanks",True
@cs-60_kirtantank77,2024-03-06T05:40:07Z,0,"36:14 Sir, I had one doubt, after we are done with dot product similarity (and softmax), why do we need to generate Values for each word, why can't we just use the embedding values directly. sorry if i am being naive here. Please help me clear that.  Thanks.",True
@mailanbazhagan,2024-03-05T17:42:34Z,1,I love the video to its full... The best lecture I have ever seen! Lovely!,True
@Waterlmelon,2024-03-05T17:00:30Z,1,amazing view thanks StatQuest!,True
@leechi-feng3393,2024-03-05T06:24:11Z,0,"Just wonder, why ""go"" has more relationship with ""let's"", rather than ""go"" itself ?",True
@bhogade,2024-03-04T09:53:26Z,1,Thanks! This is the best explanation and illustration of a very complex topic ! Keep them coming..,True
@aR-zf2wd,2024-03-04T00:33:39Z,1,Brilliant!,True
@lizzy9874,2024-03-03T13:49:16Z,1,"Hi Josh, it's such a great video to help me understand the basic concept of transformer. I have a quick question - even though you've mentioned the key value, the query value, the values, and the self-attention values can be calculated at the same time, the calculation of the self-attention value can only be started after the key, query, and value are completed since the self-attention value is based on the ""weights"" gotten from the word similarity. Is my understanding correct?",True
@GucciJohanne,2024-03-02T11:41:40Z,1,GOAT üíØüôå,True
@mohammadkazemsadoughi3880,2024-03-01T16:54:19Z,0,"Love all your videos. Just one question about embedding. What you are describing more sounds like first one hot encoding of the tokens. But more I am reading on this topic, it seems that neither during the inference or training, the input of embedding layer is not encoded in such a way. It basically integer encoded.",True
@Sujack-nt2mh,2024-03-01T04:52:10Z,1,This is the best video I have watched.you are awesome. I love u!!!ü•∞,True
@artem_isakow,2024-02-27T12:43:51Z,1,Always the best! üòÅ,True
@jhfoleiss,2024-02-26T18:19:14Z,1,"Great Video, Josh!  I'm curious about the rationale behind using sines at different phases and frequencies for positional encoding! Do you have any intuition behind this to share?   Thanks!  Love from Brazil!",True
@beauzeta1342,2024-02-26T08:31:56Z,0,"Thank you for the very clear explanation! At 16:05, I am wondering why Query network is necessary? Can we simply compute similarities between pair of Keys?",True
@shreyanshpriyadarshi781,2024-02-25T09:25:47Z,0,Can you please make a video on Hyena attention? Thanks.,True
@zzzosirozzz689,2024-02-25T05:16:02Z,0,"Your video is really interesting about Transformer Neural Network but I really still don't understand how they designed it, where is the relationship between Q, K, V and those cos functions?",True
@regivm123,2024-02-24T07:21:59Z,0,Thanks very much. But I am not clear how the transformer picked up the word 'vamos'. Is it from the entire list of Spanish words?,True
@shilpavpurushothaman,2024-02-23T17:57:27Z,1,excellent tutorial on transformers,True
@joewang9398,2024-02-23T10:03:26Z,0,"Great intro video! Some questions raise up after this video and hopefully I could get some of your time to help me understand better! Bam! When we talk about encode, we talked about the ability to do parallel computing 1. This came back to the original question that RNN is trying to resolve, the input length could be different. In the video example, it is only one example ""lets go"", but when we train the neural network, we might try ""lets go"", ""how is your day"", stuffs with different length. I wonder in this scenario, how do we resolve it? Do we predefine a max input length and we will pad up 0s for all the inputs which is shorter, or is this a dynamic neural network that changes its arch based on different input? 2. In the encoder diagram, self-attention cell from ""lets"" seems pretty independent of self-attention cell from ""go"". However when we calculate the self-attention values, we use Q and K from both cells collectively to calculate the percentage, then multiply it with V to yield self-attention values. So with this dependency, self-attention cells are not fully independent in a way. Is this understanding correct? 3. It seems we are only able to do parallelism on encoder but not decoder right? As decoder has to translate words one by one. Then is the statement at 27:12 wrong, since decoder can not be paralleled?  4. For the decoder, assuming we are generating 5 Spanish words here instead of 1. Then it seems K and V from <SOS>(at 27:47) will be used on every Spanish word translation(for percentage and self-attention value calculations), I wonder how do we optimize the weights which yield K and V for <EOS>, since they are used in different part of the neural network, I guess they will have multiple different gradients to do the updates?    Again much appreciate it for the video! Triple Bam!!!",True
@kunwei2005,2024-02-21T06:04:05Z,0,can't imagine this whole idea come out from some human's minds,True
@erikleichtenberg3950,2024-02-20T06:07:09Z,1,1 million subscribers and still taking the time to answer questions from his viewers. Absolute legend,True
@imten5518,2024-02-18T13:21:53Z,1,Thanks to Norm and Squatch for asking the same questions we had in mind....they indeed know telepathy :D,True
@ageofkz,2024-02-18T11:53:32Z,0,"Around @27:31, josh mentioned that the Q,K,V values can be computed all at the same time for all the words. But to calculate the self attention values, we need to do the dot product of the query vector for the i word with all other words and input it into a softmax function. So in a sense, to get the self-attention values, we need to wait for all words to calculate finish their key values?",True
@rocketPower047,2024-02-17T11:34:41Z,1,This was amazing! Great job!,True
@jytou,2024-02-15T15:54:47Z,1,Absolutely great video and explanation. I just discovered your channel. Subscribed. Bam!,True
@anggipermanaharianja6122,2024-02-15T14:40:21Z,1,BAM BAM,True
@BooleanDisorder,2024-02-10T21:45:20Z,1,"This is so mindblowingly complex and impressive. Great video! ‚ù§ The transformer architecture is also complex and impressive, ofc. üòä",True
@bugiothales,2024-02-10T14:08:33Z,1,Incredible video,True
@kothanos,2024-02-06T14:30:26Z,0,"This positional encoding seems to rely on the semantic information of the token being spread evenly across all dimensions of the word embedding. Let's say some token is NOT evenly spread, e.g. the embedding is (1, 0, 0, 0...), then it will have the same positional encoding if it's placed in positions 1, 3, 5, and so on... Could you give some intuition on how this is not a deal-breaker for encoding positions?",True
@hi.guin69,2024-02-05T09:39:35Z,0,I think it would be much easier to understand if you explained what QKV values mean. I needed to look for analogy examples on Stackoverflow to understand the concepts from Information Retrieval System.,True
@The_One_Who_Moves_the_Stars,2024-02-04T01:36:19Z,0,ÌïòÎÇòÌïòÎÇò Í≥ÑÏÇ∞ ÎêòÎäî Í≥ºÏ†ïÏùÑ Îã§ Î≥¥Ïó¨Ï£ºÎÑ§. ÎÇòÎèÑ Ïù¥Í±∞ Ìï¥Î≥¥Î†§Îã§ ÎÑàÎ¨¥ Îπ°ÏÑ∏ÏÑú Í¥ÄÎíÄÎäîÎç∞. ÎåÄÎã®ÌïòÎã§.,True
@jakob2946,2024-02-01T13:26:02Z,1,"THe main thing i was confused about was how you could have multiple heads(they had 8 in the paper u mentioned). What was the point of that. I understand you computed the relation of a query to every key, but how would that be any different with multiple heads, i.e what is the difference between the heads.",True
@weichen4061,2024-01-30T10:11:36Z,1,"fabulous,bam!",True
@AbhishekSinghSambyal,2024-01-28T21:38:37Z,1,Dot product can be so powerful. Transformers are beautiful. Thank you for this amazing video.,True
@ankitpersie1,2024-01-27T06:04:26Z,0,"@statequest : in the positional encoding example , were nt the weights supposed to same for different words in the example sets. Only the inputs were supposed to be varied.This is what i understood from explanation int he previous section on encoding.",True
@joajoa9584,2024-01-25T13:19:14Z,0,"It's counterintuitive that EOS gets ""Vamos"" as the output, and ""vamos"" gets EOS instead. Although thinking about it, it seems like the input for the decoder is how the output (of the Neural Network) is going, and the output (of the Decoder) is what word goes next (in the output of the NN). In addition, searching for the self-attention encoding for EOS doesn't sound useful. I guess it's because it's the start of the output, so it doesn't have yet other words to relate to.",True
@pratyushrao7979,2024-01-25T07:37:52Z,1,I had never struggled so much with understanding a concept before. But you cleared all the doubts. Thank you!,True
@omarboukherys5216,2024-01-25T06:16:48Z,0,"Hello Josh thank you the amazing content; I have a note here in 12:34 when doing positional encoding for let's go; In Vocabulary you worked with 4 token, lets and to and go and end of sentence; when doing token positional embeding of sentence ""lets go"" you considered token ""go"" as second word  although it is 3rd and not 2nd word.",True
@user-gq3uo8dl1l,2024-01-21T23:35:03Z,0,,True
@robertfontaine3650,2024-01-20T02:02:55Z,0,knurled networks,True
@user-me9tt8bn3n,2024-01-17T11:22:21Z,0,"I have one doubt about positional encoding. Why do we add the values of positional encoding to word embeddings to get the final (word + position) embedding?? we can also concatenate the values of positional embeddings to word embeddings to get final (word + positional) embeddings. For me, concatenation feels more intuitive than addition becoz how model is gonna know what fraction of a number represents position and what fraction of a number represents the semantic meaning of a word. Please let me know the answer if anyone knows...",True
@starlord3286,2024-01-16T15:06:29Z,0,"I like how he says ""In this example we kept things super simple"". Great video, thank you!",True
@aryamansinha2932,2024-01-16T10:07:49Z,1,what is self-attention on the decoder side? i.e. what is it getting context wrt.?,True
@tumultuousgamer,2024-01-15T23:29:33Z,0,‚ÄúNow it‚Äôs time for some shameless self promotion‚Äù got me at the end!! üòÇüòÇüòÇ,True
@sndrstpnv8419,2024-01-15T21:54:05Z,0,are you going to create python code from scratch as you did for xgboost ? we would happy to buy it ...,True
@AdrianBoyko,2024-01-15T21:41:17Z,0,"Could ‚Äúthey‚Äù have picked a less descriptive name than ‚ÄúTransformer‚Äù? I mean, is there ANY neural network that CAN‚ÄôT be considered a ‚Äútransformer‚Äù in the usual meaning of the word?",True
@brucewilliams2106,2024-01-11T16:20:55Z,0,I thought it was vamonos,True
@yashsvidixit7169,2024-01-10T18:59:48Z,1,A lot of hard work must have gone into these videos. And the results are these brilliant super helpful videos. Thanks a lot for these videos.,True
@user-if6ny5dk9z,2024-01-09T17:40:48Z,1,Thank You Sir...,True
@BlochSphere,2024-01-07T18:41:13Z,0,I wonder if we will have a statquest on quantum machine learning,True
@anlozcan6686,2024-01-06T21:06:51Z,1,that should be called QUADRA BAAM !,True
@heike_p,2024-01-06T17:34:01Z,1,I'm following an advanced master of Artificial Intelligence. This whole NN playlist has saved me while studying for my exams! Thanks a bunch!,True
@eburgwedel,2024-01-03T21:11:05Z,1,Mindboggingly brilliant video. THANK YOU.,True
@adarshvemali2966,2024-01-03T20:56:07Z,1,"What a legend, there is no better channel than this!",True
@101alexmartin,2024-01-02T20:18:49Z,0,"Thanks for the great video, Josh. Since in the video you don't stress out (unless I missed it) the concept of Masked Self-attention on the decoder, does it exist a transformer model that doesn't mask the self-attention of the decoder?",True
@jiayiwu4101,2024-01-02T18:53:52Z,0,"Question about 11:58. During the training steps, for example, we input multiple Wikipedia into the model. In the training step, each Wikipedia may have different positional embeddings but the same word embeddings?",True
@sheelstera,2024-01-01T10:15:28Z,0,"24:00 @statquest Fantastic video..is the input to the decoder only the words of the Spanish translation of the input English sentence or is it the entire possible set of spanish words in the spanish dictionary/vocabulary? I.e. is the decoder supposed to guess the next output word ""from the finite set of words in the correct already available output in the training data"" or ""from the exhaustive set of words available in The spanish  dictionary""?",True
@beraterendincer5864,2024-01-01T09:35:55Z,1,Thanks! You are the best,True
@bobbymath2813,2023-12-31T04:29:20Z,13,"How a model like this was created is just beyond me. There‚Äôs so many different moving parts. You could write a whole book on the fully-connected network alone. Add in all the other stuff? Wow.   Thank you, Josh, for explaining this so well!",True
@prathameshdinkar2966,2023-12-29T05:29:35Z,1,"So nicely explained! I have searched for ""how transformers work"" but no one on youtube explained with both concept and math! Keep the good work going üòÅüëç",True
@brycerogers5050,2023-12-28T16:01:08Z,0,"Running through each word in the translation until you arrive at the EOS token makes sense, but what about if you start with SOS-- how does it know to work backwards from the end of the sentence? Thank you and God bless you Josh.",True
@eshuapoorv7188,2023-12-28T08:02:29Z,1,"Great video josh, just one thing i was curious about, at 19:45 isn't it weird that the query for word go is more similar to the key for let's than to its own key? With itself similarity is -7.2 and with Let's its 18.5, weird. Great video btw, you are constantly saving me in my nlp course",True
@matthewchan4847,2023-12-26T20:36:34Z,0,i'm bammed out,True
@renjithprasad7435,2023-12-24T16:22:54Z,0,Excellent video. Thank you for all the intuitive videos. I have 2 queries)why is in self attention word check attention for itself along with other words?  The similarity with itself will be anyway large and this might interfere with words that we actually care of attention right? 2) What is the intuitive difference between self-attention and encoder-decoder attention. Is it just the fact that self-attention works in the encoder layer with input sentence and encoder-decoder attention takes in both?,True
@logosfabula,2023-12-23T21:29:05Z,0,@6:40 wouldn't it be more precise to mention gradient descent as the iterative process and back-propagation as the technique by which the actual calculation of the derivation is made (genuine question)?,True
@ansumansatpathy3923,2023-12-23T10:06:43Z,0,Started well but then went downhill with too much crammed in...the length of this video should have been more.,True
@ShadowD2C,2023-12-21T08:33:28Z,0,as a CV enthusiathist this only confused me more tbh,True
@miltonborges7356,2023-12-21T02:58:48Z,1,Amazing,True
@giangbui7171,2023-12-18T07:15:09Z,0,"Fantastic video! I just purchased the book after watching it. I have a question: at 19:53, why did the attention value for 'go' end up being the same as the one for 'Let's'? Was it a coincidence?",True
@juanbermudezroldan2392,2023-12-18T00:42:25Z,1,This is absolutely amazing.,True
@sonalkumar6885,2023-12-16T05:53:50Z,0,"Sir I have a doubt that if the softmax(Q.K) is already representing the linear superposition of the words (let's and let's),then in 18:50 we are again using values(V) of the word (let's) to scale it,we can directly use the softmax(Q.K) in this case,its already representing the context of the words in the input sequence na?",True
@shaktisd,2023-12-16T02:43:20Z,1,"One of the best explanation of encoder / decoder architecture. Esp. the self attention part. I really liked the way you colored Q,K, V to keep track of how things are moving . Looking forward to more such videos",True
@peterwan1501,2023-12-14T17:53:31Z,1,men you are honestly a legen >_<,True
@jinxmei3842,2023-12-14T08:12:34Z,1,"Why 2 activation functions at the beginning, not 3 or 4?",True
@gordongoodwin6279,2023-12-13T00:53:53Z,0,"Love your videos, but it would be REALLY helpful if you used dimensions that vary to make the matrix algebra easier to follow for those trying to use your vids to supplement more complex study. For example, in all of your videos on Deep Learning I've seen, you always use an embedding vector of the same dimesnionality as your input sequence (4-D input, and then 4-D embedding, or 2-D ""Let's"" ""Go"" input and 2-D positional encoding). It would be a lot easier to work this out by hand if you had say a 4-D embedding for a 2-D input sequence, just to keep things mentally separate.",True
@jiayuemao4985,2023-12-11T15:58:52Z,1,"Nice video! Thank you for explaining so clearly! As a starter, this video helps a lot.",True
@islemrouba7585,2023-12-10T18:52:54Z,1,BAM!,True
@autumn_moon,2023-12-09T18:21:32Z,1,this video is not sponsored by ABC - Always Be Curious üòø,True
@MinChitXD,2023-12-09T13:38:22Z,4,"I've just learned machine learning for a month, my major is a pure business student. I've been working as a Data Analyst for 2 months as the internship and I believe machine learning will be essential if I want to go further in this industry. Out of all tutorials videos I've watched, your videos brought up the clearest and most concise concepts for me to understand. All the videos walked me through from the series of neural network, back propagation, cross entropy with backward propagation, recurrent, LSTM and convolutional neural network, lastly, this video. Really appreciate for your understandings and amazing storytelling through your videos, your contents always make me eager to keep learning machine learning myself. Thanks a lot",True
@phoneix24886,2023-12-08T07:31:58Z,1,@Josh Starmer I bought your book on machine learning and it helped a lot. I request you to publish a book on reinforcement and deep learning too. It will be very helpful. There are millions of eager learners waiting for this. Please make it happen Josh.,True
@anupmandal5396,2023-12-04T18:56:56Z,1,Nobody can explain transformer as easy as this much.,True
@mixshare,2023-12-03T17:39:47Z,1,epicÔºÅÔºÅÔºÅ,True
@pussiestroker,2023-12-03T17:39:31Z,0,"I am glad Josh took the time to clearly explain how Q, V and K are computed and how they are related.  P.S. You forgot to update theo Self-Attention Values for ""go"" to -2.7 and 1.5.  It's no really biggie.  üôÇ",True
@luisherneymedinajimenez6314,2023-12-02T19:06:57Z,0,In which application do you do your animations?,True
@MathewWakefield-Is_here,2023-12-01T04:42:21Z,1,Thanks,True
@dadmehrdidgar4971,2023-11-30T22:00:38Z,1,Great video; thanks! :),True
@maged_helmy,2023-11-26T21:06:55Z,1,This was amazing! Can you please explain LoRa and QLoRa next?,True
@ghostbuster8377,2023-11-26T20:20:06Z,0,2:17 why are the self attention values the same?  I think the value on right should be -2.7 and 1.5,True
@taherbenyemna5568,2023-11-26T18:37:07Z,0,"i have a question pls : if the output sentence is composed let's say of 5 words, how can we decode the third token? Meaning, how can we generate a query in the decoder based on the two previous tokens? because as i can see we need to take into consideration the two previous tokens in order to decode the third one. and thanks ‚ô•",True
@user-qn9nq5yq2j,2023-11-25T19:59:39Z,1,"This was immensely helpful across the board. Thank you very much. As a quick point of clarification, when the self-attention was being computed for ‚Äúgo‚Äù relative to ‚Äúlet‚Äôs‚Äù, why is the similarity score based on the dot product of the query values of ‚Äúgo‚Äù relative to the key values of ‚Äúlet‚Äôs‚Äù and ‚Äúgo‚Äù giving a higher score of 18.5 for ‚Äúlet‚Äôs‚Äù and a lower score of -7.2 for ‚Äúgo‚Äù? Shouldn‚Äôt ‚Äúgo‚Äù have a higher similarity score to ‚Äúgo‚Äù than ‚Äúlet‚Äôs‚Äù?",True
,2023-11-25T03:07:06Z,1,Nice music!,True
@luis2arm,2023-11-23T07:53:54Z,0,I have a question regarding the 'masking' at the first Self-Attention module in the decoder. Is there a reason to omit them? Great video! I used it to get a first grasp and then I came back after reading some other sources and appreciate the video even more,True
@shafagh6129,2023-11-22T23:09:18Z,1,Thank you so much. That was so comprehensive and informative.,True
@robboerman9378,2023-11-22T11:03:25Z,1,"Oof, need to watch it a couple more times but definitely starting to make sense!",True
@manojsingh-nj7rl,2023-11-21T20:38:35Z,1,My brain neurans cannot create any embeddings for the word which  is enough to say thank you.... Just BAM :) üòä,True
@thomasdeneux,2023-11-21T20:12:36Z,1,thank you very much for this impressive work! it is so important that we can all have a grasp of how this works,True
@kaleab689,2023-11-20T07:33:07Z,0,"This is an amazing video on the transformer model. Thank you for making it fun and interesting. I have one question regarding the positional encoding part. I understood that it's calculated using sine and cosine functions, so here on the video, the first word ""squatch"" didn't get a higher value for the first position which it is at. Is there a learnable parameter here that the network will learn with further training or did I misunderstand the concept? Thank you again for your wonderful contribution.",True
@mohotta,2023-11-19T12:14:58Z,1,"Tried the query in chatGPT lol  Sure, here‚Äôs a song in the style of StatQuest:  (Verse 1) üéµ Deep learning, it‚Äôs a quest, To find patterns that are the best. Neural networks, wide and deep, Finding secrets that data keep.  (Chorus) üéµ StatQuest, oh StatQuest, Making sense of all the tests. With charts and graphs and songs so neat, Making learning stats a treat.  (Verse 2) üéµ Convolutional, recurrent nets, Understanding all the specs. Backpropagation sets the stage, For the gradient descent at every age.  (Chorus) üéµ StatQuest, oh StatQuest, You really are the very best. With clear explanations, no contest, You make us all love this stats fest.  (Bridge) üéµ From t-tests to PCA, You guide us every single day. With a tune that‚Äôs catchy, light, and free, You make us all love this stats spree.  (Chorus) üéµ StatQuest, oh StatQuest, With you, we‚Äôre never stressed. You make statistics fun and cool, With you, we rule the data school.  (Outro) üéµ So here‚Äôs to Josh and his guitar, Teaching stats, both near and far. With StatQuest, we‚Äôre on a roll, Understanding stats, that‚Äôs our goal! üéµ",True
@srikanthganta7626,2023-11-18T14:22:09Z,1,"Thank you for such amazing illustrations! HOW I WISH I HAD THIS DURING MY STUDIES, BUT I'M JUST GLAD I GET TO LEARN THESE AS A WORKING PROFESSIONAL. THANK YOU SO MUCH FOR ALL THE CONTENT YOU MAKE. I'M SURE YOU MAKE THOUSANDS OF LIVES BETTER. YOU'RE TRULY AN INSPIRATION JOSH!",True
@rishabhsoni,2023-11-17T12:13:32Z,0,"Just wanted to clarify: Is the query (for decoder) and key/value vector (for encoder) created from the final output post the residual connection - This is for the encoder-decoder attention? Also wanted to hear your thoughts, not sure if there is a proper explanation for why this is works but from every ML theory that i have ever read it points towards this being a massive overfit. You have curse of dimensionality with high embedding vector size, with self-attention we are looking into the future while training etc.",True
@rishabhsoni,2023-11-16T17:27:59Z,1,Just too good,True
@user-cs9lg7jk4h,2023-11-16T11:24:12Z,1,vamos!,True
@nanshen9966,2023-11-16T10:36:41Z,0,This is the least painful video to explain transformer ever!,True
@AntiLawyer0,2023-11-16T10:10:08Z,0,The best video that explains Transformer I've ever seen. Thanks for your contribution!,True
@GoodDay-ii4jb,2023-11-14T04:22:09Z,0,"Thank you for the nice explanation!  One question in 19:36, when calculate the similarity for go, why the similarity score between let's and go is 18.5, while the score between go and itself is -7.2? Would go has a higher similarity for itself? And why the softmax for let's is 1, but not 0 after softmax?",True
@adeshbhavsar6530,2023-11-13T18:46:30Z,0,so what is the unit of X axis?,True
@hamidrezahosseinkhani5980,2023-11-12T22:48:35Z,0,"It was incredible. step-by-step, clear and concise, detailed enough. great great. thank you for such an amazing video!",True
@mr8rasheed,2023-11-11T15:38:19Z,0,"awesome video but how tf did they come up with this, i keep getting lost after each added component this is way too complex damn",True
@prithvi1138,2023-11-10T11:00:51Z,2,It was a BAAAAAAAAAAAAAAAAAAAAAAAM!,True
@howardhao-chunchuang6742,2023-11-10T05:50:05Z,1,"Thank you for your wonderful work and crystally clear explanations. Finally K, Q, & V make sense.",True
@Hwrd-sw5do,2023-11-09T22:15:02Z,1,i love u. Thanks from the bottom of my heart <3,True
@vohiepthanh9692,2023-11-08T04:43:13Z,1,"Penta BAM!!! All of your videos are extremely easy to understand in a peculiar way, they have helped me a lot, thank you very much.",True
@markusdabell1656,2023-11-08T03:12:05Z,1,Excellent explanation. One of the best teachers I've had!,True
@rathinarajajeyaraj1502,2023-11-08T00:25:21Z,1,Awesome explanation,True
@TekeshwarHirwani,2023-11-05T05:47:43Z,1,Best video on Transformer I have seen in Youtube! Amazing ! huge respect for you,True
@zenchiassassin283,2023-11-04T14:34:22Z,1,Amazing,True
@naromsky,2023-11-03T19:56:05Z,1,Bamtastic.,True
@EdwinMartin,2023-11-03T19:49:25Z,0,"In the example, it compares ‚Äúit‚Äù to ‚Äúpizza‚Äù and ‚Äúoven‚Äù and concludes that ‚Äúit‚Äù has a higher similarity to ‚Äúpizza‚Äù. Isn‚Äôt that wrong? Isn‚Äôt it that ‚Äútastes good‚Äù has a higher similarity to ‚Äúpizza‚Äù in contrast to ‚Äúoven‚Äù?",True
@harshadmane8785,2023-11-02T18:55:51Z,1,Great explanation,True
@noirblanc9446,2023-11-02T15:35:57Z,0,"Question : how did Vamos get ""found"" by the decoder? I understand the self attention and encoder-decoder attention, but I couldn't follow where it took the numbers to match Lets's Go to Vamos, but rather the math after the fact, what am I missing?",True
@sahararaghi8523,2023-10-29T22:56:50Z,0,"Thank you so much for the great video. I have a question and would be grateful if you could answer it. Based on what was explained, I understand how the decoder determines the first word that needs to be translated. However, I‚Äôm unsure about how it continues the process. How does it decide on the second and third words? Thanks!",True
@tamoghnamaitra9901,2023-10-27T16:04:46Z,1,"As a Gen AI consultant, this video was amazing. Absolutely perfect",True
@iwokeupdead1093,2023-10-26T05:27:55Z,1,"I'm currently studying for job interviews and I don't know what I would do without you, thank you! When I get paid from my first job I will donate to you :)",True
@legendary2192,2023-10-24T13:56:13Z,1,loved it!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!,True
@adithyakumar1111,2023-10-23T19:09:11Z,1,"Thank you Josh for this fantastic video. One of the best videos to explain the math behind the Query, Key and Values.",True
@isseym8592,2023-10-23T14:38:33Z,6,"As a computer science student getting into the field of NLP, I really can't thank you enough for making a video that breaks down Transformer like this. Our uni doesn't go in depth about NLP related topics and with a very brief explanation they do, the uni expects us to have a full understanding about NLP. I can't thank you enough!",True
@hey-its-me239,2023-10-22T18:26:47Z,1,Thank youuu so much for sharing the BEST EVER CONCEPTS in such a brain-friendly manner!!! Really it means a lot :),True
@urazc5917,2023-10-19T13:05:14Z,2,This video is a treasure in a world where <put any complex topic here> is explained in 2 minutes. Thank you Josh!,True
@SergiuIacob,2023-10-18T09:17:34Z,1,Brilliant.,True
@nobiaaaa,2023-10-17T00:46:24Z,5,"Only videos like this can have ""clearly explained"" in the title.",True
@kosprov69,2023-10-16T03:49:11Z,1,Great explanation. I had a doubt regarding the prediction phase. How do we make predictions with this architecture? During prediction we have no idea of the next tokens from the decoder. How is self attention determined when the other tokens haven't been predicted yet?,True
@pravindesai6687,2023-10-11T12:55:00Z,1,Thanx  a lot for this video!! You have explained self attention with example better than top universities.,True
@debayantalapatra2066,2023-10-09T11:39:46Z,1,This is the best of all that is available right now on Transformers. Thank you!!,True
@fsouza,2023-10-09T03:35:09Z,1,Thanks!,True
@alexanderwu,2023-10-08T18:37:37Z,1,"I never thought it would be possible to explain something so complicated in such detail without even assuming any calculus or linear algebra background, but I am glad to be proven wrong",True
@user-rs9zs8kj7o,2023-10-08T12:49:08Z,6,You're the only person on social media that can explain such complicated topics in an easy to understand manner. Keep up!,True
@70ME3E,2023-10-07T11:31:32Z,1,I sympathize with StatSquach. Dude has good questions.,True
@potre297,2023-10-06T11:51:34Z,0,https://www.youtube.com/watch?v=zWNrjZXKOtU,True
@RealCatDev,2023-10-05T15:58:41Z,0,"The networks for word embedding are like dictionaries (for example one would have cat, dog, monkey and second apple, orange, banana) and if you want to get the value for the cat you get the animals dictionary and supply 1 in the first slot and 0s for others? Just curious if I got it right.",True
@mermich,2023-10-05T10:41:17Z,0,"Can anyone explain to me how do we train the weights for the network that gives the output of the Word Embedding values? I mean the input are the words, and the output are the numerical values of each word. But then we don't have the desired values of the numerical word embedding output or each word , how do we do back propagation and train the weights of this word embedding neural network then?",True
@maymoonabashir3993,2023-10-01T19:39:06Z,1,"I cant be the only one who typed ""write an awesome song in the style of StatQuest"" on ChatGPT.",True
@user-el8bw6fg3v,2023-10-01T07:06:25Z,0,"Thank you very much for this truly outstanding video!  One question regarding how the Q, K, and V are derived. In the video, different tokens (""let's"" and ""go"") use the same matrices to derive Q:  (position-encoded) embedding vector mulitply with [[1.1, 0.6], [-2.8, 2.4]]. Same for K and V. Is that just a simplification for brevity, or it is generally true? One would guess that for different tokens, weights of the the linear transformations from the embedding vectors to the Q's are also different.",True
@fitcommonman,2023-09-30T13:26:32Z,1,Just blown away by this video. Thank you so much.,True
@akbarhday,2023-09-28T14:11:25Z,1,almost 7 years ago I learned about computer vision using CNN as the state of the arts and here I now trying to study again as there's a new state of the arts. technological development never bored me,True
@d_b_,2023-09-28T13:54:19Z,0,"So the output is a softmax of known words, right? Two questions. How do these LLMs understand/parrot back words that it hasn't encountered before? Even things like typos or ""aslvhoasejf"" it will repeat back. Second question, these have the ability to return different outputs for the same exact input, is that baked in to the LLM architecture or is it part of an implementation step? I have not played around with open source base models, but I imagine I'd find the answer quickly by attempting.",True
@harryspeaks,2023-09-28T04:32:49Z,9,Definitely the clearest walkthru of Transformer. It's very good that you put heavy emphasis on the parallelizability of Transformer since IMO it is the most important feature that made Transformer so useful,True
@akeshagarwal794,2023-09-27T21:32:29Z,1,God level explanation ‚ù§,True
@andrewdouglas9559,2023-09-27T21:00:13Z,1,I don't know how I'd learn DataScience/ML without this channel.  Thanks so much for doing what you do!,True
@matthewhaythornthwaite9910,2023-09-27T11:26:37Z,4,"Thanks Josh, another great video, I‚Äôve been following your channel for years now and your videos have massively helped me to change career so huge thanks.   On to the transformer network, there‚Äôs something about the positional encoding that makes me feel a little uneasy. It feels we‚Äôve gone through great effort to train a word embedding model that can cluster similar words together in n-dimensional word embedding space (where n can be very large, often 1,000).  By then applying positional encoding before our self-attention, whilst you very clearly explained with your example how important adding this information to the model is, seems to me to mess up all the effort we put into word embedding to get similar words clustered together. The word pizza, instead of being positioned in the same place can now jump around word/positional embedding space. Instead of one representation of pizza in space, it can now move around to be in many different positions, and not move locally around its own 'area' but because we add the positional encoding to the word embedding, scaled equally, it can jump around a great deal of space. To me it would seem adding this much freedom to where the word pizza can be represented in space would make it much much harder to train the model. Is my understanding correct or is there something I‚Äôm missing?",True
@Jay-el1sm,2023-09-26T20:24:08Z,1,"This might be the best video on deep learning I've seen period, rivalled only by 3Blue1Brown's content. Thank you so much!",True
@nikoskodo9303,2023-09-26T16:02:27Z,1,JUST RESPECT BROTHER!!,True
@oxfordsculler8013,2023-09-25T18:28:58Z,0,"Hey Josh,   Mistake at 11:22 , -0.9 +-0.2 = -1.1 not  -1.2 . Also -0.9 +-0.9 = -1.8 , not 0.   In Pizza's1st and 3rd word and position combined.   Just thought you might like to know, check out the quest. ;- )  P.S Great video.",True
@RachelPun,2023-09-25T11:18:17Z,0,"Why does positional encoding work? Adding the position values to the embedding values seem arbitrary, doesn't it? Because the position values are made up and there doesn't seem to be a reason to change the embedding with a random number on a squiggle. Doesn't this move obfuscate the embedding?",True
@user-rp4yq3dl3u,2023-09-24T05:59:32Z,0,Can you please make a video on how transformers work on images,True
@kaandonbekci3870,2023-09-23T19:23:19Z,1,This is the SOTA for ML educational content. Bam!,True
@Santoshpbagalkot1,2023-09-23T09:50:58Z,1,Thanks,True
@atticus1886,2023-09-20T18:19:13Z,0,"Hi, Josh. Where do we get the sets of the Spanish words (ir, vamos y)? Do they come from all the vocabs that we have?",True
@AI_Financier,2023-09-20T01:14:19Z,0,"By far, this is the best explanation of the transformer I v ever seen. the only thing missing (maybe 5 min only) is how the training is being done.",True
@jarsal_firahel,2023-09-19T07:32:16Z,1,"Amazing video, the clearest I have seen on this topic",True
@k999ford,2023-09-18T01:36:45Z,1,maaaannnnnn if I had this when I was trying to figure out transformers a year ago üò≠üò≠üò≠,True
@doctorshadow2482,2023-09-17T13:53:15Z,0,"Thanks again for the video. We discussed this in a separate thread, but somehow my answer there disappeared, so let me repeat it from scratch here.  16:08 - You never explained where from we get this ""Query"" and what it intended for and now you introducing a new essence ""Key"". That's great that you just show how it is implemented in the original model, but for the student this is already second value which comes out of blue sky and never explained.  You answered ""Once you understand the basics of neural networks, the ""key/value/query"" concepts will make more sense. Just start at the start and work through a few videos until you are more comfortable with how they work."", but this actually didn't help. The key/value/query in this meaning comes only with this part and with the ""Attention Is All You Need"" article. If I missed something, could you please, pinpoint the disconnection more precisely? I will be thankful for link to the specific video and time mark.",True
@cezarystorczyk1722,2023-09-17T08:50:37Z,1,"Man, you are best teacher i have ever listen to. Thanks",True
@TheSuperFlyo,2023-09-15T19:33:39Z,1,We have been waiting for this!! Awesome,True
@Zaznobable,2023-09-15T18:05:17Z,1,THe subject is really complicated but it was really clearly explained.,True
@zhangeluo3947,2023-09-15T08:34:50Z,0,"Hey Sir, thank you gain for your teaching! But I have a question for decoder statge that during the translation let's say when you have decoded several tokens(# number of tokens>1) (still not met with <EOS>), now for the next step to decode you fed into the previous decoded token as new input to compute the self attention, is true that the self attention of that new input(from previous step) is computed as weighted sum of all so far decoded v(including itself) with weights being its q against all other ks (including itself) ? Could you help me for clarification?",True
@dewierwan,2023-09-14T12:43:49Z,0,I think you forgot to mention how the weights / values are calculated in the various matrices (see 2:20),True
@supernenechi,2023-09-14T11:15:19Z,1,"Oh my god! Now it makes sense why training a transformer model takes FAR more compute than it does to run them! I can run them on my CPU, but training a big one may take weeks to months on entire GPU clusters! So many values to optimize and SO many nodes, inputs and outputs! That's insane! And it also shows why the models are SO large! There are SO many weights and biases, SO many connections and an entire world of vocabulary to store in it and connect up!  This is SO impressive!",True
@manuelapacheco9129,2023-09-14T01:42:58Z,1,"man i love you for this video thank you so much, there's absolutely no way i'd have understood all of this without your help",True
@mtopscriterion,2023-09-13T06:58:02Z,1,"Thanks to Josh for being the legend that breaks this down for us smart yet simplicity loving developers. But also, thanks to the the genius minds that created Transformer Models #gamechanger",True
@mohdnomaan7449,2023-09-12T17:29:11Z,1,"149,267th BAM!!!",True
@mtopscriterion,2023-09-12T07:06:34Z,0,"Josh do we need to do weight embedding if we are already working with numbers, for example stock market prices?",True
@spartan9729,2023-09-11T12:11:38Z,0,This is your only video that I had to see twice to get complete idea of the topic. Transformers really is a decently tough topic.,True
@amitvyas7905,2023-09-11T03:47:12Z,0,"At 8:48, you said 4-word embeddings but in the example, it's 3 right?  Also, for each word, can you explain the reason behind why there are maybe 1000s of word embeddings created in real word scenarios? I think it's because since they (the initial word embedding values) are random and each word would need attention or value w.r.t the other words in the sentence that is why 1000s or word embeddings are created and then optimized by loss function/ gradient calculations, right?",True
@rahulmathew4970,2023-09-10T10:12:17Z,0,"what if we take dot product of query of ""go"" key of ""let's"" and compute the dot product. Wouldn't we get a different result? So does that mean that relationship between ""let's"" and ""go"" is different that  ""go"" and ""let's""?",True
@rey40,2023-09-08T22:39:43Z,0,"Thank you so much for this video, it really helped! I have one question on 19:35 , I was expecting that since the word ‚Äògo‚Äô is more similar to itself than it is to ‚ÄòLet‚Äôs‚Äô, then ‚Äògo‚Äô would have more influence on its encoding than ‚ÄòLet‚Äôs‚Äô; therefore when doing the Query*Key similarity calculation would give a high value for ‚Äògo‚Äô (and even higher than that for 'let's') instead of a negative one ( -7.2). Is there anything I'm not fully understanding there, or is it because we are using the same weights for queries, keys, etc? Thanks!",True
@prachijadhav9098,2023-09-07T15:18:16Z,0,"It's a great explanation, Josh! Can you explain Embedding vs Encoding? AFAIK embedding captures semantics among words while encoding doesn't.",True
@hardiksharma9157,2023-09-06T09:21:28Z,0,@statquest where did the consine and sine squiggles come from?,True
@pradhumantiwari9754,2023-09-05T20:12:08Z,1,Thanks a lot sir. I hope you get everything you want in your life.,True
@TheLLdoesAJ,2023-09-05T18:29:25Z,1,"This was a good video in reviewing the concept. It is good in learning the basics. However, I found that you went back and forth a lot, which is hard to follow at times. Perhaps a more 'linear' way of doing it, going from start to finish may be better? 5/5 Video though!",True
@shuaishuai2009,2023-09-05T05:43:02Z,0,what about back propagation in LSTM or Transformer. I want to see how the parameters updated.,True
@papzgaming9412,2023-09-03T22:18:56Z,1,"This is the best Transformer video i have ever seen, Thank you.",True
@davidkim8354,2023-09-03T19:38:05Z,0,"Squatch's first question at 15:23 was about the Query. However, the response at 21:17 addressed the new Self-Attention value.  If the Value and Key are multiplied by a pair of weights, wouldn't it seem that the Query could be used as the same value as the Position-encoded values?",True
@izainonline,2023-09-03T19:26:28Z,0,How the values coming from calculations,True
@fgfgdfgfgf,2023-09-02T19:57:38Z,7,"I've been looking for tutorial about transformers for a long time. This is the smoothest tutorial. It does not hide any complexities(making me confident that I actually understand the concept instead of its dumbed down version for mortals that won't end up ever using the knowledge), but also does not get lost while explaining those complexities and clearly calls out what else I can learn about to understand the side concepts better. Super !!!",True
@aanchaldogra,2023-08-31T12:55:21Z,2,"I owe my data science job to so many beautiful people on youtube, you are one of them. Thank you",True
@airxperimentboom,2023-08-30T15:03:16Z,1,"Thank you, merci beaucoup, muchas gracias, danke sch√∂n, grazie mille, xiexie, arigatou gozaimasu, shukran, dhanyavaad, spasibo, k√∂sz√∂n√∂m.  TRIPLE BAM !",True
@HIMANSHUSINGH-re7ij,2023-08-29T19:08:06Z,1,"Hii Josh , can‚Äôt thank you enough for all your lectures and the level of understanding you provide , best playlist and best teaching style ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§",True
@bobaktadjalli6516,2023-08-28T22:41:41Z,0,"Hi, at 11:30, I couldn't understand how the numbers shows the order. Because the number of ""Squatch"" after summation, is not the highest number and I can't find any relation to consider the numbers' order. Would you explain more about it, please?",True
@puvennaidu1023,2023-08-28T20:53:13Z,0,"Hi Josh, thanks for your great video as always . Just a small addition mistake when adding the position values with the word embedding at https://youtu.be/zxQyTK8quyY?t=681 . Otherwise, a great watch ! BAM",True
@emanelsheikh6344,2023-08-28T14:53:25Z,2,I've searched a lot about the transformers but seriously this is the best explanation I've ever got. Amazing!‚ù§,True
@navidghasemi9685,2023-08-26T22:12:58Z,0,"great man, but it would be great if you go further and in more detail in your next videos about these transformers",True
@arthan1011,2023-08-26T07:52:29Z,0,What a fantastic video! Sadly I had to watch it twice to fully grasp the concept.,True
@redberrypiethurgo,2023-08-25T02:34:23Z,0,Why are the positional encodings summed with the word embeddings rather than using concatenation? I would have thought that two words with different meaning could become conflated with an entirely different word at a different position.,True
@meirgoldenberg5638,2023-08-24T08:29:46Z,0,"By 23:38, we have encoded the whole phrase. Here is what's strange. The number of dimensions depends on the number of words (i.e. the number of words times the number of embedding values per word). But usually we want a fixed number of dimensions, such as 384 or 768. How is a vector representation with the required number of dimensions computed?",True
@meirgoldenberg5638,2023-08-24T08:16:31Z,0,"At 16:12, is there any connection between the weights used to compute the query and the weights used to compute the key? If not, why do we expect cosine similarity to be meaningful? In particular, what makes ""let's"" be similar to itself?",True
@meirgoldenberg5638,2023-08-24T08:05:14Z,0,"For positional encoding, what is the significance of using sine and cosine instead of just a fixed sequence of random numbers between -1 and 1, which would also give a unique sequence for each word with a very high probability?",True
@meirgoldenberg5638,2023-08-24T07:53:27Z,0,Is the whole explanation leading to 5:22 just to say that the number representation of the phrase is just concatenation of the embeddings of the constituent words? Am I missing something?,True
@mohitsrivastava5880,2023-08-23T14:23:06Z,1,"YouTube should provision a ""superLike"" button because I always feel hitting the ""like"" button on your videos is not enough. Great content, kudos!",True
@user-rn6xm7he7p,2023-08-22T14:48:38Z,1,"josh... .in recent videos, esp about NLP and ChatGPT topics, youre focusing so much on explaining the basic numerical (math) operations, that youre losing the overall concepts that make the Transformer a specialized kind of CNN....      drop the excess details, focus on the concepts like you used to...   jussayin  mike",True
@PeterSigurdson,2023-08-22T13:17:53Z,1,Excellent AI Teaching content.  as to your Singing.... Don't quit your day job.,True
@amberstiefel9748,2023-08-22T06:05:51Z,1,This is making me think quantum computing is another fancy toy - for the time being. But I say that because it doesn't easily scale,True
@carleanoravelzawongso,2023-08-22T03:43:01Z,2,"Please create more vids!! Your explanations are truly beautiful, such a work of art. I couldn't agree more that you are one of the most brilliant teachers at statistic and ML! Actually, I wanna hug you right now haha",True
@DanOneOne,2023-08-22T02:34:07Z,1,"you didn't explain the meaning of each step, you just explained the step. Why do they use 2 numbers for each token/word? What is the meaning of adding position value? What is this Query, Key, what do they query? why do they need a key? Just explaining ""Oh, this sums up with this, and this multiplies with this"" is not enough. So I don't know why they praise you so much. I guess others explain it even worse. I'll  have to look for a better explanation. Just because you or somebody else understands it themselves, it doesn't mean it's ""obvious"" to all of us as well.",True
@kurtosis4573,2023-08-22T01:32:14Z,6,I just finished watching almost all the videos on this channel and i have to say that this is probably the best place to learn stats and machine learning. I also bought the ML book and it captures the essence of the style of teaching on this channel really well and is very handy to go back and quickly look up some details. You are doing great work!,True
@Isakilll,2023-08-21T08:41:39Z,1,"Just wanted to say that I understood everything about LMs (thanks to your videos), except the part on transformers cuz the video wasn't out yet ahah. Well now that my dear squash teacher explained it, everything's clear. So really THANK YOU for your hard work and dedication, it made all the difference in my understanding of Neural Networks in general <333",True
@AI_ML_iQ,2023-08-20T23:02:06Z,0,"Attention mechanism does not require different matrices for query and key, both in self attention and cross attention mechanisms.  See paper by R. V. R. Pandya titled ""Generalized Attention Mechanism and Relative Position for Transformer"" .",True
@roshanbajaj3370,2023-08-20T22:28:14Z,1,"I try very much to understand transformer but I got 100% satisfaction from your video , your explanation is like unbelivable . I enjoy study very much when you taught any topic",True
@maha_13,2023-08-20T19:51:50Z,0,at the same time,True
@erikcollvila9171,2023-08-20T19:05:49Z,1,God content.,True
@tusharkelkar3888,2023-08-20T06:06:54Z,0,"Where did any of the weights actually come from in these example? E.g. word embedding, positional encoding, the key, query and values etc?",True
@danberm1755,2023-08-20T05:30:04Z,1,Fantastic video üëç,True
@lolololo-cx4dp,2023-08-20T05:06:13Z,1,This really clear my doubts about attention mechanisms.,True
@lolololo-cx4dp,2023-08-20T05:04:17Z,1,Ah yes transformer,True
@dracomal4555,2023-08-18T10:50:16Z,0,Hi @statquest  It was a very informative video I got all my doubts related to it cleared. BAM!!!  just a request please make some videos on SOTA computer vision algorithms too like YOLO etc. Thanks and please keep up with the good work.,True
@johnwilson7680,2023-08-17T20:38:05Z,1,I really wanted to watch this video but I couldn't make it through a full minute. I understand you're trying to make it simple but it's incredibly annoying to some of us.,True
@sdsa007,2023-08-17T07:17:10Z,2,"Transformers! More than meets the eye!? I think there is a lot of value in knowing this technology well! Thank you for your humor and learning support, I can't wait to return the favor!",True
@robertofirmino5283,2023-08-17T03:57:13Z,2,"Brilliant animations and explanation!! I have a small question, during the training process of the weight and biases, do we train the entire thing (all different parts in encoder + decoder) as a whole or can we train different parts separately (like can we train the weight and biases in the self-attention layer on its own)? I feel like the whole thing is complexly related to each other in one way or another, so I am not too sure how the backpropagation work, bam?",True
@VeloFX,2023-08-16T15:41:18Z,14,The explanations in your videos are incredibly precise and efficient at the same time. There is nothing better to watch when learning any ML topic! üëç,True
@user-kb3sk4pl4e,2023-08-15T21:20:24Z,2,This channel is awesome! Now I'm thinking how I can implement this with pytorch lightning ü§î,True
@sunnyson1219,2023-08-15T06:51:32Z,1,this man is a GEM,True
@prabhakarp9861,2023-08-14T14:11:15Z,0,Is there any way to post a video without the squatch character :) ?,True
@bestsagittarius7925,2023-08-14T08:54:06Z,0,"Josh, Can you compose a vedio for ResNet? Thx!",True
@johannesgutenburg9837,2023-08-14T05:53:22Z,0,"terrible explanation, far too specific to word-based applications of transformers rather than how they actually work in a general case.",True
@alif_ni,2023-08-14T05:47:57Z,1,"Nice way to refresh your grasp of transformer models :) By the way, did you intentionally skip explaining the masking mechanism in this video?",True
@PavelKovalev-uk7ql,2023-08-12T23:19:50Z,0,"16:08 I don't really understand where the weights that give the Query and Key values come from. (For example, for the query, the first pair of weights is (1.1, 0.6); the second pair of weight are (-2.8, 2.4) -- these are the weights I mean.) Presumably those weights are obtained after training the relevant neural network with backpropagation, but it is not clear what the training process should try to achieve (i.e., which value it should maximize/minimize). The same question applies for the weights for Key and Value.",True
@mukhtarbimurat5106,2023-08-12T19:38:43Z,1,"Hi, great video thanks! one question:  Let's compare sentences: The pizza came out of the oven and it tasted good I buy oven and it is expensive  first case it refers to pizza not oven, second case it refers to oven  how transformer knows to give high attention score in second case? does it uses other context info while calculating keys and queries",True
@nilson_001,2023-08-12T18:04:54Z,30,"Thanks to your engaging visualization and clear explanation, I've grasped the Stanford CS224n course! Your content is neatly condensed but doesn't miss a thing. It's like you've taken all the complex concepts and served them up on a platter. Triple Bam!",True
@EngRiadAlmadani,2023-08-12T12:01:18Z,1,i just want to see how you will explain the math of diffusion models it will be like dream,True
@williamflinchbaugh6478,2023-08-12T04:04:30Z,3,Great video! I'd love to see a pytorch + lightning tutorial on transformers similar to the LSTM video!,True
@tupaiadhikari,2023-08-11T15:45:58Z,0,"Prof. Starmer, can you explain how multiple attention heads are eventually connected togather and how do they eventually compute the final encoder-decoder residual connection which is taken as an input to the fully connected layer?",True
@tupaiadhikari,2023-08-11T15:39:03Z,4,"Prof. Starmer, Thank You very much. You are an inspiration to all the aspiring Machine Learning Enthusiasts. Respect and Gratitude from India. #RESPECT",True
@imolafodor4667,2023-08-11T12:45:02Z,0,"hi, context is learnt (at some level) also using an embedding with just word2vec.. like that, for calculating the position encoded values we could use the pre-trained word2vec embeddings and not retrain as part of the transformer, no? like this probably reducing the quantity of self-attention cells, since some context is already ""encoded"" in the pre-trained embeddings",True
@mahayat,2023-08-11T01:46:43Z,1,Simply great! Thanks a lot!,True
@solomononuchefaruna5022,2023-08-11T01:31:37Z,1,"You are such a great teacher. I love your style of teaching. Apart from StatQuest illustrated guide to machine Learning, do you have any other book that focus on the implementation of machine Learning using python. If yes, I need the list of these books. If no, writing a python book for machine Learning should be your next book. Thank you for all you do",True
@user-nn7wi9tx8m,2023-08-11T01:13:23Z,1,"Very good this statquest, im looking forward to The Next one, What Will be The Next Theme?",True
@andreasbinder6166,2023-08-10T22:01:19Z,0,"in the decoder MLP, is the output dimension the whole vocabulary? I suppose you fix the length ahead of time",True
@ticonderoga2906,2023-08-10T09:23:15Z,0,Could you please do videos about ARIMA and SARIMAX...,True
@alwega2923,2023-08-10T06:29:31Z,0,"But ""let's"" is 2 words ?!?",True
@alihakimtaskran7809,2023-08-09T16:59:24Z,2,That's great. I am ready to implement transformer from scratch,True
@user-xh7hv4vk6p,2023-08-09T08:16:13Z,1,"Stat""GOD""",True
@danielivanescobar6656,2023-08-09T03:04:54Z,1,Que chido.,True
@tonalddrumpboe5151,2023-08-09T00:05:24Z,1,this video was a quadruple bam!!!!,True
@anbuselvan2004,2023-08-08T19:15:35Z,1,"The best video on transformers, period !",True
@tobiasekman1,2023-08-08T12:10:04Z,0,"Maybe I am missing something but @19:49, why are the self attention scores for Go, the same as Let's? Shouldn't go be more similar to itself than let's?  edit: I can also see why a relationship between numbers should be the same, from whichever word's ""perspective"" you are looking at.",True
@naveenagrawal_nice,2023-08-07T18:45:24Z,1,"You are the most wonderful teacher I have ever got, Thank you so so much. üôè",True
@user-jq2wp3ir3q,2023-08-07T06:12:58Z,0,can you please give me the pdf of this video? which is really awesome,True
@Drestanto,2023-08-07T02:33:57Z,1,this sums up my 1 semester NLP class,True
@augustaseptemberova5664,2023-08-06T19:41:18Z,1,"This explanation is just amazing, thank you so much.",True
@soumyabanerjee4205,2023-08-06T17:14:21Z,0,"I liked your previous videos, but this video is too complex for me to take up at once. Lots of numbers, and calculation. Plus, I don't know about attention, residual, query-key-value stuffs.",True
@khaikit1232,2023-08-06T16:26:34Z,0,"Hi Josh  Thank you for the video! The concepts are explained really well and I hope you can help to clarify my further questions  1) I can't seem to intuitively understand how the use of sine and cosine curves, whose shape changes for each embedding position, can help to represent or track the position of the input/output words 2) How would normalising the values after each step work? To my understanding, to normalize a set of values, we first calculate the mean and standard deviation of the same set of values. So with reference to your video, could you elaborate what is the ""set of values""?",True
@SylwiaNano,2023-08-06T15:30:05Z,1,Thanks!,True
@xjp89644,2023-08-05T10:22:38Z,0,"Hi, it is really helpful video and I learned so much from it. However, I wonder if you make a typo when you calculated self-attetion value for ""go""? I thought you got (-2.7,1.5) but later when you shrink the part you gave it (2.5, -2.1), the same as ""Let's""? Or am I not understanding correctly? Could you please inform me why you do it?",True
@polaschwobel8998,2023-08-04T14:33:00Z,0,"Amazing video! Positional encodings make sense for the encoder part (since the sentence is given to me in order), but for the decoder how do I know the positions of the words before I have actually translated anything? Do they get added token by token as I generate?",True
@RonaldDraxer-rb4qm,2023-08-04T09:51:08Z,1,My man is back,True
@saumaydudeja7423,2023-08-03T17:40:25Z,1,This is probably the most awesome video about transformers ever,True
@bernardtiongingsheng85,2023-08-03T10:07:37Z,0,Awesome explanation! I wonder how we train the neuron network weights?,True
@SynaTek240,2023-08-03T07:33:49Z,1,"Wow, incredibly surprised by how litttle views this video has considering the current interest in transformers",True
@KE-qu3ty,2023-08-03T02:01:09Z,1,"Thank you so much! as a Cancer Biology PhD student, I never thought I could understand such complicated topics!!  Also, if you are taking requests, can you please put these topics into your list?  Markov Chain and HMM Fast Fourier Transform Monti Carlo and MCMC Random walks Wrapper methods Recursive Feature Elimination Forward/Backward stepwise selection  Again, thank you much. You are one of the greatest teachers I have ever seen.",True
@dkkkkkkk,2023-08-02T20:53:17Z,1,"This is a masterpiece! Appreciated it, Squatch!",True
@coolsai,2023-08-02T20:25:11Z,10,BEST EVER VIDEO ABOUT CHAT GPT! I watched many videos but this video is just BAM!,True
@vijetavashishtha6487,2023-08-02T14:31:35Z,1,I really wanted a video from you sir on this topic! Thank you.,True
@suman.michael,2023-08-02T08:59:11Z,1,"Thanks..! This is the best explanation, I've heard for transformers. And big shout out to Squash, those were the exact questions in my mind while reading transformers.",True
@lospaturno,2023-08-01T15:25:06Z,1,"As usual, you do the impossible.",True
@jonwakefield813,2023-08-01T14:48:54Z,0,"Once he called a sine wave a ""squiggle"" I was out. Clean it up.",True
@psg9278,2023-08-01T12:16:51Z,0,Plzz explain BERT,True
@alefalfa,2023-08-01T11:46:10Z,289,"Its kinda hilarious that StatQuest videos give the impression they were menat for 5 year olds, yet are exploring legitimately complex topics. No jargon, no overcomplicated diagrams. Josh really tries to explain things and not show off his supirior understanding of neural networks. Thanks Josh!",True
@parthgupta5058,2023-07-31T20:26:10Z,1,Stat God Josh,True
@kridsumangsri964,2023-07-31T11:13:46Z,1,thx  for the best explain very useful for me,True
@user-ls9zb3dy1i,2023-07-31T01:21:09Z,3,"Your neural networks playlist including this video gave me an intuitive understanding of transformers in less than a week which is something that would have taken an entire semester otherwise. I stumbled onto them while searching for a better understanding of Q,K,V, which everyone seems to say is as simple as querying a database‚Ä¶but what does that even mean?? Your explanations are brilliant, and I will be sharing with everyone I know who wants to learn more about this topic. I look forward to future videos. Thank you!",True
@amitvyas7905,2023-07-30T22:47:41Z,0,One quick question. When we do all the calculations parallelly on GPUs then does each GPU/ compute unit have the entire sentence?,True
@tashijawed5472,2023-07-30T16:10:17Z,0,"Next Stop: Generative Modeling, specifically Generative Pretrained Transformers or GPT model.",True
@brahimmatougui1195,2023-07-30T15:15:29Z,1,Grrrrr !!  I need to watch a video like 10 times to understand it :),True
@martin777xyz,2023-07-29T22:32:54Z,1,"I disagree with the self attention explanation for associating ""it"" with ""pizza"". I would say the meaning of ""it"" (in this case) depends the remaining sentence. Tasted good is the clue that pizza is correct. If the sentence finished ""cooked efficiently"" that would imply oven was the it reference.",True
@randomforest_dev,2023-07-29T20:18:20Z,1,this looks sick even though i do not understand a thing after first watch üòÇ i am gonna watch this 10 times üòä,True
@genericchannel8589,2023-07-29T19:20:49Z,1,Uploaded 5 days ago? An my exam on transformers is in 2 days. How lucky I am!! ;),True
@drunkayylienz3706,2023-07-29T17:38:35Z,2,"wow these are such a cool math ideas under the hood, thanks for the video",True
@lcoandrade,2023-07-29T14:46:47Z,2,"Your videos are the most clear ones out there. Even with transformers, that can be considered complex, you can make a clear explanation! Thank you very much!",True
@pumbo_nv,2023-07-29T14:37:38Z,0,"Am I the only one who undestood 10% of the video? The author gives example with concrete numbers (weights) how a simple transformer works, but doesn't explain many aspects why theses things are there in the first place (like query, key, value and so on)",True
@gyuio100,2023-07-29T14:08:21Z,1,"Very clear and builds up the concepts in a step by step manner, rather than starting with the overall architrcture.",True
@galdutro,2023-07-29T13:47:11Z,0,"In the wide world of data, where confusion takes its toll, We need a guiding light, to help us reach our goal. With rows and columns aplenty, and dimensions galore, How do we make sense of it, what is all this for?  Machine learning, oh, machine learning,  Shining your light in the data storm. Machine learning, oh, machine learning, With your algorithms, patterns form.  You take our structured and unstructured data, You slice and dice, it couldn't be greater. Machine learning, your prowess we hail, Turning big data into a tale.  From clustering to regression, your techniques never fail, Uncovering the patterns, you leave a useful trail. In decision trees and neural nets, you weave your magic spell, Solving complex problems, only the start of the tell.  Machine learning, oh, machine learning,  Shining your light in the data storm. Machine learning, oh, machine learning, With your algorithms, patterns form.  Through the lens of statistics, and linear algebra's might, You harness computation, in the vast data night. With Python or with R, or even with C++, You make sense of the world, without any fuss.  You take our structured and unstructured data, You slice and dice, it couldn't be greater. Machine learning, your prowess we hail, Turning big data into a tale.  So here's to machine learning, our data's best friend, On this exciting journey, where insights never end. With every new question, and every new quest, We turn to you, machine learning, truly you're the best.   I hope this song captures some of the excitement and intrigue of machine learning, with a touch of the playful spirit of StatQuest. Remember, the rhythm and melody will also greatly influence how the song is perceived, so feel free to adjust the lyrics as needed to match your envisioned tune.",True
@kartikchaturvedi7868,2023-07-29T06:24:12Z,1,Superrrb Awesome Fantastic video,True
@Joy-dn8yz,2023-07-28T21:13:38Z,9,"words cannot describe how happy I am to be able to watch this video. You really helped me with my studies. It is you who made me so interested in AI and think that I am actuaaly able to understand what is going on. Thank you for your simplified models. They really help when larning more complex stuff on this or that theme. But everytime there's a theme I do not know, the first thing I do is go to statquest. Thank you, Josh!",True
@wd8222,2023-07-28T19:49:14Z,9,Best explanation I found in the whole Internet ! although I admit I needed 2 full turns. well done Josh !,True
@user-ok8uz2vp6u,2023-07-28T19:36:26Z,0,Would you be able to do a video on Liquid time neural networks and their implementation in code?,True
@aamirmiyajiwala1241,2023-07-28T18:17:57Z,1,superb!,True
@yaroslavishchuk,2023-07-28T13:27:25Z,1,Brilliant explanation!!!!,True
@fritstuininga8159,2023-07-28T11:48:13Z,2,"I asked ChatGPT to write a song in the style of statquest. Here is the result:   (Verse 1) I was lost in data's maze, with numbers all ablaze, In the realm of uncertainty, I sought clarity's embrace, Then along came StatQuest, with a guiding light so true, David's voice like magic, leading me to knowledge new.  (Pre-Chorus) From logistic regression to the chi-square test, In the world of statistics, I found my quest, With every barplot, violin, and boxplot grand, I dance with joy as I begin to understand.  (Chorus) Oh, StatQuest, you're my guiding star, Through p-values and coefficients, you've taken me far, You break down complexities, make it all so clear, In the realm of data science, you're always near.  (Verse 2) With decision trees and random forests, you showed me the way, Feature importance, overfitting, no longer led astray, In the land of machine learning, you're the sage, With every concept learned, I turn a new page.  (Pre-Chorus) From PCA to t-SNE, you demystified, In the world of data visualization, you're my trusted guide, With scatter plots and heatmaps, we dive so deep, I see patterns emerge, as insights start to creep.  (Chorus) Oh, StatQuest, you're my guiding star, Through p-values and coefficients, you've taken me far, You break down complexities, make it all so clear, In the realm of data science, you're always near.  (Bridge) And when confusion matrix haunts my mind, You unravel it all, one step at a time, Oh, StatQuest, you're my beacon of light, In this journey of discovery, you make everything right.  (Guitar Solo - Instrumental)  (Chorus) Oh, StatQuest, you're my guiding star, Through p-values and coefficients, you've taken me far, You break down complexities, make it all so clear, In the realm of data science, you're always near.  (Outro) So here's my ode to StatQuest, a tribute from my heart, For the knowledge you've bestowed, I'll never be apart, With gratitude, I'll walk the path you've paved, In this world of statistics, your wisdom I'll crave.",True
@theneumann7,2023-07-28T07:29:00Z,1,<3,True
@aayushsmarten,2023-07-28T07:04:48Z,200,"This is the complet-est, precious-est, pur-est, brilliant-est video ever. Can't imagine how much work you've put into creating these illustrations. It's just brilliant. Hats off.",True
@kaanvural2920,2023-07-28T06:02:29Z,0,Thanks a lot Mr Josh. I could not clearly understand how adding the positional encoding values to embeddings works fork remembering which index is the object in.,True
@BlunderMunchkin,2023-07-28T02:43:06Z,1,Why are you talking to us like we're four years old?,True
@wadihpazos8342,2023-07-27T19:44:50Z,1,"Amazing Video, best one out yet.",True
@michaelongmk,2023-07-27T16:54:00Z,1,Love these Quests! Kudos for explaining these complex data science concepts in layman terms but also with great depth ‚ù§,True
@mugemeliskocabey4834,2023-07-27T16:11:36Z,1,Much expected !! Thank you !!,True
@toddcooper5077,2023-07-27T14:26:51Z,0,"So, transformers are different from Markov chains due to back-propagation? If so, Chomsky et al. is wrong thinking that LLM's are only linear?",True
@jyotsnachoudhary8999,2023-07-27T12:13:25Z,1,Thanks a lot @Josh for this comprehensive video on Transformers. It was really helpful!,True
@andrea-mj9ce,2023-07-27T07:20:51Z,0,Where in the course is the next word prediction dealt with? Is it on the 1 0 0 0 value?,True
@ZOBAER496,2023-07-27T06:12:27Z,0,Can you please make a video on autoencoder?,True
@exxzxxe,2023-07-27T04:17:07Z,1,I have been transformed by your video on transformers!,True
@capitanmarmota8562,2023-07-27T01:52:20Z,1,How do I install it in my brain?,True
@OpenAITutor,2023-07-26T21:10:39Z,1,"BAM, StatQuest made the abstruse, clear.",True
@jijunchen4451,2023-07-26T20:02:57Z,1,you must be ml newbie saver,True
@midolion8510,2023-07-26T19:49:44Z,82,I can't imagine how much effort it took for ai scientists to make this model. I really admire your illustration üòÄ,True
@doctorshadow2482,2023-07-26T17:58:37Z,0,"Thank you. A great one, as always.  Although there are many questions still on the table after the explanation. Let me start with some examples, if they answered by someone, I will provide more:  1) 9:09 - since and cosine are periodic functions, so the value could be the same for many different distances. If so, this could lead to unexpected and hardly debuggable issues. What can't we just use ascending or descending functions which: (a) will give the unique values for every distance; (b) value will have clear dependency from distance. 2) 14:15 - ""If you look at a lot of sentences about pizza..."". Are you suggesting to make all this work manually? If not, what is the way computer can make this conclusion? How, even with many examples, could it associate ""it"" with ""pizza"", not ""oven""? 3) 16:08 - you never explained where from we get this ""Query"" and what it intended for and now you introducing a new essence ""Key"". That's great that you just show how it is implemented in the original model, but for the student this is already second value which comes out of blue sky and never explained.  So, as a student, I am lost at 16:08 and further explanation getting harder since it introduces new values without any explanation; just ""this is it"".   It would be nice if someone can help me to catch this or if author stay in my shoes and explain for a person who wants to understand, not just know.",True
@ipsdon,2023-07-26T16:59:26Z,0,"probably most concrete explanation!!  Can you make another video explaining, let say I have 10 documents in my universe to train a model using transformer. What is the process? How to specify the ‚Äòcorrect‚Äô outcome so the back propagation can optimize all these parameters in the neural net? I assume this process is fairly automatic because no amount of manual curation can be practical. big thx!",True
@trendsetters89,2023-07-26T15:48:00Z,0,How does backprop work for word embeddings? i.e. What is the loss function that we are minimizing?,True
@andrewsiah,2023-07-26T13:52:54Z,1,You've finally landed!,True
@ORION31,2023-07-26T13:37:52Z,0,can you make a tutorial  of data structures and algorithms,True
@gnorts_mr_alien,2023-07-26T11:31:28Z,1,christmas came early,True
@therealgigamosh,2023-07-26T06:58:24Z,2,why do u keep saying bam?,True
@human_agi,2023-07-26T05:30:44Z,0,"Ir vamos y, it doesn‚Äôt make sense , it should ‚Äúvamos a ir‚Äù",True
@rossk4770,2023-07-26T03:53:57Z,0,"Awesome video! Though I think I see a typo at 11:24 , for the pizza encoding: -0.9 + -0.2 = -1.1 =\= -1.2",True
@alfredoespinozapelayo,2023-07-26T03:02:40Z,1,Brilliant!!!,True
@brianprzezdziecki,2023-07-26T02:39:00Z,1,Holy crap I‚Äôve been waiting for this for months!!! Finally!,True
@shreejanshrestha1931,2023-07-26T02:21:05Z,1,"Damn! just 1 day old. Lucky me, found early.",True
@user-lk8bx6kn6p,2023-07-26T01:12:51Z,0,"The reason why ""it"" is more related to ""pizza"" but not ""oven"", is because there is a ""taste"" after ""it"", and ""taste"" is much more relevant to ""pizza"" than ""oven"".",True
@wuhaipeng,2023-07-26T00:02:47Z,1,I love the song in Statquest style,True
@maymoonabashir3993,2023-07-25T22:02:01Z,0,thanky ou so much! can we get a video on boltzmann machine,True
@gama3181,2023-07-25T20:22:18Z,0,"super BAM!!! i have a question please :)   There are many definitions in NLP. some transformers are called encoder-only, decoder-only and encoder-decoder. But, to my knowledge, all of these variants contains both the encoder and decoder module. And the main difference are the loss function used to train the transformer, that is:  1) an autorregressive loss to predict the next token, or 2) a masking loss to reconstruct a sequence.   This is correct? If so, why everyone use the terms  encoder/decoder-only?",True
@amirjutt0,2023-07-25T19:14:00Z,0,"Beloved josh , You have pretty decent knowledge. But , It would be better if you use library that's used by 3Blue1Brown Youtuber for much better videos .",True
@kosukenishio9670,2023-07-25T19:01:52Z,19,"For slowpokes like me: The example assumes total vacabulary size of 4 for each language. Thanks Josh for providing some of the best content on the subject! Finally the K, Q, V made clear sense",True
@znull3356,2023-07-25T17:18:54Z,0,LFG. This one has been a long time coming.,True
@ItIsJan,2023-07-25T16:48:54Z,0,9:58 why dont we use a singular sine / cosine function and add the result of the index of the word to all embeddings values?,True
@ItIsJan,2023-07-25T16:37:41Z,1,we have been waiting for so long! thanks,True
@EmaMazzi76,2023-07-25T16:18:37Z,1,Best explanation of the subject I found so far‚Ä¶üëç,True
@seifeddineidani3256,2023-07-25T13:58:37Z,1,"great timing, thanks üòç Triple BAAAM!",True
@manfredoweber3562,2023-07-25T12:53:30Z,1,for the work you have done you get a like and subscribe...  ...and appropriatly purchases of your book...  ... like a thousend times :D,True
@BronzeRasta,2023-07-25T11:55:19Z,1,Been waiting daily for this. Here we go! Thanks!,True
@bin4ry_d3struct0r,2023-07-25T09:54:48Z,0,"The amount of detail that went into this must've taken A LOT of work. Kudos!!  On a side note: the GPT variants are decoder-only (i.e., they do not employ an encoder component).",True
@andrewdunbar828,2023-07-25T09:00:26Z,1,Been waiting for this one!,True
@TudorTatar-ny8zw,2023-07-25T08:47:38Z,3,The positional encoding explanation truly was a BAM!,True
@tuongnguyen9391,2023-07-25T08:37:09Z,0,I have a really weird suggestion. Can you show up your process of gathering information and break things down into digestive part. Your way of break down information is truly remarkable. Say thank you from Vietnam :v,True
@ar3568row,2023-07-25T06:54:43Z,2,Perfectttttt upload timing,True
@ruicai9084,2023-07-25T06:49:45Z,1,I feel so lucky that I just started learning Transformer and found out StatQuest made a video for it one day ago!,True
@grahamhenry9368,2023-07-25T04:09:04Z,0,You should do a video on Decoder only transformer networks like the ones Large Language Models use.,True
@jessiondiwangan2591,2023-07-25T03:20:58Z,4,"(Verse 1) Here we are with another quest,  A journey through the world of stats, no less, Data sets in rows and columns rest,  StatQuest, yeah, it's simply the best.  (Chorus) We're diving deep, we're reaching wide,  In the land of statistics, we confide, StatQuest, on a learning ride,  With your wisdom, we abide.  (Verse 2) From t-tests to regression trees,  You make understanding these a breeze. Explaining variance and degrees,  StatQuest, you got the keys.  (Chorus) We're scaling heights, we're breaking ground, In your lessons, profound wisdom's found, StatQuest, with your sound, We'll solve the mysteries that surround.  (Bridge) With bar charts, line plots, and bell curves, Through distributions, we observe, With every lesson, we absorb and serve, StatQuest, it's knowledge we preserve.  (Chorus) We're traversing realms, we're touching sky, In the field of data, your guidance, we rely, StatQuest, with your learning tie, You're the statistical ally.  (Outro) So here's to Josh Starmer, our guide, To the realm of stats, you provide, With StatQuest, on a high tide, In the world of statistics, we stride.  (End) So get ready, set, quest on, In the realm of stats, dawn upon, StatQuest, till the fear's gone, Keep learning, till the break of dawn.",True
@ps3301,2023-07-25T01:56:24Z,0,Would you be kind enough to explain what a liquid neuron is and how it works?,True
@kidley17,2023-07-24T23:40:35Z,1,"Although is way beyond my area of knowledge I love to watch your videos, it brings me a warm nostalgic feeling from college and reminds me how awesome statistics are.",True
@giyaseddinbayrak5828,2023-07-24T22:18:42Z,2,"Year 2024, transformers are explained in the kindergartens.",True
@tangchunxin979,2023-07-24T21:38:06Z,1,The videos are really fantastic!!! First time ever that helps me understand every single detail!! Thank you!!! Plz keep posting!!,True
@d_b_,2023-07-24T20:37:06Z,2,"Amazing video. I don't know how many times I will have to rewatch this to grok it, 20:50 has me crosseyed. One question comes to mind. For the residual connection, and the position encoded values, does the addition not cause collisions in meanings? Isn't there a possibility that the encoding of a word in position 1 and an encoding of a different word in position 5 end up having the same result? Or is that possibility just so low because these are continuous numbers.",True
@groesusgott9783,2023-07-24T19:45:13Z,0,"Hey Josh, awesome video. I only do not understand one part. At 30:47 you say that the transformer can be flexible with the input and output length. However, isn't the max input and output size not determined during the initialization of the transformer? Especially the Q, K, and V sizes should depend on the max length. So, for example, if the max number of input tokens is 10. The maximum sentence length would be ten, and if a shorter input is used, the remaining tokens will be filled with a ""zero"" in order to be still able to perform the matrix multiplication.",True
@darshagarwal8307,2023-07-24T19:35:42Z,1,"As always I loved the video! Thank you so much for producing such easy, fun and clear videos explaining these concepts. Always looking forward to more!",True
@Raghunaik4ever,2023-07-24T18:51:01Z,0,"Thank you so much for another great video. I have a question about your explanation in the video. According to the information from the original transformers paper, the architecture includes 6 encoder and decoder stacks. In this setup, the output of each encoder serves as input to the subsequent encoder, and the output (consisting of key and value) from the last encoder is directed to each encoder-decoder attention head in the decoder block. However, during the process of performing dot product and multiplication with the value vector, it seems that a single vector is generated, representing a single word. I'm curious about how the output of the Feedforward Network actually consists of two different vectors: one for Key and another for Value. These vectors are then passed as inputs to the encoder-decoder attention heads in the decoder block. Could you please explain in more detail how this works? Thank you!",True
@AntonPanchishin,2023-07-24T18:29:02Z,0,"[I stand corrected, the values are added!  Thanks StatQuest] At 11:48 you state that the position values are added to the word embedding values.  In the original paper the values are concatenated.  Great video!",True
@rahulmathew4970,2023-07-24T18:03:24Z,0,why keep me waiting bro. I have been waiting for your video on transformers ever since I saw attention video,True
@fgh680,2023-07-24T17:58:10Z,1,The most AWESOME 36 MINUTES - What an explanation of Transformers!,True
@maximeentsi2205,2023-07-24T17:36:27Z,9,"I try-harded deeply to understand transformers in few mouths ago, I can say that this video is a must have.  Thank you Josh",True
@gnostikas,2023-07-24T16:54:01Z,1,Wow,True
@chandraprakash934,2023-07-24T16:45:15Z,1,This video is amazing just as other videos of yours ! Thank you for spreading knowledge ! Eagerly waiting for upcoming videos.,True
@rikki146,2023-07-24T16:39:59Z,5,"That is a lot of stuff in a single video!! For those who are wondering, ChatGPT is a decoder only neural network, and the main diff between an encoder and a decoder is that a decoder uses masked attention - thus ChatGPT is essentially an autoregressive model. Notice how ChatGPT generates a response in sequential order, from left to right. Anyway, good stuff!",True
@jacksonthering5978,2023-07-24T16:32:56Z,0,"For the back propagation in the start, how does that work when we dont know what the output should be? For the neural network you showed, we wanted to know what the words went to.",True
@jediknight120,2023-07-24T15:52:24Z,772,"As a Computer Science professor who teaches Machine Learning, this is probably my most anticipated video ever. I regularly use your videos to brush up on/review ML concepts myself and recommend them to my students as study aids. You explain these concepts in the clear, straightforward way that I aspire to. Thank you!",True
@linhdinh136,2023-07-24T15:50:31Z,26,"Thanks, Josh, for keeping your promise to make a video about Transformers. I learned a lot and truly appreciate your effort in explaining this concept. I just placed an order to buy your book and made a donation to support the channel. I'm looking forward to more content on Machine Learning and hope to see videos about GPT and BERT models. ‚ô•",True
@BlueRS123,2023-07-24T14:27:11Z,0,How do we determine the sine and cosine functions? Does it work this way?  1st: sine function with period 1 2nd: cos function with period 1 3rd: sine function with period 2 3rd: cos function with period 2 etc......,True
@curiousboy7015,2023-07-24T14:27:05Z,2,People like you are the positive side of the world ,True
@abeeRidge,2023-07-24T13:42:24Z,1,"What a clean, easy to follow video!",True
@unknown-otter,2023-07-24T13:41:15Z,1,Oh boy...,True
@yuanyuan524,2023-07-24T13:14:14Z,1,Been waited this video for sooooo long,True
@abdoualgerian5396,2023-07-24T13:08:19Z,3,"We wanna more NLP material please, tiny bam !",True
@ISK_VAGR,2023-07-24T12:27:50Z,1,Despitethe complexity of the theme an amazing explanation,True
@AmitBhor,2023-07-24T12:14:34Z,118,22:12  8 heads because 8 gpu clusters are common and hence can compute in parallel . The embedding dimension are 512 and that leaves each head has 64 query size. Great video üëç,True
@johnas3,2023-07-24T12:01:56Z,1,Thank you!! Still need some time to digest such a big concept‚Ä¶ But worth for waiting! Hoorayüéâ,True
@vladimirmihajlovic1504,2023-07-24T11:37:51Z,17,Hey @statquest - here is a quick suggestion. Another convenient way to explain positional encoding might be by drawing clock with minute and hour hand. Then - instead of sin() and cos() functions you could simply track the x and y coordinates of the tip of the minute and hour hand. It gives much more convenient intuition behind mechanics of the encoding.  (a) it shows its repetitive nature (b) ties encoding position with sense of time (which is intuitive since speech is tied to time as well). Speech is the most common way we use language (c) it explains why we use both sin() and cos() functions (to track circular motion of the clock hand) (d) it provides intuition on why having two pair of sin() and cos() functions is better than just one,True
@apah,2023-07-24T11:37:37Z,2,Man oh man the crazy timing .. I just watched your video on attention yesterday !! TRIPPLE BAAAAM your rock josh thanks :D,True
@silver_soul98,2023-07-24T11:34:54Z,1,was waiting for this one. thanks so much man.,True
@rishabhjain1468,2023-07-24T11:22:39Z,1,"much awaited and anticipated video!!, Tysm",True
@fgfanta,2023-07-24T10:20:52Z,2,The explanation of transformers that the Internet missed!,True
@patriciachang5079,2023-07-24T09:40:56Z,2,You really explaining these concepts in a clear way! Will you do more explanation video on statistic like Cox model for survival ? Thanks! :),True
@elmehditalbi8972,2023-07-24T09:13:40Z,1,LETS GOOOOOOO,True
@pypypy4228,2023-07-24T09:06:52Z,1,A long anticipated video! ‚ù§,True
@rathinarajajeyaraj1502,2023-07-24T08:51:13Z,1,much awaited one... going through now,True
@user-yc4uo1uo1n,2023-07-24T08:46:01Z,0,"Thanks for another awesome video! It seems to me that positional information is actually accounted for even WITHOUT positional encoding, because softmax in multi-head attention PRESERVES THE ORDER, which is then submitted to MLP, thus MLP will apply weights depending on the input order. Or do I lose smth?",True
@jordanconnolly1046,2023-07-24T08:33:06Z,1,I was confused but now I understand bam!,True
@mitchynz,2023-07-24T08:31:26Z,1,awwwwwww yeah...... it's finally here! The culmination of the StatsQuest cinematic universe,True
@cuberootme,2023-07-24T08:27:17Z,2,Nice,True
@berkk1993,2023-07-24T08:11:07Z,4,"I've spent a good deal of time studying attention, the critical concept behind transformers. Don't anticipate a natural understanding of the Q, K, and V parameters. We aren't entirely certain about their function; we can only hypothesize. They could still function effectively even if we used four parameters instead of three. One crucial point to remember is that our intuitive understanding of neural networks (NNs) is far from complete. The matrices for Q, K, and V aren't static; they're learned via backpropagation over lengthy training periods, thus changing over time. As a result, it's not as certain as mathematical operations like 1+1=2. The same applies to the head count in transformers; we can't definitively state whether eight is a good number or not. We don't fully grasp what each head is precisely doing; we can only speculate.",True
@thanhtrungnguyen8387,2023-07-24T07:21:30Z,1,Finally,True
@berkk1993,2023-07-24T07:09:02Z,1,Long waited transformers,True
@sedthh,2023-07-24T06:27:53Z,1,IT'S HAPPENING!,True
@shichengguo8064,2023-07-24T06:12:40Z,0,"Awesome. Maybe next topic will be pre-training and fine-tuning, correct?",True
@TheTimtimtimtam,2023-07-24T06:07:58Z,1,Thank you,True
@prettywitty9051,2023-07-24T05:41:51Z,0,Word embedding sounds a lot like an embedded if statement in Excel,True
@venkideshk2413,2023-07-24T05:31:53Z,1,I think chatgpt is using only the decoder part of the transformer.,True
@HD-Grand-Scheme-Unfolds,2023-07-24T05:28:02Z,1,it felt like a lifetime waiting... I haven't even watched it yet... why am I commenting then ehh. lol.,True
@NeoShameMan,2023-07-24T05:26:09Z,0,"Word order for world order üòÇ query a d keys are related to cbow and skipgram, ie the context of the word and the word relative to context. That is what's the probability of a given context rekative to a center word, and what is the probability of a given center word for a given a specific context, ie if the given center word (query) slots in the specific context (key).",True
@wong4359,2023-07-24T05:23:40Z,1,I finally can watch your video from 5 years ago to 1 hour ago!,True
@leshawnho6349,2023-07-24T04:59:04Z,0,"very nice, but i need to watch it again",True
@tdv8686,2023-07-24T04:54:44Z,4,"OMG, I waited for it for so long!!, thank you, Josh!",True
@vinny2688,2023-07-24T04:53:25Z,3,THIS is what I've been waiting for!,True
@sandesh3273,2023-07-24T04:41:59Z,1,I was just checking if this video was released or not. üòÑ,True
@juansilva-fy6cw,2023-07-24T04:41:50Z,1,Nice video Mr Bam,True
@gvascons,2023-07-24T04:39:57Z,42,And so we reach the state-of-art!! Congrats Josh :D,True
@okay730,2023-07-24T04:38:08Z,1,I HAVE BEEN WAITING SO LONG FOR THIS VIDEO TYSM,True
@mrbeanfan7217,2023-07-24T04:33:39Z,1,ABSOLUTELY AMAZING!,True
@saranyav2581,2023-07-24T04:33:11Z,1,Thank you,True
@user-vw3pr5gq9g,2023-07-24T04:14:01Z,2,Interesting and fun as always,True
@vidbot4037,2023-07-24T04:03:29Z,5,HE HAS DONE IT YET AGAIN!,True
@sunilravilla6938,2023-07-24T04:02:44Z,1,Finally üéâ,True
@ksrajavel,2023-07-24T04:02:35Z,1,Wow! Finally,True
@bekai,2023-07-24T04:02:27Z,1,Omg what an honour,True
@statquest,2023-07-23T10:17:02Z,46,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
