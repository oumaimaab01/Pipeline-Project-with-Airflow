author,updated_at,like_count,text,public
@statquest,2023-08-21T21:36:45Z,5,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@adityasuryan37,2024-05-29T21:54:49Z,0,Loved the video but what about Encoder only Transformer?,True
@ID10T_6B,2024-05-26T12:26:21Z,1,This is the only video on youtube that explains how such a complicated thing works so simply.,True
@jugsma6676,2024-05-26T07:15:16Z,1,This is a god level YoutTube channel,True
@pfever,2024-05-07T08:09:29Z,1,I don't understand why we need the residual connections.... =''( isn't the word and position encoded values information already included in the masked self-attention values? or is most of the information lost so we need to directly add the word and position encoded values?,True
@sreerajnr689,2024-05-06T09:02:32Z,0,"In an encoder-decoder transformer encoder was trained in English and decoder was trained in Spanish which made it possible to do translations. But here, only English is used for both encoding and decoding which makes it impossible to convert the English encoding to Spanish output. So here, would we used both language datasets combined to train the model to enable it to do translations as well?",True
@sreerajnr689,2024-05-06T08:50:59Z,0,Is it the same network that is being used in BERT and GPT? What makes them different?,True
@user-se8ld5nn7o,2024-04-26T03:14:14Z,1,"Hey, fantastic video as usual! Getting hard to find new ways to compliment, haha. Just one quick question since you mentioned positional encoding. When generating embeddings from GPT embedding models (e.g., text-embedding-3-large), do the embeddings contain both positional encoding layer and masked-self-attention info in the numbers?",True
@by301892,2024-04-24T03:46:34Z,0,"I feel it‚Äôs a bit misleading that it seems the tokens of the input sequence is fed in one by one, and that when you put in the first token, it predicts the second token but just ignores it, where in reality it feeds the entire sequence to predict the next target token, and on next iteration, you append the input sequence with the target token as input, and predicts the second target token, and so on. Right?",True
@shamshersingh9680,2024-04-21T07:16:07Z,0,"Hi Josh, thanks a ton for making such a simple video on such a complex topic. Can you please explain what do you mean when you say """,True
@user-my8vx3ls2u,2024-04-19T12:39:57Z,1,"With all this high complexity stuff the SQ lost the point a little bit, IMHO. It's better to create separate videos on detailed understanding of how each concept (e.g. PO) works and then make references and compose the knowledge into only one new concept in this video.",True
@sharjeel_mazhar,2024-04-19T03:37:39Z,0,"Can you please make a tutorial on it using Pytorch? And maybe train it on any text dataset, so that all of us get a gist of how we can make our own decoder only transformer? Like ChatGPT but mini scale?",True
@aryamansinha2932,2024-04-18T16:22:12Z,0,"one more question... why is there one common FC layer used in the decoder bit (predict statquest given ""what is"") vs (predicting awesome when given EOS token and ""what is statquest"")...i would think they would be separate FC layers for both of them since one is predicting the next word..the other is predicting the word in the middle?",True
@hanhtrannguyen7791,2024-04-11T07:32:27Z,1,liked and subscribed also,True
@hanhtrannguyen7791,2024-04-11T07:30:36Z,1,Greate explanation. It help me a lot. A million heart for u!!,True
@terryliu3635,2024-04-10T02:45:57Z,0,"Another great session, thank you!!! Quick question, how do we decide what numbers to use for the Keys and Values?",True
@OliviaB-xu1vc,2024-04-05T13:18:40Z,0,"Thank you so much for another great video! I did have a question -- I'm confused about why you can train word embeddings with only linear activation functions because I thought that linear activation functions wouldn't allow you to learn non-linear patterns in the data, so why wouldn't you just not use an activation function at all in that case or use only one?",True
@victorluo1049,2024-04-03T21:16:00Z,1,"Hello Josh, thank you again for your video ! I had one question concerning training the model on next token prediction: As training data, would you use ""What is statquest <EOS>"" or ""What is statquest <EOS> awesome"" ? What I mean by that, is when training the model by feeding it an input prompt such as ""What is statquest <EOS>"", do you also feed the model the word that comes after it (for calculating the loss), here ""awesome"" ?",True
@PetrBorkovec-wk1ux,2024-04-01T08:23:02Z,0,"I don¬¥t undersatnd how is it possible to add numbers for word embeding to positional encoding and then  to self-attention. I think it is the same as to add together for instance length, weight and temperature? Could anybody help me, please??!",True
@PetrBorkovec-wk1ux,2024-03-31T15:50:16Z,0,"I can¬¥t understanda, how it is possible tu sum numbers for word embeding, positional encoding, self-attention together. It seems to me it is the same as sum weight, length and temperature together. Cam anybody help me to understand, please??!",True
@ayush_stha,2024-03-26T16:37:09Z,6,"This explanation is essential for anyone looking to understand how ChatGPT works. While more in-depth exploration is necessary to grasp all the intricacies fully, I believe this explanation couldn't be better. It's exactly what I needed.",True
@vuhuynh8740,2024-03-26T08:38:45Z,1,StatQuest is awesome!!,True
@JanKowalski-dm5vr,2024-03-20T11:48:40Z,0,"Great video. Do I understand correctly DNN which is responsible for word embedding, it not only converts the token to its representation as a numeric vector, but already predicts as the next word should be returned ?",True
@adam.phelps,2024-03-17T22:09:45Z,1,I really enjoyed this video!,True
@harshmittal63,2024-03-16T01:21:23Z,1,‚ô•,True
@396me,2024-03-13T14:23:28Z,0,How do the positional squiggles are formed,True
@nmfhlbj,2024-03-13T09:07:24Z,0,"hi ! thankyou so much for your great video! i have some questions that i havent understand..  1. how to interpret query, key, and value as in definition? cause ive been watching a lot of attention videos but i still dont get what are they actually and how to convert the input to became the Q,K,V 2. im doing a research for forecasting stock prices using transformer, but i still dont get it how to do embedding with numerical values as the input (every other videos explained it with words as the inputs).. do you know how? 3. what is the attention's output shape? is it a matrix or just a regular vector?  thank you !",True
@BigAsciiHappyStar,2024-03-08T00:30:06Z,0,If you convert the words into the correct numbers then That's Numberwang!!!! üòÅüòÅüòÅ,True
@aryamansinha2932,2024-03-06T14:37:37Z,0,"hello..first off thank you for this great content ¬†  I had a question/s  could you give an example of how the embedding neural network is trained? i.e.  what is the input and output in the embedding neural network during training? The neural networks I have worked with statements that go along the lines of ""given a set of pixels determine whether the picture is a cat or not"".. I do not know what the equivalent is with embedding neural networks  and follow up question..can the embedding neural network be the same for an encoder-decoder model and a decoder-decoder model?",True
@Lzyue0092youtube,2024-03-06T03:46:56Z,1,your series almost save me...love from Chinaüí•,True
@random-ds,2024-03-03T22:51:56Z,0,"Hello Josh! First of all thank you for this great video, as usual it's very simplified and straightforward. However, I have a little question. I saw your videos on transformers and this one, but every time I feel like the output is already there waiting to be embedded and then predicted. I mean that why the answer can't be ""great"" in stead of ""awesome"", what was the probablities given by the model for ""great"" and for ""awesome"" to make the final prediction. Here I gave the example of one extra word (great) but in real life it's the whole dictionary of words that can be predicted. So when generating the output, does it compute the ""query"" and ""key"" of the whole dictionary of words and then hopefully the right word has the best softmax probability?  Thanks in advance for the clarification.",True
@YumanKumar,2024-02-29T18:32:11Z,1,Amazing Explanation! Double Bam üòäüëç,True
@nivethanyogarajah1493,2024-02-26T20:27:00Z,1,Incredible!,True
@laurentlusinchi519,2024-02-22T22:46:29Z,0,"The embedding values for ""what"" and ""Statquest"" are identical before the positional encoding. Is that not a typo ?",True
@SriramSrinivasan-fg9nx,2024-02-22T04:07:12Z,0,"Hi Josh, Can you please make a video about Encoder-only Transformers, like Google's BERT",True
@josephsueke,2024-02-18T17:01:36Z,1,incredible! this is such a clear explanation. thank you!,True
@BooleanDisorder,2024-02-10T22:14:58Z,0,"Dude, can you make a video on state space models like Mamba? It's super interesting!",True
@arijitchaki1884,2024-02-03T05:15:49Z,0,"In word embedding how came two words ""What"" & ""statquest"" has same embedded numbers?",True
@Primes357,2024-01-30T03:15:50Z,0,"I didn't understand just one part: how are the weights to calculate Q, K and V for each word in the sentence calculated? Is it also an optimization process? If so, how is the loss function calculated?",True
@chihhaohuang9858,2024-01-29T16:55:47Z,1,BAM... You really killed it. Thanks for your explanation.,True
@ayseguldalgic,2024-01-25T21:38:13Z,1,Hey Josh! You're a gift for this planet üòç so thanks this awsome explanations..,True
@acasualviewer5861,2024-01-21T17:29:16Z,0,"The word ""similarity"" may be confusing when talking about self-attention.   Often when speaking of embeddings people think the embedding only encodes meaning (as in word2vec), but in Transformers these embeddings encode some notion of relationship with other words that would help predict the next word (each head encoding different aspects of this).   So when self-attention compares each word with the previous word, it isn't figuring how ""similar"" the word ""it"" is to ""pizza"", but rather ""how related"" these are based on things like gramatical rules, word order, parts of speech, or even meaning.   ""Similarity"" may be misleading here, though strictly speaking ""how related"" is a type of ""similarity"" score but only in the ML sense. Not the usual sense. There's nothing ""similar"" about ""it"" and ""pizza"" but the two are related.",True
@ishaansehgal2570,2024-01-21T05:35:32Z,0,I am a bit confused why are we encoding the input prompt and generating the next predicted word for each word in the input prompt. We don't use this information at all when generating the output part right? For generating the output part we just use the KQV from the input prompt and continue from there? How are the two parts connected,True
@huiwencheng4585,2024-01-18T09:53:07Z,1,Ë¨ùË¨ùÔºÅ,True
@Nana-wu6fb,2024-01-04T20:02:05Z,1,Thanks!,True
@101alexmartin,2024-01-02T20:14:24Z,1,"Thanks for the great video, Josh. I got a question for you. What should drive my decision on which model to choose when facing a problem? In other words, how to choose between an Encoder-Decoder transformer, Decoder-only transformer or Encoder-only transformer? For instance, why ChatGPT was based on a Decoder-only model, and not on a Encoder-Decoder model or an Encoder-only model (like BERT, which has a similar application)",True
@StevenTuAI,2023-12-27T05:09:04Z,0,"Hi Josh, when predicting next word, where do the weights (1.5, 1.2), (-2.5, 4.1), (1.2, 1.6), (1.7, -3.0), (-0.6,  -1.5) come from?",True
@mattk1358,2023-12-22T04:25:42Z,1,So the machine can now code and learn what to pay attention to hmm what could go wrong,True
@colmon46,2023-12-13T13:56:06Z,1,Your videos are awesome! I've never thought I could learn machine learning in such an easy way. Love from china,True
@julianh7305,2023-12-04T22:00:53Z,1,"Hi Josh, great video, as always. I was wondering if you would also make a video about Encoder-only Transformers, like Google's BERT for instance, which can also be used for a great variety of tasks.",True
@arnabkumarpan5615,2023-12-02T09:07:45Z,0,Garibo ka sam altman‚ù§,True
@arnabkumarpan5615,2023-12-02T09:07:22Z,0,Long time,True
@bhaskersuri1541,2023-11-29T11:14:34Z,1,This the most brilliant explanation that I have seen!!!!!! You are just awesome!!!!,True
@cosmicfluke3718,2023-11-27T13:31:55Z,1,We dont have to ask gpt to know stat quest is awesome reply from gpt BAM!!! BAM!! BAM!!,True
@txxie,2023-11-27T12:28:31Z,0,"Thank you, your video is great! But I'm really confused about the EOS token. Why does the model keep generating new words after generating the EOS token in the prompt? Should it just stop? What is the difference between the EOS tokens in the prompt and the output?",True
@dicksonkinyanyi4306,2023-11-23T07:16:27Z,0,"The softmax percentages have been flipped the other way around kindly check 20:00 it should be  0.0, 0.0 and 1.0 and not 1.0, 0.0 and 0.0 up to the end",True
@adityarajora7219,2023-11-22T04:12:24Z,0,"Begging, Please teach us the BERT model, BAM!!",True
@edphi,2023-11-17T03:57:01Z,0,Damn i have learn the whole of decoder and encoder models from start to finish including training and deploying but not understand the math the way you opened the pandora box. Now the sine and cosine and query key value and everything is flying out in my head,True
@cromi4194,2023-11-15T16:07:05Z,3,Wow this series culminating in a perfect explanation of GPT is the most magnificent piece of education in the history of mankind. Explaining the very climax of data science in this understandable step-by-step way so I can say that I understood it should earn you the noble prize in education! I am so grateful that you never used linear algebra in and of your videos. Professors at university don't understand that using linear algebra prevents everyone from actually understanding what is going on but only learning the formula.  I have an exam in Data Science on Friday in a week. Can you make a quick video about spectral clustering by Wednesday evening? I will pay you 250$! :),True
@jossevandekerchove1020,2023-11-15T15:22:22Z,1,Can you please make a video about GNN? You are reaaallyy good at explaining,True
@Max-ry9wl,2023-11-14T06:34:08Z,0,Hey Josh! I need to solve generation task using decoder only model. How I should preprocess corpus for this? I think that splitting in 2 parts and separate parts with token <sep> is good solution. But I dont understand how train this model and calculate loss. Input for model is tokens_first_part + <sep> tokens_second ann output[index of sep:] of model compare with input[indx of<sep>:],True
@user-ff1qi2yk9q,2023-11-08T08:41:53Z,0,"20:56 I think the value of the word ""is"" is miswritten it should be 1.1 , 0.9 not 2.9,1.3  it should  not be same with the value of word 'what'  Thank you for your videos btw ur explanation is awesome.",True
@namunamu5258,2023-11-04T19:52:54Z,3,"Thank you so much! It is an amazing video and I haven't seen a video teaching AI/ML techniques like this anywhere! You're talented. And my research areas span Efficient LLM (LoRA, Quantization, etc). It cannot be better if I can see those concepts",True
@karlnikolasalcala8208,2023-11-03T16:18:22Z,2,YOU ARE THE BEST TEACHER EVER JOSHH!! I wish you can feel the raw feeling we feel when we watch your videos,True
@SakvaUA,2023-11-02T18:03:28Z,0,"So, when one picks encoder-decoder architecture and when decoder only is sufficient?",True
@tamoghnamaitra9901,2023-10-30T11:43:45Z,1,"Great Video. If possible, please do a video on model fine-tuning techniques like PEFT/LoRA",True
@Modern_Nandi,2023-10-30T04:21:43Z,1,Brilliant,True
@sidereal6296,2023-10-27T03:55:45Z,2,"I just want to say you are AMAZING. Thank you so much. I would personally love to see a video on backprop to train this, or even just training an RNN since we saw multi dim training, but not training once we get the state machine / unrolling involved. Loved the whole series üéâ",True
@dineth9d,2023-10-25T16:38:17Z,5,"Hey Josh, I‚Äôve been really digging your videos! They‚Äôre not only informative and helpful for my studies, but they‚Äôre also super entertaining. In fact, you‚Äôve played a big part in my decision to continue pursuing AI Engineering. Could you please do a video about low-rank adaptation(LoRA). I am not good with that.",True
@nobiaaaa,2023-10-17T01:47:23Z,0,"Great explanation! Btw, what is the manuscript that first described the original GPT?",True
@RayGuo-bo6nr,2023-10-13T21:40:10Z,1,"What a wonderful video!!! BTW, When will you publish your CD? I will buy it tooüòÑThanks!",True
@shinoo5004,2023-10-13T16:24:24Z,0,Hi josh. Would you mind  making a video for retention network?,True
@raminziaei6411,2023-10-12T19:32:14Z,0,"Hi Josh! I'm a little bit confused about the whole idea of generating the input first, compare it to the actual input and use to modify weights and biases in the training phase. I cannot find anywhere on the internet that it is mentioned. All I see is that the masked self attention is used on the input sequence to make contextualized version of each word and then, they are used to generate the target tokens. Nowhere can I find that generating the input sequence and compare it to the actual input is part of the process. Can you please clarify?",True
@AndreasAlexandrou-to5pw,2023-10-10T11:16:28Z,0,"Had a couple of questions regarding word embedding: - Why do we represent each word using two values? Couldn't we just use a single one? - What is the purpose of the linear activation function, can't we just pass the summation straight to the embedder output?  Thanks for the video!",True
@hoangminhan460,2023-10-10T10:16:50Z,1,that's perfect. Can you do more lectures on LLMs? Thanks a lot.,True
@juaneshberger9567,2023-10-08T02:45:28Z,1,"great vids, any chance you could make videos on Q-Learning, Deep Q-Learning, and other RL Topics! Keep up the good work!",True
@ytpah9823,2023-10-07T09:26:40Z,0,"üéØ Key Takeaways for quick navigation:  00:00 ü§ñ Decoder-only Transformers are used in ChatGPT to generate responses to input prompts. 01:48 üìä Word embedding is a common method to convert words into numbers for neural networks like Transformers. 08:09 üåê Positional encoding is used in Transformers to maintain word order information in input data. 10:53 üß© Masked self-attention in Transformers helps associate words in a sentence by calculating similarities between words. 16:28 üßÆ Softmax function is used to determine the percentage of each word's influence on encoding a given word in self-attention. 19:56 üß† Reusing sets of weights for queries, keys, and values allows Transformers to handle prompts of different lengths. 23:52 ü§ñ Decoder-only Transformers both encode input prompts and generate responses, enabling training and evaluation. 25:58 üß† The decoder-only Transformer process involves several steps, including word embedding, positional encoding, masked self-attention, residual connections, and softmax for generating responses. 29:09 ü§ñ Masked self-attention in a decoder-only Transformer ensures it keeps track of significant words in the input when generating the output. 32:23 üîÑ Key differences between a decoder-only Transformer and a regular Transformer include using the same components for encoding and decoding in the decoder-only Transformer, using masked self-attention all the time, and including input and output in the attention mechanism. 34:15 üìö During training, a regular Transformer uses masked self-attention on known output tokens to learn correct generation without cheating, while a decoder-only Transformer uses masked self-attention throughout the process.",True
@gvlokeshkumar,2023-10-04T14:32:18Z,4,"Quests on attention, transformer and decoder only transformer are of immeasurable value! Thank you so much! Keep the quests coming!",True
@victorluo1049,2023-09-28T13:07:33Z,0,"Hello Josh, thank you very much for your videos, they are by far the most informative I have seen !  I had a question regarding the training of generative transformers: Can a generative encoder-decoder transformer (we expect it to behave like gpt-3 or llama) be trained with next token prediction ?  Because from what I understand, for inference, to generate the output, we encode the input sentence, then we feed <EOS> to the decoder (embedding layer), then we get the prediction of the first token, which we re-feed to the decoder to generate the next token, until we get a <EOS>. So we get a sentence as output.  However, if the training was done with next token prediction, it means that given an input (sentence), we only try to predict the very next token, which means that we encode the input, we feed <EOS> to the decoder, we get the token prediction and that's it. In that case, the decoder's embedding layer never sees tokens other than <EOS> in the training.  So during inference, how could it comprehend tokens other than <EOS> ? Maybe my assumption about the decoder only receiving <EOS> during next token prediction pre-training is false.",True
@shixiancui6870,2023-09-24T22:31:01Z,0,"Looks like when we encode the prompts, we only need to compute K and V for each input words, then generate outputs token by token starting with EOS. Is this true? I'm a bit confused here because previous half of your video shows that we also need to compute the whole self-attention values for prompts i.e. Q*K*V.  Edit: maybe it's because of that we need to reuse the same masked self-attention cell for encoding, and cannot avoid computing Q*K*V for prompts.",True
@lolololo-cx4dp,2023-09-23T04:17:39Z,0,"In chat gpt we can input random gibberish that certainly doesn't exist in the Training tokens, but it still can generate answer using that gibberish. Maybe they encode random gibberish to a specific token hmm.",True
@cheolyeonbyun9640,2023-09-20T15:06:07Z,3,Congrats on 1 million subs statquest!! All the Love from Korea!!,True
@jarsal_firahel,2023-09-19T13:39:37Z,1,"Ok after seeing a bunch of awesome videos about machine learning I decided to subscribe to.... ""guitar""... StatQueeeeest.",True
@thanhtrungnguyen8387,2023-09-15T13:09:02Z,0,"In 25:42, when the model generates the wrong word, it will be fixed by backpropagation if this is the training process and it will be ignored if this is the generation process, right?",True
@zhangeluo3947,2023-09-15T12:01:13Z,0,"The last question is how stacking all possible different cells of K,Q and V work? By just averaging their different ouputs or linearly transformed by a certain matrix W0?",True
@zhangeluo3947,2023-09-15T11:30:03Z,0,"My another question is just for training the encoder-decoder transformer, we can just do Masked-Self-Attention to all the ground true(known) decoded tokens at the same time? Is that right?",True
@zhangeluo3947,2023-09-15T11:14:05Z,0,"Hey sir, in terms of training that decoder-only generative transformer, each time for training a input prompt, we just need <SOS> x<1> ,..., x<Tx> (all tokens besides the last <EOS>) all those tokens' Masked Self-Attention vectors to feed into softmax and observe their ouputs(which are generated tokens immediately after them) against those real(ground true) prompt tokens? Is that true for training only?",True
@gustavsnolle8424,2023-09-15T05:16:52Z,2,What an awesome video and channelüòÅüëç. Would you consider doing a video on deep q learning models? I believe everyone would benefit from a video on such a fundamental topic. Thank you for your invaluable workü§©,True
@spartan9729,2023-09-12T07:15:43Z,2,"Oh my. Thanks for the recap, it was so necessary for this video. It made the concept extremely clear.",True
@jacksonrudd3886,2023-09-08T21:00:11Z,0,"Thank you for the incredible content. Josh, quick question for you. I didn't see you mention vertically stacking the decoders in a way where the output of one decoder is the input for the next. From the 'Illustrated Transformer' page (I can't link b.c. youtube won't let me) it seems to be a core aspect of transformers. Thanks again.",True
@NJCLM,2023-09-08T20:08:52Z,1,Awsome as always  from you !! now we only need en real tutorial with python to creat a mini transformer model. hops it is on the making as my wish list,True
@namratanath7564,2023-09-07T16:33:34Z,0,Do you still identify as a geneticist?,True
@shahrizalmuhammadabdillah3127,2023-09-06T13:23:07Z,0,"I am student in agriculture. now i'm curious about machine learning, and this channel is pretty lame for me. Hope you, Josh, always kind and help every people who get through with statistic or ML with your video and content.",True
@ruksharalam173,2023-09-06T10:23:51Z,1,A thorough explanation üòÄ,True
@AbuDurum,2023-09-04T12:44:20Z,0,Hey Josh. I just want to ask what software you use to make the diagrams?,True
@kartikchaturvedi7868,2023-09-04T09:48:55Z,1,Superrrb Awesome Fantastic video,True
@ciciparsons3651,2023-09-03T03:24:29Z,1,"awesome, really helpful. Can't wait for another exciting episode!!",True
@al8-.W,2023-09-02T16:17:15Z,9,"This video is proof that repetition is prime when teaching advanced concepts. I've watched many similar videos in the past and could never get all of these numbers to finally make sense in my mind. With your previous transformer video, I was getting closer but somewhat got lost again with the QVK values. Having to this second video to watch in a row made it clearer for me what all these numbers do and why we need them.",True
@mitch7w,2023-09-02T15:05:53Z,1,Thanks for the excellent explanation!,True
@garychow7719,2023-09-02T12:29:26Z,1,thank you! the video is really nice,True
@asheeshmathur,2023-09-02T10:42:04Z,1,"Delighted to watch one of the Most Brilliant videos. Hats off. Will join the channel tomorrow, first thing. Meanwhile do you have a one on Probability Density Function.",True
@yasboyy,2023-09-01T00:26:34Z,0,"I just have a question. The Word Embedding network contains weights that were obtained with back propagation. But on which data was it trained ? Is it like a huge superset of our current ""what is Statquest awsome EOS"" vocabulary ?",True
@gabip265,2023-08-31T16:12:10Z,2,Another great video as always! Would be amazing if you could continue with Masked Language Models such as BERT in the future!,True
@linhdinh136,2023-08-31T15:57:34Z,0,"Thank you, Josh, for yet another excellent video on GPT. I find myself slightly puzzled regarding the input and output used to train the Decoder-only transformer in your example. In the normal Transformer model, the training input would be ""what is statquest <EOS>,"" and the output would be ""awesome <EOS>.""  However, in the case of the Decoder-only model, as far as I understand, the training input remains ""what is statquest <EOS>,"" but the output becomes ""what is statquest <EOS> awesome <EOS>."" Could you help to clarify this? If my understanding is correct, I'm wondering how the Decoder-only transformers know when to stop during inference, considering that there are two <EOS> tokens within the generated response.",True
@razodactyl,2023-08-30T21:17:46Z,47,Bruh. This channel is criminally underrated.,True
@enchanted_swiftie,2023-08-30T12:59:57Z,2,"You didn't use the innocent, cozy, soft bear for softmax üß∏üò¢ _(in most of the parts)_",True
@danberm1755,2023-08-29T16:38:11Z,1,"Thanks again Josh! I noticed that many GPTs are decoder only. Thanks for clarifying!  BTW saw that Yannic had a video on history rewrites. Probably not a topic for this channel, but still pretty cool üòÅ",True
@gama3181,2023-08-29T05:02:19Z,1,autoregressive Baaaaaaaaaaa ... MMMM!!,True
@user-ou8ol4wu2s,2023-08-28T22:57:32Z,0,"Have to say that alot of concepts have not been explained in detail in the transformer series, the ml and statistics series is great though",True
@jazzeuphoria,2023-08-28T20:47:44Z,2,Bedankt,True
@antonindusek3725,2023-08-28T20:24:35Z,9,"Hello Josh, i am enjoying your videos as they are helping me so much with my studies as well as entertaining me. You are kinda a reason i decided to continue studying bioinformatics. Since you are covering chatGTP and stuff now, could you maybe make a video about AlphaFold architecture in the future? I understand it might not be your topic of interrest, but i would love to lear in more deeply (pun intended). Thanks either way!",True
@brucewayne6744,2023-08-28T16:40:22Z,1,"Perfect video! Quick question, how are you drawing your lines? This line style is awesome!",True
@exoticcoder5365,2023-08-28T12:09:05Z,3,"Hey Josh ! Would you mind making videos about graph neural networks ( GNN ) or graph convolutional network ( GCN ), and most importantly, the graph Attention Network ( GAT ) ? I have briefly gone over the maths these days, I already knew the matrix manipulation stuff but I think with your help, it would be much clear like your Transformer series, especially on the attention mechanism in the graph attention network ( GAT ), many Thanks üôèüèªüôèüèªüôèüèªüôèüèª appreciated !",True
@NTesla00,2023-08-28T11:57:44Z,8,"Haven't had a single stats course in over 3 years but I still keep up with this channel from time to time! Neural networks are way more complex than what I've ever had to deal with, but you manage to break down even these topics into bite size pieces...Bam!!",True
@MrHummerle,2023-08-28T09:10:08Z,1,"Hi there! Came to YT in hope you had a nice video of Rank Robustness. Would be amazing, if you wanted to make a video about it! Keep it up! Also: nice Dinosaurs!",True
@iProFIFA,2023-08-28T08:58:49Z,2,would love to learn about bidirectional transformers next ;-),True
@timmygilbert4102,2023-08-28T06:41:27Z,1,Hexacontatetraconta BAM!,True
@LifeObserver-007,2023-08-28T06:06:50Z,1,"Big thanks! Appreciate your efforts. I enjoyed your great book as well.   Can you make a video explaining how chatGPTs and other LLMs do logical reasoning?   The differences you presented between normal tramsformer and decoder transformer does not explain, which is better for what application?",True
@konstantinlevin8651,2023-08-28T05:55:08Z,1,"Woahh, this is actually cool. We appreciate it a lot Josh!",True
@ulamss5,2023-08-28T05:30:51Z,1,17:23 things are getting out of hand üòÇ,True
@TheTimtimtimtam,2023-08-28T05:22:03Z,2,"Sir Josh, Thank you for making this public. May God Bless you.",True
@xspydazx,2023-08-28T04:53:05Z,0,"Chat gpt is still a dialog system at its heart and has many different models which it gets results from . It softmaxes the outpits acording to the intent , ... So intent detection plays a large role in the chatgpt response .. the transformers are doing major works .. its super interested despite bqttling away with vb net !",True
@xspydazx,2023-08-28T04:48:51Z,3,"After painstaking research on this topic, I have realized that all the information is not in the transformer itself. The transformer is just the neural network (i.e., the processor or function). The information is actually in the embedding model. The sequences are all stored there.  When training the transformer, the word embedding model needs to be fully trained or trained with the same data. This allows the learned embeddings to fit the transformer model. Here, overfitting is beneficial as the model will be a closed model after training. The transformer can be retrained (tuned) to your specific task. The main task should be text generation corpus/tuned for question and answer.  The learned embeddings model can actually generate data as well as the transformer neural network, making it a rich model. I also noticed that to get an effective word embeddings model, the optimization or ""fine-tuning"" should involve classified and labeled data. This allows clusters of similar terms to appear, as well as emotionally aware embeddings.  The transformer is essentially performing an ""anything in, anything out"" operation, forcing the input and output to converge despite any logical connection between them. Given an input, it produces the desired outcome. As many similar patterns are entered, it will fit this data using the masking strategy. Transformers need to be trained as masked language models to be effective, highlighting the drawbacks of LSTM and RNN models which cannot produce the same results.  The more distanced the output is from the input, the more layers and heads you need to deploy.",True
@yuanyuan524,2023-08-28T04:07:59Z,1,Thanks for clear explanation,True
@aseemlimbu7672,2023-08-28T04:03:34Z,17,Triple BAM ‚ù§‚ù§üëåüëå,True
@peerbr7849,2023-08-24T01:58:05Z,15,And I thought you'd stop at ChatGPT. Thanks for never stopping to learn and teach!,True
@statquest,2023-08-21T21:36:45Z,5,To learn more about Lightning: https://lightning.ai/ Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
