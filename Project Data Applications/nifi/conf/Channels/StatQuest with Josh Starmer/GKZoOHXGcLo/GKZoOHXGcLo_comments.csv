author,updated_at,like_count,text,public
@statquest,2021-09-11T13:53:45Z,13,"The full Neural Networks playlist, from the basics to deep learning, is here: https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@user-qe2xo6hu2i,2024-05-30T04:08:49Z,0,"Is it important for my small nural network's forward, loss, back propogation and weight-bias Initialising function to work for all sort of model, containing different number of hidden layers and number of nurons in it. In other words is it important to make a nural network general.",True
@wcottee,2024-05-11T13:04:16Z,0,"OK, i missed something...in part 1 we optimized w3, w4 and b3. Now we start over by assigning random values to them. What was the purpose of optimizing w3, w4 and b3 in part one if we just start over? Why is this called Back propagation if we don't used the part 1 optimized w3, w4 and b3? Thanks in advance!!!",True
@sreerajnr689,2024-05-04T10:33:55Z,1,This is such a fun to learn!! üòÄüòÄ,True
@avinashbabudebbati1095,2024-04-27T14:17:30Z,1,"""THE CHAAAAIN RULEEEEEEE"" gets me every time üòÇ",True
@suateymenduran6780,2024-04-26T21:51:26Z,0,"Your videos have helped me a lot! I wrote some code myself and it has been working very good so far, however I stumbled on a problem. I can't understand how to calculate the gradients when there are more than one input and desired output pairs, for example lets say I want the network to output [0, 2, 4] when i input [1, 2],  but [0, 1, 2] at the input [4, 5]. How do we calculate the gradients at such scenario? Edit: To clarify, my calculations work at one data point, but they don't when there are multiple data points. I am calculating the loss for each data point and then calculating the gradients based on the loss and input, and then I do this for every data point and add up the gradients, then apply them.",True
@nandakumar8936,2024-04-23T21:18:01Z,2,every time you mention the soft plus activation function you show the toilet paper pic. Every time. That's commitment right there.,True
@tangh146,2024-03-11T17:46:58Z,3,the preschool vibe really does make everything much less intimidating. love u josh ü•∫,True
@Luxcium,2024-03-10T16:27:09Z,1,This was pretty cool to watch üòÖbut the more I watch the less remains üò¢ which is so sad given how JS is so busy with the rest of his life and stuff üòÆ I don‚Äôt know if he will ever have enough time to make videos awesome again MVAA,True
@bhavikdhandhalya1853,2024-03-09T12:45:01Z,1,Thank you for wonderful videos.,True
@lizzy9874,2024-02-28T14:24:05Z,1,"Hi Josh, it's a great video to help me understand NN. I have some questions regarding the parameter optimization. Is it possible that different parameters would need different numbers of steps to reach the optimal value? If so, how to determine the maximum # of steps or learning rate that works the best for all the parameter optimization?",True
@SuperYkf,2024-02-13T05:13:38Z,1,OMG This is amazing! üò≠,True
@sarvinasalohidinova4881,2024-02-07T01:31:43Z,1,Soft Plus :))))),True
@juliank7408,2024-01-27T18:53:41Z,1,Thank you very much! Appreciated!,True
@jiayiwu4101,2024-01-01T19:56:22Z,1,I like this beginning song a lot!,True
@1243576891,2023-12-27T21:49:17Z,1,Thanks for the video. This is awesome!,True
@BooleanDisorder,2023-12-16T15:07:09Z,0,"What I got from this video... is a headache. ü§î  Kidding, but damn lots happen in neural networks... üòÇ",True
@namanjha4964,2023-12-11T09:41:13Z,1,Thanks! Loved it!!!!!!!,True
@ghostbuster8377,2023-12-04T21:08:23Z,0,I still dont understand why you have to add the other 2 curves. I get how you get the 2 curves but not the adding them together part.....üòä,True
@azizmouhanna8996,2023-11-28T22:46:37Z,1,"Thank you from the bottom of my heart, from a phd student who learns a lot from you üôè",True
@pavorealstudios,2023-11-27T17:45:26Z,0,What is derivative of sigmoid function ?,True
@tunvas,2023-10-20T03:56:40Z,0,"How do we use Backpropagation with a neural network with thousands of weights and biases? Do we have to find all the derivatives by hand like this? (I guess not) Btw, your explanation was excellent!",True
@zhancao7909,2023-10-14T15:33:55Z,1,Thanks!,True
@bennydirty,2023-10-09T10:06:22Z,0,"Great video. The question is: how to choose proper initial values? I reproduced your steps and initialized weights using random values from N(0, 1). The model couldn't converge. Only when I put the same initial weights and biases as you did the model fitted well (similar what you get). But this doesn't work for another random initial values. Is the way how to force convergence?",True
@LEELEE-dg3xd,2023-10-03T07:41:22Z,1,This video really helped me a lot!,True
@ek_minute_,2023-10-02T06:08:32Z,1,thanks for the toilet paper reference of soft plus now i will always revise the backpropagation Every time i go to toilet.,True
@ankurmazumder5590,2023-09-22T02:10:30Z,0,"why can we independently update parameters of neural network in backpropagation? Does the change in value of a paramter not affected by the values of the previous layer and also by the changes in value of parameters in the current layer? Since the gradient wrt each parameters that is being calculated is considering the other parameters remain constant, but the moment you change the any one of the other parameters, shouldn't all the other changes in paramters become invalid?",True
@pauldevereaux5537,2023-09-11T13:58:38Z,0,"Hi Josh, thank you so much for your hard work with this series. I am currently programming my own neural network in R. I was trying to replicate your results of all the derivatives at 11:00 but I do not get all of your values. My values for w1 and w2 and b3 are pretty much the same as yours but for the other derivatives I get different values. I have absolutely no idea what is going wrong in my backpropagation code because I have pretty much checked everything I can think of. So I was wondering, are you 100% sure those values are correct? No offence meant. Thanks in advance!",True
@julienpawlak6239,2023-09-11T11:37:46Z,0,"awesome video I have a question, in this case we used the literal expressions of the derivative to determine each weight and bias But in practice, do we use a numerical derivatives ? Meaning we calculate the value (SSR(w1+dw1)-SSR(w1))/dw1 with dw1 a small variation of w1 and so on for all variables",True
@brahimmatougui1195,2023-09-04T09:30:14Z,1,"Hi Josh, I have made the Python code that implements the two plots in this lesson and the previous one. Would you allow me to share them on my Github?",True
@ipsdon,2023-08-15T04:44:19Z,0,"I rewatch this video and part 1 so many times. Part2 seems to calculate the gradient decent for all w's and b's simultaneously. Question is where is the back propagation, where u have to calculate b3. w3 and w4 before w1, w2 , b1 and b2?",True
@willw4096,2023-08-08T09:34:18Z,1,Thanks for the great video! My notes: 0:59 9:49 10:54 - 11:07,True
@Vanadium404,2023-07-30T08:39:47Z,10,This is the best explanation on YT. No fancy animations just pure calculation. Better than 3B1B and others.,True
@lucasliu2116,2023-07-23T04:25:28Z,1,Nice song! i love it,True
@simplifiedscience7497,2023-07-21T15:02:26Z,1,You are so amazing!! You really made me love machine learning!!,True
@Mastin70,2023-07-16T10:34:40Z,1,"Very well explained, thanks a lot. But what if there are multiple minimums in gradient descent?",True
@BillHaug,2023-07-03T18:38:00Z,1,Song is a banger!,True
@user-ef5ue7uq1q,2023-06-26T10:56:01Z,0,log(z) should be 1/zln(z),True
@mitchynz,2023-06-17T03:33:48Z,1,I love this explanation so much.... It actually helped me intuitively understand the chain rule too. I just purchased your book on machine learning which is the perfect compendium to this series.,True
@pauldevereaux5537,2023-06-15T19:25:54Z,0,"I was wondering, does the logic of building the derivatives for the weights and biases extend to neural networks with more than one hidden layer? I‚Äôd say that you need to add the  (yi * wi)s for all the layers to get the predictedi but would the equation remain the same other than that? Many thanks in advance!",True
@DharmendraKumar-DS,2023-06-08T18:01:19Z,0,We calculate the optimal values for all the parameters of neural networks using chain rule and gradient descent simultaneously but still this process is called Back Propagation. Why?...is any specific parameter's value calculated first than other?,True
@aycc-nbh7289,2023-06-06T14:17:54Z,0,"How would this work in a neural network with more than three layers? Would derivatives of y with respect to x need to be compounded or would they only apply to the current layer? For example, if there were two hidden layers with the softplus activation function, would the derivatives of the hidden layers need to be compounded or would you find each one individually?",True
@amnont8724,2023-06-04T19:44:06Z,0,"9:52 Hey Josh, is there a specific reason that we use the standard normal distribution to initialize all the weights?",True
@gf1987,2023-05-25T10:52:29Z,1,very informative ty,True
@Mica_No,2023-05-14T13:15:51Z,0,what if you would change predicted and the observed Value and calculate their derivative?,True
@Xayuap,2023-04-27T17:08:33Z,0,"hi, when doing softmax,  do we have to do ssr for the firsts W parameters?  if so, as the net has various outputs, ¬øhow do we do with the (observed - predicted)? when the firsts w paremeters are being optimized  bams in advance",True
@aloksharma4611,2023-04-24T01:13:21Z,1,Thanks!,True
@Xayuap,2023-04-22T15:33:09Z,0,so there is a step size for each dimension in the gradient?  so we do tiny steps to each dimension into the descending?  is it that the same as calculate the vector wich points down?,True
@imadboukhari8033,2023-04-08T13:13:46Z,1,THANK YOU SO MUCH !,True
@subhashcs8553,2023-04-02T08:32:30Z,0,"from 9:28, you explain that all parameters (weights and biases) are optimized simultaneously. But in back propagation, don't they get updated iteratively (layer by layer, starting from the last layer and going until the input layer is reached)?   ( Anyone, please correct me if I am wrong. )  so, the weights and bias of the last layer (w3, w4, b3) are updated first. Then the weights and biases of the last but one layer (w1, b1) and (w2, b2) are updated using the updated weights and bias of the last layer- w34 b4, b3. But in your explanation, the direction of updates is the opposite (forward).",True
@GeekyMan,2023-03-19T12:13:48Z,0,"Hi Sir,  First of all, nothing to say about the explanation, it's the best I have ever seen. But one question I have is that if all the weights and biases are updated simultaneously, then why is it known as backpropagation?",True
@BuzaBuza,2023-03-02T07:22:20Z,0,"Thanks sir for this through excellent tutorials. I only have one doubt. at 10:09 you said it doesn't matter which derivative we start with. But from my understanding, it will be more effeictive if we started by the output layer and went back. as we always use the gradients from later layers in former layers. And hence the name backpropagation. Am I correct?",True
@aaronraid282,2023-02-20T20:40:17Z,0,"Hi Josh, why did it take almost 500 gradient descents to find the best green squiggle? Is this normal? This is a small NN so I didn't expect it to take so long. How long to real NN's take to optimize? Thanks, just bought the book!",True
@akshayrameshwar4869,2023-01-24T13:23:45Z,0,"Question??? I have a question regarding steps, iteration and epochs  Q1) Let's say we are using gradient descend to optimize weights, in which instead of considering all the samples (lets say 3) at once and then calculating total SSR and updating weights, We are going to update weights after each sample. Here we are going to calculate SSR for 1st sample and then update its weight but the new weight we calculated here are not optimal weight so do we iterate over the same 1st sample few more times to get new predicted values to get optimal weights? Or do we move to the next sample (2nd sample). If we move to the second sample, we won't be able to find optimal values of weight for the first sample, and similarly we would not get optimal value for 2nd sample, because after updating the weight just once we will move towards the 3rd sample? So where are the minimum 1000 steps we take to find optimal weights?  Q2) Is Epochs are the steps we use to reduce gradient? Let's say we have 1000 samples with 2 feature, and we set the batch size as 500 then in 1 Epoch there will 2 iteration or 2 times weight will be updated, so we will calculate average SSR for first 500 samples then update its weight just once (similar to above scenario we wouldn't able to find optimal value) and move to the second  batch of 500 then done the same thing. Now if we had set epochs = 20 then we will do the above steps 20 times ?  In question 2 instead of updating weight after each sample, we are using batches but still the 1000 (or fewer) steps to find optimal weights are missing.  Q3) What if we take batch size as 1000 the weights will update after each epoch, and it will be similar to the example taken in the above video. So the epochs and steps should be equal in this scenario, right?",True
@adamwagnerhoegh9071,2023-01-10T09:03:31Z,1,Tak!,True
@hasansoufan,2022-12-27T23:06:22Z,1,‚ù§‚ù§‚ù§‚ù§,True
@anilkumar-ki1xb,2022-12-26T00:17:35Z,1,"Josh, You are a true saviour....",True
@lokeshreddypolu250,2022-12-20T08:12:05Z,0,How do we decide learning rates?,True
@MrDemultiplexer,2022-12-19T06:34:29Z,1,I think this series is better than 3Blue1Brown's,True
@devharal6541,2022-12-18T09:42:35Z,2,You are best Josh!!,True
@programmer49,2022-12-17T08:25:47Z,1,All teachers must be your students first,True
@tongzhou2007,2022-12-10T17:24:17Z,0,"Great video and explanation! There's still just one thing I am trying to figure out is, when deriving the ""d SSR/d w1"", we used predicted_i, y1_i etc, which represent one sample. It looks like when deriving the derivatives w.r.t to each parameter, we can only do one sample at a time in the equation. Once we calculated the gradiants of each parameter for one sample at a time, we just add them up to get the final gradiants? Why would this work? Like why do we sum up all the gradiants for the samples, instead of do some other kind of reduction? Is this related to the loss function being MSE? Thanks a lot!",True
@tapanaritete,2022-11-30T12:39:19Z,1,–ë–ª–∞–≥–æ–¥–∞—Ä–∏–º –≤–∏!,True
@fengjeremy7878,2022-11-29T16:41:35Z,1,Three lectures about backward propagation. Finally understand what's going on in this fancy technique. Thank you sir!,True
@Gho0O0ost35,2022-11-14T20:06:14Z,0,"It's not clear how to apply that though. 3input x 4x2 network for example. all weights will be entangled with each other and you cannot  follow a linear path as w1,w3",True
@raa__va4814,2022-10-16T06:32:31Z,0,"Hey Josh! So at 9:42 you initialize the weights and biases ... you use the standard deviation for this case ...first off I'm assuming this is the standard deviation graph created using all of the observed values? and if so please remind me then what was the next step that lead you to pick out 2.74, -1.13 and so on in that particular fashion/sequence",True
@tagoreji2143,2022-09-27T05:42:59Z,1,"Educating along with Entertaining. That too for a complicated topic.Thank you very much, Professor",True
@elhairachmohamedlimam9640,2022-09-10T23:32:49Z,1,"Thank you so much, I have wasted a lot of time to understand these things, but really, after watching your videos things become very easy, thank you a lot",True
@parshanjavanrood368,2022-09-05T20:12:12Z,2,"With all the hype around AI and machine learning, it's not easy to find sources that teach the statistics behind these subjects, and the ones that do teach the math behind it, make it very hard to understand(from the perspective of a second-year bachelors student), but what you do is awesome. Thanks for this great series.",True
@user-ur2en1zq4f,2022-09-04T12:19:35Z,1,"Sir, you are gold. Thanks",True
@giorgosmaragkopoulos9110,2022-09-04T00:00:53Z,2,"Thanks for the video Josh! Now I can flex that I coded my first neural net from scratch :D It works fine with small samples but when I create a large sample size, it fails (I guess that's why ""batch size"" exists in tensorflow instead of throwing everything at once). Do we know why this happens? Thanks",True
@amyma2204,2022-09-03T21:42:33Z,1,You saved tons of confused souls by this amazing explanation.,True
@crazy-boy6143,2022-08-26T19:10:45Z,0,"I created the code to create a Neural Network, and was about to implement backpropagation, however, I got a problem. I can make the code for a simple neural network like the one presented in this video. But what should I do when there are multiple outputs? Each output will be connected to all the ""boxes"" within the hidden layer which is prior to the outputs layer. What should I do in this case? There will be multiple outputs that affect the same weights and biases when optimising them. Can you make a video explain the process of optimising a more complex network? Just like the one presented in the video whose title is ""Neural Networks Pt. 4: Multiple Inputs and Outputs"". Thank you in advance üòä",True
@veljkonikolic7788,2022-08-01T15:55:37Z,0,One small question üôãüèª‚Äç‚ôÇÔ∏è: I tried implementing these derivatives in Julia programming language and once I plug in x-axis values in the derivative of the activation function it diverges to Inf. But when I plug in OUTPUTS of the individual neurons in their derivatives of their weights and biases it magically works. My specific question is: Should I put x-axis values or outputs of that specific neuron in the derivative of the activation function (moment 10:38 that I question)?  I hope this makes sense üòÖ,True
@mayankbhatt1308,2022-07-30T08:27:47Z,1,Tiple Billion BAM.....there is nothing more beautiful in the world than the one who breaks your ignorant blind mental chains....u give me cognitive ecstacies brother...dont even know how to thank you,True
@pavapequeno,2022-07-13T05:34:53Z,1,"Hi Josh, what you have made was for me the first definitive guide to advanced stats and ML that is accessible and understandable by anyone with a basic scientific background. Multiple Bam! I searched a while before stumbling on this treasure trove. Thank you so much! Will you also be releasing some videos on graph embeddings, GNNs/GCNs? I think you would have a very eager audience (certainly including me at least)!",True
@lisun7158,2022-07-10T17:35:00Z,4,"[Notes] 1:27 2:45 3:06  Because of the Chain Rule, we get the formula for the derivative of SSR with respect to w1. (w3 is used)                            7:31 formula for the derivative of SSR with respect to b1 (w3 is used)                            9:10 formula for the derivative of SSR with respect to w2  (w4 is used)                            9:15 formula for the derivative of SSR with respect to b2  (w4 is used)                                    formula for the derivative of SSR with respect to b3 = (d SSR / d Predicted)*(d Predicted/ d b3) [ref.13:24 https://www.youtube.com/watch?v=IN2XmBhILt4&t=277s&ab_channel=StatQuestwithJoshStarmer]  Based on formula above, (The best StatQuest gives the succinctest formula I've ever seen) I get the solution for question below, Q: why called ""backward""? why dynamic programming? A: The backpropagation algorithm computes the gradient of the loss function with respect to each weight by the chain rule.  The calculation of the gradient proceeds backwards through the network, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule. [ref. wikipedia]   11:00 use each derivative to calculate respective step size and each new value. -- optimize all parameters of the NN simultaneously",True
@yonasabebe,2022-07-07T06:53:42Z,1,You make it look so easy. Thanks for your effort and contribution.üôè,True
@ericli228,2022-07-06T19:00:25Z,0,"Question on the backpropagation: when you adjust a weight or bias using gradient descent, wouldn't that spill over and affect the tweaking of other weights and biases? For instance, wouldn't the change to weight 1 during one step influence the calculation of weight 3's derivative in the next step since weight 3's derivative involves a multiplier dependent on weight 1?   How does this affect the backpropagation's convergence toward the lowest point in the error function? Unlike your video on Gradient Descent, in which the parameters getting tweaked (a line's slope and its intercept), the parameters in a neural network appear to influence each other rather than being independent.",True
@RafaelAlves-nt6zb,2022-06-21T20:38:53Z,1,Just fucking amazing video. Thank you.,True
@xjacket2831,2022-06-17T07:40:23Z,3,This is the best thing ever... it's like you help me understand the complex concepts in 3Blue1Brown in the simplest ways in these videos....you both are the epitome of Bam!......,True
@franciscoruiz6269,2022-06-13T22:10:47Z,1,You're a master! You have gain all my respect.,True
@ramyav4689,2022-05-25T07:10:54Z,2,"Dear Josh, Thank you so much for making this video. I have always been intrigued by the way you explain the mathematics of complicated concepts! It is just amazing.",True
@durrotunnashihin5480,2022-04-26T16:16:19Z,1,"Question please: Why we use backpropagation and forward propagation in training process, while only forward propagation could possibly find the optimal parameters? Is it faster than only forward propagation? Or any other reason? Anw, I watched a lot of videos from your channel, it is very interactive and make the complexity simpler.. thank you for the effort :)))",True
@shivamgaur8624,2022-04-20T11:36:40Z,1,I know I'm enjoying a video when I like it even before it starts. Amazing work!!,True
@TheGabytls,2022-04-05T11:06:56Z,1,"Thank you, thank you, thank you thank you, thank you!!! I never thought I could understand this type of thing. If somebody asks me to explain this to them I'll say... THE CHAAAIN RULE!!! Thanks Josh! I'll donate part of my first salary to you eventually :)",True
@kostjamarschke4613,2022-03-23T18:08:16Z,1,"Love the video and explanation, but I love the SoftPlus commercial every time you mention the activation function even more.",True
@user-lk1fd7lz3c,2022-03-12T13:30:48Z,1,thx,True
@xinyuechang6062,2022-03-09T21:51:41Z,0,Not me clapping my hands watching that green line fit perfectly with data üêï,True
@mahdimohammadalipour3077,2022-03-02T15:13:48Z,0,Thank you for this wonderful series. I came up with a Q in my mind. In the software and packages in order to calculate gradient they easily apply numerical differentiation by changing very little a parameter and measuring its effect on the loss function change and then calculate derivative with respect to that parameter by easily dividing  those changes. am I thinking in a right way ?,True
@panku077,2022-02-13T02:36:53Z,1,A phenomenal example of teaching from first principles. Josh's brand of elegance and charisma kept me engaged and on track to mastering this topic.,True
@semenbondarenko3512,2022-02-01T22:19:17Z,1,"I'm your fan, seriously you are the best teacher I have ever met.  ~~~Triple Thanks~~~",True
@superk9059,2022-01-31T15:32:56Z,1,"throughly, clearly, amazing, awsome!!! Thounds BAM!!!",True
@nikolatotev,2022-01-30T12:24:04Z,1,"I have a question about backpropagation: (edit after finishing writing, actually 2)  In a real implementation when performing backpropagation do the weight values get after each layer is reached or does the algorithm go through the whole network, saving how each weight & bias should change, and then after reaching the start of the network all of the values get updated?  And a question that is related to a more complicated version of neural networks - In Convolutional neural networks that use Skip Connections, during the forward pass results from the previous layers gets concatenated with a deeper layer. My question is - when performing a backpropagation are the skip connections used to pass the gradient directly to a layer closer to the start of the network or are the skip connections just ignored.   I'm not sure if anyone else is struggling with backpropagation in CNNs, if there are more people a video on the topic with your teaching style would be amazing!",True
@ruggeromorelli5063,2022-01-29T16:20:06Z,1,"Hi Josh, first of all thank you for your enlightening contents.. there is one thing I didn‚Äôt get though: Why is it call ‚ÄúBackpropagation‚Äù if it actually optimise all parameters together? I thought it first optimised the closest parameter to the output and then backwards until the first parameters. Thank you if your (or anyone else‚Äôs) feedback ‚ù§Ô∏è",True
@aakarshanraj1176,2022-01-09T13:39:19Z,1,"you really explained it better than anyone on youtube. Thanks a lot, it was really helpful.",True
@DrRiq,2022-01-09T04:32:20Z,0,"Excellent video as always, though I have one question. I don't get how the back propagation videos are any different to the gradient descent ones; they seem to all be applying gradient descent to each parameter..? So what is this process called 'back propagation' ultimately, why does it deserve a separate name?",True
@yacinek85,2021-12-27T20:39:43Z,1,Big thanks,True
@vincentjonathan,2021-12-23T02:58:02Z,1,"I have question, why the weight and bias among that 2 activation function is different? I think it should be same because the activation function is same  #NeedHelp",True
@matteobarberis1149,2021-12-03T13:22:11Z,0,can someone explain why we can't put the derivatives of those equations equal to 0 and solve them? Why do we use gradient descent?,True
@lin1450,2021-11-26T11:18:21Z,1,"Thank you so much for your content. I will be forever grateful. They way you convey the information with such simplicity, step-by-step and humor makes it so fun to watch and builds up motivation. Concepts that seemed way out of reach for me are becoming something I'm slowely building trust to truly and deeply understand them one day! Thank you so much! <3  Ps: Would be cool to see one or two more T-Shirt or Hoodie logos. Already got the ones out there, but I'd really love to support you in this kind of way and show you and your great work to the hole world :D",True
@tymothylim6550,2021-11-10T06:53:10Z,1,Wonderful video! Now I will always think of toilet paper when seeing the softplus activation function :),True
@siddharthmodi2740,2021-11-06T13:10:34Z,1,"I dont believe , how can someone explain this weird topic with this level of simplicity.Hats off to your efforts. Thank you josh",True
@bdev1444,2021-10-19T17:29:31Z,0,"11:13 and repeat until the predictions do not in improve very much. If these 3 datapoints come from a dataset of lets say 300 datapoints and the network performs calculations in batches of 3, wouldnt the network get overfit to the last batch of 3?",True
@nabeelhasan6593,2021-10-06T08:42:33Z,1,Understanding Back Propagation always give me Panic Attack but your style and simplicity is beyond amazing. Thanks for making such a complicated topic to be so easy to understand,True
@tupaiadhikari,2021-09-16T18:51:01Z,2,"Thank You Josh, for these valuable Videos. I am immensely grateful to you for making these videos. You are truly a Legend in the Data Science Community. Love and Gratitude from Kolkata, India.",True
@harishbattula2672,2021-09-15T13:31:27Z,1,Thank you for the explanation.,True
@ritwikamajumdar5967,2021-09-15T11:29:30Z,1,Thank you so much,True
@statquest,2021-09-11T13:53:45Z,13,"The full Neural Networks playlist, from the basics to deep learning, is here: https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@dijkstra4678,2021-09-09T15:29:17Z,1,Any other videos always omit the most key details and only generally explain the concepts such that you come out of that video having learned absolutely nothing on how it actually works. The world needs more videos like yours which actually explain these concepts in mathematical detail without getting too difficult either.,True
@fadygamilmahrousmasoud5863,2021-09-08T06:08:35Z,1,i love you,True
@marcus2441,2021-09-05T16:01:52Z,0,"Hi, I have a problem 11:07 I created a calculated function and got the same result as yours at 1st loop, but after the 5th loop the values not improve anymore. And predict value is something like 0.4 0.31 0.28. Am I missing something here?",True
@sattanathasiva8080,2021-09-02T02:06:16Z,1,Best videos for stat,True
@marccrepeau6853,2021-08-25T18:56:51Z,5,"Thanks for this great series of tutorials! One thing I'm confused about: in this video some of the final parameter estimates (after the 450 gradient descent steps) end up larger than the original (random) parameter estimates, and some end up smaller.  But the derivatives calculated for the first gradient descent step are all positive, so if you multiply them by a constant (positive) learning rate you will end up *decreasing* all parameters (by subtracting a positive value from them).  Thinking about that parabola in your gradient descent tutorial, you would be starting with tangent lines (derivatives) all on the right side of the parabola.  All derivatives are positive and thus all the tangent lines have positive slopes.  Gradient descent will subtract positive step sizes from all parameters.  All parameters will thus *decrease*.  So how do some final parameter values end up greater than the original (random) estimates? (For example w1 is originally set at 2.74 and the final estimate is 3.34)",True
@faezeabdolinejad731,2021-08-25T16:32:45Z,1,"Its amazing,,,, thank youuuuuuuü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞",True
@joy2000cyber,2021-08-09T20:49:58Z,0,"So artificial neuron network is just a squiggle fitting program finding derivative minimum, has nothing to do with neuron network. That name is so fucking misleading. The animation at the end shows how it works. Most other channels intentionally or unintentionally mystify the concept with the fucking name.",True
@amarnathmishra8697,2021-08-08T11:15:19Z,0,Amwazing! Can i get documentation or notes  of your series? Please.,True
@mathscorner9281,2021-08-05T06:44:49Z,2,"I wish I could find videos on every topic in which I m having problem on this channel. Sir, you are really a fabulous teacher.",True
@krizroycetahimic4087,2021-07-29T12:59:56Z,1,You made me realize what AI truly is. THANKS!,True
@Nandeesh_N,2021-07-20T06:38:23Z,6,"""Quadruple bam"" is what I feel as soon as I learn something from your video. It's so amazing and when I get any doubt, first thing I do is to check a relevant video on your channel! Thank you Josh for these amazing videos!",True
@karannchew2534,2021-07-14T19:18:44Z,1,"Why is it called ""back"" ""propagation"" please? Isn't gradient decence working simultaneously? What is actually propagating backwards?   A separate point: though the gradient descent works simultaneously, not all of them reach their respective final point at the same time, right?",True
@phungtruong6698,2021-07-10T10:07:59Z,1,Bammmm!!! I totally understood how to update parameters in Neural Network. :v :v,True
@lakshman587,2021-06-08T10:39:55Z,1,Million billion trillion thanks for this Neural networks videos Josh! BAM!!! You are really awesome!!!!,True
@TheGoogly70,2021-06-01T20:52:07Z,3,"Awesome! It appeared daunting in the beginning, but at the end it was so easy to understand. Great job!",True
@a_sun5941,2021-05-27T07:22:48Z,2,"I might have missed this from the videos. But would like to ask that: since all the parameters are updated at the same time, why it is called 'back propagation'. The parameters aren't really updated from the last layer to the first layer; they are updated all at the same time, why it is called 'back'?",True
@pelocku1234,2021-05-25T20:06:23Z,0,"This video was great and made me want to build this in R. One note to others that are trying to use the same starting values and get to the same optimal values, you will need to use a learning rate that is not .1 because this will find different optimal values that are not quite the same as we see here. Just wanted to drop this note in case someone else tried to recreate it like I did. Again, it was the teaching that inspired me to do it.",True
@anasmomani647,2021-05-14T21:02:33Z,2,u literally should get Nobel prize for your videos <3,True
@praveerparmar8157,2021-05-13T19:21:56Z,2,"As a matter of fact, I was going bonkers trying to understand Backpropagation until I watched this video. Now I'm stable üòÅ",True
@aishnaarora9842,2021-05-08T08:07:33Z,0,"I have a doubt.. why isn't the (in any case, lets suppose the last layer) Predicted = green squiggle = softmax(Y1,i * w3 + Y2, i * w4 + b3) and is just  Predicted = green squiggle = Y1,i * w3 + Y2, i * w4 + b3 ?",True
@jancvrcek1541,2021-04-26T16:38:37Z,2,"Erudite, funny and free of charge (triple BAM!!) what more could we wish? Absolutely love your videos!",True
@samerrkhann,2021-04-13T13:07:07Z,0,"Bam! This explanation is dope. One little question, isn't it better to optimize starting from b_3 all the way to w_1? Maybe because that's why the algorithm has a ""back"" in name.",True
@brucebalfour1002,2021-04-08T10:51:56Z,2,"I love your videos so much. So helpful. I always look forward to the BAMs, not only because they are fun, but also because it gives a sense of fulfillment. BAM from Switzerland.",True
@MADaniel717,2021-03-03T20:37:13Z,2,I'm back Josh! Exactly what I needed. Now I'm going to try to implement a neural network from scratch in Python using your videos :D,True
@hunter8831,2021-03-02T08:10:54Z,1,You are such a great teacher that you make machine learning seem so easy and motivate anyone to learn more about it,True
@powangsept,2021-02-23T14:09:29Z,1,Watching the videos in your channel let me love the statistics more and more! Thank you for the great videos!,True
@MelonsIncorporated,2021-02-20T04:02:08Z,47,"I'm surprised more teachers don't know about this channel, I'm not exaggerating when I say it's the MOST incredibly useful tool in helping me understand material in a class that makes no sense.",True
@maryguty1705,2021-02-19T08:07:51Z,0,"hi, what happened to video 69? just curious.",True
@nandankakadiya1494,2021-02-12T10:49:22Z,4,The best and perfect videos on Machine learning I have ever seen bammmm thank you ......,True
@84mchel,2021-02-06T14:53:22Z,0,I recreated this nn in Python and when I use only 3 values to train on the output doesn‚Äôt match. With 5 or more training data it does work!,True
@justinwhite2725,2021-01-29T22:59:06Z,0,Thank you for the animation at the end. I've been purplexed why my network with 2 hidden layers seems to find a 'happy medium' where all the outputs are muddled values around the median. Your graph showed this is normal.  I suspect this means I need more time to train the deeper layers towards the inputs.,True
@otahirs,2021-01-18T13:22:36Z,0,"Hi,  this video is missing in the ""Machine Learning"" playlist.",True
@pratyushsinha374,2021-01-18T13:19:57Z,1,This is goldmine,True
@yacinerouizi844,2021-01-08T15:18:33Z,1,"best tutorials on machine learning, thank you!",True
@maksims4669,2021-01-03T00:08:15Z,65,"Throughout all my years of Bachelor studies I was avoiding computer sciences and statistics as much as it was possible, for I could not understand them. However, when I enrolled for Masters I had no other choice but to figure it out. Last semester I had a compulsory course in Computational Intelligence, so in order to understand the material I had to find some additional sources. That's how I encountered StatQuest. You explained everything so well that through the summer I was inspired to make an additional course in Machine Learning. This semester I took several courses in statistics and optimization, next semester I will certainly take more and now I really consider to connect my life to one of these fields. All this with help of your channel. I have no doubts that there are and will be much more people like me, to whom you became a guiding light in their studies. Thank you for your work, Josh, and keep up helping the curious ones in finding answers.  Have a great year 2021!",True
@John-wx3zn,2021-01-02T08:14:30Z,1,"You are freakin funny. ""a green sqiggle""",True
@joehsiao6224,2021-01-01T05:36:52Z,0,awesome! so it doesn't matter which weight to start with!,True
@sureshkumar-cc1jq,2021-01-01T02:56:55Z,1,You're gene is tunned with teaching.,True
@karthikdeepan1998,2020-12-31T13:17:06Z,0,Absolutely love this video and crystal clear explanation !  You are indeed a great teacher.  What videos are coming up in Neural Network Series ?,True
@azul_azure,2020-12-30T08:53:48Z,1,BAAAAAAAAAAAMMM,True
@mikhailbaalberith,2020-12-28T22:41:26Z,1,Bam from Brazil,True
@alejandrocanada2180,2020-12-28T09:30:32Z,3,I wish all the teachers in the world explain as good as you do. Thank you very much!,True
@deepakrout5330,2020-12-16T20:50:54Z,0,"when are the deep learning CNN, RNN coming ?",True
@Luthea,2020-12-13T21:33:56Z,1,And this is the first step in my adventure to create an earthquake early warning system. BAM!,True
@zahraahm371,2020-12-11T22:59:58Z,1,"Thank u so much for making it that simple & awesome and understandable (:  Also, thanks for telling about Kite, I wounder if it works with online environment like Colab, and online version  of Jupyter?  I finally got the theoretical part with u, can u suggest me where to start learning Deep Learning coding/practical part, if u have a suggestion for somewhere awesome as like u, it will be great,,, I mean, it will be Trible BAM! (: Thanks again & again",True
@jasonliu6239,2020-12-11T01:44:35Z,2,"Very helpful! Question: does it mean there is no order of optimizing different weights and bias (w1, w2, b1, b2, w3, etc.) at the same time? Also, does it optimize w1 based on initial w3 guess? I mean if there is a different initial w3, how it gonna impact w1 optimization? Thanks",True
@rameshmitawa2246,2020-12-06T15:57:00Z,1,This guy is just too awesome :),True
@parijatkumar6866,2020-12-06T15:49:11Z,2,As always brings a smile along with great clarity...,True
@ouche71,2020-12-05T23:24:34Z,2,"I really liked this series, but I still feel confused what happens when we have more than one feature. Keep the great work!",True
@satrakolhem3224,2020-11-24T07:13:24Z,0,"How do we get the value of W1, W2, W3, and W4 for the first time to calculate the first predicted value? Anyway, your video is Amazing. <3",True
@huidezhu7566,2020-11-23T00:48:44Z,2,This is the clearest explanation I can find. Amazing work,True
@madhanbaburs9594,2020-11-21T13:40:23Z,1,Black box demystified!,True
@rajathk9691,2020-11-18T17:03:44Z,59,I'm really confused right now because the quality of the content really makes me want to promote you but I also wanna keep this channel a secret to myself.,True
@kalpaashhar6522,2020-11-17T07:15:20Z,2,"Brilliantly explained. Not only is it easy to assimilate the information due to the colours, the style in which you have broken down a complex explanation is a skill not many teachers have. Keep up the awesome work",True
@yiuhotam1215,2020-11-15T10:57:44Z,1,Million BAM !,True
@bhaskersuri1541,2020-11-14T05:07:43Z,1,statquest dot O M G,True
@seanfitzgerald6165,2020-11-11T20:27:44Z,7,"There were so many BAM's in this video it made my head spin, and I love it.",True
@TheLLMGuy,2020-11-10T10:57:18Z,214,i think you are one of the few teachers on the planet who understands that the secret of understanding is simplicity! Thank you,True
@zihaozhou1256,2020-11-10T01:15:12Z,5,"Honestly, some of the illustration is better than the college professors.",True
@ivanb.2914,2020-11-09T22:13:17Z,2,Can you do a video on DOMINANCE ANALYSIS?,True
@sandeshbirla4331,2020-11-09T10:36:16Z,1,More power to you!,True
@SenoorMoradee,2020-11-09T07:27:38Z,1,Super nice!  I like your videos. They are very helpful  Thank you so much,True
@BiduKadri,2020-11-08T20:20:59Z,42,"Josh, I cannot stress how much of an empathetic teacher you are. I went through years and years of education in engineering and I tend to forget the low level math behind it all.  Here I can rescue that with you. You are just an amazing human being :)",True
@magtazeum4071,2020-11-08T14:21:12Z,2,this channel is lovely..Thank you Josh...,True
@haozeli9660,2020-11-07T18:31:09Z,2,"damn, the quality of the content of this channel is just over the top. Hope u can get more views because u definitely deserve it!!!!!",True
@ignaciozamanillo9659,2020-11-06T19:21:24Z,1,Thanks as simple and good as always! Planning about a tutorial of Neural Networks in R / Python? Would be great,True
@ArifDolanGame,2020-11-05T22:44:19Z,1,glad I found this channel! very helpful üëç,True
@arthurlee4294,2020-11-05T06:30:24Z,1,"hello,for the content of your video,can you provide citations or reference so that i can read more detail? espacially k-maens clustering and bias and variance series. thank you so much.",True
@jitpackjoyride,2020-11-05T01:51:02Z,2,Thank you for this! Love learning from statquest  Hope there‚Äôs more videos for CNN,True
@wenyange2607,2020-11-04T13:44:41Z,5,"Thank you for making this video. It's very clear, easy to follow, and super helpful for understanding the algorithms.",True
@twandekorte2077,2020-11-03T21:24:40Z,1,Great video. You are exceptional with regards to explaining difficult concepts in an easy and straightforward way :),True
@rajmalkar2687,2020-11-03T15:29:36Z,1,Thanks!,True
@kanui3618,2020-11-03T08:34:05Z,1,"Keep it up, joshüî•",True
@yeyuan4235,2020-11-03T08:31:40Z,2,Thanks Josh for another great video! Do you plan to introduce RNN and CNN at some point? Looking forward to your clear explanations on these topics.,True
@mounikyerusu5058,2020-11-02T22:00:34Z,1,"Hey josh, could we expect videos where you write code in python and implement ML models?. I know you did one on decision trees and I really loved it.   I know you gave us a conceptual understanding of Ml in all of your videos but what I'm requesting is to give examples for every popular Ml algorithm along with python code in three or two levels of complexity(basic, intermediate) etc.  There are other channels who do this, but if we could learn it from your teaching approach  it would totally be one of a kind.   The reason I ask you this is because I genuinely think you're a teacher who puts himself in the shoes of a beginner's mind who is laboring to understand all the data science and statistics jargon, and tries to explain stuff  so that it stokes up interest and ambition in them. It certainly worked for me. I was never good at math let alone stats. But your videos helped me tremendously and now I am a little better and thanks to you for that.   I know I asked you a lot and you may not have the time for all that I requested, but hopefully one day.   Thank you Josh.",True
@RubenMartinezCuella,2020-11-02T10:03:05Z,0,Is this the last video of the series on Neural Networks?  Thank you again for the great video!,True
@mariusconstantin3781,2020-11-02T08:30:28Z,0,Hi Josh! Thank you for elaborating such detailed and rich tutorials!  Can you please recommend a software that can help solving Neural Network issues?,True
@mrglootie101,2020-11-02T08:21:21Z,26,"Finally!, tbh you're really good in teaching Josh, simple but detail, keep it!  Bam! From Indonesia",True
@engindenizalpman,2020-11-02T08:19:11Z,1,Amazing as always üòéüëç,True
@suryatejakothakota7742,2020-11-02T05:24:12Z,3,Bam from India üòç,True
@petercourt,2020-10-28T11:20:19Z,3,Fantastic work Josh! :),True
