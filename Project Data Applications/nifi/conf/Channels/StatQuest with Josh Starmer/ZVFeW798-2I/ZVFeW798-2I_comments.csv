author,updated_at,like_count,text,public
@statquest,2020-03-29T16:09:11Z,23,Corrections: 1:16 The Lambda should be outside of the square brackets.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/,True
@mostafakhalid8332,2024-05-11T16:09:10Z,0,Second order Taylor polynomial is used only  to simplify the math?  is there another objective?,True
@abhzz3371,2024-03-05T14:58:11Z,0,"7:43, how did you get 104.4? I'm getting 103.... could anyone explain?",True
@jiaqint961,2024-02-16T03:08:36Z,1,OMG... How you break down complicated concept to simple concept is amazing. Thank you for the content.,True
@nielshenrikkrogh5195,2024-02-06T05:47:24Z,1,as always a very structured and easy to understand explanation......many thanks!!,True
@user-gq3uo8dl1l,2024-01-20T13:31:04Z,0,ü§©,True
@jingzhouzhao8609,2023-12-25T12:27:54Z,0,"Merry Christmas Josh, üòä Just a quick observation: at 11:00, I noticed that p_i represents the previous predicted value, therefore, p_i-1 might be a better notation to denote this.",True
@pratt3000,2023-12-08T01:24:45Z,0,I understand the derivation of Similarity score but didnt quite get the reasoning behind flipping the parabola and taking the y coordinate. Could someone explain?,True
@user-fy4mu7tp6h,2023-11-11T02:23:17Z,1,Very nice explanation on the math. love it !,True
@user-ns2en2gs5h,2023-10-10T17:03:13Z,1,100 years later ai will come to this Channel to Learn what their grand grand father looks likeüòä,True
@user-fi2vi9lo2c,2023-09-24T11:42:38Z,1,This series about XGBoost is marvellous! Thanks!,True
@hoomankashfi1282,2023-07-31T09:11:14Z,0,"you did a great job with this quest, could you please make another quest and describe how does XGBoost handle multi class classification tasks? there are several strategies in sk learn but understanding them in another issue. Good luck",True
@CrazyProgrammer16,2023-07-21T18:56:42Z,1,Very well explained. Thank you.,True
@pedroramon3942,2023-07-19T08:55:22Z,1,Thank you very much for explaining all this very hard math in the original article. I did all the calculations and now I can say I understand xgboost in deep.,True
@ineedtodothingsandstuff9022,2023-07-07T02:31:22Z,0,"Hello thanks for the video. Just one questions is the splits done based on the residuals all the time, or gradients? For instance if I use a different loss function, gradient might have different calculation. In this case, do we still use residuals to do the splits or we use respective gradients from the given loss function?  Thanks a lot!",True
@karthikeyapervela3230,2023-06-14T18:46:22Z,0,"@statquest I am trying to workout a problem on pen and paper but just 4 features instead of 1, so once the split it is made on 1 feature does it proceed to another feature? What happens next?",True
@christianrange8987,2023-06-09T08:06:03Z,0,"Great video!! Very helpful for my current bachelorthesis!üôè Since I want to use the formulas for Similarity Score and Gain in my thesis, how can I reference them? Do you know if there is any official literatur like book, paper etc. where they are mentioned or do I have to show the whole math in my thesis to get from Tianqi Chen's formulas to the Similarity Score?",True
@jingyang2865,2023-04-03T03:48:18Z,1,This is the best resource I can find online on explaining XGboost! Million thanks to you!,True
@abhashpr,2023-03-11T14:52:34Z,1,Wonderful explanation .. did not se this sort of thing anywhere else,True
@knightedpanther,2023-02-04T20:18:43Z,0,"Thanks Josh. You are awesome. Please let me know if I got this right:  For Gradient Boosting, we are fitting a regression tree so the loss function is just sum of squared residuals. When deciding a split we just try to minimize the sum of squared residuals.  For XGboosting they modified the loss function by adding the regularization term. So when deciding a split, we can just try minimizing this new loss function. However they decided to flip it for clarity (or other purposes like maximization instead of minimization which we don't know) and called it similarity and we try to maximize it when deciding a split.",True
@FedorT54,2023-01-30T14:22:05Z,0,"Hi!  Can you please explain me, Similarity score for model with number of trees =1 and depth =1 is its logloss minimum value?",True
@FedorT54,2023-01-30T12:08:58Z,0,Hi great video! Can you please explain me for classification p_i is log odds? Not direct probability? https://youtu.be/ZVFeW798-2I?t=681,True
@auzaluis,2022-12-16T00:52:16Z,2,gosh!!! such a clean explanation!!!,True
@anunaysanganal,2022-11-16T00:23:31Z,1,Thank you for this great tutorial! I had a question regarding the similarity score; why do we need a similarity score in the first place? Why can't we just use a normal decision tree with MSE as a splitting criterion like in GBT?,True
@ayenewyihune,2022-11-03T07:48:19Z,1,Super clear,True
@dr.kingschultz,2022-10-11T23:29:44Z,1,your videos are awesome,True
@mohammadelghandour1614,2022-08-24T20:23:13Z,0,"in 11:10 I understood You used the second order Taylor approximation because of this term ""Pi+Ovalue"",which makes differentiation with respect to Ovalue difficult,  Similarly,  in Gradient boost lesson part 2, https://www.youtube.com/watch?v=2xudPOBz-vs&t=322s  , in 19:03 similar term  existed ""F(X)+Gamma"", However, U preferred to replace F(X) with its value ""73.3"" and that made differentiation with respect to ""gamma"" simple and easy. Why didn't you use the same simple substitution  method with XGBoost loss function instead of second order tayolor approximation?",True
@wibulord926,2022-08-20T09:25:24Z,0,it easy to understand p1 and p2 but come to p3 seem overwhelme for me lol !!!!,True
@nishalc,2022-08-19T10:48:59Z,1,"Thanks for the great video.  I'm wondering how other regression methods such as poisson, gamma and tweedie relate to what is shown in the video here.  I imagine the outputs of the trees in these cases are similar to the case of regression, as we are estimating the expected value of the distribution in question. On the other hand, the loss function would be the negative log likelihood for the distribution in question.   If anyone has any details of how these methods work it would be much appreciated!",True
@venkateshmunagala205,2022-08-12T01:52:09Z,0,Can you please me understand why we multiplied the equation with negative (-) to get similarity score which makes parabola to get inverted ?@ time stamp 21:29,True
@mengzhou193,2022-07-12T15:23:46Z,1,"Hi Josh! Amazing videos! I have one question at 6:39, you replace p_i with (initial prediction+output value), but according to part 1&2, I think it should be (initial prediction+eta*output value), am I right about this?",True
@wert572,2022-07-11T15:07:02Z,0,"hello , josh, thank you for demystifying the loss function of xgboost regression model. I have a small doubt. Where is the regularisation term related to L1 penalty(lasso)? Could you provide a related reference including this term.",True
@robertocorti4859,2022-07-07T22:13:44Z,0,"Hi! Amazing video, thank you for this great content!  I have a question, maybe it's stupid but it's just for having everything's clear: after that you proved that the optimal output value is computed on a minimization of a Loss + L2 penalty made by approximating this function with a 2nd order taylor approximation, I still don't get the next step when you improve the prediction by creating nodes such that the gain in terms of similarity is bigger. Of course I know that by building such a tree you would improve the optimal output value since the first guess comes from a 2nd order approximation, but I still don't get how do you prove this mathematically.   Thank you again!",True
@3Jkkk2,2022-07-07T19:26:50Z,1,Josh you are the best! I love your songs at the beginning,True
@mohammadelghandour1614,2022-05-23T21:56:05Z,0,In 18:06 How (1-yi) log(1-Pi) ended up like this : log(1+e^(log(odds))) ?,True
@maddoo23,2022-05-18T19:51:04Z,0,"At 5:27, wouldnt gamma also decide if a node gets built(going by original paper, not able to post link)? You wouldnt have to prune a node if you dont build it.",True
@chrischu2476,2022-04-17T09:00:48Z,0,"This is the best educational channel that I've ever seen. There seem like a little problem in 18:02, when you convert L(yi, pi) to  L(yi, log(odds)i). I thought pi is equal to (e^log(odds) / 1 + e^log(odds)). Please tell me if I am wrong or misunderstand something. Thanks a lot.",True
@ilia8265,2022-03-31T19:49:07Z,1,Can we have a study guide for XGBoost plz plz plz plz üòÖ,True
@Vivekagrawal5800,2022-03-29T03:39:23Z,2,Amazing Video!! Makes the Maths of XGBooost super simple. Thank you for your efforts...      ,True
@cici412,2022-02-25T03:30:00Z,0,"Thanks for the video.  I have one question that I'm struggled with. At 7:58, why the new predicted value is not (0.5 + learning rate X Output Value)?  Why is the ""learning rate"" omitted to compute the new predicted value?",True
@laveenabachani,2022-02-19T01:22:34Z,1,Amazing! The human race thanks you for making this vdo.,True
@sayantanmazumdar9371,2021-12-17T06:51:24Z,1,"If I am right here , then finding  output value is just like gradient descent of the loss function .Like we do in neural networks",True
@Stoic_might,2021-11-21T15:43:16Z,0,What is the number of decision trees should be there in our XGBoost Algorithm?,True
@Stoic_might,2021-11-21T15:43:06Z,0,What is the number of decision trees should be there in our XGBoost Algorithm? And how do we calculate this,True
@Cathy55Ms,2021-11-09T03:06:07Z,2,Great tutorial materials to whom need the fundamental idea of those methods! Do you plan to publish videos on ligntGBM and catGBM too?,True
@davidd2702,2021-10-15T11:46:42Z,0,Thank you for your fabulous video! I enjoy it and understand well!  Could you tell me the output from the xgb classifier giving 'confidence' in a specific output (allowing you to assign a class) ? is this functionally equivalent to statistical probability of an event occuring?,True
@damp8277,2021-09-07T08:20:29Z,1,Watching this video with the original paper open is like deciphering forgotten texts. Thanks so much! <3,True
@iraklimachabeli6659,2021-08-11T22:39:13Z,11,"This is a brilliant and very detailed explanation of math behind XGBoost. I love that notation uses minimal subscripts. I was scratching my head for a day after looking at original paper by Chen and Guestrin. This video clearly laid out all the steps , taylor expansion of loss function and then gradient of second order approximation with respect to trees current  prediction. Now it so obvious that gradient is  wrt to current prediction,  but somehow it was not clear before.",True
@rubyjiang8836,2021-07-26T20:33:16Z,1,cool~~~,True
@amitbisht5445,2021-07-19T15:04:59Z,0,"Hi @JoshStarmer, Could you please help me in understanding, how taking the second order gradient in taylor series helped in reducing the loss function?",True
@salhjasa,2021-06-06T06:26:10Z,3,This channel is awesome. After searching and searching for somewhere to explain this clearly this is just perfect.,True
@iraklisalia9102,2021-05-31T18:09:16Z,0,"Thank you Josh for the great explanation! I was confused at the part where Cover equaled denominator minus lambda as I thought we were supposed to subtract Lambda and got confused in the last video, but here it clicked that you meant minus as in without lambda :D   I'm super stoked about Taylor series as it seems like quite an important part in ML, any chances that you will do Taylor series clearly explained video in a near future? :)",True
@praveerparmar8157,2021-05-25T18:49:15Z,1,Thank God you skipped the fun parts üòÖüòÖ. They were already much fun in the Gradient Boost video üòÅüòÅ,True
@rushilv4102,2021-05-02T15:24:29Z,3,Your videos are really really helpful and easy to comprehend. Thank you so much!,True
@Gabbosauro,2021-04-13T15:34:15Z,0,Looks like we just apply the L2 Ridge reg param but what about the L1 Lasso regularization parameter? Where is it applied in the algorithm?,True
@SophiaSLi,2021-04-12T21:08:45Z,4,"Thank you so much for the excellent explanation and illustration Josh!!! This is the best (clearest, best-organized, most comprehensible, most detailed) XGBoost lecture I've ever seen... I don't find my self having the need to ask follow-up questions as everything is explained so well!",True
@junbinlin6764,2021-03-29T10:17:15Z,6,"Your youtube channel is amazing. Once I find a job related to data science after uni, I will donate this channel fat stacks",True
@karannchew2534,2021-03-26T14:06:40Z,0,21:50 Why does the highest point of the parabola give the Similarity Score please?  What exactly is the Similarity Score definition?,True
@strzl5930,2021-02-17T18:52:16Z,0,Are the output values for the trees that are denoted as O in this video equivalent to the output values denoted as gamma in the gradient boosting videos?,True
@hampirpunah2783,2021-02-13T14:26:12Z,0,"I have a question, I did not find your formula in the xgboost tianqi chen paper, can you explain the original formula XGboost ?",True
@NaimishBaranwal,2021-02-03T06:34:30Z,0,This is hell lot of maths,True
@dollylollapaloosa,2021-01-22T03:33:04Z,6,"Hey, Josh! I really enjoy your videos and I could not express my gratitude enough!",True
@iOSGamingDynasties,2021-01-01T17:05:17Z,1,"I am learning XGBoost and this has helped me greatly! So thank you Josh. One question, at 7:18, in the loss function, the term p_i^0 is the total value from previous trees? That being said, p_2^0 would be initial value 0.5 + eta * (output value of the leaf from the first tree), am I right?",True
@Theviswanath57,2020-12-26T14:44:05Z,0,Why we are multiplying with negative -1 at  21.30 minute?,True
@midhileshmomidi2434,2020-12-11T17:00:15Z,0,"So to get ouput value and similarity score, this huge amount of calculation(double derivatives) is required No wonder why xgboost takes lot of training time One doubt Josh While running model to calculate output value and similarity score it just calculates the formulae right or it goes through all this huge process",True
@henkhbit5748,2020-12-07T11:46:59Z,1,"The math was, as always, elegantly explained. Analogous  you're support vector machine math explanation usingTaylor series for radial kernel.",True
@beautyisinmind2163,2020-11-27T12:17:50Z,41,I wish this channel live 1000 years in youtube.,True
@ParepalliKoushik,2020-11-11T07:21:24Z,0,Thanks for the detailed explanation Josh. Why XGBoost doesn't need feature scaling although it uses gradients?,True
@tulanezhu,2020-10-16T22:36:03Z,0,"Really helped me a lot understanding the math behind XGB. This is awesome!  For regression, you said XGB used 2nd order Taylor approximation to derive the leaf output, while general gradient boost use 1st order Taylor. From what I understand other than the lambda regularization term, they just end up with the same answer, which is sum of residuals/number of residuals in that leaf node, right?",True
@yujiezhao9825,2020-09-15T18:19:27Z,0,What is the difference between the method in this video compared with the gradient boost (classification & regression)? Is the only difference lie on the penalty term?  Are the gradient boost (classification & regression) you introduced previously the same as GBDT?,True
@palebluedot8733,2020-09-15T09:24:43Z,3,I cant get past the intro. Its so addictive and Im not kidding lol.,True
@zahrahsharif8431,2020-09-02T10:49:44Z,0,"Maybe this is basic, but the Hessian matrix is a matrix of what exactly ? Its the partial second order derivatives of the loss function with respect to what just the log odds?. Just trying to see bigger picture here applying it to training data",True
@vinodananddixit7267,2020-08-12T20:18:18Z,0,"Hi, At 18:17 , I can see that you have converted pi=(e^log(odds)/1+e^log(odds))? Can you please let me know how it has been converted. I am stuck at this point. Any, reference/help would be appreciated.",True
@vinayak186f3,2020-07-25T19:35:09Z,1,"I watch your videos , get the subtitles downloaded and make notes from it. I'm really enjoying doing so . THANKS FOR EVERYTHING. üòä",True
@nonamemark,2020-07-19T22:20:51Z,0,Correct Hession to Hessian on one of the slides,True
@tingtingli8904,2020-07-15T08:53:28Z,0,"Thank you so much for the videos. And I have a question . The pruning can be done after the tree built, if the difference between gain and gamma is negative , we can remove the branch. Could you explain this, can we have this conclusion from math details. Thank you",True
@ShubhanshuAnand,2020-07-03T17:13:31Z,0,"Expression of Output value looks very similar to GBDT gamma field with L2 regularizer, we can always use first order derivative with SGD optimization to get the minima as we do in other optimisation problem to solve for minima, why use taylor expansion? Is taylor expansion gives faster convergence?",True
@lucaslai6782,2020-06-19T21:00:29Z,0,"Hello Josh,  Why does  -- (yi -- pi)  must be negative?  (yi -- pi) can be smaller than 0, right?  --(negative ) is positive, right? Do I miss something?  Thank you.",True
@sureshparit2988,2020-05-16T05:10:00Z,1,Thank Josh ! could you please make a video on LightGBM or share the difference between LightGBM and XGBoost.,True
@zahrahsharif8431,2020-05-11T17:57:02Z,0,Do you need to remove outliers in your data first to reduce the loss function?,True
@pandikapinata62,2020-05-02T03:51:18Z,0,"Thanks for videos is awesome, i have a questions, im try using xgboost in python with that sample data in video (Drug Dosage) im already set lamba=0, base_score=0.5 but i got left first leaf -3.15 and in this video got -10.5, what equation for calculate is used? or leaf -3.15 is prediction of residual? I try in excel step by step use math detail in your video i got same result as your, but i dont know what calculation in library python Thank again, im need this for my undergraduate thesis",True
@sheenaphilip6444,2020-04-26T15:00:34Z,1,"Thank you so much for this series of videos on XG boost!! Has helped so much..esp in understanding the original paper on this, which can be very intimidating at first glance!",True
@muzamilshah8028,2020-04-21T11:57:10Z,0,Kindly mention paper reference,True
@xiaoyuchen3112,2020-04-18T01:50:33Z,1,"Fantastic vedio! I have a small questions, if we calculate the similarity based on gradient and second order gradient, how can these similarities be additive? That is to say, why can we add similarities in different leaves and compare it with the similarity in the root?",True
@VBHVSAXENA82,2020-04-03T18:29:02Z,1,Great video! Thanks Josh,True
@sxfjohn,2020-03-29T17:08:21Z,4,"The best  valuable and most easily explained the hard core of xgboost, Thanks!",True
@post_national_sham,2020-03-29T12:29:04Z,0,At 1:16 shouldn't we put lambda outside brackets?,True
@rrrprogram8667,2020-03-22T18:28:23Z,1,MEGAA BAAMMMMMM is backkk...,True
@henry7434,2020-03-10T10:53:16Z,1,bam,True
@georgeli6160,2020-03-08T22:22:04Z,0,"Hi guys, can someone provide some insight into why the similarity score is the maximum of -1*the loss function?",True
@benjaminlu9886,2020-02-25T15:33:44Z,0,"Hi Josh, what is Ovalue? Is that the result that the tree outputs? Wouldn't then the output value be 0.5/predicted drug effectiveness in the first example? Or is the output value a hyper parameter that is used in regularization? Also, BIG thanks for all these videos!",True
@mamk687,2020-02-23T04:58:01Z,0,"Thank you for the great video as usual! I am currently confused by differences of the words' definitions, such as method, algorithm, technique, method, and approach. Could you tell me how you use those of words appropriately and give me some references I can see?",True
@ranerev2480,2020-02-22T16:57:24Z,0,"thanks for the video! could you explain why the y-axis coordinate for maximum value of the ""flipped"" parabola is actually the similarity score?",True
@alex_zetsu,2020-02-19T14:16:38Z,3,"I knew enough calculus to know what the second derivative with respect to Pi would be, but even though you spoke normally and I could see it coming, ""the number one"" seemed so funny after doing all that.",True
@wenzhongzhao627,2020-02-18T18:54:33Z,0,"Thanks Josh for the great series of ML videos. They are really ""clearly explained"". I have a question regarding calculation of g_i and h_i for the XGBoost classification case, where you used log(odds) as the variable to take the first/second derivatives. However, you used the p_i as the variable to perform the Taylor expansion. Will that cause any issue? I assume that in the classification case you have to use log(odds) to perform the Taylor expansion and variable update in stead of p_i as in the regression case.",True
@rahelehmirhashemi5213,2020-02-16T07:46:51Z,1,love you man!!! :D,True
@sayantandutta8353,2020-02-15T10:41:33Z,2,"I just completed the Ada Boost, Gradient Boost and XGBoost series, it was awesome. Thanks Josh for the awesome contents!",True
@kunlunliu1746,2020-02-13T01:17:36Z,5,"Hi Josh, great videos, learned a ton. Are you gonna talk about the other parts of XGBoost, like quantile? Looking forward it!",True
@suzyzang1659,2020-02-13T00:47:50Z,1,"I was waiting for this for a very long time, cannot wait to learn!! May I please know when the part 4 will come out? Can you help to introduce how to realize XGBoost in R or Python?  Thank you!!",True
@jhlee8796,2020-02-10T22:33:04Z,0,Thanks for great lecture. Where can I get your beamer pdf file?,True
@damianos17xyz99,2020-02-10T15:00:33Z,9,"Oh after finish my project from Xgboost classification - max score I get! I have watched first two parts and it was really helpful, thanks! Now is the part there, yes! :-) What a helpful man !?",True
@MrDiego163,2020-02-10T11:09:21Z,0,Great video! Could make one about Naive Bayes classifier? :),True
@mikhaeldito,2020-02-10T11:04:45Z,2,I couldn't give a lot but I am a proud patron of your work now! I hope others who are financially capable would also donate to StatQuest. BAM!,True
@aditya4974,2020-02-10T08:38:47Z,1,Triple Bam with part 3!! Thank you so much.,True
@helenjude3746,2020-02-10T07:03:43Z,0,I would like to point out that the hessian in XGBoost for Multiclass Softmax is not exactly pi(1-pi). It is actually twice that.  See source code: https://github.com/dmlc/xgboost/blob/master/src/objective/multiclass_obj.cc   See here:   https://github.com/dmlc/xgboost/issues/1825  https://github.com/dmlc/xgboost/issues/638,True
@Erosis,2020-02-10T05:49:21Z,4,It's crazy to think a graduate student (Tianqi Chen) came up with this... Very impressive.,True
@tucheng8496,2020-02-10T05:43:58Z,1,Yeh!,True
@bktsys,2020-02-10T05:33:12Z,2,Keep going the Quest!!!,True
@ayoubmarah4063,2020-02-10T05:02:29Z,1,first comment horray for me,True
