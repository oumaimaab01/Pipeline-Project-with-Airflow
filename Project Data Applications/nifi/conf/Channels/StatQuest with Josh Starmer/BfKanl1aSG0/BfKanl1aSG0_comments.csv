author,updated_at,like_count,text,public
@statquest,2020-06-17T14:15:22Z,56,"NOTE: In statistics, machine learning and most programming languages, the default base for the log() function is 'e'. In other words, when I write, ""log()"", I mean ""natural log()"", or ""ln()"". Thus, the log to the base 'e' of 2.717 = 1.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@saltedfish_is_good,2024-05-23T11:25:32Z,0,Hello 8:30 while transforming the log(odds) prob we use X as weight and Y as probability of Obesity right. I was so confused on does the logistic regression changes its line on probability plot.,True
@Dwika34,2024-05-08T00:49:25Z,0,bro how do you project the point on infinite to the line üò¢,True
@KshitizShah-on9pr,2024-05-07T17:18:46Z,0,What about cost funtion of Logistic regression,True
@sayyamplay,2024-05-02T17:02:27Z,0,what I don't understand is : Why do you transform the y-axis to the log(odds) ? why can't we just use the maximum likelihood on the S-curve ? why do we need Bete_0 and Beta1,True
@ag1805x,2024-04-29T07:23:42Z,0,"How can we calculate the value on the x-axis given a probability of coefficient? Basically, I need to identify the value (say weight) that separates (or forms the boundary) the two groups (obese vs not obese).",True
@CCyuu,2024-04-19T15:38:03Z,1,@ 10:00 the fact that we both said cool at the same time was pretty cool,True
@gunasekhar8440,2024-04-19T02:19:40Z,0,"Hi Josh, at the time stamp 6.52 we know that red points are not obese mouse. I have a doubt that if I'm using scikit learns predict_proba() function then it returns two columns. The first column is negative probability and the second column is a positive probability. So, now if we want to calculate the likelihood then how can we do? I mean simply take log products of positive class probability!! Or else is there something else?",True
@PunmasterSTP,2024-04-16T13:28:47Z,1,StatQuest gives the maximum likelihood...of learning!,True
@zombieeplays3146,2024-04-13T17:02:49Z,0,"3:55 So basically log odds to probability transformation is nothing but substituting sigmoid equation p = 1/(1+e^-z) with z = log(odds). Since sigmoid converts any real number to a value between 0 and 1, this should make sense, right? And we also know that log odds is nothing but b0 + b1X, so basically we are using the fit line equation.",True
@sunyog,2024-04-07T11:06:16Z,0,"The line that we're fitting in the data, is it what we call a decision boundary?",True
@stephenventura4075,2024-04-03T12:52:52Z,1,I will be using these videos for the rest of my life. Thank you Josh,True
@barnowl2832,2024-04-03T07:08:11Z,0,Why cant we just do the least squares method using the S-curve to find the best fitting s-curve and then afterwards transform the s-curve of best fit?,True
@enicay7562,2024-03-27T13:11:05Z,1,Amazing,True
@karimmohanad2k01,2024-03-18T00:14:23Z,0,"Hi josh, I have a question. How in 2:45 do we plot our actual data at infinity and negative infinity, while if i understand correctly that the x axis is the p, so there are some points plotted that are beyond infinity since the corresponding p value on x axis > 1, and at p=1 log odds is infinity",True
@dhavalpatel3289,2024-03-05T22:46:07Z,1,with zero dislikes the odds of this video being liked are infinity,True
@turtleyoda7703,2024-02-23T16:59:18Z,1,"This is awesome, thank you so much",True
@abdurrehmansiddique4480,2024-02-13T13:48:45Z,1,"One of the best videos on Logistic Regression.... Awesome...  In your words ""Tripple BAM""... üòä",True
@michaeljagdharry,2024-01-31T06:50:55Z,1,we love you josh,True
@brendawilliams8062,2024-01-25T14:31:32Z,1,It seems to prove a useful model. Thankyou,True
@tysonliu2833,2024-01-05T22:40:32Z,0,"so essentially we need to find a straight line(linear), which for those who are not obese, maximize their distance from 1 (aka the likelihood of not obeses), and for those who are obese, minimize their distance from 1 ( aka maximize their p of being obese)",True
@ahmetjeyhunov4435,2023-12-17T05:02:58Z,2,"So far, the best explanation of maximum likelihood estimation on Youtube. Log odds of me being better at math would be significantly high had you been my math teacher at highschool. Thanks Josh.",True
@julianeventurelli1,2023-11-15T13:45:27Z,1,Exceptional.,True
@alejandroramirezluna2895,2023-11-08T05:26:47Z,1,I love it,True
@faizalrafi,2023-10-14T20:41:09Z,1,Thanks,True
@nicholasyuen9206,2023-10-07T10:16:39Z,0,Great video! I have a question regarding the candidate line of the logit function. How is the line initialised? And is it less important to tweak (if possible) the candidate line since its mentioned that the algorithm to find the maximum likelihood is pretty smart as tweaking might only shorten the train time for the best parameters? Thanks!,True
@Arthur-so2cd,2023-10-04T22:47:34Z,0,"why can't we just use squared residuals to the squigle? if it's a non obese predicted with p=0.1, the residual would be 0-0.1 and residual squared is 0.01",True
@PhuongNguyen-lh5qu,2023-10-04T09:55:17Z,0,"Can anyone please explain to me the part, after calculating the value p, how can we come up with the squiggle line? I thought that p values calculated were the values on the y-axis already? Since you calculated the example p of 0.1. But when doing the likelihood maths, you used 0.01. I don't get where 0.01 comes from and why don't we use 0.1 instead?",True
@randywa,2023-09-24T05:11:06Z,0,"There‚Äôs probably something I‚Äôm not seeing but i feel that we might actually be able to use gradient descent after all.   If we pick some line, with the form y=mx + b, we can plug it into the exponentiated function: e^y/(1 + e^y). Then we can use least squares again but with 1 or 0 as the desired value. For example: (1 - e^y/(1 + e^y))^2.   Then we can take the derivative with respect to the slope and intercept within y again and then adjust based on whatever we get. Even if an obese rat should be at positive infinity on the line, I feel like the line can still approach correctness in the same way this algorithm in the video does right?",True
@minerodo,2023-08-30T03:52:17Z,0,"Hello! thank you! amazing video! Just a question, how do you define the first intercept and the first slope? Is it random or is there any way to define the one that will be easier for the rotation algoritm (if it is done manually i mean)?",True
@sheershsaxena9357,2023-08-28T20:04:18Z,1,If your explanations would not have been this good I would probably have sued you in Court for such horrible introductory songs.  But I sincerely thank you for such amazing explanations.,True
@seeyoutube6430,2023-08-26T16:04:25Z,1,I am 15 and really your videos are really helpful to me‚ù§,True
@sukritimacker7562,2023-07-31T02:09:59Z,2,You are Stats God!! Thank you üôáüèª‚Äç‚ôÄ,True
@tranthucnhubui623,2023-07-17T11:54:28Z,0,Thank you for the very clear explanation. Can you explain the idea behind maximizing the likelihood? Like I understand how it‚Äôs done to find the best fit but I can‚Äôt see why it helps find the best fit. Thank you very much!,True
@television80,2023-06-17T15:30:25Z,1,"Thank you!!! Thank you!!! Thank you!! for your dedication on the videos, you are helping a lot!!",True
@jayasingh4666,2023-06-08T03:48:29Z,1,"You are an awesome teacher!!! Thank you for the visualizations during your teaching, it helps to learn the concept so well!!",True
@user-ym2kf2ky2i,2023-05-29T20:53:41Z,0,"At around the 6:00 mark in this video, you stated that the likelihood is the same as the probability, but before you said that likelihood is not the same as probability. So, then is it true that probability and likelihood are the same with logistic regression.",True
@RB-sn6gn,2023-05-26T15:33:13Z,0,I wish this video as well as all others would contain a clear table of the example data analyzed in the videos. This would allow one to run the data with his statistical software and practice. Is such  data set available?,True
@hingaglaiawong7815,2023-05-26T08:14:20Z,0,"@3:20, is the candidate line there for the projection randomly initialized? If not, where could we get the 'candidate line' on the log-odd graph during the 1st iteration of the maximum likelihood estimate?",True
@drachenschlachter6946,2023-05-12T22:52:32Z,0,Hey Josh please always label the axis!,True
@atrayeedasgupta2872,2023-04-26T10:39:31Z,3,"The best explanation ever on logistic regression which includes every details. Thanks a lot Josh, i adore you so much!",True
@trevorvanier7627,2023-04-07T22:42:17Z,0,"any1 else catch the line ""pull the p out""",True
@dantitchener1713,2023-03-14T09:39:48Z,0,Why do you ignore signs when you project the points onto the line?,True
@MengyanSun,2023-02-25T19:36:00Z,0,Hi Josh WHY is the coordinate x of y(infinite)=0 always 2 (weight)? Thank you so much!!,True
@mandarbagul3008,2023-02-06T12:09:28Z,0,1. in logistic regression we find log(odds) for each data point and plot those log(odds) on line (mx + c) 2. then convert this line with the help of sigmoid function into the probabilities between 0 to 1 (sigmoid line is decided using maximum likelyhood) 3. which ever value we get it will get assigned to the corresponding class using thread hold value is my understanding about logistic regression right?,True
@shadwal,2023-02-06T11:20:12Z,7,"In the past one year of my MBA, I have been taught concepts of logistic regression by Math PhDs and Analytics Gurus, but no one could beat the simplicity and elegance of your explanation.",True
@rishabsaini2080,2023-01-17T16:34:45Z,2,"I am student of data science, when i see this logistic algorithm calculation , i am scared and think that i could survive in this field or not, But after seeing your content of this algorithm i gonna play with this .Thankyou so much sir for this valuable content",True
@markusheimerl8735,2023-01-11T12:42:09Z,0,"Hello Josh, first of all I wanted to thank you for these enormously helpful, world class educational videos. They are my lifeboat at the moment in my ""Data Literacy"" class. One thing I noticed was: @3:35 it should say -1.4. This is not irrelivant since the ""fancy looking formula"" produces values closer to 0 for negative values. Again, thank you so much for what you did here. I would probably have to go back to copying slides by hand which omit all the details and assume way to much previous knowledge. Best Markus",True
@veeek8,2022-12-13T07:39:14Z,1,Thank you!!,True
@riyaz8072,2022-12-12T15:07:25Z,6,"Hi Josh.. I hope you know how you are changing people's life directly.. a lot of people are earning very good salary bz of your quality content.. if one person gets a job, his entirely family is getting benefited bz of him.. so, you are not just helping one single person but thousands of families.. May the almighty bless you and keep you happy, healthy and wise..",True
@brucewayne000,2022-11-22T19:34:23Z,1,Liking the video immediately after the song!!,True
@user-ul2mw6fu2e,2022-11-13T22:25:23Z,0,Why likelihood is product of pdf?,True
@user-ul2mw6fu2e,2022-11-08T21:58:15Z,0,Thank you very much for videos. I dont understand differences between gradient descent and maximum likelihood estimation. Can you make videos about this,True
@rahulranjan8682,2022-11-03T00:45:22Z,1,how do we find the the log(odds of obesity) 0n the candidate line as in the log(odd) scale all points are at -infinity or +infinity?,True
@suchaimmuchwow9776,2022-10-31T10:11:50Z,0,"Hey Josh, are you on Spotify?",True
@lcc1648,2022-10-25T05:57:58Z,1,Thank you sooooo much!!!!,True
@baharehbehrooziasl9517,2022-10-18T20:57:38Z,1,"Great video, thank you so much for clearly explaining the topic!",True
@rajatchopra1411,2022-10-16T07:45:16Z,0,4:14 Why didn't you cancel out E and LOG on the right hand side of this equation like you did on the left hand side?,True
@riyaz8072,2022-10-15T10:18:13Z,1,could anyone help me understand Where is Sigmoid function coming into picture here ? also can anyone help me understand where are we using log loss function to optimize things ?,True
@bernardoleivas8296,2022-09-24T13:39:00Z,1,"Hey Josh, I can only resonate everybody¬¥s thankful words here. I can¬¥t be more grateful. You make me think that statistics can be very daunting most of the time for the lack of experts like you that can really explain its details in a very simple way. For me, this is true sign of mastery! I had one question regarding this video though, based on your other video where you explain the difference between probability (P(data | mean, sd) )and likelihood (L (mean, sd| data)). Where the in the former you find the distribution of the data under a fixed hypothesis and the latter you find the best distribution that fit the data.   Somehow in this video about logistic regression i feel that you talk about the likelihood in terms of the probability. As in ""the likelihood of that this mouse is obese [given the shape of the squiggle] is the same as the predicted probability."" So here the likelihood of the data point (the mouse) is based on a fixed distribution? Could you explain that if you have time? :) Again, really appreciate all that you do!!!",True
@vedantthakur2669,2022-09-23T07:32:35Z,0,at 9:16 are you talking about Gradient Descent,True
@vedantthakur2669,2022-09-23T07:10:15Z,0,Can you tell me how to get this squiggle line equation  as we fit data (Using formula) in this squiggle line  or those this 'S' shape line is just imaginary,True
@carterash2588,2022-09-22T15:48:01Z,1,"Hey there, can you speak a to the process of 'projecting the observed data' onto the candidate line? In your video, you do not show what the actual values of x are which makes it very easy to projects the values onto the line, but in reality (when working with real data) this becomes a bit confusing.   I have ticket prices for the titanic (range of 0.0 - 500.0), I have classified the data per passenger whether they survived or not (1 or 0). When I choose a candiate line (y-intercept = -3.5, slope=0.5) I get a line. I am having troube with how to determine where the observed x-axis data would land on this line in order to perform max-log-likelihood on the new y-value .",True
@carterash2588,2022-09-19T20:52:52Z,1,"Hey Josh, I have a question on the process of iterating the fitting of the candidates line. We start off with a random line, this line is then put through the process of max (log) likelihood. After this is done, we simply change the line and the slope around until we find the best likelihood fit. I have 2 questions.  1. When do we know if the likelihood is right? you show one that is log(of-data-likelihood) = -3.77. I am confused on when I will be able to say, ""yes this line has the best likelihood, done."" 2. Is there a method to iterating through the possible 'best' fitted line candidates? You mention some algorithms for choosing. I think since I am not sure which likelihood would be optimal, I can not visualize what algorithm would test the next line.   Your videos are fantastic and I am learning more from you than I ever did in school. Really appreciate your teaching methods.",True
@ishaankulkarni49,2022-09-15T17:47:58Z,1,Just amazing!,True
@kartiksagar6354,2022-09-13T16:17:48Z,1,BAM !!!! I just found a gem of a teacher.,True
@Karen-ww7js,2022-08-30T03:05:19Z,0,Hi Josh! Thank you so much for the videos. I want to study the association between two independent categorical variables and one dependent categorical variable. How would I go about this in R? Would this be considered a log linear regression? Thank you!,True
@taruchitgoyal3735,2022-08-27T18:24:14Z,0,"Hello Sir,  I need your help to understand and clarify.  In squiggly line graph, we plotted the y axis with probability of obesity and then transformed the y axis by using logit function and get log(odds of obesity) value in the new graph where y axis ranges from -infinity to +infinity.  Then how exactly we plot the candidate line in the new graph where y axis is log(odds of obesity)? Thank you Taruchit",True
@barryfeng6602,2022-07-27T19:38:59Z,0,"Amazing video!!! I have a question, do you get the linear line given by random coefficient and slope in the beginning at 2:52?",True
@karannchew2534,2022-07-25T22:16:14Z,0,"Hi Josh, Re: 05:36, ""The likelihood the mouse is obese, given the shape of the squiggle, is the same as the predicted probability. In this case the probability is not calculated as the area under the curve, but instead is the Y-axis value, and that's why it's the same as the likelihood.""  But in other videos, you mentioned probability is not the same as likelihood. Could you elaborate a little please? I often use time interchangeably and been been confused. Thanks.",True
@stoicism-101,2022-07-19T13:33:07Z,0,"Dear Sir, The candidate line in Logistic regression is also fitted using Gradient Descent, If not on what basis do we rotate the line?",True
@pereeia9048,2022-07-15T22:32:35Z,5,"Hi Josh, you are single-handedly carrying me through my Masters program with your videos. I was seriously considering dropping out earlier this year cause I was having a lot of difficulty understanding anything, but a lot of things are starting to click now thanks to you. Your vids are a godsend to students everywhere.",True
@taiworidwan194,2022-07-04T04:31:47Z,1,"Thanks Josh for these great videos.  I'm wondering if it's possible to optimize the coefficients of a Logistic Model using a Genetic Algorithm. If yes, please can you demonstrate how?  Many thanks.",True
@andrewknapton7665,2022-06-15T08:36:40Z,0,"I have a question about how you're re-arranging the equation at 4:14. your first step is to exponentiate both sides the equation reads:  LHS: p/(1-p) RHS: e^(log(odds)).  I'm confused because it looks like some terminology has changed: on the LHS the ""log"" has disappeared, suggesting that you what you actually did was 10^(log(p/(1-p)) = (p/(1-p)),   but, on the right hand side, you've got ""e^"" and not ""10^""  so it looks like you've done different things to both sides and I can't figure out if it's because one of the logs is secretly a natural log or something",True
@tdao9741,2022-06-11T02:19:55Z,2,"Coming from lecture from MIT on generalized linear model which was difficult to get a grasp, this series on logistic regression helps me understand GLM better. Thank you! I also wonder if you would make a video that deeper explains multinomial logistic regression.",True
@monoarul_islam_3,2022-05-16T00:22:55Z,1,"You need to learn Binomial distribution, Bernoulli trial, likelihood, MLE, loss function, Gradient Descent, odds, log(odds), Logit function, sigmoid function, decision boundary, and expected values. And of course the mathematical intuitions too. This is overwhelming for a beginner like me. But Josh your part 1 and 2 of Logistic regression solves many problems for me.  Thank you and a lot more to learn from you. BAM!!!",True
@deeptysarder6797,2022-04-26T13:09:59Z,1,Thanks,True
@ravi.chennaram,2022-04-02T04:24:16Z,0,"Dear Josh, How to update slope and intercept for  2 independent variable and 1 dependent variable in gradient descent (used for deriving deviation in logistic regression) ?",True
@donnasantos6930,2022-03-18T08:20:59Z,4,"Thank you for this. You make our life easier, especially for data analytics students like me. Your explanations are so great that is s easy to understand. Such a talent.",True
@marvinbcn2,2022-03-07T12:02:25Z,0,"Great video as usual. You make perfectly clear how rotating the line (increasing or decreasing the slope) results in different likelihoods. But doesn't the rotation assume that the axis-point (= the intersection of the line with the x-axis), which the line rotates around, has already been found? What would be the algorithm's procedure to find it? Intuitively, I've got the feeling there's some trade-off between best-fitting slope and best-fitting axis point.",True
@TJ-wo1xt,2022-03-03T14:28:57Z,0,how do u decide the candidate line after converting the probabilities into lo odds???,True
@mahdimohammadalipour3077,2022-02-19T09:40:06Z,0,Can somebody explain why do we write  (1 - P(obese)) in maximum likelihood when we deal with red dots and we don't include P(obese) directly ?,True
@mahendra4352,2022-01-30T10:04:38Z,1,Thank you. This is a genius way to explain the concept to a dummy like me. The rotating graph on 08:55 is icing on the cake.,True
@pranavgupta7518,2022-01-12T18:34:22Z,0,"Josh I loved your explanation as always, however help me understand, as mentioned in video you said we can find the best fit line by calculating the likelihood, multiplying them for all points. So where does the cross entropy loss come into picture to train our model if we can do it just by using likelihood. I'm kinda stuck with this doubt and can't seem to find an answer, thanks!",True
@MalgosO,2022-01-08T17:25:05Z,1,"I have a big thank you for you, there ya go Thaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnkkkkkkkkkkkkkkkkkk UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU",True
@Shawn-vw9gk,2022-01-03T04:53:00Z,7,"I teach graduate level operations research courses which require some understanding of probability and statistics. My students really had limited understanding of some fundamental knowledge even if they took some related courses before. I found your videos organized and concise. I will recommend your channel to my future students. Thank you, Josh!",True
@rabbitazteca23,2021-12-22T08:11:53Z,0,"What does the maximum likelihood value tell us about the model and why does the number have to be the maximum after rotating our line? If I am not mistaken shifting the squigly line, for example, shifts the the probability of the values....thus we want to shift it in a way that clearly obese values FALL under 0.5 and CLEARLY  non-obese values fall above 0.5, right? But shifting the squigly line that favors the 2 blue dots (1 being near 0.5 and the other below 0.5) and give it  probability of higher than 0.5 means almost giving the red dot at the top a value of 1, when we want it to have a value of less than 0.5 in the first place since we clearly know that it is obese.... It seems there is a trade of? How do you decide when the tradeoff is okay? Or am I going about this all wrong? lol  Sorry my comment just turned into a rant lol.",True
@rabbitazteca23,2021-12-22T07:54:43Z,1,this was soooo good,True
@RadomName3457,2021-12-13T09:51:15Z,0,"Thanks again for your video Josh, my question might sound dumb. But why is the higher the likelihood, the better the model is? :)",True
@Underwatercanyon,2021-12-10T14:13:49Z,0,3:16 how do we choose what the candidate line is and 8:20 how do we know how much to rotate the line by?,True
@pg4234,2021-12-01T16:30:49Z,0,"At 9:03, how does this algorithm work to find the optimal fit? Is it like gradient descent?",True
@biochemputer,2021-12-01T05:46:45Z,0,Excellent instruction! 8:22 mentioned rotation of line. Around which point will this line rotate? Do we assume the line pass the average weight point on X-axis?,True
@challalalitha6770,2021-12-01T03:52:22Z,1,Next time please rap before the video...hahhahaha that was good explanation,True
@ahmetfurkanemrehan,2021-11-30T19:49:38Z,0,"Hi from Istanbul, thank you for your revolutionary perfect lessons. I have a vital question for me. I try to understand the essence of logistic regression.  How do you calculate of ""log(odd)"" value of first red point (from left to right) as -2.1 at minute 4:52 ?",True
@siddharthvm8262,2021-11-30T18:39:04Z,1,You're awesome üôè,True
@superdahoho,2021-11-15T02:52:34Z,0,8:39 how do you determine which value is better? like why is -3.77 better than -4.15?,True
@rachelrex8368,2021-11-02T01:44:01Z,0,why would you use logistic regression vs. Discriminant analysis ?,True
@vivianbarros6843,2021-10-14T14:31:01Z,0,"Hey Josh! On 6:15s, you mention ""likelihood of data given the squiggle'. Shouldn't it be 'likelihood of squiggle given the data' based on this video 'Probability is not Likelihood. Find out why!!!'?",True
@dr.prateekpandey7137,2021-08-28T07:01:34Z,0,"Hey Josh, can you tell how to  project the original data points from -inf or +inf onto the candidate line?",True
@heyiamsra5207,2021-08-21T17:19:22Z,0,hello sir there was a mistake in 4:12 you took exponential both sides but it must be property of log a to the base e = c which is why we get a = e to the power of c,True
@user-dj7ju2ce7m,2021-08-18T11:53:59Z,0,"Thank you for the video.  I have a question. Can logistic regression be used if I only know the data for one value of the dependent variable?  I am a political science student and am currently working on an article exploring the relationship between revolutionary regime change and various socio-economic factors. Accordingly, the dependent variable in my equation is 1 if the revolution happened, and 0 if not. The problem is that in the first case, I use the values ‚Äã‚Äãof independent variables (GDP, education level, etc.) that existed just before the revolution. And in the second case, the political regime could have existed for decades, and throughout this time the values ‚Äã‚Äãof the independent variables changed.  Can I just collect data from those countries that have had a revolution and create a model based on them?",True
@ravisrikakulapu6901,2021-08-17T06:49:48Z,0,"Hi josh, But in the video, you are actually considering only the dependent variables.  may I know where to consider the independent variables in logistics regression?  And what is the difference between sigmoid and logit function? If I am wrong can you explain",True
@Paprika526,2021-08-10T15:23:43Z,0,04:28 Pulling my p out is what me got dismissed from my statistical course in the first place.,True
@ebenezerscrounge8398,2021-07-18T14:50:24Z,0,I cannot understand on what basis the original data gets projected onto the candidate line. Are these points randomly assigned to a finite log(odds) value?,True
@kapilkumar2650,2021-07-04T10:30:29Z,0,"Hi Josh, I didn't understand why the positive and negative points are pushed to positive and negative infinity in the graph between log odds and the predictor, ex weight of mouse. I am curious as you mentioned this to be the reason for not using least squares. Can you explain why the points are pushed to infinity on log odds scale because later when you take these points projection on fitted line, their log odds is given by y intercept.",True
@bhartichambyal6554,2021-06-13T04:14:44Z,1,"nice video sir, but i have a question here sir you draw the best fit line does this line draw with the help of y=mx+c  what algo you used here is it maximum likelihood  but how we can find it when we have multiple variables  ? you say that we can't use ols method here then we use gradient descent here or not if we use gradient descent then why we are not using it when we have single variable  ? does maximum likelihood only work with single variable?",True
@yancheeyang2918,2021-06-11T21:10:19Z,0,Quick question (Hopefully) 1. where / how to pick the first line (candidate line you said in the video)? 2. How does the algo know when to stop?,True
@wliw3034,2021-06-06T23:56:49Z,1,Such a WoW!!! Perfect,True
@esperanzazagal7241,2021-06-06T03:08:18Z,1,üëèsong,True
@whatstop1010,2021-06-05T06:11:13Z,0,"what is the intuition behind rotating a line to calculate the new likelihood, is it the same as using a different sample of data to fit a new candidate line or is it like updating the coefficients using some optimizer to achieve the maximized log likelihood ?",True
@linhdinh136,2021-05-14T07:47:59Z,0,"Hi Josh, excellent videos to learn about Logistic Regression. One basic question, in practice do we really rotate the line in linear space and compute the Likelihood until we find the best parameter of the squiggly line? I was thinking there are many issues with this approach like what is the initial choice of the line (there are infinite choices I can begin with), how do I know when to stop, the cost to compute every rotation given one dataset is big etc.? You mentioned in the video there is a algorithm that can find optimal fit after few iterations, what is this algo? Will be great if you could make a video about this algo :)",True
@iraklisalia9102,2021-05-12T20:20:42Z,1,"Excuse me if this was explained in the video(I guess it went over my head), but I have one question:  At 3:51 we already have a squiggle with data points projected onto it. Then we transform it with log(p/(1-p)) and then we transform it back to the squiggle. I wonder why is this done, couldn't we automatically get the probabilities by projecting the data onto the squiggle?   Sometimes I think I understand why it's needed to transform the data but it just doesn't click. Could you please elaborate?",True
@lastikasethi356,2021-05-11T15:43:33Z,0,"Hi Prof Josh, these videos are beyond amazing and they make me look forward to understanding statistical concepts that underpin most machine learning algorithms!  I also have a question: Maximum likelihood is finding the optimal mean/standard deviation or 'something else' for distribution to optimally fit the data.  1)What is that optimal 'something else' in this case that helps us determine the maximum likelihood of observing our measurements? 2) Why do we use the product of all individual likelihoods to determine our maximum likelihood?",True
@billaspiel,2021-05-08T20:02:19Z,0,"Hi thanks for the cool video , where is the decision boundary here ?     @3.28 you projected the points on the regression line  y=-3.48 +1.83 *weights  for the first 2 red point value was less than zero  -2,-1.5 as seen  in the regression line all other values are greater than zero so where is the decision boundary here ?   We apply sigmoid on  value returned by the regression line  y=-3.48 +1.83 *weights , is the regression line  considered as  decision boundary ?   I have seen  people saying if decision boundary represent by slope *X+intercept > 0   the if its return value > 0 then data is positive class else negative class but in your example all values got using   slope *X+intercept  @3.28 are greater than zero  except  frist 2 red points  , which means all are positive classes and there are 2 negative classes  it's bit confusing ,pls guide  on approach on how can we find the decision boundary and do classification using it in your example  ?  Is this intuition correct ? 1. project data on line and it's values > 0 can be considered as positive class  2.project data on line and it's values < 0 can be considered as negative  class  3.project data on line and it's values =0 can be considered as decision boundary  thanks for the video ,",True
@dleo2,2021-05-07T16:11:13Z,1,Thank you so much for this great video!,True
@MrAdeelAH,2021-05-02T18:57:43Z,0,"My first thought when learning about this was why couldn't we use least squares on the log odds thing graph, then straight away it's explained with the pos/neg infinity thing. Nice.",True
@balamuralikannaiyan6164,2021-04-26T18:21:50Z,147,"Dear Josh, I want you to know that there are many of us who are so thankful to have you.",True
@4wanys,2021-04-25T00:24:01Z,0,"i didn't got what the relation  between  the calculated likelihood and the rotate of the line ,is it change the slop of the line?",True
@KetakiGadgil_15,2021-04-24T13:38:09Z,1,Amazing you are!!,True
@rogertea1857,2021-04-15T07:58:26Z,0,"Hi Josh, I think logistic regression implements sigmoid function to transform y=wx+b to h(x) which ranges from 0-1. A little bit confused now. Could you please check on this? I'll be much appreicated.",True
@adamfialka7850,2021-04-12T16:18:32Z,1,"If this channel doesnt help you with your course, I dont think there is anything else to try, then bribery. Great videos, thanks!",True
@robertovilchis6068,2021-04-09T19:02:52Z,0,"Awesome video! One question: to get the maximum likelihood, you rotate until you find the coefficient for the slope, but how do you find the y intercept? Is there some sort of ""pivot"" you rotate the line around? (Maybe the point with the coordinates of both means for size and weight?)",True
@saikatroy3818,2021-04-04T18:51:13Z,1,Superb!!!! What a explanation...,True
@Dollar123Bills,2021-03-30T18:27:25Z,0,Is there a singular formula to calculate the best fitting line instead of having to constantly rotate the line to find the best log likelihood?,True
@matthewgraham790,2021-03-17T20:33:37Z,0,"So with maximum likelihood, normally you have a density function and then for the set of data you can take the product of the density values at the given observation points to find the likelihood, however with logistic regression it is not a density function, it is a mapping from an input value to an output value (that just happens to be probability), but has nothing to do with density. I am struggling to see the connection here (other than the method of calculation), but why are they both called ""likelihood"" when they seem to be so very disparate, is there some underlying unification I am missing?",True
@Han-ve8uh,2021-03-17T02:53:27Z,0,"I see at 6:25 there is a switch in thinking from Obese to Not Obese points, causing an extra step of doing 1-prob rather than directly using prob previously. How would this operation work when we have more than 2 classes, like 3 instead? I assume we can't calculate MLE from this logistic regression anymore. I know we can still use this framework by doing OVR, but are there other ways of calculating MLE for 3 classes if we're not doing OVR? Not sure if the ""1-prob"" thing will still be done, and what new graphs we'll be reading from to get the probabilities for calculating MLE.",True
@samngai6179,2021-03-08T16:24:55Z,1,You are life-saver,True
@preslavtonkov2831,2021-03-06T14:41:58Z,3,The songs are great haha,True
@josephgan1262,2021-02-16T03:26:48Z,0,"Hello Josh, thanks for the video! Have a questions, when the log line rotates to find the maximum likelihood, Does it use the concept of finding derivative of the log equation = 0?",True
@k.akenou6675,2021-02-08T01:50:16Z,1,Words cannot express how grateful I am for this amazing explanation.,True
@salmatolba7875,2021-02-02T22:53:17Z,0,"good explanation thank you very much, but please i want the material or websites or a book to have a good material in logistic regression thanks in advance",True
@ivinsertion6759,2021-01-30T17:09:56Z,4,"Great tutorials.  Something I missed: in previous video the probability was used to calculate the Log. Here the Log(odd) is used to calculate back the probabilities.  I guess that 80-85 kg individuals were clinically distributed to obese/not-obese and this ratio provides the probability for this weight range. So, why we need to convert to log, and back, as probabilities are the raw data?",True
@pribumim9638,2021-01-30T16:22:55Z,0,"Josh: ""if you want to support StatQuest click the like button bellow and consider buying one or two of...""  Me: ""...ebooks about statistics"" Josh: ""my original songs"" Me: wait, don't betray me like this",True
@deepakchowdary1641,2021-01-22T03:27:12Z,0,bro tell me about cost function and gradient descent in logistic regression,True
@monishmartin7007,2021-01-21T07:48:41Z,1,The best tutorial on logistic regression Thank you. :),True
@jiayiwu4101,2021-01-19T21:26:54Z,0,The log likelihood part around 6:00 is really confusing about why likelihood here are the same as probabilities. Would you help to explain more? Please!!! I saw a lot of people in comments are confused too.....,True
@micahdelaurentis6551,2020-12-11T21:31:09Z,0,"you don't dumb it down, you smart it down",True
@arijitkumar4763,2020-12-09T08:31:39Z,0,"Wow.. just WOW! Not sure about you but this video had at least two Triple Bams for me. instead, I found none :( What clear explanation... thanks to Youtube algo as well for bringing this Gold mine to me :D :D :D",True
@sreelalb1729,2020-12-05T07:41:45Z,0,"Thank you very much for the explanation. But one doubt I am having is that if we are finding the best fit line like this, what is the relevance of the sigmoid function (Y = 1 / 1+e -z) which is highlighted in many other tutorials? Thanks in advance.",True
@kiranmahato5034,2020-12-02T11:40:26Z,0,"log[p/(1-p)] = log(odds), the base of log on both sides is e right!? so after exponentiating, why is there only p/(1-p) on the left side while there's e^(log(odd)) on the right?  Thanks in advance!!!  And Josh, can't thank you enough for these vids!!! :)",True
@rajibkumartah5133,2020-11-29T18:30:54Z,1,You are too good. All the best wishes for the good work.,True
@samiulsaeef2076,2020-11-24T10:44:21Z,0,"Isn't logistic function the CDF of logistic distribution? I mean, we define CDF as P(X < x), e.g. probability of mouse weight being less than x. But in the video you said  ""probability of this mouse being obese"" ;  shouldn't it be instead  ""the probability of this mouse or any mouse lighter than this being obese"" being the curve a CDF ? little confused",True
@lalakuma9,2020-11-16T19:56:54Z,1,These StatQuest videos are giving me Homestar Runner vibes... but educational,True
@akshaygupta8837,2020-11-15T14:55:00Z,0,"@joshstarmer   Q1: Could you pls explain why the probability is the same as likelihood here.  Q2: Why was the likelihood for the red mice was calculated as 1-p. We are supposed to calculate the likelihood for all the mice being obese, right?",True
@GauravSingh-ku5xy,2020-10-26T15:57:49Z,0,"We used a single parameter here i.e. weight, to classify mice as obese or not obese. What happens when we have multiple parameters like height, genes, etc? How is maximum likelihood calculated then?",True
@GauravSingh-ku5xy,2020-10-26T15:49:12Z,1,Thanks man,True
@taotaotan5671,2020-10-23T17:22:38Z,0,Quick question: why does line rotate but not move with other manners (like horizontally move etc.) Are there any constraints between the coefficient and intercept?   Thanks for the in-depth explanation of logistic regression.,True
@jingxihuang8911,2020-10-17T07:00:33Z,0,Love your intro! You are like u suck at cooking in science fields,True
@jingxihuang8911,2020-10-17T06:59:39Z,1,And the logistic regression brought me back to uüò≥üò≥üò≥,True
@Mau365PP,2020-10-16T17:32:45Z,1,I have a question (I don't really know about regression)  Why at 7:56 when he calculates the log(likelihood)=Sum[log(p)] + Sum[log(1-p)] He is not doing it as the formula log(likelihood)=Sum[ *(Y)* log(p)]+Sum[ *(1-Y)* log(1-p)]  Where is (Y) and (1-Y) ??,True
@beatricemalnati1761,2020-10-12T10:36:33Z,2,"Finding this channel was a pure blessing, especially now that my econometrics classes are held online due to the pandemic and it‚Äôs even harder to understand the material. Thank you so much providing free content with such a high academic value (and with very lovely jingles too)!",True
@amoghbharadwaj9252,2020-10-12T06:36:46Z,0,"Hello Josh, thank you for the wonderful video. I have a question. How do we fit the S curve from the log-likelihoods line ( 8.50--9.00 section of this video). Given the log-likelihood line, I calculate the probabilities of the points using the formula, then once I have the probabilities, how do I fit my S line? Like My S line can curve at some random point, have huge width at the bottom and less at the top, how do we optimize the fit, once we have the probabilities for better future predictions? Want to understand how the shape of S if formed :) Thank you.",True
@mic_135,2020-09-05T16:35:01Z,1,Damn great videos. So the idea of Maximum Likelihood method is to choose the model that maximazes the product of likelihoods of getting the right results from your observations.,True
@kriti3011,2020-09-02T23:34:11Z,1,wow! :),True
@signs80,2020-08-20T23:12:10Z,1,"Hey Josh, one question, I'm a little confused on the WHY of the projections of the data points onto the candidate line of the log(odds) graph before it gets transformed with the logit function. I haven't worked with stats theory in a couple of years so I'm trying to remember a lot of this stuff. Is it because you're considering the candidate line to be the MLE for the data points that are given?",True
@ca177,2020-08-14T02:54:15Z,2,"Love your lessons so far.. FUN, engaging, and doesn't feel academic.. I'm only on my 3rd video.. Thanks !!",True
@auzaluis,2020-07-30T19:53:07Z,1,"This guy is just an amazing e-teacher. The world doesn't have many people like him, so, what are you waiting for suppport him on patreon?!",True
@akashprabhakar6353,2020-07-24T14:39:59Z,0,Thanks for the video sir! One doubt...when we tranform data from original line to the probability(P) graph...we get P of obese being obese on y axis and non obese being non obese on y axis?...so thats why we do (1-P) for non obese points being obese....?? Is this right ?..,True
@akshaybaura,2020-07-13T22:03:20Z,0,"Hi Josh I feel a little under confident about absorbing this approach. Firstly, if it is a classification problem aren't we supposed to have random points on the x-y plane ( for single single feature) and the ask would have to be to fit a straight line between two classes? Then to find the perfect line that could compartmentalize the points, we should use gradient ascent to maximize the likelihood of data on the sigmoid function? Is this correct? Secondly, if that's correct, could you tell, how I can find similarity between this method and the one you are teaching please?",True
@clintonarum6935,2020-07-04T21:55:19Z,1,"Hi Josh, this is wonderful, thanks for your series of presentations. Very informative. Well understood!",True
@changning2743,2020-07-04T04:33:19Z,0,"Hi sir, I am quite confused that why you put the dots all directly to the infinity. I mean as a single dot, I have no idea where it should put when the y axis is log(odds) instead of probability.",True
@jeflee2787,2020-07-01T19:45:40Z,1,Lol such a nice intro!!!,True
@kunalkiran3318,2020-06-29T11:30:13Z,0,"8:20 Sir, when you ""rotate"" the line, I find that the pivot is a point on c axis where the logodds is 0. Does that mean that the X value for which logodds is 0, is the same point irrespective of the coefficients/squiggle?",True
@statquest,2020-06-17T14:15:22Z,56,"NOTE: In statistics, machine learning and most programming languages, the default base for the log() function is 'e'. In other words, when I write, ""log()"", I mean ""natural log()"", or ""ln()"". Thus, the log to the base 'e' of 2.717 = 1.  Support StatQuest by buying my book The StatQuest Illustrated Guide to Machine Learning or a Study Guide or Merch!!! https://statquest.org/statquest-store/",True
@shaiksajidpasha4297,2020-06-17T13:04:10Z,1,at 8:05 when i am calculating  log(likelihood of the data given to suiggle) = -3.37  Where as in my case i got around -1.67.  Can you please clarify me..,True
@munloonkok4928,2020-06-06T06:47:23Z,1,This is a brilliant video - in-depth discussion of how to fit a logistic regression line for non-mathematicians. Do consider creating a lesson giving more details of the conversion between the squiggle and the log-likelihood graph. Bet it will be awesome! BAM :) PS - purchased all your lesson :),True
@wasifmasood969,2020-06-05T19:42:59Z,0,"at time 4:15, after exponenting both sides, why there is still log in the power of e on LHS, shouldnt both e and log cancel out each other, so what you should have on the LHS e^odds, or?",True
@empaulstube6947,2020-06-03T07:32:45Z,0,I did not understand..üò≠,True
@cjstevens6405,2020-05-30T22:52:35Z,1,"Whoa, Black Betty (Double BAM!!) Whoa, Black Betty (Double BAM!!) Black Betty had a child (Double BAM!!) The deviant thing gone wild (Double BAM!!) Learnin' stats all day (Double BAM!!) Fittin' them lines (Double BAM!!) I said ""Oh, StatQuesty"" (Double BAM!!) Whoa, StatQuesty (Double BAM!!)",True
,2020-05-30T02:27:01Z,0,"Thanks for another great video. I have a doubt, I get the intuition from the video, but could you tell a straightforward way to understand why the log of odds is equal to the linear combination of the input variables plus the bias term. I am not sure if is an assumption or there is a reason.",True
@yshuai1021,2020-05-29T01:22:38Z,0,"On 7:16, should the likelihood/probability that the first two red mice are obese 0.1 as 4:55?",True
@jiayoongchong2606,2020-05-28T10:34:17Z,0,6:16 squiggle is all it matters,True
@shradhadash9312,2020-05-22T04:27:53Z,0,"Hi Josh. I hope you are doing well. My question is regarding the maximum likelihood (6:41): when calculating, the probability values are being taken. so the blue dots represent the probability values for obese and for the same we can get the probability values for *not obese*. My question is, instead of taking the probability values of not obese*, why (1- *not obese* )value is considered?",True
@DANstudiosable,2020-05-19T05:51:55Z,0,Can u make a video on derivation of loss function for logistic regression? It'll be really helpful!,True
@cristianoronaldo-pl8zs,2020-05-19T04:47:15Z,0,"you are a genius Josh!! I had one doubt, why does Logistic Regression perform well on linearly separable classes. Its like we try to separate the classes using a hyperplane and stuff....i didnt get that :(",True
@RajeshSahu-ey8kw,2020-05-19T03:53:28Z,1,Love from India.. thanks sir for ur awsome content.,True
@vyshnavidigala1992,2020-05-18T16:48:24Z,0,"Thank you Josh for this great content. I have a doubt, u said log. Regression is a linear classifier. How are we able to see a line but not the sigmoid as a decision boundary between classes in a graph?!",True
@DANstudiosable,2020-05-17T08:09:51Z,0,"And one more last thing, Why didn't you do the derivatives while finding the likelihood? Because in your explanation of Max. Likelihood. You find out the likelihood by taking the derivative and appyling log on top of all values.",True
@DANstudiosable,2020-05-17T08:03:02Z,2,"Hey Josh, I really like your explanations but the logistic regression is where i got stuck real bad. But it's clear now. The problem is that you never linked the log(odds) and line equation. I saw from the internet that there's a link function between them and that's how we can use line equation in place of log(odds). If you'd have included that explanation as well, i'd have not depended on other explanations. But anyways thank you, am always  a fan of your explanations. Keep up the good work!!!! BAAAMMM!!!!!",True
@DANstudiosable,2020-05-16T12:42:10Z,0,"Hey Josh, Here, while calculating logit, we need to consider line equation in place of odds?! like this, log(odds) = log(m*x+c) (So here we're indirectly considering our odds as line?)",True
@DANstudiosable,2020-05-16T09:34:17Z,0,"Hey Josh! Am working on a binary classification dataset, and i have found log likelihood value by following all your steps. Can u tell me how to say if that is optimal or not?(value is -4.126....)  And how to optimise m and c values? Instead of trial and error.",True
@badoiuecristian,2020-05-15T18:15:59Z,1,"You mention at 9:25 of the algorithm that finds the perfect line in few iterations, are you reffering to gradient desccent? Awesome video as always!",True
@hemantdas9546,2020-05-14T21:52:58Z,0,Hi Josh! Is this method different from gradient descent? There we try to reduce gradient descent here we try to maximize the likelihood. Also how to apply ridge regularization here. Please answer really confused.,True
@alessandro49946,2020-05-12T09:38:20Z,0,is it possible that from logistic regression instead of getting a sigmoidal curve we get a unimodal curve? I'm studying the effect of nitrogen concentration on butterfly presence/absence and in same papers I saw that graphs obtained with logistic regression !,True
@mthetree,2020-05-10T23:48:08Z,0,What software is used to create these videos ?? Great stuff btw :) BAM,True
@ramnareshraghuwanshi516,2020-05-09T05:49:08Z,2,Thanks Josh!! your channel is the best thing happen in my quarantine period :) love from India!,True
@MrWincenzo,2020-05-08T15:07:46Z,0,"hi Josh, first of all let me say that your videos whatever is the topic are always the most clear and effective i can find, your job is simply amazing.  After i wish to ask you about a passage that i can't understand, when we are in log(odds) space you said, doing the math, that the log(odds) of each class are respectively +inf and -inf, why next after projecting those point on the fitting line we define different values for log(odds)? I mean this is contraddictory, why can we change the values of log(odds) by simply projecting them?",True
@buliuyingify,2020-05-07T19:47:59Z,3,I would be able to design a space station instead of being a data analyst now if all my teacher were like you:),True
@mrweisu,2020-05-07T17:34:52Z,0,"When explaining maximum likelihood, you mentioned 'rotating' the line many times. But you did not explain around which point the rotation happens. Can you explain it a bit more? Thanks.",True
@michelewang5108,2020-05-05T19:39:33Z,3,This was so helpful! You explained the likelihood really intuitively (calculate likelihood of this data given shape of the squiggle) and it makes a lot of sense now. thank you!!!,True
@weiqingwang1202,2020-05-01T16:20:32Z,0,Why do you call it likelihood instead of probability?,True
@blacksiddis,2020-04-29T12:44:07Z,0,I would love to see how MLE works for ARIMA and GARCH as well!,True
@EvgeniaOlimp,2020-04-28T21:04:09Z,10,"Dear Josh, your videos are amazing and I would have never passed my qualification exam without them. Also, this is the first time I actually understood how MLE works. Thank you so much!",True
@michalispapadopoulos5090,2020-04-24T21:29:13Z,1,THANK YOU DOC!,True
@DrATIF-ij9hy,2020-04-21T15:54:36Z,1,Clearly explained. Good job,True
@iraidaredondo5008,2020-04-20T15:37:35Z,1,"I am your biggest fan. Thanks for explaining these things in a way they are understable. Thank you, thank you!!! :-)",True
@Andynath100,2020-04-11T14:59:54Z,0,"Hi,¬†I have a question. In Intro. to statistical learning it is mentioned that logistic regression can be minimised using non-linear least squares. Can you please explain how that is possible if the residuals are at infinity ?",True
@bilalmoiz5990,2020-04-09T22:08:14Z,0,one thing I'm a bit confused about is how exactly are the values projected onto the linear line if the initial values are negative/positive infinity. I tried googling it but haven't been able to find any good examples of that.,True
@mahammadshikalgar213,2020-04-09T13:25:59Z,1,Thank you sir your video awesome and  help me  clearly understand the logistics regression,True
@NitinKumar-en6dc,2020-03-26T09:24:14Z,1,This is my new favourite channel for ML really love the explanations and the speed at which you talk is easy to follow. I am a person who learns by examples so it would be great if you added more examples. Thanks for the great content.,True
@shivc22,2020-03-25T16:48:13Z,0,A question - isn't e^log(odds) = odds?. If so when you exponentiate both side you''ll simple get  p/(1-p)=odds,True
@sojanprajapati5217,2020-03-21T09:07:01Z,3,"The song reminds me of Pheobe! Jokes apart, I really loved the series.",True
@weiyangshi4729,2020-03-16T11:31:17Z,0,This might be a dumb question but when does the sigmoid function come into play here? Really confusedüò≠,True
@saraning498,2020-03-05T02:37:21Z,1,That song at the beginning reminds me of Phoebe from Friends. Anyone??,True
@mrslyslyf,2020-03-01T22:16:30Z,1,I just discover your channel and it is one of the way to learn stats! Thank you so much!,True
@thetruthsreality,2020-02-28T00:44:26Z,0,"Hi, very clear theory. But it would be more helpful if illustrated with an example interpretation and coding in R",True
@fawadnazir6978,2020-02-24T20:50:12Z,1,thanks for the videos. i just have one question. How does Logistic Regression handles outliers?,True
@saanikagupta1508,2020-02-23T07:14:39Z,2,You are an amazing teacher!! Made it crystal clear!! Thank you soo much!! :),True
@harishjulapalli448,2020-02-21T07:11:53Z,3,This is the first time I clearly understood Maximum Likelihood and Logistic Regression. Thanks for your videos.,True
@MrCracou,2020-02-08T20:18:00Z,0,"The plot at 4.08 is confusing for me as the top left one should display all points at 0 or 1, no?",True
@UncleLoren,2020-02-07T18:31:45Z,0,"So, let me get this straight. We're supposed to calculate the likelihood by multiplying the collective set of probabilities (or adding the log of those probabilities) at each point...and the probability for each point is defined by a (squiggly) line). And that optimal line is a line that produces probabilities at each point, that when multiplied by each other, results in the maximum likelihood. The apparent circularity of this statement is EXTREMELY confusing...but your videos are excellent. Thank you.",True
@jnspincliffe2,2020-02-05T01:51:34Z,2,Your videos are AWESOME.,True
@anuradhasriram7117,2020-02-04T07:02:34Z,1,Thank you so much for the excellent video. Keep up the good work.,True
@sudeshnadutta5702,2020-01-28T17:30:01Z,0,"Hi Josh, one question - so do we basically use MLE on the log(odds) to find the best parameter estimates i.e we fit this log(odds) line to estimate the S-curve or as you say squiggle? I am not able to understand on what are we applying MLE? Basically I want to know why do we pose an equation like log(odds) = b0 + b1 x1 + b2x2 + ...... Y writing like this is useful?",True
@siddharthkshirsagar2545,2020-01-27T08:12:24Z,1,You are the best,True
@InsightsWithAkshay,2020-01-20T17:55:19Z,4,"Hey Josh, thanks for the awesome explanation. I wanted to know what will happen if have multiple independent variables. You have explained using one independent variable, it would be great if you can explain the multiple variable part. Also, how will this work when we have both continuous and categorical data at the same time.",True
@jacksmith870,2020-01-17T13:14:36Z,0,"hey josh! i couldn't understand the part at 5:58 . could you please explain it further?  Also, why didn't we project the data directly in the squiggly line without changing it to log of odds? and thanks a lot. BAAM!",True
@gogoko4654,2020-01-15T15:32:25Z,1,"well, this is the second channel i want to watch alllll of the videos.. thank you for your enthusiasm and good illustration of conceptsü•∞",True
@amoghbharadwaj9252,2020-01-04T21:17:58Z,0,"Hey, Josh, thank you so much for the brilliant explanation. Just a thought....the linear line tells the log-likelihood of every point when projected on to the line, can we not sum the log-likelihood of individual points and get the value, in this way chose the line which has the maximum value for the sum and convert them to squiggle at the end to get the best fit instead of converting likelihood to probability at each step.  Or this is how it works in reality, by converting the linear line to squiggle only at last? Please let me know, thank you :)",True
@spencerarnott9284,2020-01-04T20:53:59Z,0,Projecting to the candidate line is explained in the coefficients video around 6:00.,True
@adeniyiadeboye3300,2019-12-25T12:55:38Z,0,"Hi Statquest,   I would like to ask how to derive *co-ordinate line* in the first place before rotating it to get the one with the maximum likelihood. Thanks.",True
@princezard3506,2019-12-24T08:14:45Z,0,"Does this mean that the closer of the value of a log likelihood to the 0, the better the line is?",True
@jacobmoore8734,2019-12-22T22:38:10Z,2,"Shameless self promotion, I implemented a gradient-descent based logistic regression model in python. If anybody would like to see what this looks like in code, see https://github.com/jdmoore7/Binary-Logistic-Regresion/blob/master/Logistic%20regression.ipynb Hopefully you find this helpful!",True
@ayoubmarah4063,2019-12-22T19:56:29Z,1,i like when you say squiggle,True
@elnurazhalieva1262,2019-12-12T17:33:27Z,1,"By the way, Maximum likelihood is a method for estimating parameters, right? Then, when it comes to our case with logistic regression, are we trying to estimate the slope and y-intercept of a line (further a squiggle) with maximum likelihood? Like, what are the parameters that maximize the likelihood of getting the data that we already have. Am I right? Just to be clear...",True
@elnurazhalieva1262,2019-12-12T11:43:24Z,0,This is hard((,True
@chinmaymaganur7133,2019-12-11T14:14:17Z,0,"3:24 ,you said that we project the original data on the candidate line,  and you got values 2.1 and so on. is this the assumption that these points are falling in that regions on the line .",True
@nampai,2019-12-08T16:47:42Z,43,Easily the most intuitive and detailed explanation of logistic regression + max likelihood on the web. period.,True
@richardj1666,2019-12-07T08:18:17Z,0,"Awesome video. Can you help explain why one can calculate the intercept and coefficients  if there is one categorical variable easily by hand? Say for example my outcome is cancer and my exposure is smoking or not, where 5 smokers develop cancer, 1 smoker doesn‚Äôt, 1 non smoker develops cancer, and 7 non smokers don‚Äôt, I can easily calculate beta0 and then other coefficient (I.e assuming non-smoke is ref, solve for beta0=ln(1/7) and then can solve beta1). But as soon as I run two or more variable models my software calculates differently than what I expect by hand. I‚Äôm thinking it has to do with MLE and you‚Äôre a genius so would appreciate the help!",True
@andhikaadyatmaabhirama9324,2019-12-03T05:58:30Z,1,Really Helpfull!!!,True
@moaazyoussef5274,2019-11-29T20:36:27Z,0,why don't share slides ?,True
@gabrielintriagov,2019-11-16T00:53:34Z,0,Why do you rotate the line?????,True
@VivaldiHeroes,2019-11-15T13:09:34Z,1,"This Logistic Regression series is one of the best means I've found online for understanding the theoretical foundations behind those models in a simple way. In both Linear and Logistic Regression, you reference the idea of iterating and optimizing the line. Do you imply the usage of Gradient Descent? Do you consider solving those problems using partial derivatives less relevant?",True
@MrVishyG,2019-10-29T00:49:44Z,0,Where did log (p / 1 - p) come from?,True
@kmachine5110,2019-10-21T10:04:07Z,1,Beautiful.,True
@ajinkyaghadge3106,2019-10-19T04:02:48Z,1,"Thanks a lot for making this video, your the best.",True
@RaviTeja-bv8xj,2019-10-10T10:49:51Z,1,"Hi josh, your explanation is superb but i have a doubt @ 8:15 ""-3.77"" is an intercept or a slope ?. I have gone through pt 1 video also.Please throw your knowledge light on this area so that i would be out of this dark. Thank you",True
@notnilc2107,2019-10-09T14:48:24Z,155,"This explanation is so good that I feel kinda guilty for having access to it. There is no doubt that if people in the past had access to this, then their lives would've been a lot easier. I feel like a spoiled brat. This explanation is too good for this world.",True
@GurjasSingh92,2019-09-26T11:09:39Z,0,"@8:14, you said the likelihood is -3.77 and then rotated the line. Can you please help me out what exactly was the meaning of this line was. Thanks :)",True
@dipanshunegi9805,2019-09-21T03:20:52Z,0,Will threshold value effect in  rotation of the line?   What will be the axis of rotation?...,True
@Tyokok,2019-09-06T02:50:33Z,0,"Josh, do you have have a video for multivariate logistic regression? wondering how multi discrete or even mixed discrete and continuous logistic regression works. Thank you so much!",True
@jacobmoore8734,2019-09-03T16:18:46Z,0,"Maybe I‚Äôm confused - instead of MLE, can we use cross-entropy/log-loss to determine the goodness fit and optimize from there?",True
@kyoungsub,2019-08-29T14:32:25Z,1,I really really really really love your videos!! But I have one question to ask you.. I‚Äôve watched your video about maximum likelihood and it says it is about fitting a distribution to a data. Then exactly what distribution are we trying to fit in the case of logistic regression?? There should be some type of distribution we use for the calculation.. Maybe you just didn‚Äôt state it since it‚Äôs not important.. Thank anyways for the wonderful videos!!,True
@probaldas6654,2019-08-21T05:56:59Z,0,why we transform probability of obese to log(odds(obese))?,True
@jorgebretonessantamarina18,2019-08-20T08:57:25Z,3,These videos are amazing!! Congratulations on this fine job!,True
@lambuth,2019-08-02T03:12:19Z,1,This dude is a saint. These videos really condense the ideas to some easy to follow steps.,True
@TheAbhiporwal,2019-07-30T11:07:54Z,1,I must say you are a magician. You have the tricks to communicate and deliver just what people want. We want more like you. Thank you so much in shaping the world.,True
@Lj-zn6ej,2019-07-29T07:51:24Z,188,This is where our tuition fees should have gone to.,True
@vikashbhandi3668,2019-07-23T09:55:08Z,5,I wish I could see the backend efforts you put to make concepts so easy I understood all concepts god bless you bro,True
@DANstudiosable,2019-07-19T13:05:23Z,0,"Everything is clear! Only there's a small doubt that, when u say that we project those points on the line, how is it done mathematically?",True
@saurabhchoudhary4572,2019-06-28T11:17:47Z,0,Could you please explain the feature selection process for logistic regression,True
@rafaelcoelho6246,2019-06-25T01:40:31Z,1,You're awesome! Thank you very much,True
@surendrabhargava6798,2019-06-21T19:21:45Z,2,Brilliant!! From the very depth...explanation is splendid..this is all i wanted! Thank you so much,True
@shashankbarole,2019-06-21T17:38:39Z,2,Thank you very much Sir ! . Very informative ! . You're a  great teacher. Quest ON!,True
@amanirouihem58,2019-06-04T21:27:55Z,1,perfect,True
@shyram,2019-05-28T17:07:49Z,0,Í∞ÑÎã®ÌïòÍ≥† Î™ÖÌôïÌïú ÏÑ§Î™Ö Í∞êÏÇ¨Ìï©ÎãàÎã§! thank you!!,True
@anujlahoty8022,2019-05-27T10:52:27Z,1,BAM!!!!,True
@ahmedabdulrahman8567,2019-05-23T22:43:18Z,0,So useful .... Thanks,True
@Shshank_Malviya,2019-04-20T21:20:20Z,0,Is likelihood values are same as  the probability value of mice being obese or not which we get before applying the logit function ?,True
@tumul1474,2019-03-28T16:11:10Z,24,this is called GENIUS AT TEACHING !!,True
@kaitlinkahler7007,2019-03-13T21:45:41Z,1,BAM!!!,True
@vrush0022,2019-03-06T06:17:46Z,0,Is there any other way to maximize the log liklihood without using gradient descent?,True
@chyanitsingh7052,2019-03-04T16:27:42Z,0,"It should have been the log likelihood of the line given the data, We never calculate likelihood for data instead it is done for the estimates",True
@leandadearaujo5559,2019-02-26T04:50:33Z,3,hey josh in the above video how did you find the x intercept  about which your rotating the  the line  to find the max likelihood,True
@nishu761,2019-02-14T16:52:27Z,3,"You're straight up G, Josh ! Keep up the great work, I hope you sell loads of your songs and really grow Statquest to the next level !",True
@jamesrobisnon9165,2019-02-08T17:34:53Z,0,I didn't get how you calculated the log odds of the original samples in order to fit a straight line. I mean we don't have the probabilities at first so how did you calculate log(p/1-p) to get the transformed points into -infinity and +infinity?,True
@ericaltf4,2019-02-06T18:45:04Z,1,If p is always equal to 0 or 1 and therefore the log(odds) are always +/- infinity how are you converting your scatter plot to a sigmoid curve?,True
@farhatyasmin6543,2019-02-02T09:36:52Z,1,I use your method of log-odds in my question but not satisfied. May I calculate coefficient using Eviews software? Eviews shows the same coefficients in logistic as well as in linear and on fitted line but coefficient calculating from log-odds is not same to above stated? Where I'm doing wrong?,True
@farhatyasmin6543,2019-02-02T09:31:39Z,0,And thanks a lot of you for replying the comments. I'm trying to calculate coefficients manually without any software and my question is related to hours studied by students to pass the exams. And my problem is to calculate coefficients manually as we do in linear regression simply using formulas.,True
@farhatyasmin6543,2019-02-02T09:25:41Z,0,Values of betas remain same in logistic regression which we obtain by using linear regression formulas?? I want to clear that value of intercept and coefficients using in logistic are those which we obtain using regression formulas or in liner fitted line?,True
@brandonarias1549,2019-01-28T20:50:32Z,0,"@3:15 Hi Josh! I am confused as to how you knew that the red and blue observations fell exactly at THAT particular x-value before doing your candidate line. This is confusing because the x-axis here is the probability (0,1). You don't know the value (p) of these observations unless you randomly guess them (p-hat). Because of this assumption, you then projected these values onto the candidate b0-hat and b1-hat (ratio-odds) regression line. Can you thoroughly explain this piece more extensively?",True
@marcosikino444,2019-01-24T17:54:29Z,1,"Hi, congratulations for your explanations! But let me ask you something. I think in your video you were clear, but just for my better understanding. In your example, the mice that are obese and not obese were pre-defined before the determination of the log(odds) in the candidate fit line, that¬¥s correct? Thanks in advance.",True
@reajulchowdhury8534,2019-01-24T12:29:12Z,1,How do we project the data point on the candidate line?,True
@ChrisS-bq4pj,2019-01-16T18:15:51Z,2,"Hi Josh, first of all thank you for your video. But I have a question: I summed up all logs (at 7:56) but my calculator gives me -1.637 instead of what you get (-3.77). I did: log(.49) + log(.9) + log(.91) + log(.91) + log(.92) + log(.1) + log(.70) + log(.99) + log(.99) = -1.637. What am i doing wrong?",True
@shekarreddydevireddy4408,2019-01-16T09:46:34Z,1,Very nice explanation. I was not able to understand it even after spending hours of time but with a single video in 10 minutes i got a clear picture. Can you please let me know where can i find the document you used for explanation in this video?,True
@khlifimariem9429,2018-12-27T12:27:08Z,2,Thank you very much for another cool video!  I have a question if you allow me to ask it: isn't the linear function that you derived from the logistic function the perfect choice because you have already derived its values and drew it using the values of the first logistic function (which happened to be the squiggle found using maximum likelihood)?,True
@user-zt8uv6gi7x,2018-12-17T07:41:59Z,1,what to do when there's no maximum from the MLE? (linearly separable data),True
@Raptor3Falcon,2018-12-16T22:15:31Z,1,"At 5:31, its not the likelihood but the probability. Likelihood is p/1-p . After the transformation, you calculated p. So, the 0.49 is probability not likelihood",True
@shivamupadhyay6325,2018-12-14T03:48:55Z,1,"In practice, how do you actually project the data on the line? Can somebody explain?",True
@zhihaoguo103,2018-12-08T23:43:07Z,14,"When you use the train data to derive the coefs, how do you know the log-odds from the binary response in the train data set?  in 3:31,  how do you get the log(odds) for the first response is -2.1? How do you derive that candidate line? To get log(odds) we have to know p(y=1) right? When you train the model, you actually dont have know how to convert binary (0,1) response to p(y=1). I am wondering is the candidate line used to project the data from binary to log(odds) selected by random, then we keep improve it?",True
@meekee7553,2018-12-01T04:41:28Z,1,Your voice sounds too sarcastic that I can't decide to believe you or not,True
@andresmendez3873,2018-11-28T19:14:24Z,3,"Hey Josh. Nice video.  Have a question on equation in 4.11, when using exponential on both sides to cancel the log. Shouldn't the log disappear on both sides?",True
@etsevnevo1315,2018-11-12T08:43:10Z,4,The minus sign may have been left out: 3:32 Should be (-)2.1  3:37 Should be (-)1.4  Thank you for producing this lesson!,True
@arbanafal,2018-10-31T23:43:11Z,1,I can‚Äôt like this video enough!!,True
@s2johnteoh943,2018-10-28T06:56:52Z,1,"Hi, at 1.47 in your video, you mention that the estimate linear line is rotated, to recalculate the sum of the residuals. May I know for both logistic and linear regression model, the rotation of the line is based on what basis?",True
@sachof,2018-10-25T21:38:33Z,1,"With regression, I can draw a line following the line equation f(x) = ax+b.  I wonder why the sigmoid equation is not used here f(x)=1/(1+e^-a). How can I draw it ?",True
@youssefesseddiq7453,2018-10-17T14:02:31Z,1,"Hi Josh! It's me again xD...If there was no overlap between classes and they were well-separated, does it mean that we can fit several lines that would yield same maximum log-likelihood or at least really close ones..Is this accurate?",True
@youssefesseddiq7453,2018-10-17T09:54:59Z,7,"Best explanation for this concept in the internet, most just say: ""a bunch of iterative stuff that statistical software do"" -_- ! Thanks again sir!",True
@amnairfan2144,2018-10-10T13:14:51Z,3,This was the best explanation on youtube. I wish I found this before. You‚Äôre awesome!,True
@amnairfan2144,2018-10-10T13:12:47Z,1,"Is it just me, or he sounds like Kevin from The office?",True
@Sn-nw6zb,2018-09-18T04:44:06Z,1,"Clear explanation about maximum likelihood. It is just confusing to understand difference between likelihood and probability. I watched earlier videos likelihood vs probability, which made sense. But in logistic regression, not sure why is it called likelihood? And I think most of the library uses Gradient Descent to fit line, which is pretty cool technique.",True
@dominicj7977,2018-09-14T06:29:53Z,1,How to determine which direction to rotate the line and by what angle?. Any material on this?,True
@glowish1993,2018-09-09T13:51:43Z,5,"Thanks so much, stats doesn't seem so hard any more with your videos. You truly have a talent for teaching :)",True
@aspdeepak4yt,2018-08-17T09:14:20Z,1,Fantastic explanation!!,True
@aop2182,2018-08-12T04:15:34Z,0,"Thank you so much! I love those visualizations. It would be awesome if you could explain some questions I had when I  was in school(some are interview questions):  1) the data is binary when they were plotted I can only see 0 and 1s where does the probability come from? since you compares it with linear regression.   2) Why not just fit a straight line, like linear regression, to binary data instead of a squiggle? What's the impact if we do this?   3)Why no error term in logistic regression?  4)Why do we want to use logit?",True
@tusshhargupta7070,2018-08-06T20:07:08Z,2,"Hey Josh, for calculating the overall likelihood, why are we multiplying  (Likelihood of Obese) X (Likelihood of Not Obese) ?  Why not multiply the (Likelihood of Obese) for all and check for maximum ?",True
@hcgaron,2018-07-25T00:53:36Z,0,"Josh, I have a question. At 3:33 you mention the log(odds) for a point below the x axis = 2.1. But if we exponentiate exp{1.4} this would give us { p / (1-p) }  = 4.05520...    Shouldn‚Äôt the log odds values below the x axis be negative? Or are they to be evaluated as absolute values and plugged into the sigmoid function?  Edit: I continued the video and see you do evaluate the points below the x axis as negative. I think it may have been a typo at 3:33. Sorry for my confusion.",True
@hajer3335,2018-06-25T13:07:15Z,1,What is the iteratively reweighted least squares( IRLS) ? and is it another approach to fitting the line in logistic regression or a way to find MLE (which the method to fitting the line)? I find this https://stats.stackexchange.com/questions/236676/can-you-give-a-simple-intuitive-explanation-of-irls-method-to-find-the-mle-of-a   but it is make me more confused and not fully understand. How you can help me?,True
@hajer3335,2018-06-25T08:47:38Z,1,How we can rotate the line??,True
@hajer3335,2018-06-25T08:24:32Z,2,The fancy function in 3:46 minute is called logistic function or model.,True
@dominicj7977,2018-06-20T13:48:54Z,1,"Is that line that you started with, just a random line ? Because we don't have an observed set of dependent variable (logodds) when you started right?  So if a computer is calculating it, it would start with guess values for coefficients as 0.   Am I right or what?",True
@dominicj7977,2018-06-20T13:16:05Z,1,""" transformation pushes raw data to + and - infinity"".   What do you mean by this?",True
@poojakunte9402,2018-06-17T17:20:42Z,1,Thankyou so much for all your videos! Pls keep posting!,True
@Felicidade101,2018-06-15T12:43:36Z,1,epic !,True
@vlavla1737,2018-06-14T16:18:41Z,1,"my exam is in two days, you could really help me doing another video",True
@rrrprogram8667,2018-06-12T23:04:56Z,3,Can you please make an example of using logistic regression in R,True
@urjaswitayadav3188,2018-06-12T04:00:16Z,1,Love the likelihood series. Thanks!!,True
@panpiyasil790,2018-06-12T00:11:47Z,1,Nicely done,True
@rrrprogram8667,2018-06-11T22:53:21Z,1,Fantastic video josh.... Clear cut explanation...,True
@juheesingh1157,2018-06-11T18:04:23Z,1,Your videos have cleared my basics . Thankyou for these ‚ò∫Ô∏è‚ò∫Ô∏è,True
